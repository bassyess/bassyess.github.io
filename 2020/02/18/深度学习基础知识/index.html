<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bassyess.github.io","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="深度学习SGD,Momentum,Adagard,Adam原理1. SGD、BGD和Mini-BGD:SGD(stochastic gradient descent):随机梯度下降，算法在每读入一个数据都会立刻计算loss function的梯度来更新参数，假设loss function为L(w),下同。 w-&#x3D;\eta \bigtriangledown_{w_{i}}L(w_{i})优点：收敛的">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习基础知识">
<meta property="og:url" content="https://bassyess.github.io/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="深度学习SGD,Momentum,Adagard,Adam原理1. SGD、BGD和Mini-BGD:SGD(stochastic gradient descent):随机梯度下降，算法在每读入一个数据都会立刻计算loss function的梯度来更新参数，假设loss function为L(w),下同。 w-&#x3D;\eta \bigtriangledown_{w_{i}}L(w_{i})优点：收敛的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bassyess.github.io/images/Sigmoid.png">
<meta property="og:image" content="https://bassyess.github.io/images/Sigmoid%E5%AF%BC%E6%95%B0.png">
<meta property="og:image" content="https://bassyess.github.io/images/tanh%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="https://bassyess.github.io/images/Relu%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="https://bassyess.github.io/images/LeakyRelu%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="https://bassyess.github.io/images/ELU%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="https://bassyess.github.io/images/%E5%BD%92%E4%B8%80%E5%8C%96%E5%8C%BA%E5%88%AB.png">
<meta property="og:image" content="https://bassyess.github.io/images/pooling.png">
<meta property="og:image" content="https://bassyess.github.io/images/LeNet5.png">
<meta property="og:image" content="https://bassyess.github.io/images/LeNet5%E5%8F%82%E6%95%B0.png">
<meta property="og:image" content="https://bassyess.github.io/images/AlexNet.png">
<meta property="og:image" content="https://bassyess.github.io/images/AlexNet.png">
<meta property="og:image" content="https://bassyess.github.io/images/VGG16.png">
<meta property="og:image" content="https://bassyess.github.io/images/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%8D%B7%E7%A7%AF%E5%B1%82.png">
<meta property="og:image" content="https://bassyess.github.io/images/Inception1.png">
<meta property="og:image" content="https://bassyess.github.io/images/Inception.png">
<meta property="og:image" content="https://bassyess.github.io/images/Inceptionv3.png">
<meta property="og:image" content="https://bassyess.github.io/images/Residual.png">
<meta property="og:image" content="https://bassyess.github.io/images/bottleneck.png">
<meta property="og:image" content="https://bassyess.github.io/images/DenseNet.jpg">
<meta property="og:image" content="https://bassyess.github.io/images/Denseblock.jpg">
<meta property="og:image" content="https://bassyess.github.io/images/IOU.png">
<meta property="og:image" content="https://bassyess.github.io/images/RCNN.png">
<meta property="og:image" content="https://bassyess.github.io/images/rcnn2.png">
<meta property="og:image" content="https://bassyess.github.io/images/sppnet.png">
<meta property="og:image" content="https://bassyess.github.io/images/rcnn_vs_spp.png">
<meta property="og:image" content="https://bassyess.github.io/images/SPP.png">
<meta property="og:image" content="https://bassyess.github.io/images/ROImap.png">
<meta property="og:image" content="https://bassyess.github.io/images/frcnn.png">
<meta property="og:image" content="https://bassyess.github.io/images/faster%20rcnn.png">
<meta property="og:image" content="https://bassyess.github.io/images/RPN.png">
<meta property="og:image" content="https://bassyess.github.io/images/Anchor.png">
<meta property="og:image" content="https://bassyess.github.io/images/fasterrcnn.png">
<meta property="og:image" content="https://bassyess.github.io/images/ROIAlign.png">
<meta property="og:image" content="https://bassyess.github.io/images/ROIAlign2.png">
<meta property="og:image" content="https://bassyess.github.io/images/maskrcnn.jpg">
<meta property="og:image" content="https://bassyess.github.io/images/Yolo.png">
<meta property="og:image" content="https://bassyess.github.io/images/Yolo2.png">
<meta property="og:image" content="https://bassyess.github.io/images/Yolo3.png">
<meta property="og:image" content="https://bassyess.github.io/images/SSD.png">
<meta property="og:image" content="https://bassyess.github.io/images/SSD2.png">
<meta property="og:image" content="https://bassyess.github.io/images/SSDloss.png">
<meta property="article:published_time" content="2020-02-18T13:13:44.000Z">
<meta property="article:modified_time" content="2020-02-25T12:04:46.538Z">
<meta property="article:author" content="Kay">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bassyess.github.io/images/Sigmoid.png">

<link rel="canonical" href="https://bassyess.github.io/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>深度学习基础知识 | Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Home</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习基础知识
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-18 21:13:44" itemprop="dateCreated datePublished" datetime="2020-02-18T21:13:44+08:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-25 20:04:46" itemprop="dateModified" datetime="2020-02-25T20:04:46+08:00">2020-02-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="SGD-Momentum-Adagard-Adam原理"><a href="#SGD-Momentum-Adagard-Adam原理" class="headerlink" title="SGD,Momentum,Adagard,Adam原理"></a>SGD,Momentum,Adagard,Adam原理</h2><h3 id="1-SGD、BGD和Mini-BGD"><a href="#1-SGD、BGD和Mini-BGD" class="headerlink" title="1. SGD、BGD和Mini-BGD:"></a>1. SGD、BGD和Mini-BGD:</h3><p>SGD(stochastic gradient descent):随机梯度下降，算法在每读入一个数据都会立刻计算loss function的梯度来更新参数，假设loss function为L(w),下同。</p>
<script type="math/tex; mode=display">w-=\eta \bigtriangledown_{w_{i}}L(w_{i})</script><p>优点：收敛的速度快，可以实现在线更新，能够跳出局部最优<br>缺点：很容易陷入到局部最优，困在马鞍点<br>BGD(batch gradient decent):批量梯度下降，算法在读取整个数据集后累加来计算损失函数的梯度。</p>
<script type="math/tex; mode=display">w-=\eta \bigtriangledown_{w}L(w)</script><p>优点：如果loss function为convex(凸函数)，则基本可以找到全局最优解<br>缺点：数据处理量大，导致梯度下降慢；不能实时增加实例，在线更新；训练占内存<br>Mini-BGD(mini-batch gradient descent):选择小批量数据进行梯度下降，这是一个折中的方法，采用训练集的子集(mini-batch)来计算loss function的梯度：</p>
<script type="math/tex; mode=display">w-=\eta \bigtriangledown_{w_{i:i+n}}L(w_{i:i+n})</script><p>这个优化方法用的比较多，计算效率高且收敛稳定，是现在深度学习的主流方法。<br>上面的方法都存在一个问题，就是update更新的方向完全依赖计算出来的梯度，很容易陷入局部最优的马鞍点。能不能改变其走向，又保证原本的梯度方向，就像向量变换一样，我们模拟物理中物体流动的动量概念（惯性），引入Momentum的概念。</p>
<h3 id="2-Momentum"><a href="#2-Momentum" class="headerlink" title="2. Momentum"></a>2. Momentum</h3><p>在更新方向的时候保留之前的方向，增加稳定性而且还有摆脱局部最优的能力。</p>
<script type="math/tex; mode=display">\Delta w=\alpha \Delta w- \eta \bigtriangledown L(w)</script><script type="math/tex; mode=display">w=w+\Delta w</script><p>若当前梯度的方向与历史梯度方向一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。一种形象的解释是：我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，<script type="math/tex">\eta</script>可视为空气阻力，若球的方向发生变化，则动量会衰减。</p>
<h3 id="3-Adagrad"><a href="#3-Adagrad" class="headerlink" title="3. Adagrad"></a>3. Adagrad</h3><p>Adagrad(adaptive gradient)自适应梯度算法，是一种改进的随机梯度下降算法。以前的算法中，每一个参数都使用相同的学习率<script type="math/tex">\alpha</script>,而Adagrad算法能够在训练中自动对learning_rate进行调整，出现频率较低参数采用较大的<script type="math/tex">\alpha</script>更新，出现频率较高的参数采用较小的<script type="math/tex">\alpha</script>更新，根据描述这个优化方法很适合处理稀疏数据。</p>
<script type="math/tex; mode=display">G=\sum ^{t}_{\tau=1}g_{\tau} g_{\tau}^{T} \quad s.t. \ g_{\tau}=\bigtriangledown L(w_{i})$$　
对角矩阵
$$G_{j,j}=\sum _{\tau=1}^{t} g_{\tau,j\cdot}^{2}</script><p>这个对角线矩阵的元素代表的是参数的出现频率，每个参数的更新：</p>
<script type="math/tex; mode=display">w_{j}=w_{j}-\frac{\eta}{\sqrt{G_{j,j}}}g_{j}</script><h3 id="4-RMSprop"><a href="#4-RMSprop" class="headerlink" title="4. RMSprop"></a>4. RMSprop</h3><p>RMSprop(root mean square propagation)也是一种自适应学习率方法，不同之处在于，Adagrad会累加之前所有的梯度平方，RMSprop仅仅是计算对应的平均值，可以缓解Adagrad算法学习率下降较快的问题。</p>
<script type="math/tex; mode=display">v(w,t)=\gamma v(w,t-1)+(1-\gamma)(\bigtriangledown L(w_{i}))^{2}</script><p>其中 $\gamma$ 是遗忘因子</p>
<p>参数更新</p>
<script type="math/tex; mode=display">w=w-\frac{\eta}{\sqrt{v(w,t)}}\bigtriangledown L(w_{i})</script><h3 id="5-Adam"><a href="#5-Adam" class="headerlink" title="5. Adam"></a>5. Adam</h3><p>Adam(adaptive moment eatimation)是对RMSprop优化器的更新，利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。优点：每一次迭代学习率都有一个明确的范围，使得参数变化很平稳。</p>
<script type="math/tex; mode=display">m_{w}^{t+1}=\beta_{1}m_{w}^{t}+(1-\beta_{1}) \bigtriangledown L^{t}</script><script type="math/tex; mode=display">v_{w}^{t+1}=\beta_{2}m_{w}^{t}+(1-\beta_{2}) (\bigtriangledown L^{t})^{2}</script><p>其中，m为一阶矩估计，v为二阶矩估计，然后进行估计校正，实现无偏估计</p>
<script type="math/tex; mode=display">\hat{m}_{w}=\frac{m_{w}^{t+1}}{1-\beta_{1}^{t+1}}</script><script type="math/tex; mode=display">\hat{v}_{w}=\frac{v_{w}^{t+1}}{1-\beta_{2}^{t+1}}</script><script type="math/tex; mode=display">w^{t+1} \leftarrow=w^{t}-\eta \frac{\hat{m}_{w}}{\sqrt{\hat{v}_{w}}+\epsilon}</script><p>Adam是实际中最常用的算法</p>
<h2 id="L1、L2范数"><a href="#L1、L2范数" class="headerlink" title="L1、L2范数"></a>L1、L2范数</h2><p>在机器学习中几乎可以看到在损失函数后面后悔添加一个额外项，常用的额外项一般有两种，称为L1正则化和L2正则化，或者L1范数和L2范数。L1范数和L2范数可以看做是损失函数的惩罚想，所谓“惩罚”是指对损失函数中的某些参数做些限制。对于现行回归模型，使用L1范数的模型叫做Lasso回归，使用L2范数的模型叫做Ridge回归（岭回归）。</p>
<h3 id="L1范数"><a href="#L1范数" class="headerlink" title="L1范数"></a>L1范数</h3><p>L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”(Lasso regularization)。稀疏的意思是可以让权重矩阵的一部分值等于0。为什么L1范数会使权值稀疏？有一种回答“它是L0范数的最优凸近似”，还存在一种更优雅的回答：任何的规则算子，如果他在$w_{i}=0$处不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子可以实现稀疏。</p>
<script type="math/tex; mode=display">||x||_{1}=\sum_{i}|x_{i}|</script><p>L1范数可以实现稀疏，而实现稀疏的作用为：<br>1) 可解释性：可以看到到底是哪些特征和预测的信息有关<br>2) 特征选择：输入的x的大部分特征与输出y是没有关系的，如果让参数矩阵w中出现许多0，则可以直接干掉与y无关的元素，也就是选择出于y真正相关的特征。如果不这么做，那么x中本来与y无关的特征也加入到模型中，虽然会更好的减小训练误差，但是在预测新样本时会考虑到无关的信息，干扰了预测。</p>
<h3 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h3><p>L2范数是指向量中各元素的平方和然后再求平方根，也叫做“岭回归(Ridge Regression)”，或叫做“权值衰减(weight decay)”。</p>
<script type="math/tex; mode=display">||x||_{2}=\sqrt{\sum_{i}x_{i}^2}</script><p>L2范数与L1范数不同，它不会让参数等于0，而是让每个参数都接近于0。L2范数的优点是：<br>1) 防止过拟合。一般的用法是在损失函数后面加上w的L2范数，即$||x||_{2}$，这是一种规则。<br>2) 优化求解变得稳定快速。简单地说它可以让$w$在接近全局最优点$w^*$的时候，还保持较大的梯度。这样可以跳出局部最优，也使得收敛速度变快。<br>总之，L1会趋向产生少量的特征，而其他的特征都是0，L2会产生更多地特征但都会接近于0。L1在特征选择时候非常有用，而L2就是一种规则化而已</p>
<h3 id="L1不可导的时候该怎么办"><a href="#L1不可导的时候该怎么办" class="headerlink" title="L1不可导的时候该怎么办"></a>L1不可导的时候该怎么办</h3><p>当损失函数不可导，梯度下降不再有效，可以使用坐标轴下降法。梯度下降是沿着当前点的负梯度方向进行参数更新，而坐标轴下降法是沿着坐标轴的方向。假设有m个特征个数，坐标轴下降法进行参数更新的时候，先固定m-1个值，然后再求另外一个的局部最优解，从而避免损失函数不可导问题。坐标轴下降法每轮迭代都需要O(mn)的计算，和梯度下降算法相同。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>神经网络的每个神经元接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入神经元节点会将输入属性值直接传递给下一层（隐藏层或输出层）。上层函数的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数。如果不用激活函数，在这种情况下每一层节点的输入都是上一层输出的线性函数，很容易验证，无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron），那么网络的逼近能力就相当有限。我们引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大，几乎可以逼近任意函数。</p>
<h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p>Signoid是常用的非线性的激活函数，它的数学形式如下：</p>
<script type="math/tex; mode=display">f(z)=\frac{1}{1+e^{-z}}</script><p>Sigmoid的几何如下：<br><img src="/images/Sigmoid.png" alt="Sigmoid函数"><br>特点：它能够把输入的连续实值变换为0和1之间的输出，对于非常大的负数则输出为0，非常大的正数则输出为1.<br>缺点：<br>1) 在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。Sigmoid函数的倒数如下所示：<br><img src="/images/Sigmoid导数.png" alt="Sigmoid的导数"><br>如果我们初始化神经网络的权值为[0,1]之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，没传递一层梯度值都会减少为原来的0.25倍，如果神经网络隐藏层特别多时，那么梯度在多层传递之后就变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为$(1,+\infty)$区间的值，则会出现梯度爆炸情况。<br>2) Sigmoid的输出不是0均值，这会导致后一层的神经元将上一层的神经元输出的非0均值的信号作为输入。产生的结果是：如果$x&gt;0, f=w^Tx+b$，那么对$w$求局部梯度则都为正，这样反向传播的过程中$w$要么都向正方向更新，要么都往负方向更新，使得收敛缓慢。当然，如果按batch训练，那么那个batch可能会得到不同的信号，这个问题可以缓解一下。非0均值问题虽然会产生一些不好的影响，不过跟梯度消失问题相比还是要好很多。<br>3) 其解析式中含有幂运算，计算求解时相对比较耗时。对于规模较大的深度网络，这回较大地增加训练时间。</p>
<h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>tanh函数的解析式：</p>
<script type="math/tex; mode=display">tanh(x)=\frac{e^x-x^{-x}}{e^x+e^{-x}}</script><p>tanh函数及其导数的几何图像如下：<br><img src="/images/tanh函数.png" alt="tanh函数及其导数"><br>tanh函数解决了Sigmoid函数不是zero-centered输出的问题，然而，梯度消失的问题和幂运算的问题仍然存在。</p>
<h3 id="Relu函数"><a href="#Relu函数" class="headerlink" title="Relu函数"></a>Relu函数</h3><p>Relu函数的解析式：</p>
<script type="math/tex; mode=display">Relu=max(0, x)</script><p>Relu函数及其导数的图像如下图所示：<br><img src="/images/Relu函数.png" alt="Relu函数及其导数"><br>ReLU其实是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。<br>优点：1）解决了梯度消失问题(gradient vanishing)问题（在正区间）。 2）计算速度非常快，只需要判断输入是否大于0。 3）收敛速度远快于sigmoid和tanh函数。<br>ReLU也有几个需要注意的问题：<br>1) ReLU的输出不是zero-centered。<br>2) Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不会被更新。有两个主要原因可能会导致这种情况：参数的初始化或learning_rate太大导致训练过程中参数更新太大进入这种状态。<br>解决的方法是采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。Xavier初始化方法是一种很有效的神经网络初始化方法，为了使得网络中信息更好的流动，每一层输出的方法应该尽量相等。</p>
<h3 id="Leaky-ReLU函数"><a href="#Leaky-ReLU函数" class="headerlink" title="Leaky ReLU函数"></a>Leaky ReLU函数</h3><p>函数表达式：</p>
<script type="math/tex; mode=display">f(x)=max(\alpha x, x)</script><p>Leaky Relu函数及其导数的图像如下图所示：<br><img src="/images/LeakyRelu函数.png" alt="Leaky ReLU函数及其导数"><br>图中左半边直线斜率非常接近于0，所以看起来像是平的。为了解决Dead ReLU Problem，通过将ReLU的前半段设为$\alpha x$而不是0，通常$\alpha =0.01$。理论上讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但在实际操作中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<h3 id="ELU函数"><a href="#ELU函数" class="headerlink" title="ELU函数"></a>ELU函数</h3><p>函数表达式为：</p>
<script type="math/tex; mode=display">
f(x)=
\begin{cases}
x &\text(if\ x>0)\\
\alpha(e^x-1) &\text{otherwise}
\end{cases}</script><p>函数及其导数的图像如下：<br><img src="/images/ELU函数.png" alt="ELU函数及导数图像"><br>显然，ELU有ReLU的基本所有优点，以及不会有Dead ReLU问题，输出的均值接近于0，是zero-centered。它的一个小问题在于计算量稍大。</p>
<h2 id="神经网络权重初始化方式"><a href="#神经网络权重初始化方式" class="headerlink" title="神经网络权重初始化方式"></a>神经网络权重初始化方式</h2><p>在深度学习找那个，神经网络的权重初始化方法(weight initialization)对模型的收敛速度和性能有着至关重要的影响。神经网络其实就是对权重参数w的不停迭代更新，以期达到较好的性能。在深度神经网络中，随着层数的增多，在梯度下降的过程中，极易出现梯度消失或者梯度爆炸。因此，对权重w的初始化显得至关重要，一个好的权重初始化虽然不能完全解决梯度消失和梯度爆炸问题，但是对于处理这两个问题是由很大的帮助的，并且十分有利于模型性能和收敛速度。</p>
<h3 id="初始化为0"><a href="#初始化为0" class="headerlink" title="初始化为0"></a>初始化为0</h3><p>在线性回归和logistics回归中可以使用，因为隐藏层只有一层。在超过一层的神经网络中就不能够使用了。因为如果所有的权重参数都为0，那么所有的神经元输出都是一样的，在反向传播时向后传递的梯度也是一致，将无法发挥多层的效果，实际上相当于一层隐藏层。</p>
<h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>卷积层的方差为：</p>
<script type="math/tex; mode=display">Var(w_ix_i)=E[w_i]^2Var(x_i)+E[x_i]^2Var(w_i)+Var(w_i)Var(x_i)</script><p>使用高斯随机初始化时要把W随机初始化到一个相对较小的值，因为如果X很大的话，W又相对较大，会导致输出值特别大，这样如果激活函数是sigmoid，就会导致sigmoid的输出值为1或0，导致更多的问题。但是随机初始化也有缺点，在均值为0，方差为1的高斯分布中，当神经网络层数增加时，会发现越往高层的激活函数（tanh函数）的输出值几乎都接近于0，使得神经元不被激活。</p>
<h3 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h3><p>每层的权重初始化为：</p>
<script type="math/tex; mode=display">W\sim U[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}]</script><p>服从均匀分布，$n_j$为输入层的参数，$n_{j+1}$为输出层参数。<br>Xavier是为了解决随机初始化问题而提出的一种初始化方式，其思想是尽可能让输入和输出服从相同的分布，这样能够避免高层的激活函数的输出值趋向于0。虽然Xavier初始化能很好地用于tanh函数，但是对于目前最常用的ReLU激活函数，还是无能为力，因此引出He initialization。</p>
<h3 id="MSRA-He-initialization"><a href="#MSRA-He-initialization" class="headerlink" title="MSRA/He initialization"></a>MSRA/He initialization</h3><p>Xavier初始化对于Relu激活函数表现非常不好，因此何恺明针对ReLU重新推导，每层的初始化公式为：</p>
<script type="math/tex; mode=display">W\sim U[0, \sqrt{\frac{2}{n}}]</script><p>是一个均值为0，方差为$\frac{2}{n}$的高斯分布。<br>缺点是：MSRA方法只考虑一个方向，无法使得正向反向传播时方差变化都很小。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="损失函数、代价函数与目标函数"><a href="#损失函数、代价函数与目标函数" class="headerlink" title="损失函数、代价函数与目标函数"></a>损失函数、代价函数与目标函数</h3><p>损失函数(Loss Function)：是定义在单个样本上的，是指一个样本的误差。<br>代价函数(Cost Function)：是定义在整个训练集上，是所有样本误差的平均值，也就是所有损失函数值的平均。<br>目标函数(Object Function)：是指最终需要优化的函数，一般来说是代价函数+正则化项。</p>
<h3 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h3><p>（1）0-1损失函数(0-1 loss function)</p>
<script type="math/tex; mode=display">
L(y, f(x))=
\begin{cases}
1, &\text{y}\ne\text{f(x)}\\
0, &\text{y}=\text{f(x)}
\end{cases}</script><p>即，当预测错误时，损失函数为1，当预测正确时，损失函数值为0.该损失函数不考虑预测值与真实值之间的误差程度。<br>（2）平方损失函数（quadratic loss function）</p>
<script type="math/tex; mode=display">L(y,f(x))=(y-f(x))^2</script><p>是指预测值与实际值差的平方。<br>（3）绝对值损失函数（absolute loss function）</p>
<script type="math/tex; mode=display">L(y,f(x))=|y-f(x)|</script><p>该损失函数只是取了绝对值而不是求平方值，差距不会被平方放大。<br>（4）对数损失函数(logarithmic loss function)</p>
<script type="math/tex; mode=display">L(y, p(y|x))=-logp(y|x)</script><p>该损失函数用到了极大似然估计思想。P(Y|X)通俗的解释是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该越小，因此再加个符号取反。<br>（5）Hinge loss<br>Hinge loss一般分类算法中的损失韩式，尤其是SVM，其定义为：</p>
<script type="math/tex; mode=display">L(w,b)=max{0, 1-yf(x)}</script><p>其中$y=+1$或$y=-1$，$f(x)=wx+b$，当为SVM的线性核时。</p>
<h3 id="常用的代价函数"><a href="#常用的代价函数" class="headerlink" title="常用的代价函数"></a>常用的代价函数</h3><p>（1）均方误差(Mean Squared Error)</p>
<script type="math/tex; mode=display">MSE=\frac{1}{N}\sum_{i=1}^N(y^{(i)}-f(x^{(i)}))^2</script><p>均方误差是指参数估计值与真实值之差的平方的期望值，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精度。（i表示第i个样本，N表示样本总数）。<br>通常用来做回归问题的代价函数。<br>（2）均方根误差</p>
<script type="math/tex; mode=display">RMSE=\sqrt{\frac{1}{N}\sum_{i=1}^N(y^{(i)}-f(x^{(i)}))^2}</script><p>均方根误差是均方误差的算术平方根，能够直观观测预测值与真实值的离散程度。通常用来作为回归算法的性能指标。<br>（3）平均绝对误差（Mean Absolute Error）</p>
<script type="math/tex; mode=display">MAE=\frac{1}{N}\sum_{i=1}^N|y^{(i)}-f(x^{(i)})|</script><p>平均绝对误差是绝对误差的平均值，平均绝对误差能更好地反映预测值误差的实际情况。通常用来作为回归算法的性能指标。<br>（4）交叉熵代价函数(Cross Entry)</p>
<script type="math/tex; mode=display">H(p,q)=-\frac{1}{N}\sum_{i=1}^Np(x^{(i)})log(x^{(-i)})</script><p>交叉熵是用来评估当前训练得到的概率分布于真实分布的差异情况，减少交叉熵损失就是在提高模型的预测准确率。其中p(x)是指真实分布的概率，q(x)是模型通过数据计算出来的概率估计。<br>对于二分类模型的交叉熵代价函数：</p>
<script type="math/tex; mode=display">L(w,b)=-\frac{1}{N}\sum_{i=1}^N(y^{(i)}logf(x^{(i)})+(1-y^{(i)})log(1-f(x^{(i)})))</script><p>其中f(x)可以是sigmoid函数或深度学习中的其他激活函数，而$y{(i)}\in 0,1$。<br>交叉熵通常用作分类问题的代价函数。</p>
<h3 id="为什么分类问题用-cross-entropy，而回归问题用-MSE"><a href="#为什么分类问题用-cross-entropy，而回归问题用-MSE" class="headerlink" title="为什么分类问题用 cross entropy，而回归问题用 MSE?"></a>为什么分类问题用 cross entropy，而回归问题用 MSE?</h3><p><a href="https://blog.csdn.net/weixin_41888969/article/details/89450163" target="_blank" rel="noopener">优秀解答</a><br>对于多分类的标签，从本质上看，通过One-hot操作，就是把具体的标签（Label）空间，变换到一个概率测度空间（设为 p）。而对于多分类问题，在Softmax函数的“加工”下，神经网络的实际输出值就是一个概率向量，设其概率分布为q。现在我们想衡量p和q之间的差异（即损失），一种简单粗暴的方式，自然是可以比较p和q的差值，如MSE（不过效果不好而已）。一种更好的方式是衡量这二者的概率分布的差异，就是交叉熵，因为它的设计初衷，就是要衡量两个概率分布之间的差异。总之分类标签可以看做是概率分布（由one-hot变换而来），神经网络输出（经过softmax加工）也是一个概率分布，现在想衡量二者的差异（即损失），自然用交叉熵最好了。<br>当MSE和交叉熵同时应用到多分类场景下时，（标签的值为1时表示属于此分类，标签值为0时表示不属于此分类），MSE对于每一个输出的结果都非常看重，而交叉熵只对正确分类的结果看重。交叉熵的损失函数只和分类正确的预测结果有关系，而MSE的损失函数还和错误的分类有关系，该分类函数除了让正确的分类尽量变大，还会让错误的分类变得平均，但实际在分类问题中这个调整是没有必要的。但是对于回归问题来说，这样的考虑就显得很重要了。所以，回归问题熵使用交叉上并不合适。</p>
<h2 id="Batch-Normalization原理"><a href="#Batch-Normalization原理" class="headerlink" title="Batch Normalization原理"></a>Batch Normalization原理</h2><p>Batch Normalization就是在训练过程中使得每一层神经网络的输入保持相同分布的。BN的基本思想相当直观：因为深层神经网络在做非线性变换前的激活输入值随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或变动，做所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区域的上下限两端靠近，所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。而BN就是通过一定的规范化手段，把层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回到标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就是导致损失函数较大的变化。即这样使得梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。<br>如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同吗？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层网络就没有意义，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力又下降了。所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的$x$又进行了scale加上shift操作($y=scale*x+shift$)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把输入值从标准正太分布左移或右移一点并拉伸或缩短一点，每个实例的挪动情况不一样，这样等价于非线性函数的值从正中心的线性区域往非线性区域动了动。核心思想是想找到一个线性核非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠近非线性区两头使得网络的收敛速度太慢。<br>BatchNorm在网络中的作用：BN层添加在激活函数前，对输入激活函数的输入进行归一化，这样解决了输入数据发生偏移和增大的影响。<br>BatchNorm的优点：1）极大提升了训练速度，使得收敛过程大大加快；2）还能增加分类效果，一种解释是这类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；3）调参过程也简单了，对于初始化要求没那么高，可以使用大的学习率。</p>
<h2 id="LRN-局部归一化响应"><a href="#LRN-局部归一化响应" class="headerlink" title="LRN 局部归一化响应"></a>LRN 局部归一化响应</h2><p>为什么要有LRN 局部归一化响应？<br>在神经生物学有一概念叫 “侧抑制”，指的是被激活的神经元会抑制相邻的神经元。局部响应归一化借鉴了“侧抑制”的思想来实现局部抑制，对局部神经元的活动创建竞争机制，使得相应比较大的值相对大，小的更小。<br>优点：可提高模型泛化能力，当使用Relu时这种“侧抑制”的方法很管用。</p>
<h2 id="深度学习几种归一化（BN、LN、IN、GN）"><a href="#深度学习几种归一化（BN、LN、IN、GN）" class="headerlink" title="深度学习几种归一化（BN、LN、IN、GN）"></a>深度学习几种归一化（BN、LN、IN、GN）</h2><p> BN、LN、IN和GN这四个归一化的计算流程几乎是一样的，可以分为四步：1）计算出均值；2）计算出方差；3）归一化处理到均值为0，方差为1；4）变化重构，恢复出这一层网络所要学到的分布。<br> 我们先用一个示意图来形象的表现BN、LN、IN和GN的区别，在输入图片的维度为（NCHW）中，HW是被合成一个维度，这个是方便画出示意图，C和N各占一个维度。<br> <img src="/images/归一化区别.png" alt="BN,LN,IN,GN的区别"><br> Batch Normalization：<br>1)BN的计算就是把每个通道的NHW单独拿出来归一化处理<br>2)针对每个channel我们都有一组γ,β，所以可学习的参数为2*C<br>3)当batch size越小，BN的表现效果也越不好，因为计算过程中所得到的均值和方差不能代表全局。<br>Layer Normalizaiton：<br>1)LN的计算就是把每个CHW单独拿出来归一化处理，不受batchsize 的影响<br>2)常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理<br>Instance Normalization:<br>1)IN的计算就是把每个HW单独拿出来归一化处理，不受通道和batchsize 的影响<br>2)常用在风格化迁移，但如果特征图可以用到通道之间的相关性，那么就不建议使用它做归一化处理<br>Group Normalization:<br>1)GN的计算就是把先把通道C分成G组，然后把每个gHW单独拿出来归一化处理，最后把G组归一化之后的数据合并成CHW<br>2)GN介于LN和IN之间，当然可以说LN和IN就是GN的特列，比如G的大小为1或者为C</p>
<h1 id="卷积神经网络模型"><a href="#卷积神经网络模型" class="headerlink" title="卷积神经网络模型"></a>卷积神经网络模型</h1><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>输入图像的位数通常很高，例如，1000x1000大小的彩色图像对应于三百万维特征。因此，继续沿用多层感知机中全连接层会导致庞大的参数量。大参数量需要繁重的计算，而更重要的是大参数量会有更高的过拟合风险。卷积是局部连接、共享参数的全连接层。这两个特征使参数量大大降低。卷积层中的权值通常被称为滤波器(filter)或卷积核(convolution kernel)。<br><strong>局部连接</strong>：所谓局部连接，就是卷积层的节点仅仅和前一层的部分节点相连接，只用来学习局部特征，而全连接层中，每个输出通过权值(weight)和所有输入相连。在计算机视觉中，图像中的某一块区域中，像素之间的相关性与像素之间的距离同样相关，距离较近的像素之间相关性强，距离较远则相关性比较弱，所以局部相关性理论同样适用于计算机视觉的图像处理领域。在卷积层中，每个输出神经元在通道方向保持全连接，在空间方向上只和上一层一部分输入神经元相连。局部感知采用部分神经元接收图像信息，再通过综合全部的图像信息达到增强图像信息的目的。这种局部连接的方式大大减少了参数数量，加快了学习速率，同时也在一定程度上减少过拟合的可能。<br><strong>共享参数</strong>：如果一组权值可以在图像中某个区域提取出有效的表示，那么它们也能在图像中的另外区域提取出有效的表示。也就是说，如果一个模式(pattern)出现在图像中的某个区域，那么它们也可以出现在图像中的其他任何区域。因此，卷积层不同空间位置的神经元共享权值，用于发现图像中不同空间位置的模式。权值共享其实就是整张图片在使用同一个卷积核内的参数提取特征，但是不同的卷积核提取的是不同特征，因此不同卷积核间的神经元权值是不共享的。卷积层在空间方向共享参数，而循环神经网络在时间方向共享参数。<br><strong>描述卷积的四个量</strong>：一个卷积层的配置由如下四个量确定。1）卷积核个数。使用一个卷积核进行卷积可以得到一个二维的特征图(feature map)。使用多个卷积核进行卷积，可以得到不同特征的feature map。2）感受野（receptive field）F，即卷积核的大小。3）零填充(zero-padding)P，随着卷积的进行，图像的大小将缩小，图像边缘的信息将逐渐丢失，因此再卷积前，我们在图像上下左右填补一些0，使得我们可以控制输出特征图的大小。4）步长(stride)S，卷积核在输入图像上每移动S个位置计算一个输出神经元。<br>假设输入图片大小为$I\times I$，卷积核大小为$K\times K$，步长为S，填充的像素为P，则卷积层输出的特征图大小为：</p>
<script type="math/tex; mode=display">O=(I-K+2P)/S+1</script><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化层根据特征图上的局部统计信息进行下采样，在保留有用信息的同是减少特征图的大小。和卷积层不同，池化层不包含需要学习的参数，最大池化层(max-pooling)在一个局部区域选最大值输出，而平均池化(average pooling)计算一个区域的均值作为输出。<br><img src="/images/pooling.png" alt="池化层"><br><strong>池化层的作用</strong>：1）增加特征平移不变性，池化可以提高网络对微小位移的容忍能力。2）减小特征图大小，池化层对空间局部区域进行下采样，使下一层需要的参数量和计算量减少，并降低过拟合风险。3）最大池化可以带来非线性，这是目前最大池化更常使用的原因。</p>
<h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><p>给定一张图像，图像分类任务旨在判断该图像所属类别。</p>
<h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>LeCun等将BP算法应用到多层神经网络中，提出了LeNet5模型，并将其用于手写数字识别，卷积神经网络才算正式提出。<br><img src="/images/LeNet5.png" alt="LeNet5网络模型"><br><img src="/images/LeNet5参数.png" alt="LeNet5网络模型参数"><br>网络输入32*32的手写字体图片，这些手写字体包含0~9数字，也就是相当于10个类别的图片；输出：分类结果在0~9之间。LeNet的网络结构十分简单且单一，卷积层C1、C3和C5除了输出维数外采用的是相同的参数，池化层S2和S4采用的也是相同的参数。</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>2012年Krizhevsky使用卷积神经网络在ILSRC 2012图像分类大赛上夺冠，提出了AlexNet模型。AlexNet网络的提出对于卷积神经网络具有里程碑式的意义，相较于LeNet5的改进有以下几点：<br>1）数据增强：水平翻转、随机裁剪（平移变换）、颜色光照变换<br>2）Dropout:Dropout方法和数据增强一样，都是防止过拟合。dropout能按照一定的概率将神经元从网络中丢弃，dropout能在一定程度上防止过拟合，并且加快网络的训练速度。<br>3）ReLU激活函数：ReLU具有一些优良的特性，在为网络引入非线性的同时，也能引入稀疏性。稀疏性可以选择性激活和分布式激活神经元，能学习到相对稀疏的特征，起到自动化解离的效果。此外，ReLU的导数曲线在输入大于0时，函数的导数为1，这种特性能保证在其输入大于0时梯度不衰减，从而避免或抑制网络训练时梯度消失现象，网络模型的收敛速度会相对稳定。4）Local Response Normalization:局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。5）Overlapping Pooling：即Pooling的步长比Pooling Kernel对应边要小。6）多GPU并行：极大加快网络训练。<br><img src="/images/AlexNet.png" alt="AlexNet网络模型"><br><img src="/images/AlexNet.png" alt="AlexNet网络模型参数"></p>
<h3 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h3><p>VGGNet是由牛津大学计算机视觉组合Google DeepMind项目的研究员共同研发的卷积神经网络模型，包含VGG16和VGG19两种模型。<br><img src="/images/VGG16.png" alt="VGG16网络模型"><br>从网络模型中可以看出，VGG16相比于AlexNet类的模型具有较深的深度，通过反复堆叠$3\times 3$的卷积层和$2\times 2$的池化层，VGG16构建了较深层次的网络结构。与AlexNet主要有以下不同：<br>1）Vgg16有16层网络，AlexNet只有8层；<br>2）在训练和测试时使用了多尺度做数据增强。</p>
<h3 id="GoogleNet-22层"><a href="#GoogleNet-22层" class="headerlink" title="GoogleNet(22层)"></a>GoogleNet(22层)</h3><p>GoogleNet进一步增加了网络模型的深度，但是单纯的在VGG16的基础上增加网络的宽度会带来以下的缺陷：1）过多的参数容易引起过拟合；2）层数的过深，容易引起梯度消息现象。<br>GoogleNet的提出受到论文Network in Network(NIN)的启发，NIN有两个贡献：1）提出多层感知卷积层，使用卷积层后加上多层感知机，增强网络提取特征的能力。普通的卷积层和多层感知卷积层结构如图所示，Mlpconv相当于在一般卷积层后加一个1*1的卷积层。2）提出了全局平均池化代替全连接层，全连接层具有大量的参数，使用全局平均池化代替全连接层，能很大程度减少参数空间，便于加深网络，还能防止过拟合。<br><img src="/images/多层感知卷积层.png" alt="普通卷积层和多层感知卷积层结构图"><br>GoogleNet根据Mlpconv的思想提出了Inception结构，该结构有两个版本，下图是Inception的naive版，该结构巧妙的将$1\times 1$、$3\times 3$和$5\times 5$三种卷积核和最大池化层结合起来作为一层结构。<br><img src="/images/Inception1.png" alt="Inception结构的native版"><br>而Inception的native版中$5\times 5$的卷积核会带来很大的计算量，因此采用了与NIN类似的结构，在原始的卷积层之后加上$1\times 1$卷积层，最终版本的Inception如下图所示：<br><img src="/images/Inception.png" alt="降维后的Inception模块"></p>
<h3 id="Inception-v3-v4"><a href="#Inception-v3-v4" class="headerlink" title="Inception v3/v4"></a>Inception v3/v4</h3><p>在GoogleNet的基础上进一步降低参数，其和GoogleNet有相似的Inception模块，但将$7\times 7$和$5\times 5$卷积分解为若干等效$3\times 3$卷积，并在网络中后部分把$3\times 3$卷积分解为$1\times 3$和$3\times 1$卷积，这使得在相似的网络参数下网络可以部署到42层。此外，Inception v3使用了批量归一化层。Inception v3是GoogleNet计算量的2.5倍，而错误率较后者下降了3%。Inception v4在Inception模块基础上结合residual模块，进一步降低了0.4%的错误率。<br><img src="/images/Inceptionv3.png" alt="Inceptionv3模块"></p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>卷积神经网络模型的发展历程一次次证明加深网络的深度和宽度能得到更好的效果，但是后来的研究发现，网络层次较深的网络模型的效果反而不如较浅层的网络，称为“退化”现象。退化现象产生的原因在于当模型的结构变得复杂随机梯度下降的优化变得更加困难，导致网络模型的效果反而不如浅层网络。针对这个问题，MSRA何凯明团队提出Residual Network，该网络具有Residual结构如下所示：<br><img src="/images/Residual.png" alt="Residual结构"><br>ResNet的基本思想是引入了能够跳过一层或多层的“shortcut connection”，即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换为F(x)+x，作者认为这两种表达的效果相同，但是优化的难度却并不相同。这个Residual block通过将shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的叠加，这个简单地加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度，提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。对于shortcut的方式，作者提出了三个策略：1）使用恒等映射，如果residual block的输入和输出维度不一致，对增加的维度用0来填充；2）在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致；3）对于所有的block使用线性投影。论文最后用三个$1\times 1,3\times 3, 1\times 1$的卷积层代替前面说的两个$3\times 3$卷积层，第一个$1\times 1$用来降低维度，第三个$1\times 1$用来增加维度，这样可以保证中间的$3\times 3$卷积层拥有比较小的输入输出维度。<br><img src="/images/bottleneck.png" alt="更深的residual block"></p>
<h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p>DenseNet脱离了加深网络层数(ResNet)和旁路网络结构(Inception)来提升网络性能的定式思维，从特征的家督考虑，通过特征重用和旁路（Bypass）设置，既大幅度减少了网络的参数量，又在一定程度上缓解了gradient vanishing问题的产生。DenseNet作为另一种较深层数的卷积神经网络，具有如下优点：<br>1）相比于ResNet拥有更少的参数数量；2）旁路加强了特征的重用；3）网络更容易训练，并具有一定的正则效果；4）缓解了gradient vanishing和model degradation的问题。<br><img src="/images/DenseNet.jpg" alt="DenseNet网络结构图"><br>如图所示，第i层的输入不仅与i-1层的输出相关，还与所有之前层的输出相关。记作：<br>$X_l=H_l([X_0,X_1,…,X_{l-1}])$<br>其中[]代表concatenation（拼接），既将$X_0$到$X_{l-1}$层的所有输出feature map按Channel组合在一起，这里所用到的非线性变换H为BN+ReLU+Conv(3x3)的组合。<br>由于在DenseNet中需要对不同层的feature map进行cat操作，所以需要不同层的feature map保持相同的feature size，这就限制了网络中的down sampling的实现。为了使用down sampling在实验中transition layer由BN+Conv(1x1)+average-pooling(2x2)组成。<br><img src="/images/Denseblock.jpg" alt="Denseblock网络图"></p>
<h2 id="图像检测"><a href="#图像检测" class="headerlink" title="图像检测"></a>图像检测</h2><p>分类任务关系整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息。相比于分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置）。因此，检测模型的输出是一个列表，列表的每一项使用一个数据组给出检测目标的类别和位置（常用矩形检测框的坐标表示）。<br>目前主流的目标检测算法主要是基于深度模型，大致可以分成两大类别：（1）One-Stage目标检测算法，这类检测算法不需要Region Proposal阶段，可以通过一个Stage直接产生物体的类别概率和位置坐标值，比较典型的算法有YOLO、SSD和CornerNet；（2）Two-Stage目标检测算法，这类检测算法将检测问题划分为两个阶段，第一个阶段首先产生候选区域（Region Proposals），包含目标大概的位置信息，然后第二个阶段对候选区域进行分类和位置精修，这类算法的典型代表有R-CNN、Fast R-CNN、Faster R-CNN等。目标检测模型的主要性能是检测准确度和速度，其中准确度主要考虑物体的定位及分类准确度。一般情况下，Two-Stage算法在准确度上有优势，而One-Stage算法在速度上有优势。不过随着研究的发展，两类算法都在两个方面做改进，均能在准确度及速度上取得较好结果。</p>
<h3 id="IOU的定义"><a href="#IOU的定义" class="headerlink" title="IOU的定义"></a>IOU的定义</h3><p>物体检测需要定位出物体的bounding box，对于bounding box的定位精度，存在一个定位精度评价公式：IOU。<br>IOU定义了两个bounding box的重叠度，如下图所示：<br><img src="/images/IOU.png" alt="IOU图像"><br>矩形框A、B的一个重合度计算公式为：</p>
<script type="math/tex; mode=display">IOU=\frac{A\cap B}{A\cup B}</script><p>就是矩形框A、B的重叠面积$S_I$占A、B并集的面积的比例：</p>
<script type="math/tex; mode=display">IOU=\frac{S_I}{S_A+S_B-S_I}</script><h3 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h3><p>在目标检测时一般会采取窗口滑动的方式，在图像上生成很多的候选框，把这些候选框进行特征提取送入到分类器，一般会得到一个得分，然后把这些得分全部排序。选取得分最高的那个框，接下来计算其他框与当前框的重合程度(IOU)，如果重合程度大于一定的阈值就删除，这样不停的迭代下去就会得到所有想要找到的目标物体的区域。</p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>算法摘要：首先输入一张图片，我们先定位出2000个物体候选框，然后采用CNN提取每个候选框中图片的特征向量，特征向量的维度是4096维，接着采用SVM算法对各个候选框中的物体进行分类识别。<br><img src="/images/RCNN.png" alt="R-CNN算法流程"><br>我们采用selective search算法搜索出候选框，由于搜索到的候选框是矩形的，而且大小各不相同，而CNN对输入的图片大小是固定的，因此需要对每个输入的候选框都要缩放到固定的大小。然而人工标注的图片中就只标注了一个正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此我们需要用IOU为2000个bounding box打标签，以便下一步CNN训练使用。在CNN阶段，如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域大于0.5，我们就把这个候选框标注成物体类别，否则我们就把它当做背景类别。最后，我们需要对上面预训练的CNN模型进行fine-tuning训练。假设需要检测的物体类别有N类，那么我们就需要将上面与训练的CNN模型的最后一层给替换点，替换成N+1个输出的神经元（还有一个背景），然后这一层直接采用参数随机初始化的方法，其他的网络层数不变。<br>R-CNN缺点：<br>1）耗时的selective search，对一帧图像，需要花费2s。<br>2）耗时的串行式CNN前向传播，对于每一个ROI，都需要经过一个AlexNet提取特征，为所有的ROI提供特征需要花费47s。<br>3）三个模块分别训练，并且在训练的时候，对于存储空间的消耗很大。<br><img src="/images/rcnn2.png" alt="RCNN算法流程框图"></p>
<h3 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h3><p><img src="/images/sppnet.png" alt="SPPNet网络流程框图"><br>算法特点：1）通过Spatial Pyramid Pooling解决了深度网络固定输入层尺寸的这个限制，使得网络可以享受不限制输入尺寸带来的好处。2）解决了RCNN速度慢的问题，不需要对每个Proposal(2000个左右)进行wrap或crop输入CNN提取feature map，只需要对整图提一次feature map，然后将proposal区域映射到卷积特征层得到全连接层的输入特征。<br><img src="/images/rcnn_vs_spp.png" alt="RCNN与SPPNet对比"><br>一、ROI在特征图上的对应的特征区域的维度不满足全连接层的输入要求怎么办？<br>事实上，CNN的卷积层不需要固定尺寸的图像，而是全连接层是需要固定大小的输入。根据Pooling规则，每个Pooling bin对应一个输出，所以最终的Pooling后的输出特征由bin的个数来决定。SPP网络就是分级固定bin的个数，调整bin的尺寸来实现多级Pooling固定输出。如图所示，SPP网络中最后一个卷积层的feature map维数为16x24，按照图中所示分为3级：<br><img src="/images/SPP.png" alt="SPP网络结构"><br>其中，第一级bin个数为1，最终对应的window大小为16x24；第二级bin个数为4个，最终对应的window大小为4x8；第三级bin个数为16个，最终对应的window大小为1x1.5(小数需要舍入处理)。通过融合各级bin的输出，最终每一个feature map经过SPP处理后，得到1+4+16维的feature map，经过融合后输入分类器。这样就可以在任意输入size和scale下获得固定的输出；不同的scale下网络可以提取不同尺度的特征，有利于分类。<br>二、原始图像的ROI如何映射到特征图？<br>下面将从感受野、感受野上坐标映射及原始图像的ROI如何映射三方面阐述。<br>1）感受野<br>在卷积神经网络中，感受野的定义是卷积神经网络每一层输出的特征图(feature map)的像素点在原始图像上映射的区域大小。</p>
<script type="math/tex; mode=display">output\ field\ size = (input\ field\ size - kernel size + 2*padding) / stride + 1</script><p>其中output field size是卷积层的输出，input field size是卷积层的输入，反过来卷积层的输入为:</p>
<script type="math/tex; mode=display">input\ field\ size = (output\ field\ size - 1) * stride - 2*padding + kernel size</script><p>2）感受野上的坐标映射<br>对于Convolution/Pooling Layer:</p>
<script type="math/tex; mode=display">p_i=s_i \cdot p_{i+1}+[(k_i-1)/2-padding]</script><p>对于Neuronlayer（ReLU/Sigmoid/…）:</p>
<script type="math/tex; mode=display">p_i=p_{i+1}</script><p>其中$p_i$为第$i$层感受野上的坐标，$s_i$为Stride的大小，$k_i$为感受野的大小。<br>何凯明在SPP-NET中使用的是简化版本，将公式中的Padding都设为$\left \lfloor k_i/2 \right \rfloor$，公式可进一步简化为：$p_i=s_i \cdot p_{i+1}$<br>3）原始图像的ROI如何映射<br>SPP-NET是把原始ROI的左上角和右下角 映射到Feature Map上的两个对应点。 有了Feature Map上的两队角点就确定了对应的Feature Map 区域（下图中橙色）。<br><img src="/images/ROImap.png" alt="ROI映射过程"><br>左上角取$x’=\left \lfloor x/S \right \rfloor+1, y’=\left \lfloor y/S \right \rfloor+1$；右下角的点取$x’=\left \lceil x/S \right \rceil-1, y’=\left \lceil y/S \right \rceil-1$。其中S为坐标映射的简化计算版本，即$S=\prod_{0}^{i}s_i$。</p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>Fast R-CNN针对R-CNN在训练时multi-state pipeline和训练的过程很耗时间和空间的问题进行了改进，主要改进为：<br>1）在最后一个卷积层加了一个ROI pooling layer。ROI Pooling layer首先可以将image中的ROI定位到feature map，然后用一个单层的SPP layer将这个feature map池化到固定大小的feature map后再传入全连接层。<br>2）损失函数使用多任务损失函数（multi-task loss），将边框回归直接加入到CNN网络进行训练。<br><img src="/images/frcnn.png" alt="Fast R-CNN流程框图"><br>首先还是采用selective search提取2000个候选框，然后对全图进行特征提取，接着使用一个ROI pooling layer在全体特征上获取每一个ROI对应的特征，再通过全连接层进行分类和检测框修正。即最后得到的ROI feature vector被分开，一个经过全连接层后用作softmax回归，用来分类，另一个经过全连接后用作bbox回归。需要注意的是，输入到后面ROI Pooling layer的feature map是在卷积层上的feature map上提取的，故整个特征提取过程只计算一次卷积。虽然在最开始也提取了大量的ROI，但他们还是作为整体输入到卷积网络的，最开始提取的ROI区域只是为了最后的bounding box回归时使用。<br><strong>联合训练</strong>：联合训练（Joint Training）指如何将分类和边框回归联合到一起在CNN阶段训练，主要难点是损失函数的设计。Fast-RCNN中，有两个输出层：第一个是针对每个ROI区域的分类概率预测，$p=(p_0, p_1, \cdots, p_K)$；第二个则是针对每个ROI区域坐标的偏移优化，$t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$，$0 \le k \le K$是多类检测的类别序号。每个训练ROI都对应着真实类别$u$和边框回归目标$v=(v_x,v_y,v_w,v_h)$，对于类别$u$预测边框为$t^u=(t_x^u,t_y^u,t_w^u,t_h^u)$，使用多任务损失$L$来定义ROI上分类和边框回归的损失：</p>
<script type="math/tex; mode=display">L(p,u,t^u,v)=L_{cls}(p,u)+\lambda [u \ge 1]L_{loc}(t^u,v)</script><p>其中$L_{cls}(p,u)=-\log p_u$表示真实类别的log损失，当$u \ge 1$时，$[u \ge 1]$的值为1，否则为0。下面将重点介绍多任务损失中的边框回归部分（对应坐标偏移优化部分）。<br><strong>边框回归</strong>：假设对于类别$u$，在图片中标注的真实坐标和对应的预测值理论上两者越接近越好，相应的损失函数为：</p>
<script type="math/tex; mode=display">L_{loc}(t^u, v) = \sum_{i \in {x, y, w, h}} \text{smooth}_{L_1}(t_i^u- v_i)</script><script type="math/tex; mode=display">\text{smooth}_{L_1}(x) = \left \{ \begin{aligned} & 0.5x^2 & |x| \le 1 \\ &|x|-0.5 & \text{otherwise}\end{aligned} \right.</script><p>Fast-RCNN在上面用到的鲁棒$L_1$函数对外点比RCNN和SPP-NET中用的$L_2$函数更为鲁棒，该函数在$(-1, 1)$之间为二次函数，其他区域为线性函数<br>存在问题：使用Selective Search提取Region Proposals，没有实现真正意义上的端到端，操作耗时。</p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p><img src="/images/faster rcnn.png" alt="Faster R-CNN流程图"><br>算法特点：1）提出了Region Proposal Network(RPN)，将Proposal阶段和CNN分类融合到一起，实现了一个完全的End-To-End的CNN目标检测模型。RPN可以快速提取高质量的Proposal，不仅加快了目标检测速度，还提高了目标检测性能。2）将Fast R-CNN和RPN放在同一个网络结构中训练，共享网络参数。</p>
<h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h4><p>Region Proposal Network(RPN)的核心思想是使用卷积神经网络直接产生Region Proposal，使用的方法本质上就是滑动窗口。RPN的设计比较巧妙，RPN只需要在最后的卷积层上滑动以便，借助Anchor机制和边框回归就可以得到多尺度多长宽比的Region Proposal。下图是RPN的网络结构图。<br><img src="/images/RPN.png" alt="RPN网络结构图"><br>给定输入图像（假设分辨率为$600<em>1000$），经过卷积操作得到最后一层卷积特征图（大小约为$40 </em> 60$）。在这个特征图上使用$3 <em> 3$的卷积核（滑动窗口）与特征图进行卷积，最后一层卷积层共有256个feature map，那么这个$3 </em> 3$的区域卷积后可以获得一个256维的特征向量，后边接cls layer和reg layer分别用于分类和边框回归（跟Fast R-CNN类似，只不过这里的类别只有目标和背景两个类别）。$3 <em> 3$滑窗对应的每个特征区域同时预测输入图像的3种尺度（128，256，512），3中长宽比（1：1，1：2，2：1）的Region Proposal，这种映射的机制称为Anchor。所以对于这个$40 </em> 60$的feature map，总共有约20000（$40 <em> 60 </em> 9$）个Anchor，也就是预测20000个Region Proposal。<br><img src="/images/Anchor.png" alt="Anchor示例"><br>这样设计的好处是什么？虽然现在也是在用的滑动窗口策略，但是滑动串口操作是在卷积特征图上进行的，维度较原始图像降低了$16 * 16$倍；多尺度使用了9中Anchor，对应了三种尺度和三种长宽比，加上后边接了边框回归，所以几遍是这9种Anchor外的窗口也能得到一个跟目标比较接近的Region Proposal。</p>
<h4 id="RPN的损失函数"><a href="#RPN的损失函数" class="headerlink" title="RPN的损失函数"></a>RPN的损失函数</h4><p>损失函数的定义为：</p>
<script type="math/tex; mode=display">L({p_i}{t_i}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^*) +\lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^*)</script><p>其中$i$表示一次Mini-Batch中Anchor的索引，$p_i$是Anchor $i$是否是一个物体，$L_{reg}$即为上面提到的$\text{smooth}_{L_1}(x)$函数，$N_{cls}$和$N_{reg}$是两个归一化项，分别表示Mini-Batch的大小和Anchor位置的数目。</p>
<h4 id="网络的训练"><a href="#网络的训练" class="headerlink" title="网络的训练"></a>网络的训练</h4><p>作者采用了4-step Alternating Training:<br>1) 用ImageNet模型初始化，独立训练一个RPN网络；<br>2) 仍然用ImageNet模型初始化，但是使用上一步RPN网络产生的Proposal作为输入，训练一个Fast-RCNN网络，至此，两个网络每一层的参数完全不共享；<br>3) 使用第二步的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的Learning Rate设置为0，也就是不更新，仅仅更新RPN特有的那些网络层，重新训练，此时，两个网络已经共享了所有公共的卷积层；<br>4) 仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个Unified Network，继续训练，Fine Tune Fast-RCNN特有的网络层，此时，该网络已经实现我们设想的目标，即网络内部预测Proposal并实现检测的功能。<br>在训练分类器和RoI边框修正时，步骤如下：<br>1）首先通过RPN生成约20000个anchor（40x60x9）<br>2）对20000个anchor进行第一次边框修正，得到修订边框后的proposal；<br>3）对超过图像边界的proposal的边进行clip，使得该proposal不超过图像范围；<br>4）忽略掉长或宽太小的proposal；<br>5）将所有的proposal按照前景分数从高到低排序，选取前12000个proposal；<br>6）使用阈值为0.7的NMS算法排除掉重叠的proposal；<br>7）针对上一步剩下的proposal，选取前2000个proposal进行分类和第二次边框修正。<br><img src="/images/fasterrcnn.png" alt="faster rcnn训练过程"></p>
<h4 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h4><p>ROI Align是在Mask R-CNN这篇论文里提出的一种区域特征聚集方式，很好地解决了ROI Pooling操作中两次量化造成的区域不匹配的问题。在检测阶段将ROI Pooling替换为ROI Align可以提升检测模型的准确性。<br>在常见的两级检测框架中，ROI Pooling的作用是根据候选框的位置坐标在特征图中将相应的区域池化为固定尺寸的特征图，以便后续的分类和bounding box回归操作。由于预选框的位置通常是由模型回归得到的，一般来讲都是浮点数，而池化后的特征图要求尺寸固定，故ROI Pooling这一操作存在两次量化的过程。1）将候选框的边界量化为整数点坐标值。2）将量化后的边界区域平均分割成$k <em> k$个单元（bin），对每一个单元的边界进行量化。经过上述的两次量化，此时的候选框已经和最开始回归出来的位置存在一定的偏差，这个偏差会影响检测或分割的精确度。在论文里作者将它归结为“不匹配问题(misalignment)”。<br>下面我们用直观的例子具体分析一下上述区域不匹配问题。如图所示，这是一个Faster-RCNN检测框架。输入一张$800 </em> 800$的图片，图片上有一个$665 <em> 665$的包围框(框着一只狗)。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是ROI Pooling 直接将它量化成20。接下来需要把框内的特征池化$7 </em> 7$的大小，因此将上述包围框平均分割成$7 <em> 7$个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。<br><img src="/images/ROIAlign.png" alt="ROIAlign示例"><br>为了解决ROI Pooling的上诉缺点，作者提出了ROI Align这一改进的方法。ROI Align的思路很简单：取消量化操作，使用双线性内插的犯法获得坐标为浮点数的像素点上的图像数值，从而将整个特征聚集过程转化为一个连续的操作。ROI Align流程如下：<br>1）遍历每一个候选区域，保持浮点数边界不做量化；2）将候选区域分割成k </em> k个单元，每个单元的边界也不做量化；3）在每个单元中计算出固定的四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化的操作。<br>需要注意的是，这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。事实上，ROI Align 在遍历取样点的数量上没有ROIPooling那么多，但却可以获得更好的性能，这主要归功于解决了misalignment的问题。<br><img src="/images/ROIAlign2.png" alt="ROIAlign示例2"><br><img src="/images/maskrcnn.jpg" alt="Mask R-CNN结构图"><br>注意的是，在Mask R-CNN中的ROI Align之后有一个“head”部分，主要作用是将ROI Align的输出维度扩大，这样在预测Mask时会更加精确。<br>在Mask Branch的训练环节，作者没有采用FCN式的SoftmaxLoss，反而是输出了K个Mask预测图（为每一个类都输出一张），并采用average binary cross-entropy loss训练，当然在训练Mask branch的时候，输出K个特征图中，也就是对应ground truth类别的哪一个特征图对Mask loss有贡献。也就是说， Mask RCNN定义为多任务损失为$L=L_{class}+L_{boxes}+L_{mask}$。<br>$L_{class}$与$L_{boxes}$与Faster RCNN没区别。$L_{mask}$中，假设一共有K个类别，则mask分割分支的输出维度是$K <em> m </em> m$，对于$m * m$中的每个点，都会输出K个二值Mask(每个类别使用sigmoid输出)。需要注意的是，计算loss的时候，并不是每个类别的sigmoid输出都计算二值交叉熵损失，而是该像素属于哪个类，哪个类的sigmoid输出才要计算损失函数，并且在测试的时候，我们通过分类分支预测类别来选择相应的mask预测。这样，mask预测和分类分支预测就彻底解耦了。</p>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p>算法特点：1）将物体检测作为回归问题求解。基于一个单独的End-To-End网络，完成从原始图像的输入到物体位置和类别的输出，输入图像经过一次Inference，便能得到图像中所有物体的位置和其所属类别及相应的置信概率。2）YOLO网络借鉴GoogleNet网络结构，不同的是，YOLO未使用Inception Module，而是使用$1 <em> 1$卷积层（此处$1 </em> 1$卷积层的存在是为了跨通道信息整合）+ $3 <em> 3$卷积层简单代替。3）Fast YOLO使用9个卷积层代替YOLO的24个，网络更轻快，速度从YOLO的45fps提升到155fps，但是同时损失了检测准确率。4）使用全图作为Context信息，背景错误（把背景错认为物体）比较少。5）泛化能力强。在自然图像上训练好的结果在艺术作品中依然具有很好的效果。<br><img src="/images/Yolo.png" alt="YOLO网络结构"><br>一、大致流程<br>1）对于一个输入图像，首先将图像划分成$7 </em> 7$的网络。<br>2）对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）。<br>3）根据上一步可以预测出$7 <em> 7 </em> 2$个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。<br><img src="/images/Yolo2.png" alt="Yolo实例"><br>二、训练<br>1）预训练分类网络：在ImageNet 1000-class Competition Dataset预训练一个分类网络，这个网络时前文网络结构中前20个卷积网络+Average-Pooling Layer+Fully Connected Layer(此时网络的输入是$224 <em> 224$)。<br>2）训练检测网络：在预训练网络中增加卷积和全连接层可以改善性能。YOLO添加4个卷积层和2个全连接层，随机初始化权重。检测要求细粒度的视觉信息，所以把网络输入也从$224 </em> 224$变成$448 <em> 448$。一幅图像分成$7 </em> 7$个网格，某个物体的中心落在这个网格中此网络就负责预测这个物体。每个网络预测两个Bounding Box。网格负责类别信息，Bounding Box负责坐标信息（4个坐标信息及一个置信度），所以最后一层输出为$7 <em> 7 </em> (2 <em> (4+1) + 20) = 7 </em> 7 <em> 30$的维度。Bounding Box的坐标使用图像的大小进行归一化0-1，Confidence使用$P_r(Object) </em> IOU_{pred}^{truth}$计算，其中第一项表示是否有物体落在网格中，第二项表示预测的框和实际的框之间的IOU值。<br>3）损失函数的确定：损失函数的定义如下，损失函数的设计目标就是让坐标，置信度和类别这三个方面达到很好的平和。简单地全部采用Sum-Squared Error Loss来做这件事会有以下不足：a)8维的Localization Error和20维的Classification Error同等重要显然是不合理的；b)如果一个网格中没有Object（一幅图中这种玩个很多），那么就会将这些网络中的Box的COnfidence降低到0，相对于较少的有Object的网络，这种做法是Overpowering的，这会导致网络不稳定甚至发散。解决方案如下：<br><img src="/images/Yolo3.png" alt="Yolo的损失函数"><br>首先更重视8维的坐标预测，给这些损失前面赋予更大的Loss Weight，记为$\lamda_{coord}$，在Pascal VOC训练中取5（上图蓝色框）。对于没有Object的Bbox的Confidence Loss，赋予小的Loss Weight，记为$\lamda_{noobj}$，在Pascal VOC训练中取0.5（上图橙色框）。有Object的Bbox的Confidence Loss（上图红色框）和类别的Loss（上图紫色框）的Loss Weight正常取1。对于不同大小的Bbox预测中，相比于大Bbox预测偏一点，小Bbox预测偏一点更不能忍受。而Sum-Square Error Loss中对同样的偏移Loss是一样的。为了缓和这个问题，将Bbox的Width和Height取平方根代替原本的Height和Width。如下图所示：Small Bbox的横轴值较小，发生偏移时，反应到y轴上的Loss(下图绿色)比Big Bbox(下图红色)要大。一个网格预测多个Bbox，在训练时我们希望每个Object（Ground True box）只有一个Bbox专门负责（一个Object一个Bbox）。具体做法是与Ground True Box(Object)的IOU最大的Bbox负责该Ground True Box(Object)的预测。这种做法称作Bbox Predictor的Specialization（专职化）。每个预测器会对特定（Size,Aspect Ratio or Classed of Object）的Ground True Box预测的越来越好。<br>三、测试<br>1）计算每个Bbox的Class-Specific Confidence Score:每个网格预测的Class信息($Pr(Class_i|Object)$)和Bbox预测的Confidence信息($Pr(Object)*IOU_{pred}^{truth}$)相乘，就得到每个Bbox的Class-Specific Confidence Score。</p>
<script type="math/tex; mode=display">Pr(Class_i|Object)*Pr(Object)*IOU_{pred}^{truth}=Pr(Class_i)*IOU_{pred}^{truth}</script><p>2）进行Non-Maximum Suppression(NMS)：得到每个Bbox的Class-Specific Confidence Score以后，设置阈值，滤掉得分低的Bboxes，对保留的Bboxes进行NMS处理就得到最终的检测结果。<br>四、存在问题<br>1）YOLO对相互靠的很近的物体（挨在一起且中点都落在同一个格子上的情况），还有很小的群体检测效果不好，这是因为一个网络中只预测了两个框，并且只属于一类。<br>2）测试图像中，当同一类物体出现的不常见的长宽比和其他情况时泛化能力偏弱。<br>3）由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强。</p>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p><a href="https://blog.csdn.net/xiaohu2022/article/details/79833786?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">SSD模型</a><br>算法特点<br>1）SSD结合了YOLO中的回归思想和Faster R-CNN中的Anchor机制，使用全图各个位置的尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster-RCNN一样比较精准。<br>2）SSD的核心是在特征图上采用卷积核来预测一系列Default Bounding Boxes的类别、坐标偏移。为了提高检测准确率，SSD在不同尺度的特征图上进行预测。<br><img src="/images/SSD.png" alt="SSD网络结构"><br>一、模型结构<br>1、多尺度特征图（Multi-scale Feature Map For Detection）<br>在图像Base Network基础上，将Fc6，Fc7变为Conv6，Conv7两个卷积层，添加了一些卷积层（Conv8,Conv9,Conv10,Conv11），这些层的大小逐渐减小，可以进行多尺度预测。<br>2、卷积预测器（Convolutional Predictors For Detection）<br>每个新添加的卷积层和之前的部分卷积层，使用一系列的卷积核进行预测。对于一个大小为$m <em> n$，$p$通道的卷积层，使用$3 </em> 3$的$p$通道卷积核作为基础预测元素进行预测，在某个位置上预测出一个值，该值可以是某一类别的得分，也可是相对于Default Bounding Boxes的偏移量，并且在图像中每个位置都将产生一个值。<br>3、默认框和比例（Default Boxes And Aspect Ratio）<br>在特征图的每个位置预测K个Box，对于每个Box，预测C个类别得分，以及相对于Default Bounding Box的4个偏移值，这样需要$(C+4)<em> k$个预测器，在$m </em> n$的特征图上将产生$(C+4)<em> k </em> m * n$个预测值。这里，Default Bounding Box类似于Faster R-CNN中Anchor是，下图所示。<br><img src="/images/SSD2.png" alt="SSD默认框"><br>二、模型训练<br>1、监督学习的训练关键是人工标注的label，对于包含Default Box（在Faster R-CNN中叫做Anchor）的网络模型（如:YOLO,Faster R-CNN,MultiBox）关键点就是如何把标注信息（Ground True Box,Ground True Category）映射到（Default Box）上。<br>2、给定输入图像以及每个物体的Ground Truth，首先找到每个Ground True Box对应的Default Box中IOU最大的作为正样本。然后，在剩下的Default Box中找到那些与任意一个Ground truth Box的IOU大于0.5的Default Box作为正样本。其他的作为负样本（每个Default Box要么是正样本Box要么是负样本Box）。如上图中，两个Default Box与猫匹配，一个与狗匹配。在训练过程中，采用Hard Negative Mining 的策略（根据Confidence Loss对所有的Box进行排序，使正负例的比例保持在1:3） 来平衡正负样本的比率。<br>3、损失函数<br>与Faster-RCNN中的RPN是一样的，不过RPN是预测Box里面有Object或者没有，没有分类，SSD直接用的Softmax分类。Location的损失，还是一样，都是用Predict box和Default Box/Anchor的差 与Ground Truth Box和Default Box/Anchor的差进行对比，求损失。<br><img src="/images/SSDloss.png" alt="SSD loss求解"><br>其中，$x_{i,j}^p=1$表示 第i个Default Box与类别p的第j个Ground Truth Box相匹配，否则若不匹配的话，则$x_{i,j}^p=0$<br>4、Default Box的生成<br>对每一张特征图，按照不同的大小（Scale） 和长宽比（Ratio）生成生成k个默认框（Default Boxes）。<br>（1）Scale：每一个Feature Map中Default Box的尺寸大小计算如下：</p>
<script type="math/tex; mode=display">s_k=s_{min}+\frac{s_{max}-s_{min}}{m-1}(k-1), \quad k\in [1, m]</script><p>其中，$S_{min}$取值0.2，$s_{max}$取值0.95，意味着最低层的尺度是0.2，最高层的尺度是0.95。<br>（2）Ratio：使用不同的Ratio值$a_=1,2,3,\frac{1}{2},\frac{1}{3}$计算Default Box的宽度和高度：$w_k^a=s_k\sqrt{a_r},h_k^a=s_k/\sqrt{a_r}$。另外对于Ratio=1的情况，还增加了一个Default Box，这个Box的Scale为$s’_k=\sqrt{s_ks_k+1}$。也就是总共有6种不同的Default Box。<br>（3）Default Box中心：每个Default Box的中心位置设置成$(\frac{i+0.5}{|f_k|},\frac{j+0.5}{|f_k|})$，其中$|f_k|$表示第k个特征图的大小$i,j\in[0,|f_k|]$。<br>5）Data Augmentation：为了模型更加鲁棒，需要使用不同尺寸的输入和形状，作者对数据进行了多种方式的随机采样。</p>
<h2 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h2><p>（待补充）</p>
<h2 id="目标追踪"><a href="#目标追踪" class="headerlink" title="目标追踪"></a>目标追踪</h2><h2 id="姿态估计"><a href="#姿态估计" class="headerlink" title="姿态估计"></a>姿态估计</h2>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" rel="prev" title="机器学习基础知识">
      <i class="fa fa-chevron-left"></i> 机器学习基础知识
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/21/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" rel="next" title="排序算法">
      排序算法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习"><span class="nav-number">1.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD-Momentum-Adagard-Adam原理"><span class="nav-number">1.1.</span> <span class="nav-text">SGD,Momentum,Adagard,Adam原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-SGD、BGD和Mini-BGD"><span class="nav-number">1.1.1.</span> <span class="nav-text">1. SGD、BGD和Mini-BGD:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Momentum"><span class="nav-number">1.1.2.</span> <span class="nav-text">2. Momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Adagrad"><span class="nav-number">1.1.3.</span> <span class="nav-text">3. Adagrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-RMSprop"><span class="nav-number">1.1.4.</span> <span class="nav-text">4. RMSprop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Adam"><span class="nav-number">1.1.5.</span> <span class="nav-text">5. Adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1、L2范数"><span class="nav-number">1.2.</span> <span class="nav-text">L1、L2范数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#L1范数"><span class="nav-number">1.2.1.</span> <span class="nav-text">L1范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L2范数"><span class="nav-number">1.2.2.</span> <span class="nav-text">L2范数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L1不可导的时候该怎么办"><span class="nav-number">1.2.3.</span> <span class="nav-text">L1不可导的时候该怎么办</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-number">1.3.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sigmoid函数"><span class="nav-number">1.3.1.</span> <span class="nav-text">Sigmoid函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tanh函数"><span class="nav-number">1.3.2.</span> <span class="nav-text">tanh函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Relu函数"><span class="nav-number">1.3.3.</span> <span class="nav-text">Relu函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Leaky-ReLU函数"><span class="nav-number">1.3.4.</span> <span class="nav-text">Leaky ReLU函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ELU函数"><span class="nav-number">1.3.5.</span> <span class="nav-text">ELU函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络权重初始化方式"><span class="nav-number">1.4.</span> <span class="nav-text">神经网络权重初始化方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化为0"><span class="nav-number">1.4.1.</span> <span class="nav-text">初始化为0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机初始化"><span class="nav-number">1.4.2.</span> <span class="nav-text">随机初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Xavier初始化"><span class="nav-number">1.4.3.</span> <span class="nav-text">Xavier初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MSRA-He-initialization"><span class="nav-number">1.4.4.</span> <span class="nav-text">MSRA&#x2F;He initialization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-number">1.5.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数、代价函数与目标函数"><span class="nav-number">1.5.1.</span> <span class="nav-text">损失函数、代价函数与目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的损失函数"><span class="nav-number">1.5.2.</span> <span class="nav-text">常用的损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用的代价函数"><span class="nav-number">1.5.3.</span> <span class="nav-text">常用的代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么分类问题用-cross-entropy，而回归问题用-MSE"><span class="nav-number">1.5.4.</span> <span class="nav-text">为什么分类问题用 cross entropy，而回归问题用 MSE?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Normalization原理"><span class="nav-number">1.6.</span> <span class="nav-text">Batch Normalization原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LRN-局部归一化响应"><span class="nav-number">1.7.</span> <span class="nav-text">LRN 局部归一化响应</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习几种归一化（BN、LN、IN、GN）"><span class="nav-number">1.8.</span> <span class="nav-text">深度学习几种归一化（BN、LN、IN、GN）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络模型"><span class="nav-number">2.</span> <span class="nav-text">卷积神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积神经网络"><span class="nav-number">2.1.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积层"><span class="nav-number">2.1.1.</span> <span class="nav-text">卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#池化层"><span class="nav-number">2.1.2.</span> <span class="nav-text">池化层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像分类"><span class="nav-number">2.2.</span> <span class="nav-text">图像分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LeNet-5"><span class="nav-number">2.2.1.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlexNet"><span class="nav-number">2.2.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGGNet"><span class="nav-number">2.2.3.</span> <span class="nav-text">VGGNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GoogleNet-22层"><span class="nav-number">2.2.4.</span> <span class="nav-text">GoogleNet(22层)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inception-v3-v4"><span class="nav-number">2.2.5.</span> <span class="nav-text">Inception v3&#x2F;v4</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet"><span class="nav-number">2.2.6.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DenseNet"><span class="nav-number">2.2.7.</span> <span class="nav-text">DenseNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像检测"><span class="nav-number">2.3.</span> <span class="nav-text">图像检测</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#IOU的定义"><span class="nav-number">2.3.1.</span> <span class="nav-text">IOU的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非极大值抑制"><span class="nav-number">2.3.2.</span> <span class="nav-text">非极大值抑制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#R-CNN"><span class="nav-number">2.3.3.</span> <span class="nav-text">R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SPP-Net"><span class="nav-number">2.3.4.</span> <span class="nav-text">SPP-Net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fast-R-CNN"><span class="nav-number">2.3.5.</span> <span class="nav-text">Fast R-CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-number">2.3.6.</span> <span class="nav-text">Faster R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Region-Proposal-Network"><span class="nav-number">2.3.6.1.</span> <span class="nav-text">Region Proposal Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RPN的损失函数"><span class="nav-number">2.3.6.2.</span> <span class="nav-text">RPN的损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网络的训练"><span class="nav-number">2.3.6.3.</span> <span class="nav-text">网络的训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mask-R-CNN"><span class="nav-number">2.3.6.4.</span> <span class="nav-text">Mask R-CNN</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO"><span class="nav-number">2.4.</span> <span class="nav-text">YOLO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SSD"><span class="nav-number">2.4.1.</span> <span class="nav-text">SSD</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像分割"><span class="nav-number">2.5.</span> <span class="nav-text">图像分割</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标追踪"><span class="nav-number">2.6.</span> <span class="nav-text">目标追踪</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#姿态估计"><span class="nav-number">2.7.</span> <span class="nav-text">姿态估计</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Kay</p>
  <div class="site-description" itemprop="description">千里之行，始于足下</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kay</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
