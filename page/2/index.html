<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bassyess.github.io","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="千里之行，始于足下">
<meta property="og:type" content="website">
<meta property="og:title" content="Home">
<meta property="og:url" content="https://bassyess.github.io/page/2/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="千里之行，始于足下">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Kay">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://bassyess.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Home</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%85%AB%E7%AB%A0_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%85%AB%E7%AB%A0_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" class="post-title-link" itemprop="url">第八章_目标检测</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:27:39 / 修改时间：12:28:02" itemprop="dateCreated datePublished" datetime="2020-03-03T12:27:39+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第八章-目标检测"><a href="#第八章-目标检测" class="headerlink" title="第八章  目标检测"></a>第八章  目标检测</h1><h2 id="8-1-基本概念"><a href="#8-1-基本概念" class="headerlink" title="8.1 基本概念"></a>8.1 基本概念</h2><h3 id="8-1-1-什么是目标检测？"><a href="#8-1-1-什么是目标检测？" class="headerlink" title="8.1.1 什么是目标检测？"></a>8.1.1 什么是目标检测？</h3><p>​    目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。</p>
<p>​    计算机视觉中关于图像识别有四大类任务：</p>
<p><strong>分类-Classification</strong>：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。</p>
<p><strong>定位-Location</strong>：解决“在哪里？”的问题，即定位出这个目标的的位置。</p>
<p><strong>检测-Detection</strong>：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。</p>
<p><strong>分割-Segmentation</strong>：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。</p>
<p><img src="/img/ch8/8.1.1.png" alt="图像识别四大类任务，图像来源于cs231n 2016课件Lecture 8"></p>
<h3 id="8-1-2-目标检测要解决的核心问题？"><a href="#8-1-2-目标检测要解决的核心问题？" class="headerlink" title="8.1.2 目标检测要解决的核心问题？"></a>8.1.2 目标检测要解决的核心问题？</h3><p>除了图像分类之外，目标检测要解决的核心问题是：</p>
<p>1.目标可能出现在图像的任何位置。</p>
<p>2.目标有各种不同的大小。</p>
<p>3.目标可能有各种不同的形状。</p>
<h3 id="8-1-3-目标检测算法分类？"><a href="#8-1-3-目标检测算法分类？" class="headerlink" title="8.1.3 目标检测算法分类？"></a>8.1.3 目标检测算法分类？</h3><p>基于深度学习的目标检测算法主要分为两类：</p>
<p><strong>1.Two stage目标检测算法</strong></p>
<p>​    先进行区域生成（region proposal，RP）（一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。</p>
<p>​    任务：特征提取—&gt;生成RP—&gt;分类/定位回归。</p>
<p>​    常见的two stage目标检测算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN和R-FCN等。</p>
<p><strong>2.One stage目标检测算法</strong></p>
<p>​    不用RP，直接在网络中提取特征来预测物体分类和位置。</p>
<p>​    任务：特征提取—&gt;分类/定位回归。</p>
<p>​    常见的one stage目标检测算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD和RetinaNet等。</p>
<p><img src="/img/ch8/8.1.2.png" alt=""></p>
<h3 id="8-1-4-目标检测有哪些应用？"><a href="#8-1-4-目标检测有哪些应用？" class="headerlink" title="8.1.4 目标检测有哪些应用？"></a>8.1.4 目标检测有哪些应用？</h3><p>​    目标检测具有巨大的实用价值和应用前景。应用领域包括人脸检测、行人检测、车辆检测、飞机航拍或卫星图像中道路的检测、车载摄像机图像中的障碍物检测、医学影像在的病灶检测等。还有在安防领域中，可以实现比如安全帽、安全带等动态检测，移动侦测、区域入侵检测、物品看护等功能。</p>
<h2 id="8-2-Two-Stage目标检测算法"><a href="#8-2-Two-Stage目标检测算法" class="headerlink" title="8.2 Two Stage目标检测算法"></a>8.2 Two Stage目标检测算法</h2><h3 id="8-2-1-R-CNN"><a href="#8-2-1-R-CNN" class="headerlink" title="8.2.1 R-CNN"></a>8.2.1 R-CNN</h3><p><strong>R-CNN有哪些创新点？</strong></p>
<ol>
<li>使用CNN（ConvNet）对 region proposals 计算 feature vectors。从经验驱动特征（SIFT、HOG）到数据驱动特征（CNN feature map），提高特征对样本的表示能力。</li>
<li>采用大样本下（ILSVRC）有监督预训练和小样本（PASCAL）微调（fine-tuning）的方法解决小样本难以训练甚至过拟合等问题。</li>
</ol>
<p>注：ILSVRC其实就是众所周知的ImageNet的挑战赛，数据量极大；PASCAL数据集（包含目标检测和图像分割等），相对较小。</p>
<p><strong>R-CNN 介绍</strong></p>
<p>​    R-CNN作为R-CNN系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。比如R-CNN pipeline中的第二步和第四步其实就属于传统的“计算机视觉”技术。使用selective search提取region proposals，使用SVM实现分类。</p>
<p><img src="/img/ch8/8.2.1-1.png" alt=""></p>
<p>原论文中R-CNN pipeline只有4个步骤，光看上图无法深刻理解R-CNN处理机制，下面结合图示补充相应文字</p>
<ol>
<li><p>预训练模型。选择一个预训练 （pre-trained）神经网络（如AlexNet、VGG）。</p>
</li>
<li><p>重新训练全连接层。使用需要检测的目标重新训练（re-train）最后全连接层（connected layer）。</p>
</li>
<li><p>提取 proposals并计算CNN 特征。利用选择性搜索（Selective Search）算法提取所有proposals（大约2000幅images），调整（resize/warp）它们成固定大小，以满足 CNN输入要求（因为全连接层的限制），然后将feature map 保存到本地磁盘。</p>
<p><img src="/img/ch8/8.1.4.png" alt=""></p>
</li>
<li><p>训练SVM。利用feature map 训练SVM来对目标和背景进行分类（每个类一个二进制SVM）</p>
</li>
<li><p>边界框回归（Bounding boxes Regression）。训练将输出一些校正因子的线性回归分类器</p>
</li>
</ol>
<p><img src="/img/ch8/8.1.5.png" alt=""></p>
<p><strong>R-CNN 实验结果</strong></p>
<p>R-CNN在VOC 2007测试集上mAP达到58.5%，打败当时所有的目标检测算法。</p>
<p><img src="/img/ch8/8.1.6.png" alt=""></p>
<h3 id="8-2-2-Fast-R-CNN"><a href="#8-2-2-Fast-R-CNN" class="headerlink" title="8.2.2 Fast R-CNN"></a>8.2.2 Fast R-CNN</h3><p><strong>Fast R-CNN有哪些创新点？</strong></p>
<ol>
<li>只对整幅图像进行一次特征提取，避免R-CNN中的冗余特征提取</li>
<li>用RoI pooling层替换最后一层的max pooling层，同时引入建议框数据，提取相应建议框特征</li>
<li>Fast R-CNN网络末尾采用并行的不同的全连接层，可同时输出分类结果和窗口回归结果，实现了end-to-end的多任务训练【建议框提取除外】，也不需要额外的特征存储空间【R-CNN中的特征需要保持到本地，来供SVM和Bounding-box regression进行训练】</li>
<li>采用SVD对Fast R-CNN网络末尾并行的全连接层进行分解，减少计算复杂度，加快检测速度。</li>
</ol>
<p><strong>Fast R-CNN 介绍</strong></p>
<p>​    Fast R-CNN是基于R-CNN和SPPnets进行的改进。SPPnets，其创新点在于计算整幅图像的the shared feature map，然后根据object proposal在shared feature map上映射到对应的feature vector（就是不用重复计算feature map了）。当然，SPPnets也有缺点：和R-CNN一样，训练是多阶段（multiple-stage pipeline）的，速度还是不够”快”，特征还要保存到本地磁盘中。</p>
<p>将候选区域直接应用于特征图，并使用RoI池化将其转化为固定大小的特征图块。以下是Fast R-CNN的流程图</p>
<p><img src="/img/ch8/8.2.2-1.png" alt=""></p>
<p><strong>RoI Pooling层详解</strong></p>
<p>因为Fast R-CNN使用全连接层，所以应用RoI Pooling将不同大小的ROI转换为固定大小。</p>
<p>RoI Pooling 是Pooling层的一种，而且是针对RoI的Pooling，其特点是输入特征图尺寸不固定，但是输出特征图尺寸固定（如7x7）。</p>
<p><strong>什么是RoI呢？</strong></p>
<p>RoI是Region of Interest的简写，一般是指图像上的区域框，但这里指的是由Selective Search提取的候选框。</p>
<p><img src="/img/ch8/8.2.2-2.png" alt=""></p>
<p>往往经过RPN后输出的不止一个矩形框，所以这里我们是对多个RoI进行Pooling。</p>
<p><strong>RoI Pooling的输入</strong></p>
<p>输入有两部分组成： </p>
<ol>
<li>特征图（feature map）：指的是上面所示的特征图，在Fast RCNN中，它位于RoI Pooling之前，在Faster RCNN中，它是与RPN共享那个特征图，通常我们常常称之为“share_conv”； </li>
<li>RoIs，其表示所有RoI的N*5的矩阵。其中N表示RoI的数量，第一列表示图像index，其余四列表示其余的左上角和右下角坐标。</li>
</ol>
<p>在Fast RCNN中，指的是Selective Search的输出；在Faster RCNN中指的是RPN的输出，一堆矩形候选框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）。其实关于ROI的坐标理解一直很混乱，到底是根据谁的坐标来。其实很好理解，我们已知原图的大小和由Selective Search算法提取的候选框坐标，那么根据”映射关系”可以得出特征图（featurwe map）的大小和候选框在feature map上的映射坐标。至于如何计算，其实就是比值问题，下面会介绍。所以这里把ROI理解为原图上各个候选框（region proposals），也是可以的。</p>
<p>注：说句题外话，由Selective Search算法提取的一系列可能含有object的bounding box，这些通常称为region proposals或者region of interest（ROI）。</p>
<p><strong>RoI的具体操作</strong></p>
<ol>
<li><p>根据输入image，将ROI映射到feature map对应位置</p>
<p>注：映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标</p>
</li>
<li><p>将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）</p>
</li>
<li><p>对每个sections进行max pooling操作</p>
</li>
</ol>
<p>这样我们就可以从不同大小的方框得到固定大小的相应 的feature maps。值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。RoI Pooling 最大的好处就在于极大地提高了处理速度。</p>
<p><strong>RoI Pooling的输出</strong></p>
<p>输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel <em> w </em> h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框。</p>
<p><strong>RoI Pooling示例</strong></p>
<p><img src="/img/ch8/8.1.11.gif" alt=""></p>
<h3 id="8-2-3-Faster-R-CNN"><a href="#8-2-3-Faster-R-CNN" class="headerlink" title="8.2.3 Faster R-CNN"></a>8.2.3 Faster R-CNN</h3><p><strong>Faster R-CNN有哪些创新点？</strong></p>
<p>Fast R-CNN依赖于外部候选区域方法，如选择性搜索。但这些算法在CPU上运行且速度很慢。在测试中，Fast R-CNN需要2.3秒来进行预测，其中2秒用于生成2000个ROI。Faster R-CNN采用与Fast R-CNN相同的设计，只是它用内部深层网络代替了候选区域方法。新的候选区域网络（RPN）在生成ROI时效率更高，并且以每幅图像10毫秒的速度运行。<br><img src="/img/ch8/8.2.3-1.png" alt=""> </p>
<p>图8.1.13 Faster R-CNN的流程图<br>Faster R-CNN的流程图与Fast R-CNN相同，采用外部候选区域方法代替了内部深层网络。<br><img src="/img/ch8/8.2.3-2.png" alt=""> </p>
<p>图8.1.14<br><strong>候选区域网络</strong></p>
<p>候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个3×3的卷积核，以使用卷积网络（如下所示的ZF网络）构建与类别无关的候选区域。其他深度网络（如VGG或ResNet）可用于更全面的特征提取，但这需要以速度为代价。ZF网络最后会输出256个值，它们将馈送到两个独立的全连接层，以预测边界框和两个objectness分数，这两个objectness分数度量了边界框是否包含目标。我们其实可以使用回归器计算单个objectness分数，但为简洁起见，Faster R-CNN使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。<br><img src="/img/ch8/8.2.3-3.png" alt=""> </p>
<p>图8.1.15<br>对于特征图中的每一个位置，RPN会做k次预测。因此，RPN将输出4×k个坐标和每个位置上2×k个得分。下图展示了8×8的特征图，且有一个3×3的卷积核执行运算，它最后输出8×8×3个ROI（其中k=3）。下图（右）展示了单个位置的3个候选区域。<br><img src="/img/ch8/8.2.3-4.png" alt=""></p>
<p>图8.1.16<br>假设最好涵盖不同的形状和大小。因此，Faster R-CNN不会创建随机边界框。相反，它会预测一些与左上角名为锚点的参考框相关的偏移量（如x, y）。我们限制这些偏移量的值，因此我们的猜想仍然类似于锚点。<br><img src="/img/ch8/8.1.17.png" alt=""> </p>
<p>图8.1.17<br>要对每个位置进行k个预测，我们需要以每个位置为中心的k个锚点。每个预测与特定锚点相关联，但不同位置共享相同形状的锚点。<br><img src="/img/ch8/8.2.3-6.png" alt=""> </p>
<p>图8.1.18<br>这些锚点是精心挑选的，因此它们是多样的，且覆盖具有不同比例和宽高比的现实目标。这使得我们可以用更好的猜想来指导初始训练，并允许每个预测专门用于特定的形状。该策略使早期训练更加稳定和简便。<br><img src="/img/ch8/8.2.3-7.png" alt=""></p>
<p>图8.1.19<br>Faster R-CNN使用更多的锚点。它部署9个锚点框：3个不同宽高比的3个不同大小的锚点（Anchor）框。每一个位置使用9个锚点，每个位置会生成2×9个objectness分数和4×9个坐标。</p>
<h3 id="8-2-4-R-FCN"><a href="#8-2-4-R-FCN" class="headerlink" title="8.2.4 R-FCN"></a>8.2.4 R-FCN</h3><p><strong>R-FCN有哪些创新点？</strong></p>
<p>R-FCN 仍属于two-stage 目标检测算法：RPN+R-FCN</p>
<ol>
<li>Fully convolutional</li>
<li>位置敏感得分图（position-sentive score maps）</li>
</ol>
<blockquote>
<p>our region-based detector is <strong>fully convolutional</strong> with almost all computation shared on the entire image. To achieve this goal, we propose <strong>position-sensitive score maps</strong> to address a dilemma between translation-invariance in image classification and translation-variance in object detection.</p>
</blockquote>
<p>R-FCN backbone：ResNet</p>
<p>ResNet-101+R-FCN：83.6% in PASCAL VOC 2007 test datasets</p>
<p>既提高了mAP，又加快了检测速度</p>
<pre><code>假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。现在我们回顾一下所有问题。在Faster R-CNN中，检测器使用了多个全连接层进行预测。如果有2000个ROI，那么成本非常高。R-FCN通过减少每个ROI所需的工作量实现加速。上面基于区域的特征图与ROI是独立的，可以在每个ROI之外单独计算。剩下的工作就比较简单了，因此R-FCN的速度比Faster R-CNN快。
</code></pre><p><img src="/img/ch8/8.2.4-1.png" alt=""></p>
<pre><code>图8.2.1 人脸检测
现在我们来看一下5×5的特征图M，内部包含一个蓝色方块。我们将方块平均分成3×3个区域。现在，我们在M中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元[2,2]处于激活状态。在左侧创建一个新的特征图，用于检测目标的左上角。
</code></pre><p><img src="/img/ch8/8.2.4-2.png" alt=""> </p>
<pre><code>图8.2.2 检测示例
我们将方块分成9个部分，由此创建了9个特征图，每个用来检测对应的目标区域。这些特征图叫做位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。
</code></pre><p><img src="/img/ch8/8.2.4-3.png" alt=""></p>
<pre><code>图8.2.3生成9个得分图
下图中红色虚线矩形是建议的ROI。我们将其分割成3×3个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角ROI区域包含左眼的概率。我们将结果存储成3×3 vote数组，如下图（右）所示。例如，vote_array[0][0]包含左上角区域是否包含目标对应部分的得分。
</code></pre><p><img src="/img/ch8/8.2.4-4.png" alt=""> </p>
<pre><code>图8.2.4
将ROI应用到特征图上，输出一个3x3数组。将得分图和ROI映射到vote数组的过程叫做位置敏感ROI池化（position-sensitive ROI-pool）。该过程与前面讨论过的ROI池化非常接近。
</code></pre><p><img src="/img/ch8/8.2.4-5.png" alt=""> </p>
<pre><code>图8.2.5
将ROI的一部分叠加到对应的得分图上，计算V[i][j]。在计算出位置敏感ROI池化的所有值后，类别得分是其所有元素得分的平均值。
</code></pre><p><img src="/img/ch8/8.2.6.png" alt=""></p>
<pre><code>图8.2.6 ROI池化
假如我们有C个类别要检测。我们将其扩展为C+1个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有3×3个得分图，因此一共有(C+1)×3×3个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。以下是数据流图，在本案例中，k=3。
</code></pre><p><img src="/img/ch8/8.2.7.png" alt=""> </p>
<pre><code>图8.2.7
</code></pre><h3 id="8-2-5-FPN"><a href="#8-2-5-FPN" class="headerlink" title="8.2.5 FPN"></a>8.2.5 FPN</h3><p><strong>FPN有哪些创新点？</strong></p>
<ol>
<li>多层特征</li>
<li>特征融合</li>
</ol>
<p>解决目标检测中的多尺度问题，通过简单的网络连接改变，在基本不增加原有模型计算量的情况下，大幅度提升小物体（small object）检测的性能。</p>
<p>在物体检测里面，有限计算量情况下，网络的深度（对应到感受野）与 stride 通常是一对矛盾的东西，常用的网络结构对应的 stride 一般会比较大（如 32），而图像中的小物体甚至会小于 stride 的大小，造成的结果就是小物体的检测性能急剧下降。传统解决这个问题的思路包括：</p>
<ol>
<li>图像金字塔（image pyramid），即多尺度训练和测试。但该方法计算量大，耗时较久。</li>
<li>特征分层，即每层分别预测对应的scale分辨率的检测结果，如SSD算法。该方法强行让不同层学习同样的语义信息，但实际上不同深度对应于不同层次的语义特征，浅层网络分辨率高，学到更多是细节特征，深层网络分辨率低，学到更多是语义特征。</li>
</ol>
<p>因而，目前多尺度的物体检测主要面临的挑战为：</p>
<ol>
<li>如何学习具有强语义信息的多尺度特征表示？</li>
<li>如何设计通用的特征表示来解决物体检测中的多个子问题？如 object proposal, box localization, instance segmentation.</li>
<li>如何高效计算多尺度的特征表示？</li>
</ol>
<p>FPN网络直接在Faster R-CNN单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义（rich semantic）特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。作者接下来实验了将 FPN 应用在 Faster RCNN 上的性能，在 COCO 上达到了 state-of-the-art 的单模型精度。在RPN上，FPN增加了8.0个点的平均召回率（average recall，AR）；在后面目标检测上，对于COCO数据集，FPN增加了2.3个点的平均精确率（average precision，AP），对于VOC数据集，FPN增加了3.8个点的AP。</p>
<p><img src="img/ch8/FPN-01.png" alt=""></p>
<p>FPN算法主要由三个模块组成，分别是：</p>
<ol>
<li>Bottom-up pathway（自底向上线路）</li>
<li>Lareral connections（横向链接）</li>
<li>Top-down path（自顶向下线路）</li>
</ol>
<p><img src="img/ch8/FPN-02.png" alt=""></p>
<p><strong>Bottom-up pathway</strong></p>
<p>FPN是基于Faster R-CNN进行改进，其backbone是ResNet-101，FPN主要应用在Faster R-CNN中的RPN（用于bouding box proposal generation）和Fast R-CNN（用于object detection）两个模块中。</p>
<p>其中 RPN 和 Fast RCNN 分别关注的是召回率（recall）和精确率（precision），在这里对比的指标分别为 Average Recall(AR) 和 Average Precision(AP)。</p>
<p>注：Bottom-up可以理解为自底向上，Top-down可以理解为自顶向下。这里的下是指low-level，上是指high-level，分别对应于提取的低级（浅层）特征和高级语义（高层）特征。</p>
<p>Bottom-up pathway 是卷积网络的前向传播过程。在前向传播过程中，feature map的大小可以在某些层发生改变。一些尺度（scale）因子为2，所以后一层feature map的大小是前一层feature map大小的二分之一，根据此关系进而构成了feature pyramid（hierarchy）。</p>
<p>然而还有很多层输出的feature map是一样的大小（即不进行缩放的卷积），作者将这些层归为同一 stage。对于feature pyramid，作者为每个stage定义一个pyramid level。</p>
<p>作者将每个stage的最后一层的输出作为feature map，然后不同stage进行同一操作，便构成了feature pyramid。</p>
<p>具体来说，对于ResNets-101，作者使用了每个stage的最后一个残差结构的特征激活输出。将这些残差模块输出表示为{C2, C3, C4, C5}，对应于conv2，conv3，conv4和conv5的输出，并且注意它们相对于输入图像具有{4, 8, 16, 32}像素的步长。考虑到内存占用，没有将conv1包含在金字塔中。</p>
<p><img src="img/ch8/FPN-03.png" alt=""></p>
<p><strong>Top-down pathway and lateral connections</strong></p>
<p>Top-town pathway是上采样（upsampling）过程。而later connection（横向连接）是将上采样的结果和bottom-up pathway生成的相同大小的feature map进行融合（merge）。</p>
<p>注：上采样尺度因子为2，因为为了和之前下采样卷积的尺度因子=2一样。上采样是放大，下采样是缩小。</p>
<p>具体操作如下图所示，上采样（2x up）feature map与相同大小的bottom-up feature map进行逐像素相加融合（element-wise addition），其中bottom-up feature先要经过1x1卷积层，目的是为了减少通道维度（reduce channel dimensions）。</p>
<p>注：减少通道维度是为了将bottom-up feature map的通道数量与top-down feature map的通道数量保持一致，又因为两者feature map大小一致，所以可以进行对应位置像素的叠加（element-wise addition）。</p>
<p><img src="img/ch8/FPN-04.png" alt=""></p>
<h3 id="8-2-6-Mask-R-CNN"><a href="#8-2-6-Mask-R-CNN" class="headerlink" title="8.2.6 Mask R-CNN"></a>8.2.6 Mask R-CNN</h3><p><strong>Mask R-CNN有哪些创新点？</strong></p>
<ol>
<li>Backbone：ResNeXt-101+FPN</li>
<li>RoI Align替换RoI Pooling</li>
</ol>
<p>Mask R-CNN是一个实例分割（Instance segmentation）算法，主要是在目标检测的基础上再进行分割。Mask R-CNN算法主要是Faster R-CNN+FCN，更具体一点就是ResNeXt+RPN+RoI Align+Fast R-CNN+FCN。</p>
<p><img src="img/ch8/Mask R-CNN-01.png" alt=""></p>
<p><strong>Mask R-CNN算法步骤</strong></p>
<ol>
<li>输入一幅你想处理的图片，然后进行对应的预处理操作，或者预处理后的图片；</li>
<li>将其输入到一个预训练好的神经网络中（ResNeXt等）获得对应的feature map；</li>
<li>对这个feature map中的每一点设定预定个的RoI，从而获得多个候选RoI；</li>
<li>将这些候选的RoI送入RPN网络进行二值分类（前景或背景）和BB回归，过滤掉一部分候选的RoI；</li>
<li>对这些剩下的RoI进行RoI Align操作（即先将原图和feature map的pixel对应起来，然后将feature map和固定的feature对应起来）；</li>
<li>对这些RoI进行分类（N类别分类）、BB回归和MASK生成（在每一个RoI里面进行FCN操作）。</li>
</ol>
<p><strong>RoI Pooling和RoI Align有哪些不同？</strong></p>
<p>ROI Align 是在Mask-RCNN中提出的一种区域特征聚集方式，很好地解决了RoI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。实验显示，在检测测任务中将 RoI Pooling 替换为 RoI Align 可以提升检测模型的准确性。</p>
<p>在常见的两级检测框架（比如Fast-RCNN，Faster-RCNN，RFCN）中，RoI Pooling 的作用是根据预选框的位置坐标在特征图中将相应区域池化为固定尺寸的特征图，以便进行后续的分类和包围框回归操作。由于预选框的位置通常是由模型回归得到的，一般来讲是浮点数，而池化后的特征图要求尺寸固定。故RoI Pooling这一操作存在两次量化的过程。</p>
<ul>
<li>将候选框边界量化为整数点坐标值。</li>
<li>将量化后的边界区域平均分割成 $k\times k$ 个单元(bin),对每一个单元的边界进行量化。</li>
</ul>
<p>事实上，经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度。在论文里，作者把它总结为“不匹配问题（misalignment）”。</p>
<p>下面我们用直观的例子具体分析一下上述区域不匹配问题。如下图所示，这是一个Faster-RCNN检测框架。输入一张$800\times 800$的图片，图片上有一个$665\times 665$的包围框（框着一只狗）。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是RoI Pooling 直接将它量化成20。接下来需要把框内的特征池化$7\times 7$的大小，因此将上述包围框平均分割成$7\times 7$个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。</p>
<p><img src="img/ch8/Mask R-CNN-02.png" alt=""></p>
<p>为了解决RoI Pooling的上述缺点，作者提出了RoI Align这一改进的方法(如图2)。</p>
<p><img src="img/ch8/Mask R-CNN-03.png" alt=""></p>
<p>RoI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值，从而将整个特征聚集过程转化为一个连续的操作。值得注意的是，在具体的算法操作上，RoI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如下图所示：</p>
<ol>
<li><p>遍历每一个候选区域，保持浮点数边界不做量化。</p>
</li>
<li><p>将候选区域分割成$k\times k$个单元，每个单元的边界也不做量化。</p>
</li>
<li><p>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</p>
</li>
</ol>
<p>这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。在相关实验中，作者发现将采样点设为4会获得最佳性能，甚至直接设为1在性能上也相差无几。事实上，RoI Align 在遍历取样点的数量上没有RoI Pooling那么多，但却可以获得更好的性能，这主要归功于解决了mis alignment的问题。值得一提的是，我在实验时发现，RoI Align在VOC 2007数据集上的提升效果并不如在COCO上明显。经过分析，造成这种区别的原因是COCO上小目标的数量更多，而小目标受mis alignment问题的影响更大（比如，同样是0.5个像素点的偏差，对于较大的目标而言显得微不足道，但是对于小目标，误差的影响就要高很多）。</p>
<p><img src="img/ch8/Mask R-CNN-04.png" alt=""></p>
<h3 id="8-2-7-DetNet（贡献者：北京理工大学—明奇）"><a href="#8-2-7-DetNet（贡献者：北京理工大学—明奇）" class="headerlink" title="8.2.7  DetNet（贡献者：北京理工大学—明奇）"></a>8.2.7  DetNet（贡献者：北京理工大学—明奇）</h3><p>DetNet是发表在ECCV2018的论文，比较新，出发点是现有的检测任务backbone都是从分类任务衍生而来的，因此作者想针对检测专用的backbone做一些讨论和研究而设计了DetNet，思路比较新奇。</p>
<ol>
<li><strong>Introduction</strong><br>&emsp;&emsp;很多backbone的提出都是用于挑战ImageNet分类任务后被应用到检测上来，而鲜有单独<u>针对检测任务设计的backbone</u>。          </li>
</ol>
<p>&emsp;&emsp;<strong>检测和分类有明显的区别</strong>：（1）不仅需要分类，还需要精确的定位 （2）最近的检测器都是基于类似FPN结构，在分类网络基础上加额外多尺度特征进行检测，应对不同尺度变化的目标。这两点又是相互补充，共同协助网络完成分类到检测任务的转变。例如分类任务是检测的一环所以必不可少，但是传统分类采用的最高级特征定位细节不够，因此很多最近网络设法用类似FPN的结构去处理尺度变化的问题，就将分类较好地过渡到检测任务上了。</p>
<ol>
<li><p><strong>DetNet</strong>  </p>
<p>2.1 <strong>Motivation</strong><br>&emsp;&emsp;主要着眼点是<strong>分辨率</strong>，从大目标和小目标分别阐述保持分辨率的重要性。所以DetNet也是从分辨率的保持着手，解决多尺度物体的识别问题。</p>
</li>
</ol>
<ul>
<li><p>Weak visibility of large objects<br>&emsp;&emsp;网络在较深层如P6（FPN）P7（RetinaNet）大目标的边界不明确使精确定位困难。</p>
</li>
<li><p>Invisibility of small objects<br>&emsp;&emsp;小目标就很惨了，降采样容易丢。这个就不赘述了，所以只要避开降采样就能防止目标丢失，但是这种方法又会导致抽象能力不够 </p>
</li>
</ul>
<p>​    2.2  <strong>DetNet Design</strong><br>&emsp;&emsp;保持分辨率有两个麻烦的问题：（1）内存消耗大，计算大 （2）降采样减少导致高层的抽象特征不足以很好地进行分类任务。下面设计时会同时考虑时间和高层抽象信息两点。<br>&emsp;&emsp;先放出DetNet的多尺度各stage的尺寸如下图， 可以看到，相比前两种方式，DetNet在P4之后就不再进一步降采样了，进行分辨率的保持。</p>
<p><img src="img/ch8/DetNet-1.png" alt=""></p>
<p>&emsp;&emsp;实现细节如下图：</p>
<p><img src="img/ch8/DetNet-2.png" alt=""></p>
<ul>
<li>采用的backbone是ResNet-50，改进设计了DetNet-59。</li>
<li>对bottlenecks进行了改进，传统的其实不止C，也包含两种，即将AB的膨胀卷积换成普通卷积。AB是新的基础模块。</li>
<li>为了减少分辨率保持带来的时间和内存成本消耗，通道数固定为256（思考：降采样和膨胀卷积都会有信息丢失，这里可以想想）。</li>
<li>DetNet也可以加FPN结构，方法类似。</li>
</ul>
<ol>
<li><p><strong>Experiments</strong><br> &emsp;&emsp;检测和训练的细节配置就不看了。</p>
<p>3.1 <strong>Main Results</strong></p>
</li>
</ol>
<p><img src="img/ch8/DetNet-3.png" alt=""></p>
<ul>
<li>在FPN基础上明显有大物体涨点，同时由于高分辨率，小物体也有不错的提升。</li>
<li>膨胀卷积提供的大感受野使得分类也不逊色<br><img src="img/ch8/DetNet-4.png" alt=""></li>
</ul>
<p>​    3.2  <strong>Results analysis</strong><br><img src="img/ch8/DetNet-5.png" alt=""></p>
<ul>
<li>从AP50看出，高好1.7；从AP80看出，高了3.7。由此可以看出确实提高了检测性能。（</li>
<li><p>从定位性能来看，大物体的提升比小物体更多。作者认为是高分辨率解决了大物体边界模糊的问题。其实有一种解释：小目标没有大目标明显，因为膨胀卷积核降采样都会丢失小目标，只是膨胀卷积可能离散采样不至于像降采样直接给到后面没了，但是没有根本性的解决，所以小目标不大。<br><img src="img/ch8/DetNet-6.png" alt=""></p>
</li>
<li><p>AR指标也有类似结论</p>
</li>
<li>AR50体现了小目标的查全率更好，这也印证上面分析的：相对降采样，膨胀卷积丢失会好点。此下大目标效果虽然提升不大但是也很高了，作者表示DetNet擅长找到更精确的定位目标，在AR85的高指标就能看出。</li>
<li>AR85看大目标丢失少，说明能够像 VGG一样对大目标效果优良。关于小目标的效果平平，作者认为没有必要太高，因为FPN结构对小目标已经利用地很充分了，这里即使不高也没事。</li>
</ul>
<p>3.3 <strong>Discussion</strong></p>
<ul>
<li>关于stage<br>&emsp;&emsp;为了研究backbone对检测的影响，首先研究stage的作用。前4个还好说，和ResNet一样，但是P5 P6就不同，没有尺度的变化，和传统意义的stage不一样了，需要重新定义。这里DetNet也是类似ResNet的方法，虽然没有尺度变化，但是AB模块的位置还是保持了，B开启一个stage（<del>听上去有点牵强</del>）。如下图，认为新加的仍属于P5。<br><img src="img/ch8/DetNet-7.png" alt=""></li>
</ul>
<p>&emsp;&emsp;验证方法是做了实验，将P6开始的block换成上图所示的A模块对比效果如下图。 发现还是加了B效果更好。（但是这个stage和传统意义很不一样，所以很多性质不能相提并论，只是B模块的改变也不好判定什么）<br><img src="img/ch8/DetNet-8.png" alt=""></p>
<h3 id="8-2-8-CBNet"><a href="#8-2-8-CBNet" class="headerlink" title="8.2.8  CBNet"></a>8.2.8  CBNet</h3><p>本部分介绍一篇在COCO数据集达到最高单模型性能——mAP 53.3的网络，论文于2019.9.3发布在ArXiv，全名是<em>CBNet: A Novel Composite Backbone Network Architecture for Object Detection</em></p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>&emsp;&emsp;名义上是单模型，实际是多模型的特征融合，只是和真正的多模型策略略有不同。作者的起点是，设计新的模型往往需要在ImageNet上进行预训练，比较麻烦。因而提出的Composite Backbone Network (CBNet)，采用经典网络的多重组合的方式构建网络，一方面可以提取到更有效的特征，另一方面也能够直接用现成的预训练参数（如ResNet，ResNeXt等）比较简单高效。</p>
<ol>
<li><strong>Proposed method</strong><br><img src="img/ch8/CBNet-1.png" alt=""><br>2.1  <strong>Architecture of CBNet</strong><br><img src="img/ch8/CBNet-2.png" alt=""></li>
</ol>
<p>&emsp;&emsp;如上图，模型中采用K个（K&gt;1）相同的结构进行紧密联结。其中两个相同backbone的叫Dual-Backbone (DB)，三个叫Triple- Backbone (TB)；L代表backbone的stage数目，这里统一设置为L=5。其中，和前任工作不同的地方在于，这里将不同的stage信息进行复用回传，以便获取更好的特征（为什么work不好说）。        </p>
<p>2.2  <strong>Other possible composite styles</strong><br><img src="img/ch8/CBNet-3.png" alt=""></p>
<p>&emsp;&emsp;相关工作的其他类似结构，大同小异。要么是前面backbone的stage往后传播，要么是往前一个传播，每个都有一篇论文，应该都会给出不同的解释；第四个结构不太一样，是类似densnet的结构，但是密集连接+多backbone assemble的内存消耗不出意外会非常大。但是脱离这些体系来看，多backbone的结构类似多模型的assemble，和单模型有点不公平。</p>
<ol>
<li><strong>Experiment</strong></li>
</ol>
<ul>
<li><strong>result</strong><br><img src="img/ch8/CBNet-4.png" alt=""></li>
</ul>
<p>COCO数据集上的结果。看来提升还是有的。但是也能看出，大趋势上，三阶级联效果不如两阶的提升大，也是这部分的特征提升空间有限的缘故，到底哪部分在work不好说。下图的研究就更说明这一点了，斜率逐渐减小。</p>
<ul>
<li><strong>Comparisons of different composite styles</strong><br><img src="img/ch8/CBNet-5.png" alt=""></li>
</ul>
<p>他的级联网络相比，作者的阐述点只落脚于特征的利用情况，但是这个东西本身就很玄乎，不好说到底怎么算利用得好。硬要说这种做法的解释性，大概就是将backbone方向的后面高级语义特征传播回前面进行加强，相当于横向的FPN传播。</p>
<ul>
<li><strong>Number of backbones in CBNet</strong><br><img src="img/ch8/CBNet-6.png" alt=""></li>
</ul>
<p>速度慢是必然的，FPN+ResNeXt为8fps，加上两个backboen后为5.5FPS；如果减去backbone的前两个stage，可以节省部分参数达到6.9FPS，而精度下降不大（整体速度太低，这个实验意义不大）</p>
<ul>
<li><strong>Sharing weights for CBNet</strong><br><img src="img/ch8/CBNet-7.png" alt=""></li>
<li>从中可以看出其实权重是否share区别不大， 不到一个点的降幅，参数量减少。</li>
</ul>
<ul>
<li><strong>Effectiveness of basic feature enhancement by CBNet</strong><br><img src="img/ch8/CBNet-8.png" alt=""></li>
</ul>
<p>从中可以看出激活响应效果更好，确实是能够提取到更为有效的特征，对物体的响应更加敏感。</p>
<h2 id="8-3-One-Stage目标检测算法"><a href="#8-3-One-Stage目标检测算法" class="headerlink" title="8.3 One Stage目标检测算法"></a>8.3 One Stage目标检测算法</h2><p>我们将对单次目标检测器（包括SSD系列和YOLO系列等算法）进行综述。我们将分析FPN以理解多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。然后我们将分析Focal loss和RetinaNet，看看它们是如何解决训练过程中的类别不平衡问题的。</p>
<h3 id="8-3-1-SSD"><a href="#8-3-1-SSD" class="headerlink" title="8.3.1 SSD"></a>8.3.1 SSD</h3><p><strong>SSD有哪些创新点？</strong></p>
<ol>
<li>基于Faster R-CNN中的Anchor，提出了相似的先验框（Prior box）</li>
<li>从不同比例的特征图（多尺度特征）中产生不同比例的预测，并明确地按长宽比分离预测。</li>
</ol>
<p>不同于前面的R-CNN系列，SSD属于one-stage方法。SSD使用 VGG16 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样），将后面的全连接层替换成卷积层，并在之后添加自定义卷积层，并在最后直接采用卷积进行检测。在多个特征图上设置不同缩放比例和不同宽高比的先验框以融合多尺度特征图进行检测，靠前的大尺度特征图可以捕捉到小物体的信息，而靠后的小尺度特征图能捕捉到大物体的信息，从而提高检测的准确性和定位的准确性。如下图是SSD的网络结构图。</p>
<p><img src="/img/ch8/SSD-01.png" alt=""></p>
<p><strong>1. 怎样设置default boxes？</strong><br>SSD中default box的概念有点类似于Faster R-CNN中的anchor。不同于Faster R-CNN只在最后一个特征层取anchor, SSD在多个特征层上取default box，可以得到不同尺度的default box。在特征图的每个单元上取不同宽高比的default box,一般宽高比在{1,2,3,1/2,1/3}中选取，有时还会额外增加一个宽高比为1但具有特殊尺度的box。如下图所示，在8x8的feature map和4x4的feature map上的每个单元取4个不同的default box。原文对于300x300的输入，分别在conv4_3, conv7,conv8_2,conv9_2,conv10_2,conv11_2的特征图上的每个单元取4,6,6,6,4,4个default box. 由于以上特征图的大小分别是38x38,19x19,10x10,5x5,3x3,1x1，所以一共得到38x38x4+19x19x6+10x10x6+5x5x6+<br>3x3x4+1x1x4=8732个default box.对一张300x300的图片输入网络将会针对这8732个default box预测8732个边界框。</p>
<p><img src="/img/ch8/SSD-02.png" alt=""></p>
<p><strong>2. 怎样对先验框进行匹配？</strong><br>SSD在训练的时候只需要输入图像和图像中每个目标对应的ground truth. 先验框与ground truth 的匹配遵循两个原则：</p>
<p>（1）对图片中的每个ground truth, 在先验框中找到与其IOU最大的先验框，则该先验框对应的预测边界框与ground truth 匹配。</p>
<p>（2）对于（1）中每个剩下的没有与任何ground truth匹配到的先验框，找到与其IOU最大的ground truth，若其与该ground truth的IOU值大于某个阈值（一般设为0.5），则该先验框对应的预测边界框与该ground truth匹配。</p>
<p>按照这两个原则进行匹配，匹配到ground truth的先验框对应的预测边界框作为正样本，没有匹配到ground truth的先验框对应的预测边界框作为负样本。尽管一个ground truth可以与多个先验框匹配，但是ground truth的数量相对先验框还是很少，按照上面的原则进行匹配还是会造成负样本远多于正样本的情况。为了使正负样本尽量均衡（一般保证正负样本比例约为1：3），SSD采用hard negative mining, 即对负样本按照其预测背景类的置信度进行降序排列，选取置信度较小的top-k作为训练的负样本。</p>
<p><strong>3. 怎样得到预测的检测结果？</strong></p>
<p>最后分别在所选的特征层上使用3x3卷积核预测不同default boxes所属的类别分数及其预测的边界框location。由于对于每个box需要预测该box属于每个类别的置信度（假设有c类，包括背景，例如20class的数据集合，c=21）和该box对应的预测边界框的location(包含4个值，即该box的中心坐标和宽高)，则每个box需要预测c+4个值。所以对于某个所选的特征层，该层的卷积核个数为（c+4）x 该层的default box个数.最后将每个层得到的卷积结果进行拼接。对于得到的每个预测框，取其类别置信度的最大值，若该最大值大于置信度阈值，则最大值所对应的类别即为该预测框的类别，否则过滤掉此框。对于保留的预测框根据它对应的先验框进行解码得到其真实的位置参数（这里还需注意要防止预测框位置超出图片），然后根据所属类别置信度进行降序排列，取top-k个预测框，最后进行NMS，过滤掉重叠度较大的预测框，最后得到检测结果。</p>
<p>SSD优势是速度比较快，整个过程只需要一步，首先在图片不同位置按照不同尺度和宽高比进行密集抽样，然后利用CNN提取特征后直接进行分类与回归，所以速度比较快，但均匀密集采样会造成正负样本不均衡的情况使得训练比较困难，导致模型准确度有所降低。另外，SSD对小目标的检测没有大目标好，因为随着网络的加深，在高层特征图中小目标的信息丢失掉了，适当增大输入图片的尺寸可以提升小目标的检测效果。</p>
<h3 id="8-3-2-DSSD"><a href="#8-3-2-DSSD" class="headerlink" title="8.3.2 DSSD"></a>8.3.2 DSSD</h3><p><strong>DSSD有哪些创新点？</strong></p>
<ol>
<li>Backbone：将ResNet替换SSD中的VGG网络，增强了特征提取能力</li>
<li>添加了Deconvolution层，增加了大量上下文信息</li>
</ol>
<p>为了解决SSD算法检测小目标困难的问题，DSSD算法将SSD算法基础网络从VGG-16更改为ResNet-101，增强网络特征提取能力，其次参考FPN算法思路利用去Deconvolution结构将图像深层特征从高维空间传递出来，与浅层信息融合，联系不同层级之间的图像语义关系，设计预测模块结构，通过不同层级特征之间融合特征输出预测物体类别信息。</p>
<p>DSSD算法中有两个特殊的结构：Prediction模块；Deconvolution模块。前者利用提升每个子任务的表现来提高准确性，并且防止梯度直接流入ResNet主网络。后者则增加了三个Batch Normalization层和三个3×3卷积层，其中卷积层起到了缓冲的作用，防止梯度对主网络影响太剧烈，保证网络的稳定性。</p>
<p>SSD和DSSD的网络模型如下图所示：</p>
<p><img src="img/ch8/DSSD-01.png" alt=""></p>
<p><strong>Prediction Module</strong></p>
<p>SSD直接从多个卷积层中单独引出预测函数，预测量多达7000多，梯度计算量也很大。MS-CNN方法指出，改进每个任务的子网可以提高准确性。根据这一思想，DSSD在每一个预测层后增加残差模块，并且对于多种方案进行了对比，如下图所示。结果表明，增加残差预测模块后，高分辨率图片的检测精度比原始SSD提升明显。</p>
<p><img src="img/ch8/DSSD-02.png" alt=""></p>
<p><strong>Deconvolution模块</strong></p>
<p>为了整合浅层特征图和deconvolution层的信息，作者引入deconvolution模块，如下图所示。作者受到论文Learning to Refine Object Segments的启发，认为用于精细网络的deconvolution模块的分解结构达到的精度可以和复杂网络一样，并且更有效率。作者对其进行了一定的修改：其一，在每个卷积层后添加批归一化（batch normalization）层；其二，使用基于学习的deconvolution层而不是简单地双线性上采样；其三，作者测试了不同的结合方式，元素求和（element-wise sum）与元素点积（element-wise product）方式，实验证明元素点积计算能得到更好的精度。</p>
<p><img src="img/ch8/DSSD-03.png" alt=""></p>
<h3 id="8-3-3-YOLOv1"><a href="#8-3-3-YOLOv1" class="headerlink" title="8.3.3 YOLOv1"></a>8.3.3 YOLOv1</h3><p><strong>YOLOv1有哪些创新点？</strong></p>
<ol>
<li>将整张图作为网络的输入，直接在输出层回归bounding box的位置和所属的类别</li>
<li>速度快，one stage detection的开山之作</li>
</ol>
<p><strong>YOLOv1介绍</strong></p>
<p>YOLO（You Only Look Once: Unified, Real-Time Object Detection）是one-stage detection的开山之作。之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。</p>
<p>YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。下图展示了各物体检测系统的流程图。</p>
<p><img src="/img/ch8/YOLOv1-01.png" alt=""></p>
<p>事实上，YOLO也并没有真正的去掉候选区，而是直接将输入图片划分成7x7=49个网格，每个网格预测两个边界框，一共预测49x2=98个边界框。可以近似理解为在输入图片上粗略的选取98个候选区，这98个候选区覆盖了图片的整个区域，进而用回归预测这98个候选框对应的边界框。</p>
<p><strong>1. 网络结构是怎样的？</strong></p>
<p>YOLO网络借鉴了GoogLeNet分类网络结构，不同的是YOLO使用1x1卷积层和3x3卷积层替代inception module。如下图所示，整个检测网络包括24个卷积层和2个全连接层。其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。</p>
<p><img src="/img/ch8/YOLOv1-02.png" alt=""></p>
<p><strong>2. YOLO的输入、输出、损失函数分别是什么？</strong></p>
<p>前面说到YOLO将输入图像分成7x7的网格，最后输出是7x7xk的张量。YOLO网络最后接了两个全连接层，全连接层要求输入是固定大小的，所以YOLO要求输入图像有固定大小，论文中作者设计的输入尺寸是448x448。</p>
<p>YOLO将输入图像分成7x7的网格，每个网格预测2个边界框。若某物体的ground truth的中心落在该网格，则该网格中与这个ground truth IOU最大的边界框负责预测该物体。对每个边界框会预测5个值，分别是边界框的中心x,y（相对于所属网格的边界），边界框的宽高w,h（相对于原始输入图像的宽高的比例），以及这些边界框的confidencescores（边界框与ground truth box的IOU值）。同时每个网格还需要预测c个类条件概率 （是一个c维向量，表示某个物体object在这个网格中，且该object分别属于各个类别的概率，这里的c类物体不包含背景）。论文中的c=20，则每个网格需要预测2x5+20=30个值，这些值被映射到一个30维的向量。<br>为了让边界框坐标损失、分类损失达到很好的平衡，损失函数设计如下图所示。</p>
<p><img src="/img/ch8/YOLOv1-03.png" alt=""></p>
<p>如上图所示，损失函数分为坐标预测（蓝色框）、含有物体的边界框的confidence预测（红色框）、不含有物体的边界框的confidence预测（黄色框）、分类预测（紫色框）四个部分。</p>
<p>由于不同大小的边界框对预测偏差的敏感度不同，小的边界框对预测偏差的敏感度更大。为了均衡不同尺寸边界框对预测偏差的敏感度的差异。作者巧妙的对边界框的w,h取均值再求L2 loss. YOLO中更重视坐标预测，赋予坐标损失更大的权重，记为 coord，在pascal voc训练中coodd=5 ，classification error部分的权重取1。</p>
<p>某边界框的置信度定义为：某边界框的confidence = 该边界框存在某类对象的概率pr(object)*该边界框与该对象的ground truth的IOU值 ，若该边界框存在某个对象pr(object)=1 ，否则pr(object)=0 。由于一幅图中大部分网格中是没有物体的，这些网格中的边界框的confidence置为0，相比于有物体的网格，这些不包含物体的网格更多，对梯度更新的贡献更大，会导致网络不稳定。为了平衡上述问题，YOLO损失函数中对没有物体的边界框的confidence error赋予较小的权重，记为 noobj，对有物体的边界框的confidence error赋予较大的权重。在pascal VOC训练中noobj=0.5 ，有物体的边界框的confidence error的权重设为1.</p>
<p><strong>3. YOLO怎样预测？</strong></p>
<p>YOLO最后采用非极大值抑制（NMS）算法从输出结果中提取最有可能的对象和其对应的边界框。</p>
<p>输入一张图片到YOLO网络将输出一个7<em>7</em>30的张量表示图片中每个网格对应的可能的两个边界框以及每个边界框的置信度和包含的对象属于各个类别的概率。由此可以计算某对象i属于类别 同时在第j个边界框中的得分：</p>
<p><img src="/img/ch8/YOLOv1-04.png" alt=""></p>
<p>每个网格有20个类条件概率，2个边界框置信度，相当于每个网格有40个得分，7x7个网格有1960个得分，每类对象有1960/20=98个得分，即98个候选框。</p>
<p><strong>NMS步骤如下：</strong></p>
<p>1.设置一个Score的阈值，一个IOU的阈值；</p>
<p>2.对于每类对象，遍历属于该类的所有候选框，</p>
<p>①过滤掉Score低于Score阈值的候选框；</p>
<p>②找到剩下的候选框中最大Score对应的候选框，添加到输出列表；</p>
<p>③进一步计算剩下的候选框与②中输出列表中每个候选框的IOU，若该IOU大于设置的IOU阈值，将该候选框过滤掉，否则加入输出列表中；</p>
<p>④最后输出列表中的候选框即为图片中该类对象预测的所有边界框</p>
<p>3.返回步骤2继续处理下一类对象。</p>
<p>YOLO将识别与定位合二为一，结构简便，检测速度快，更快的Fast YOLO可以达到155FPS。相对于R-CNN系列, YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。同时YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上做训练的YOLO，在艺术图片上可以得到较好的测试结果。</p>
<p>由于YOLO网格设置比较稀疏，且每个网格只预测2个边界框，其总体预测精度不高，略低于Fast RCNN。其对小物体的检测效果较差，尤其是对密集的小物体表现比较差。</p>
<h3 id="8-3-4-YOLOv2"><a href="#8-3-4-YOLOv2" class="headerlink" title="8.3.4 YOLOv2"></a>8.3.4 YOLOv2</h3><p><strong>YOLOv2 有哪些创新点？</strong></p>
<p>YOLOv1虽然检测速度快，但在定位方面不够准确，并且召回率较低。为了提升定位准确度，改善召回率，YOLOv2在YOLOv1的基础上提出了几种改进策略，如下图所示，可以看到，一些改进方法能有效提高模型的mAP。</p>
<ol>
<li>大尺度预训练分类</li>
<li>New Network：Darknet-19</li>
<li>加入anchor</li>
</ol>
<p><img src="/img/ch8/YOLOv2-01.png" alt=""></p>
<p><strong>YOLOv2 介绍</strong></p>
<p><strong>（1）Batch Normalization</strong></p>
<p>YOLOv2中在每个卷积层后加Batch Normalization(BN)层，去掉dropout. BN层可以起到一定的正则化效果，能提升模型收敛速度，防止模型过拟合。YOLOv2通过使用BN层使得mAP提高了2%。<br><strong>（2）High Resolution Classifier</strong></p>
<p>目前的大部分检测模型都会使用主流分类网络（如vgg、resnet）在ImageNet上的预训练模型作为特征提取器,<br>而这些分类网络大部分都是以小于256x256的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2将输入图片的分辨率提升至448x448，为了使网络适应新的分辨率，YOLOv2先在ImageNet上以448x448的分辨率对网络进行10个epoch的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2的mAP提升了约4%。</p>
<p><strong>（3）Convolutional With Anchor Boxes</strong></p>
<p>YOLOv1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLOv2去掉了YOLOv1中的全连接层，使用Anchor Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLOv2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416x416的输入，模型下采样的总步长为32，最后得到13x13的特征图，然后对13x13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。使用anchor<br>boxes之后，YOLOv2可以预测13x13x5=845个边界框，模型的召回率由原来的81%提升到88%，mAP由原来的69.5%降低到69.2%.召回率提升了7%，准确率下降了0.3%。</p>
<p><strong>（4）Dimension Clusters</strong></p>
<p>在Faster R-CNN和SSD中，先验框都是手动设定的，带有一定的主观性。YOLOv2采用k-means聚类算法对训练集中的边界框做了聚类分析，选用boxes之间的IOU值作为聚类指标。综合考虑模型复杂度和召回率，最终选择5个聚类中心，得到5个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均IOU值，这使得模型更容易训练学习。</p>
<p><strong>（5）New Network：Darknet-19</strong></p>
<p>YOLOv2采用Darknet-19，其网络结构如下图所示，包括19个卷积层和5个max pooling层，主要采用3x3卷积和1x1卷积，这里1x1卷积可以压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用BN层以加快模型收敛同时防止过拟合。最终采用global avg pool 做预测。采用YOLOv2，模型的mAP值没有显著提升，但计算量减少了。</p>
<p><img src="/img/ch8/YOLOv2-02.png" alt=""></p>
<p><strong>（6）Direct location prediction</strong></p>
<p>Faster R-CNN使用anchor boxes预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。YOLOv2沿用YOLOv1的方法，根据所在网格单元的位置来预测坐标,则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。设一个网格相对于图片左上角的偏移量是cx，cy。先验框的宽度和高度分别是pw和ph，则预测的边界框相对于特征图的中心坐标(bx，by)和宽高bw、bh的计算公式如下图所示。</p>
<p><img src="/img/ch8/YOLOv2-03.png" alt=""></p>
<p>YOLOv2结合Dimention Clusters, 通过对边界框的位置预测进行约束，使模型更容易稳定训练，这种方式使得模型的mAP值提升了约5%。</p>
<p><strong>（7）Fine-Grained Features</strong></p>
<p>YOLOv2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。YOLOv2提取Darknet-19最后一个max pool层的输入，得到26x26x512的特征图。经过1x1x64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%.</p>
<p><strong>（8）Multi-Scale Training</strong></p>
<p>YOLOv2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数{320,352,…,608}。采用Multi-Scale Training, 可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p>
<p>YOLOv2借鉴了很多其它目标检测方法的一些技巧，如Faster R-CNN的anchor boxes, SSD中的多尺度检测。除此之外，YOLOv2在网络设计上做了很多tricks,使它能在保证速度的同时提高检测准确率，Multi-Scale Training更使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡。</p>
<p><strong>YOLOv2的训练</strong></p>
<p>YOLOv2的训练主要包括三个阶段。<br>第一阶段：先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为$224\times 224$,共训练160个epochs。<br>第二阶段：将网络的输入调整为$448\times 448$,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。<br>第三个阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。<br>网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个$3\times 3 \times 2014$卷积层，同时增加了一个passthrough层，最后使用$1\times 1$卷积层输出预测结果。</p>
<h3 id="8-3-5-YOLO9000"><a href="#8-3-5-YOLO9000" class="headerlink" title="8.3.5 YOLO9000"></a>8.3.5 YOLO9000</h3><p>github：<a href="http://pjreddie.com/yolo9000/" target="_blank" rel="noopener">http://pjreddie.com/yolo9000/</a></p>
<p>YOLO9000是在YOLOv2的基础上提出的一种联合训练方法，可以检测超过9000个类别的模型。YOLOv2混合目标检测数据集和分类数据集，用目标检测数据集及其类别标记信息和位置标注信息训练模型学习预测目标定位和分类，用分类数据集及其类别标记信息进一步扩充模型所能识别的物体类别同时能增强模型鲁棒性。</p>
<p><strong>1. YOLO9000是怎么组织数据的？</strong></p>
<p>YOLO9000根据各个类别之间的从属关系建立一种树结WordTree, 将COCO数据集和ImageNet数据集组织起来。</p>
<p>WordTree的生成方式如下：</p>
<p>①首先遍历ImageNet中的类别名词。</p>
<p>②对每个名词，在WordNet(一种结构化概念及概念之间关系的语言数据库)上找到从它所在位置到根节点（设根节点为实体对象physical object）的最短路径，由于在WordNet中大多数同义词只有一个路径，所以先把将该路径上的词全都加到树中。</p>
<p>③迭代地检查剩下的名词，取它到根节点的最短路径，将该最短路径上的还没出现在层次树中的词加入到树中。<br>混合后的数据集形成一个有9418类的WordTree.生成的WordTree模型如下图所示。另外考虑到COCO数据集相对于ImageNet数据集数据量太少了，为了平衡两个数据集，作者进一步对COCO数据集过采样，使COCO数据集与ImageNet数据集的数据量比例接近1：4。</p>
<p><img src="/img/ch8/YOLOv2-04.png" alt=""></p>
<p>对于物体的标签，采用one-hot编码的形式，数据集中的每个物体的类别标签被组织成1个长度为9418的向量，向量中除在WordTree中从该物体对应的名词到根节点的路径上出现的词对应的类别标号处为1，其余位置为0。</p>
<p><strong>2. YOLO9000是怎么进行联合训练的？</strong></p>
<p>YOLO9000采用YOLOv2的结构，anchorbox由原来的5调整到3，对每个anchorbox预测其对应的边界框的位置信息x,y,w,h和置信度以及所包含的物体分别属于9418类的概率，所以每个anchorbox需要预测4+1+9418=9423个值。每个网格需要预测3x9423=28269个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的YOLOv2loss进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的loss进行反向传播。</p>
<p><strong>3. YOLO9000是怎么预测的？</strong></p>
<p>WordTree中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次softmax处理，以得到同义词集合中的每个词的下义词的概率。当需要预测属于某个类别的概率时，需要预测该类别节点的条件概率。即在WordTree上找到该类别名词到根节点的路径，计算路径上每个节点的概率之积。预测时，YOLOv2得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。</p>
<p>YOLO9000使用WordTree混合目标检测数据集和分类数据集，并在其上进行联合训练，使之能实时检测出超过9000个类别的物体，其强大令人赞叹不已。YOLO9000尤其对动物的识别效果很好，但是对衣服或者设备等类别的识别效果不是很好，可能的原因是与目标检测数据集中的数据偏向有关。</p>
<h3 id="8-3-6-YOLOv3"><a href="#8-3-6-YOLOv3" class="headerlink" title="8.3.6 YOLOv3"></a>8.3.6 YOLOv3</h3><p>YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。</p>
<p><strong>YOLOv3有哪些创新点？</strong></p>
<ol>
<li>新网络结构：DarkNet-53</li>
<li>融合FPN</li>
<li>用逻辑回归替代softmax作为分类器</li>
</ol>
<p><strong>1. YOLOv3对网络结构做了哪些改进？</strong></p>
<p>YOLOv3在之前Darknet-19的基础上引入了残差块，并进一步加深了网络，改进后的网络有53个卷积层，取名为Darknet-53，网络结构如下图所示（以256*256的输入为例）。</p>
<p><img src="/img/ch8/YOLOv3-01.png" alt=""></p>
<p>为了比较Darknet-53与其它网络结构的性能，作者在TitanX上，采用相同的实验设置，将256x256的图片分别输入以Darknet-19，ResNet-101，ResNet-152和Darknet-53为基础网络的分类模型中，实验得到的结果如下图所示。可以看到Darknet-53比ResNet-101的性能更好，而且速度是其1.5倍，Darknet-53与ResNet-152性能相似但速度几乎是其2倍。注意到，Darknet-53相比于其它网络结构实现了每秒最高的浮点计算量，说明其网络结构能更好的利用GPU。</p>
<p><img src="/img/ch8/YOLOv3-02.png" alt=""></p>
<p><strong>2.YOLOv3中怎样实现多尺度检测？</strong></p>
<p>YOLOv3借鉴了FPN的思想，从不同尺度提取特征。相比YOLOv2，YOLOv3提取最后3层特征图，不仅在每个特征图上分别独立做预测，同时通过将小特征图上采样到与大的特征图相同大小，然后与大的特征图拼接做进一步预测。用维度聚类的思想聚类出9种尺度的anchor box，将9种尺度的anchor box均匀的分配给3种尺度的特征图.如下图是在网络结构图的基础上加上多尺度特征提取部分的示意图（以在COCO数据集(80类)上256x256的输入为例）：</p>
<p><img src="/img/ch8/YOLOv3-03.png" alt=""></p>
<p>从YOLOv1到YOLOv2再到YOLO9000、YOLOv3, YOLO经历三代变革，在保持速度优势的同时，不断改进网络结构，同时汲取其它优秀的目标检测算法的各种trick，先后引入anchor box机制、引入FPN实现多尺度检测等。</p>
<h3 id="8-3-7-RetinaNet"><a href="#8-3-7-RetinaNet" class="headerlink" title="8.3.7 RetinaNet"></a>8.3.7 RetinaNet</h3><p><strong>研究背景</strong></p>
<ul>
<li>Two-Stage检测器（如Faster R-CNN、FPN）效果好，但速度相对慢</li>
<li>One-Stage检测器（如YOLO、SSD）速度快，但效果一般</li>
</ul>
<p><img src="/img/ch8/RetinaNet-01.png" alt=""></p>
<p>作者对one-stage检测器准确率不高的问题进行探究，发现主要问题在于正负类别不均衡（简单-难分类别不均衡）。</p>
<blockquote>
<p>We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause.</p>
</blockquote>
<p>作者建议通过重新设计标准的交叉熵损失（cross entropy loss）来解决这种类别不平衡（class inbalance）问题，即提出Focal Loss。</p>
<blockquote>
<p>We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.</p>
</blockquote>
<p>结合Focal Loss的one-stage检测器称为RetinaNet，该检测器在COCO上mAP可以和特征金字塔网络（feature pyramid network，FPN）或者Mask R-CNN接近，</p>
<p><strong>问：什么是类别不均衡（class imbalance）？</strong></p>
<p>答：负样本的数量极大于正样本的数量，比如包含物体的区域（正样本）很少，而不包含物体的区域（负样本）很多。比如检测算法在早期会生成一大波的bbox。而一幅常规的图片中，顶多就那么几个object。这意味着，绝大多数的bbox属于background。</p>
<p><strong>问：样本的类别不均衡会带来什么问题？</strong></p>
<p>答：由于大多数都是简单易分的负样本（属于背景的样本），使得训练过程不能充分学习到属于那些有类别样本的信息；其次简单易分的负样本太多，可能掩盖了其他有类别样本的作用（这些简单易分的负样本仍产生一定幅度的loss，见下图蓝色曲线，数量多会对loss起主要贡献作用，因此就主导了梯度的更新方向，掩盖了重要的信息）</p>
<blockquote>
<p>This imbalance causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal; (2) en masse, the easy negatives can overwhelm training and lead to degenerate models.</p>
</blockquote>
<p>简单来说，因为bbox数量爆炸。 正是因为bbox中属于background的bbox太多了，所以如果分类器无脑地把所有bbox统一归类为background，accuracy也可以刷得很高。于是乎，分类器的训练就失败了。分类器训练失败，检测精度自然就低了。</p>
<p><strong>问：为什么在two-stage检测器中，没有出现类别不均衡（class imbalamce）问题呢？</strong></p>
<p>答：因为通过RPN阶段可以减少候选目标区域，而在分类阶段，可以固定前景与背景比值（foreground-to-background ratio）为1:3，或者使用OHEM（online hard example mining）使得前景和背景的数量达到均衡。</p>
<p><strong>RetinaNet有哪些创新点？</strong></p>
<p><strong>概述：</strong></p>
<ul>
<li>New loss：提出Focal Loss函数解决class imbalance</li>
</ul>
<script type="math/tex; mode=display">
FL(p_t) = -(1-p_t)^\gamma \log(p_t)FL(pt)=−(1−pt)γlog(pt)</script><ul>
<li>New detector：RetinaNet = ResNet + FPN + Two sub-networks + Focal Loss</li>
</ul>
<p>Focal Loss更加聚焦在困难样本（hard examples）上的训练。</p>
<p><img src="/img/ch8/RetinaNet-02.png" alt=""></p>
<p>将Focal Loss与ResNet-101-FPN backbone结合提出RetinaNet（one-stage检测器），RetinaNet在COCO test-dev上达到39.1mAP，速度为5FPS。</p>
<p>RetinaNet检测器与当时最佳的其它检测器进行比较，无论是速度上还是准确率上都是最佳：</p>
<p><img src="/img/ch8/RetinaNet-03.png" alt=""></p>
<p><strong>详解：</strong></p>
<p>作者提出一种新的损失函数，思路是希望那些hard examples对损失的贡献变大，使网络更倾向于从这些样本上学习。</p>
<p>作者以二分类为例进行说明：</p>
<p><strong>交叉熵函数CE</strong></p>
<p>首先是我们常使用的交叉熵损失函数：</p>
<p><img src="/img/ch8/RetinaNet-04.png" alt=""></p>
<p>上式中，y=+1或者y=-1。p∈[0,1]是y=+1的估计概率。作者定义pt为：</p>
<p><img src="/img/ch8/RetinaNet-05.png" alt=""></p>
<p><img src="/img/ch8/RetinaNet-06.png" alt=""></p>
<p>注：对交叉熵函数不了解的，可以参考<a href="https://blog.csdn.net/chaipp0607/article/details/73392175" target="_blank" rel="noopener">理解交叉熵作为损失函数在神经网络中的作用</a></p>
<p><strong>均衡交叉熵函数</strong></p>
<p>要对类别不均衡问题对loss的贡献进行一个控制，即加上一个控制权重即可，最初作者的想法即如下这样，对于属于少数类别的样本，增大α即可</p>
<p><img src="/img/ch8/RetinaNet-07.png" alt=""></p>
<p>但这样有一个问题，它仅仅解决了正负样本之间的平衡问题，并没有区分易分/难分样本，按作者的话说：</p>
<blockquote>
<p>While α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. Instead, we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives.</p>
</blockquote>
<p>问：为什么公式(3)只解决正负样本不均衡问题？</p>
<p>答：增加了一个系数αt，跟pt的定义类似，当label=1的时候，αt=a；当label=-1的时候，αt=1-a，a的范围也是0到1。因此可以通过设定a的值（一般而言假如1这个类的样本数比-1这个类的样本数多很多，那么a会取0到0.5来增加-1这个类的样本的权重）来控制正负样本对总的loss的共享权重。</p>
<p><strong>Focal Loss</strong></p>
<p>作者一开始给交叉熵损失函数添加modulating factor：</p>
<script type="math/tex; mode=display">
(1-pt)^γ(1−pt)γ</script><p><img src="/img/ch8/RetinaNet-08.png" alt=""></p>
<p>显然，样本越易分，pt就越大（pt—&gt;1），modulating factor趋近于0，则贡献的loss就越小，同样地，样本越难分，其pt就越小，modulating factor接近于1，则贡献的loss不受影响。</p>
<p>问：为什么pt越大，FL值越小？</p>
<p>答：根据公式（4）可知，FL与log(pt)中的pt成反比，与1-pt成正比，因此FL与pt的关系成反比。这是交叉熵函数的基本性质。当pt很大时（接近于1），FL值很小；而当pt很小时（接近于0），FL值会很大。</p>
<p>注：这里有个超参数—focusing parameter γ。</p>
<p>γ 放大了modulating factor的作用。</p>
<p>举原文中的一个例子，当pt=0.9时，带有modulating factor的focal loss是CE loss的100分之一，即进一步减小了正确分类的损失。</p>
<blockquote>
<p>For instance, with γ = 2, an example classified with pt = 0.9 would have 100× lower loss compared with CE and with pt ≈ 0.968 it would have 1000× lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most 4× for pt ≤ .5 and γ = 2).</p>
</blockquote>
<p>在实际中，作者采用如下公式，即综合了公式(3)和公式(4)的形式，这样机能调整正负样本的权重，又能控制难易分类样本的权重：</p>
<p><img src="/img/ch8/RetinaNet-09.png" alt=""></p>
<p>这里的两个参数 α和γ 来控制，在实验中a的选择范围也很广，一般而言当γ增加的时候，a需要减小一点，本文作者采用α=0.25，γ=2效果最好。</p>
<p><strong>RetinaNet Detector</strong></p>
<p>RetinaNet是由backbone网络和两个特殊任务的子网络（subnet）组成（属于one-stage检测器）。Backbone用来计算feature map；第一个子网络用来object classification，第二个子网络用来bounding box regression。</p>
<p><strong>Feature Pyramid Network Backbone</strong></p>
<p><img src="/img/ch8/RetinaNet-10.png" alt=""></p>
<p><strong>Anchor</strong></p>
<p><strong>Classification Subnet</strong></p>
<p><strong>Box Regression Subnet</strong></p>
<p><img src="/img/ch8/RetinaNet-11.png" alt=""></p>
<p><img src="/img/ch8/RetinaNet-12.png" alt=""></p>
<p>RetinaNet结构注意内容：</p>
<ol>
<li>训练时FPN每一级的所有example都被用于计算Focal Loss，loss值加到一起用来训练；</li>
<li>测试时FPN每一级只选取score最大的1000个example来做nms；</li>
<li>整个结构不同层的head部分(上图中的c和d部分)共享参数，但分类和回归分支间的参数不共享；</li>
<li>分类分支的最后一级卷积的bias初始化成前面提到的-log((1-π)/π);</li>
</ol>
<p>作者：张磊_0503 链接：<a href="https://www.jianshu.com/p/204d9ad9507f" target="_blank" rel="noopener">https://www.jianshu.com/p/204d9ad9507f</a> 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>
<p><strong>实验结果</strong></p>
<p>Table1是关于RetinaNet和Focal Loss的一些实验结果。（a）是在交叉熵的基础上加上参数a，a=0.5就表示传统的交叉熵，可以看出当a=0.75的时候效果最好，AP值提升了0.9。（b）是对比不同的参数γ和a的实验结果，可以看出随着γ的增加，AP提升比较明显。（d）通过和OHEM的对比可以看出最好的Focal Loss比最好的OHEM提高了3.2AP。这里OHEM1:3表示在通过OHEM得到的minibatch上强制positive和negative样本的比例为1:3，通过对比可以看出这种强制的操作并没有提升AP。（e）加入了运算时间的对比，可以和前面的Figure2结合起来看，速度方面也有优势！注意这里RetinaNet-101-800的AP是37.8，当把训练时间扩大1.5倍同时采用scale jitter，AP可以提高到39.1，这就是全文和table2中的最高的39.1AP的由来。</p>
<p><img src="/img/ch8/RetinaNet-13.png" alt=""></p>
<p><img src="/img/ch8/RetinaNet-14.png" alt=""></p>
<h3 id="8-3-8-RFBNet"><a href="#8-3-8-RFBNet" class="headerlink" title="8.3.8 RFBNet"></a>8.3.8 RFBNet</h3><p><strong>RFBNet有哪些创新点？</strong></p>
<ol>
<li>提出RF block（RFB）模块</li>
</ol>
<p>RFBNet主要想利用一些技巧使得轻量级模型在速度和精度上达到很好的trade-off的检测器。灵感来自人类视觉的感受野结构Receptive Fields (RFs) ，提出了新奇的RF block（RFB）模块，来验证感受野尺寸和方向性的对提高有鉴别鲁棒特征的关系。RFBNet是以主干网络（backbone）为VGG16的SSD来构建的，主要是在Inception的基础上加入了dilated卷积层（dilated convolution），从而有效增大了感受野（receptive field）。整体上因为是基于SSD网络进行改进，所以检测速度还是比较快，同时精度也有一定的保证。</p>
<p><strong>RFB介绍</strong></p>
<p>RFB是一个类似Inception模块的多分支卷积模块，它的内部结构可分为两个组件：多分支卷积层和dilated卷积层。如下图：</p>
<p><img src="img/ch8/RFBNet-01.png" alt=""></p>
<p><strong>1.多分支卷积层</strong><br>​      根据RF的定义，用多种尺寸的卷积核来实现比固定尺寸更好。具体设计：1.瓶颈结构，1x1-s2卷积减少通道特征，然后加上一个nxn卷积。2.替换5x5卷积为两个3x3卷积去减少参数，然后是更深的非线性层。有些例子，使用1xn和nx1代替nxn卷积；shortcut直连设计来自于ResNet和Inception ResNet V2。3.为了输出，卷积经常有stride=2或者是减少通道，所以直连层用一个不带非线性激活的1x1卷积层。</p>
<p><strong>2.Dilated 卷积层</strong></p>
<p>设计灵感来自Deeplab，在保持参数量和同样感受野的情况下，用来获取更高分辨率的特征。下图展示两种RFB结构：RFB和RFB-s。每个分支都是一个正常卷积后面加一个dilated卷积，主要是尺寸和dilated因子不同。（a）RFB。整体结构上借鉴了Inception的思想，主要不同点在于引入3个dilated卷积层（比如3x3conv，rate=1），这也是RFBNet增大感受野的主要方式之一；（b）RFB-s。RFB-s和RFB相比主要有两个改进，一方面用3x3卷积层代替5x5卷积层，另一方面用1x3和3x1卷积层代替3x3卷积层，主要目的应该是为了减少计算量，类似Inception后期版本对Inception结构的改进。 </p>
<p><img src="img/ch8/RFBNet-02.png" alt=""></p>
<p>RFBNet300的整体结构如下图所示，基本上和SSD类似。RFBNet和SSD不同的是：1、主干网上用两个RFB结构替换原来新增的两层。2、conv4_3和conv7_fc在接预测层之前分别接RFB-s和RFB结构。 </p>
<p><img src="img/ch8/RFBNet-03.png" alt=""></p>
<h3 id="8-3-9-M2Det"><a href="#8-3-9-M2Det" class="headerlink" title="8.3.9 M2Det"></a>8.3.9 M2Det</h3><p><strong>M2Det有哪些创新点？</strong></p>
<ol>
<li>提出了多层次特征金字塔网络（MLFPN）来构建更有效的特征金字塔，用于检测不同尺度的对象。</li>
</ol>
<p>M2Det的整体架构如下所示。M2Det使用backbone和多级特征金字塔网络（MLFPN）从输入图像中提取特征，然后类似于SSD，根据学习的特征生成密集的边界框和类别分数，最后是非最大抑制（NMS）操作以产生最终结果。 MLFPN由三个模块组成：特征融合模块（FFM），简化的U形模块（TUM）和按基于尺度的特征聚合模块（SFAM）。 FFMv1通过融合骨干网络的特征图，将语义信息丰富为基本特征。每个TUM生成一组多尺度特征，然后交替连接的TUM和FFMv2提取多级多尺度特征。此外，SFAM通过按比例缩放的特征连接操作和自适应注意机制将特征聚合到多级特征金字塔中。下面介绍有关M2Det中三个核心模块和网络配置的更多详细信息。</p>
<p><img src="img/ch8/M2Det-01.png" alt=""></p>
<p><strong>FFMs</strong></p>
<p>FFM融合了M2Det中不同层次的特征，这对于构建最终的多级特征金字塔至关重要。它们使用1x1卷积层来压缩输入特征的通道，并使用连接操作来聚合这些特征图。特别是，由于FFMv1以backbone中不同比例的两个特征图作为输入，因此它采用一个上采样操作，在连接操作之前将深度特征重新缩放到相同的尺度。同时，FFMv2采用基本特征和前一个TUM的最大输出特征图 - 这两个具有相同的比例 - 作为输入，并产生下一个TUM的融合特征。 FFMv1和FFMv2的结构细节分别如下图（a）和（b）所示。</p>
<p><img src="img/ch8/M2Det-02.png" alt=""></p>
<p><strong>TUMs</strong> </p>
<p>TUM不同于FPN和RetinaNet，TUM采用简化的U形结构，如上图（c）所示。编码器是一系列3x3，步长为2的卷积层.并且解码器将这些层的输出作为其参考特征集，而原始FPN选择ResNet主干网络中每个阶段的最后一层的输出。此外，在解码器分支的上采样层后添加1x1卷积层和按元素求和的操作，以增强学习能力并保持特征的平滑性。每个TUM的解码器中的所有输出形成当前级别的多尺度特征。整体而言，堆叠TUM的输出形成多层次多尺度特征，而前TUM主要提供浅层特征，中间TUM提供中等特征，后TUM提供深层特征。</p>
<p><strong>SFAM</strong></p>
<p>SFAM旨在将由TUM生成的多级多尺度特征聚合成多级特征金字塔，如下图所示。SFAM的第一阶段是沿着信道维度将等效尺度的特征连接在一起。聚合特征金字塔可以表示为$X = [X_1,X_2,…,X_i,…,X_L]$，其中</p>
<script type="math/tex; mode=display">X_i = Concat(X_{1i}, X_{2i}, ...., X_{Li}) \in R^{W_i \times H_i \times C}</script><p>指的是尺度第i个最大的特征。这里，聚合金字塔中的每个比例都包含来自多级深度的特征。但是，简单的连接操作不太适合。在第二阶段，引入了通道注意模块，以促使特征集中在最有益的通道。在SE区块之后，使用全局平均池化来在挤压步骤中生成通道统计z∈RC。</p>
<p><img src="img/ch8/M2Det-03.png" alt=""></p>
<h2 id="8-4-人脸检测"><a href="#8-4-人脸检测" class="headerlink" title="8.4 人脸检测"></a>8.4 人脸检测</h2><p>在目标检测领域可以划分为了人脸检测与通用目标检测，往往人脸这方面会有专门的算法（包括人脸检测、人脸识别、人脸其他属性的识别等等），并且和通用目标检测（识别）会有一定的差别，着主要来源于人脸的特殊性（有时候目标比较小、人脸之间特征不明显、遮挡问题等），下面将从人脸检测和通用目标检测两个方面来讲解目标检测。</p>
<h3 id="8-4-1-目前主要有人脸检测方法分类？"><a href="#8-4-1-目前主要有人脸检测方法分类？" class="headerlink" title="8.4.1 目前主要有人脸检测方法分类？"></a>8.4.1 目前主要有人脸检测方法分类？</h3><p>目前人脸检测方法主要包含两个区域：传统人脸检测算法和基于深度学习的人脸检测算法。传统人脸检测算法主要可以分为4类：</p>
<p>（1）基于知识的人脸检测方法；</p>
<p>（2）基于模型的人脸检测方法；</p>
<p>（3）基于特征的人脸检测方法；</p>
<p>（4）基于外观的人脸检测方法。</p>
<p>由于本书着重关注深度学习，下面会着重介绍基于深度学习的人脸检测方法。</p>
<p>2006年Hinton首次提出深度学习（Deep Learning）的概念，它是通过组合低层的特征形成更高层的抽象特征。随后研究者将深度学习应用在人脸检测领域，主要集中在基于卷积神经网络（CNN）的人脸检测研究，如基于级联卷积神经网络的人脸检测（cascade cnn）、 基于多任务卷积神经网络的人脸检测（MTCNN）、Facebox等，很大程度上提高了人脸检测的鲁棒性。当然通用目标检测算法像Faster-rcnn、yolo、ssd等也有用在人脸检测领域，也可以实现比较不错的结果，但是和专门人脸检测算法比还是有差别。下面部分主要介绍基于深度学习的的人脸检测算法，基于深度学习的通用目标检测算法将在第二大节介绍。</p>
<h3 id="8-4-2-如何检测图片中不同大小的人脸？"><a href="#8-4-2-如何检测图片中不同大小的人脸？" class="headerlink" title="8.4.2 如何检测图片中不同大小的人脸？"></a>8.4.2 如何检测图片中不同大小的人脸？</h3><p>传统人脸检测算法中针对不同大小人脸主要有两个策略：</p>
<p>（1）缩放图片的大小（图像金字塔如图8.4.1所示）；</p>
<p>（2）缩放滑动窗的大小（如图8.4.2所示）。</p>
<p><img src="/img/ch8/8.4.1.png" alt=""></p>
<p>图 8.1 图像金字塔           </p>
<p>​      <img src="/img/ch8/8.4.2.png" alt=""></p>
<p> 图 8.2 缩放滑动窗口</p>
<p>​    基于深度学习的人脸检测算法中针对不同大小人脸主要也有两个策略，但和传统人脸检测算法有点区别，主要包括:</p>
<p>（1）缩放图片大小。（不过也可以通过缩放滑动窗的方式，基于深度学习的滑动窗人脸检测方式效率会很慢存在多次重复卷积，所以要采用全卷积神经网络（FCN），用FCN将不能用滑动窗的方法。）</p>
<p>（2）通过anchor box的方法（如图8.3所示，不要和图8.2混淆，这里是通过特征图预测原图的anchor box区域，具体在facebox中有描述）。</p>
<p><img src="/img/ch8/8.4.3.png" alt=""></p>
<p>图 8.3 anchor box</p>
<h3 id="8-4-3-如何设定算法检测最小人脸尺寸"><a href="#8-4-3-如何设定算法检测最小人脸尺寸" class="headerlink" title="8.4.3 如何设定算法检测最小人脸尺寸?"></a>8.4.3 如何设定算法检测最小人脸尺寸?</h3><p>主要是看滑动窗的最小窗口和anchorbox的最小窗口。</p>
<p>（1）滑动窗的方法 </p>
<p>假设通过12×12的滑动窗，不对原图做缩放的话，就可以检测原图中12×12的最小人脸。但是往往通常给定最小人脸a=40、或者a=80，以这么大的输入训练CNN进行人脸检测不太现实，速度会很慢，并且下一次需求最小人脸a=30*30又要去重新训练，通常还会是12×12的输入，为满足最小人脸框a，只需要在检测的时候对原图进行缩放即可：w=w×12/a。</p>
<p>（2）anchorbox的方法</p>
<p>原理类似，这里主要看anchorbox的最小box，通过可以通过缩放输入图片实现最小人脸的设定。</p>
<h3 id="8-4-4-如何定位人脸的位置？"><a href="#8-4-4-如何定位人脸的位置？" class="headerlink" title="8.4.4 如何定位人脸的位置？"></a>8.4.4 如何定位人脸的位置？</h3><p>（1）滑动窗的方式：</p>
<p>滑动窗的方式是基于分类器识别为人脸的框的位置确定最终的人脸，</p>
<p><img src="/img/ch8/8.4.4.png" alt=""></p>
<p>图 8.4 滑动窗</p>
<p>（2）FCN的方式：</p>
<p>​    FCN的方式通过特征图映射到原图的方式确定最终识别为人脸的位置，特征图映射到原图人脸框是要看特征图相比较于原图有多少次缩放（缩放主要查看卷积的步长和池化层），假设特征图上(2,3)的点，可粗略计算缩放比例为8倍，原图中的点应该是(16,24)；如果训练的FCN为12*12的输入，对于原图框位置应该是(16,24,12,12),当然这只是估计位置，具体的再构建网络时要加入回归框的预测，主要是相对于原图框的一个平移与缩放。</p>
<p>（3）通过anchor box的方式：</p>
<p>​    通过特征图映射到图的窗口，通过特征图映射到原图到多个框的方式确定最终识别为人脸的位置。</p>
<h3 id="8-4-5-如何通过一个人脸的多个框确定最终人脸框位置？"><a href="#8-4-5-如何通过一个人脸的多个框确定最终人脸框位置？" class="headerlink" title="8.4.5 如何通过一个人脸的多个框确定最终人脸框位置？"></a>8.4.5 如何通过一个人脸的多个框确定最终人脸框位置？</h3><p><img src="/img/ch8/8.4.5.png" alt=""></p>
<p>图 8.5 通过NMS得到最终的人脸位置</p>
<p>NMS改进版本有很多，最原始的NMS就是判断两个框的交集，如果交集大于设定的阈值，将删除其中一个框，那么两个框应该怎么选择删除哪一个呢？ 因为模型输出有概率值，一般会优选选择概率小的框删除。</p>
<h3 id="8-4-6-基于级联卷积神经网络的人脸检测（Cascade-CNN）"><a href="#8-4-6-基于级联卷积神经网络的人脸检测（Cascade-CNN）" class="headerlink" title="8.4.6 基于级联卷积神经网络的人脸检测（Cascade CNN）"></a>8.4.6 基于级联卷积神经网络的人脸检测（Cascade CNN）</h3><ol>
<li><p>cascade cnn的框架结构是什么？</p>
<p><img src="/img/ch8/8.4.6.png" alt=""></p>
</li>
</ol>
<p>级联结构中有6个CNN，3个CNN用于人脸非人脸二分类，另外3个CNN用于人脸区域的边框校正。给定一幅图像，12-net密集扫描整幅图片，拒绝90%以上的窗口。剩余的窗口输入到12-calibration-net中调整大小和位置，以接近真实目标。接着输入到NMS中，消除高度重叠窗口。下面网络与上面类似。</p>
<ol>
<li>cascade cnn人脸校验模块原理是什么？ </li>
</ol>
<p>该网络用于窗口校正，使用三个偏移变量：Xn:水平平移量，Yn:垂直平移量，Sn:宽高比缩放。候选框口(x,y,w,h)中，(x,y)表示左上点坐标，(w,h)表示宽和高。</p>
<p>我们要将窗口的控制坐标调整为：</p>
<script type="math/tex; mode=display">
（x-{x_nw}/{s_n},y-{y_nh}/{s_n},{w}/{s_n},{h}/{s_n}）</script><p>这项工作中，我们有$N=5×3×3=45$种模式。偏移向量三个参数包括以下值：</p>
<script type="math/tex; mode=display">
Sn：(0.83,0.91,1.0,1.10,1.21)</script><script type="math/tex; mode=display">
Xn：(-0.17,0,0.17)</script><script type="math/tex; mode=display">
Yn：(-0.17,0,0.17)</script><p>同时对偏移向量三个参数进行校正。</p>
<p><img src="/img/ch8/8.4.8.png" alt=""></p>
<p>3、训练样本应该如何准备？</p>
<p>人脸样本：</p>
<p>非人脸样本：</p>
<ol>
<li>级联的好处</li>
</ol>
<p>级联的工作原理和好处：</p>
<ul>
<li>最初阶段的网络可以比较简单，判别阈值可以设得宽松一点，这样就可以在保持较高召回率的同时排除掉大量的非人脸窗口；</li>
<li>最后阶段网络为了保证足够的性能，因此一般设计的比较复杂，但由于只需要处理前面剩下的窗口，因此可以保证足够的效率；</li>
<li>级联的思想可以帮助我们去组合利用性能较差的分类器，同时又可以获得一定的效率保证。</li>
</ul>
<h3 id="8-4-7-基于多任务卷积神经网络的人脸检测（MTCNN）"><a href="#8-4-7-基于多任务卷积神经网络的人脸检测（MTCNN）" class="headerlink" title="8.4.7 基于多任务卷积神经网络的人脸检测（MTCNN）"></a>8.4.7 基于多任务卷积神经网络的人脸检测（MTCNN）</h3><p><img src="/img/ch8/8.4.9.png" alt=""></p>
<p><img src="/img/ch8/8.4.10.png" alt=""></p>
<p><img src="/img/ch8/8.4.11.png" alt=""></p>
<p><img src="/img/ch8/8.4.12.png" alt=""></p>
<p>1.MTCNN模型有三个子网络。分别是P-Net,R-Net,O-Net.我想问一下，1.模型中的三个input size是指的是同一张图resize到不同尺度下喂给不同模型，还是同一张图，依次经过三个模型，然后是不同的输入尺寸？（这部分能给我讲一下吗）2.每个模型它都有对应三个结果（face classification;bounding box;facial landmark）这三个在网络上是如何对应的呢？</p>
<p>为了检测不同大小的人脸，开始需要构建图像金字塔，先经过pNet模型，输出人脸类别和边界框（边界框的预测为了对特征图映射到原图的框平移和缩放得到更准确的框），将识别为人脸的框映射到原图框位置可以获取patch，之后每一个patch通过resize的方式输入到rNet，识别为人脸的框并且预测更准确的人脸框，最后rNet识别为人脸的的每一个patch通过resize的方式输入到oNet，跟rNet类似，关键点是为了在训练集有限情况下使模型更鲁棒。</p>
<p>还要注意一点构建图像金字塔的的缩放比例要保留，为了将边界框映射到最开始原图上的</p>
<p>还要注意一点：如何从featureMap映射回原图</p>
<h3 id="8-4-8-Facebox"><a href="#8-4-8-Facebox" class="headerlink" title="8.4.8 Facebox"></a>8.4.8 Facebox</h3><p><img src="/img/ch8/8.4.13.png" alt=""></p>
<p><strong>（1）Rapidly Digested Convolutional Layers(RDCL)</strong></p>
<p>在网络前期，使用RDCL快速的缩小feature map的大小。 主要设计原则如下：</p>
<ul>
<li>Conv1, Pool1, Conv2 和 Pool2 的stride分别是4, 2, 2 和 2。这样整个RDCL的stride就是32，可以很快把feature map的尺寸变小。</li>
<li>卷积(或pooling)核太大速度就慢，太小覆盖信息又不足。文章权衡之后，将Conv1, Pool1, Conv2 和 Pool2 的核大小分别设为7x7,3x3,5x5,3x3</li>
<li>使用CReLU来保证输出维度不变的情况下，减少卷积核数量。</li>
</ul>
<p><strong>（2）Multiple Scale Convolutional Layers(MSCL)</strong></p>
<p>在网络后期，使用MSCL更好地检测不同尺度的人脸。 主要设计原则有：</p>
<ul>
<li>类似于SSD，在网络的不同层进行检测；</li>
<li>采用Inception模块。由于Inception包含多个不同的卷积分支，因此可以进一步使得感受野多样化。</li>
</ul>
<p><strong>（3）Anchor densification strategy</strong></p>
<p>为了anchor密度均衡，可以对密度不足的anchor以中心进行偏移加倍，如下图所示：</p>
<p><img src="/img/ch8/8.4.14.png" alt=""></p>
<h2 id="8-5-目标检测的技巧汇总"><a href="#8-5-目标检测的技巧汇总" class="headerlink" title="8.5 目标检测的技巧汇总"></a>8.5 目标检测的技巧汇总</h2><h3 id="8-5-1-Data-Augmentation（贡献者：北京理工大学—明奇）"><a href="#8-5-1-Data-Augmentation（贡献者：北京理工大学—明奇）" class="headerlink" title="8.5.1 Data Augmentation（贡献者：北京理工大学—明奇）"></a>8.5.1 Data Augmentation（贡献者：北京理工大学—明奇）</h3><p>介绍一篇发表在Big Data上的数据增强相关的文献综述。</p>
<ol>
<li><strong>Introduction</strong>  </li>
</ol>
<ul>
<li>数据增强与过拟合<br>验证是否过拟合的方法：画出loss曲线，如果训练集loss持续减小但是验证集loss增大，就说明是过拟合了。</li>
</ul>
<p><img src="/img/ch8/8.5.1-1.png" alt=""></p>
<ul>
<li><p>数据增强目的<br>通过数据增强实现数据更复杂的表征，从而减小验证集和训练集以及最终测试集的差距，让网络更好地学习迁移数据集上的数据分布。这也说明网络不是真正地理解数据，而是记忆数据分布。</p>
</li>
<li><p>数据增强的方法<br>（1）数据变换增强<br>包括几何变换、色彩空间变换，随机擦除，对抗训练，神经风格迁移等<br>（2）重采样增强<br>主要侧重于新的实例合成。如图像混合（mixup），特征空间的增强，GAN生成图片。一张图看明白：</p>
</li>
</ul>
<p><img src="/img/ch8/8.5.1-2.png" alt=""></p>
<ol>
<li><strong>Image Data Augmentation techniques</strong></li>
</ol>
<p>2.1 <strong>Data Augmentations based on basic image manipulations</strong>  </p>
<ul>
<li><p>Geometric transformations<br>&emsp;&emsp;如果数据集潜在的表征能够被观察和分离，那么简单的几何变换就能取得很好的效果。对于复杂的数据集如医学影像，数据小而且训练集和测试集的偏差大，几何变换等增强的合理运用就很关键。</p>
<ul>
<li><p>Flipping<br>作者提到了要衡量普遍性的观点。但是这种变换对于数字数据集不具有安全性。</p>
</li>
<li><p>Color space<br>主要提及的识别RGB通道上的变换，将三通道图进行分离，以及直方图变换增强等。（颜色空间更多增强方式可以参考A Preliminary Study on Data Augmentation of Deep Learning for Image Classification）</p>
</li>
<li><p>Cropping<br>通常在输入图片的尺寸不一时会进行按中心的裁剪操作。裁剪某种程度上和平移操作有相似性。根据裁剪幅度变化，该操作具有一定的不安全性。</p>
</li>
<li><p>Rotation<br>大幅度的旋转对数字集会有不安全性的考虑。</p>
</li>
<li><p>Translation<br>平移也需要合理设计。如车站人脸检测，只需要中心检测时，就可以加合适的平移增强。平移后空出部分填0或者255，或用高斯分布噪声。</p>
</li>
<li><p>Noise injection<br>在像素上叠加高斯分布的随机噪声。</p>
</li>
</ul>
</li>
<li><p>Color space transformations<br>&emsp;&emsp;由于实际图像中一定存在光线偏差，所以光线的增强十分有必要（但是IJCV的光流文章指出，3D建模的灯光增强实在是很难学习到，所以对于光线增强的效果不如几何也可能因为<strong>光线的复杂度更高，数据样本远远不够</strong>）。色彩变换十分多样，如像素限制、像素矩阵变换、像素值颠倒等；灰度图和彩图相比，计算时间成本大大较少，但是据实验效果会下降一些，很明显因为特征的维度被降维了；还有尝试将RGB映射到其他的色彩空间进行学习，YUV,CMY.HSV等。<br>&emsp;&emsp;除了计算大内存消耗和时间长等缺点，色彩变换也面临不安全性，比如识别人脸的关键信息是黄白黑，但是大量增强出红绿蓝，会丢信息。颜色变换的增强方法是从色彩空间角度拟合偏置，效果有限的可能性是多样的：1. 真实几何多样性比颜色更简单  2. 色彩的变化多样性更多，导致增强不够反而学不好，颜色空间的欠拟合 3. <strong>变换不安全</strong></p>
</li>
</ul>
<ul>
<li>Experiment<br><img src="/img/ch8/8.5.1-3.png" alt=""></li>
</ul>
<p><strong>随机裁剪</strong>效果最好。        </p>
<p>2.2  <strong>Geometric versus photometric transformations</strong> </p>
<ul>
<li><p>Kernel filter<br>滤波器核在图像处理用的比较广，这里提到用这种方法来增强。还提到了一种正则化增强方法PatchShuffle，在一个patch内随机交换像素值，使得对噪声的抵抗更强以及避免过拟合。<br>文章指出关于应用滤波器增强的工作尚且不多，因为这种方法其实和CNN的机制是一样的，这么做也许还不如直接在原始CNN上加层加深网络。</p>
</li>
<li><p>Mixing images<br><del>就是那篇被ICLR拒稿的采样方法</del>直接均值相加混合。  </p>
</li>
</ul>
<p><img src="/img/ch8/8.5.1-4.png" alt=""></p>
<p>&emsp;&emsp;还有非线性的mixup裁剪如下：  </p>
<p><img src="/img/ch8/8.5.1-5.png" alt=""></p>
<p>&emsp;&emsp;以及随机裁剪的图像混合：  </p>
<p><img src="/img/ch8/8.5.1-6.png" alt=""></p>
<p>&emsp;&emsp;这些混合方式是十分反人类直觉的，因此可解释性不强。只能说是可能增强了对底层低级特征如线条边缘等的鲁棒性。其实有点没有抓住关键点。</p>
<ul>
<li>Random erasing<br>随机擦除就是类似cutout的思想，通过mask的遮挡使得网络能够提高遮挡情况的鲁棒性。需要手工设计的部分包括mask的大小以及生成方式。是一种比较有效的方法。这种方式也需要考量增强的安全性，比如MNIST数据集8cutout后可能出问题。</li>
</ul>
<p><img src="/img/ch8/8.5.1-7.png" alt=""></p>
<ul>
<li>A note on combining augmentations<br>组合的增强方式往往是连续变化的，导致数据集的容量会迅速扩大，这对于小数据集领域来说容易发生过拟合 ，所以需要设计合理的搜索算法设计恰当的训练数据集。        </li>
</ul>
<p>2.3  <strong>Data Augmentations based on Deep Learning</strong></p>
<ul>
<li><p>Feature space augmentation<br>之前刚看的基于SMOTE类别不平衡的过采样法来进行特征空间的插值操作进行数据增强，就实验效果而言不算特别出众。</p>
</li>
<li><p>Adversarial training<br>对抗样本训练可以提高鲁棒性，但是实际应用中其实提高不一定明显，因为自然对抗样本的数目没有那么多。而NIPS的对抗攻击大赛很多从神经网络的学习策略下手，进行梯度攻击，更加偏向于人为的攻击了，对于普适的检测性能提高意义反而不大，更强调安全需求高的场合。</p>
</li>
<li><p>GAN‑based Data Augmentation</p>
</li>
<li><p>Neural Style Transfer</p>
</li>
</ul>
<p>不觉得这个效果会普遍很好，应该来说是针对特定域会有效（如白天黑夜），实际效果应该有限。</p>
<ul>
<li>Meta learning Data Augmentations <ul>
<li>Neural augmentation</li>
<li>Smart Augmentation<br>两个东西差不多，就是上次看到SmartAugment方法。随机采样类内图片进行通道叠加然后输出融合图像，学通过梯度下降使得输出图像的类内差距减小（没考虑类间关系，可能也不便处理）。</li>
</ul>
</li>
</ul>
<p><img src="/img/ch8/8.5.1-8.png" alt=""></p>
<ul>
<li>AutoAugment<br>谷歌最早做的自学习增强方法，走的NAS的思路RL+RNN搜索增强空间，还有后来最近发的检测增强也是大同小异，基本就是换汤不换药，问题在于<strong>搜索空间太大</strong>，复现搜索过于依赖硬件条件（<del>普通实验室玩不起</del>）</li>
</ul>
<ol>
<li><strong>Design considerations for image Data Augmentation</strong></li>
</ol>
<p>3.1  <strong>Test-time augmentation</strong><br>&emsp;&emsp;许多都论文指出在检测阶段进行同等的数据增强能够获得较好的效果。归结可以认为是训练检测阶段的一致性。当然，这种手段时间成本太高，只在如医学影像等追求精度的关键领域可以使用。        </p>
<p>3.2  <strong>Curriculum learning</strong><br>&emsp;&emsp;Bengio团队早年在ICML提出的观点，确实合理，一开始就进行大量的增强容易导致网络不收敛。<br>从一个数据集学习到的数据增强也可以迁移到其他数据集。</p>
<p>3.3  <strong>Resolution impact</strong><br>高清（1920×1080×3）或4K（3840×2160×3）等高分辨率图像需要更多的处理和内存来训练深度CNN。然而下一代模型更倾向于使用这样更高分辨率的图像。因为模型中常用的下采样会造成图像中信息的丢失，使图像识别更困难。<br>研究人员发现，高分辨率图像和低分辨率图像一起训练的模型集合，比单独的任何一个模型都要好。<br>某个实验（这里就不注明引用了）在256×256图像和512×512图像上训练的模型分别获得7.96%和7.42%的top-5 error。汇总后，他们的top-5 error变低，为6.97%。<br>随着超分辨率网络的发展，将图像放大到更高的分辨率后训练模型，能够得到更好更健壮的图像分类器。</p>
<p>3.4  <strong>Final dataset size</strong><br>&emsp;&emsp;数据增强的形式可以分为在线和离线增强。前者是在加载数据时增强，可能造成额外的内存消耗（现在都是数据容量不变的随机增强）。<br>&emsp;&emsp;此外作者提到了一个比较有意思的点：当前数据集尤其是进行增广后是十分庞大的，明显能够在一定程度上缩小数据集但是保持性能下降不多的子集效率会高得多。</p>
<p>3.5 <strong>Alleviating class imbalance with Data Augmentation</strong><br>&emsp;&emsp;这也是值得借鉴的一点。通过增强在一定程度上解决类别不平衡问题。但增强需要仔细设计，否则会面对已经学习较好的类别或者场景造成过拟合等问题。</p>
<h3 id="8-5-2-OHEM"><a href="#8-5-2-OHEM" class="headerlink" title="8.5.2  OHEM"></a>8.5.2  OHEM</h3><h3 id="8-5-3-NMS：Soft-NMS-Polygon-NMS-Inclined-NMS-ConvNMS-Yes-Net-NMS-Softer-NMS"><a href="#8-5-3-NMS：Soft-NMS-Polygon-NMS-Inclined-NMS-ConvNMS-Yes-Net-NMS-Softer-NMS" class="headerlink" title="8.5.3  NMS：Soft NMS/ Polygon NMS/ Inclined NMS/ ConvNMS/ Yes-Net NMS/ Softer NMS"></a>8.5.3  NMS：Soft NMS/ Polygon NMS/ Inclined NMS/ ConvNMS/ Yes-Net NMS/ Softer NMS</h3><h3 id="8-5-4-Multi-Scale-Training-Testing"><a href="#8-5-4-Multi-Scale-Training-Testing" class="headerlink" title="8.5.4  Multi Scale Training/Testing"></a>8.5.4  Multi Scale Training/Testing</h3><h3 id="8-5-5-建立小物体与context的关系"><a href="#8-5-5-建立小物体与context的关系" class="headerlink" title="8.5.5  建立小物体与context的关系"></a>8.5.5  建立小物体与context的关系</h3><h3 id="8-5-6-参考relation-network"><a href="#8-5-6-参考relation-network" class="headerlink" title="8.5.6  参考relation network"></a>8.5.6  参考relation network</h3><h3 id="8-5-7-结合GAN"><a href="#8-5-7-结合GAN" class="headerlink" title="8.5.7  结合GAN"></a>8.5.7  结合GAN</h3><h3 id="8-5-8-结合attention"><a href="#8-5-8-结合attention" class="headerlink" title="8.5.8  结合attention"></a>8.5.8  结合attention</h3><h3 id="8-5-9-训练tricks（贡献者：北京理工大学—明奇）"><a href="#8-5-9-训练tricks（贡献者：北京理工大学—明奇）" class="headerlink" title="8.5.9  训练tricks（贡献者：北京理工大学—明奇）"></a>8.5.9  训练tricks（贡献者：北京理工大学—明奇）</h3><p>介绍一篇2019.2.4亚马逊挂在ArXiv的目标检测训练tricks的文章（之前亚马逊发了篇分类的tricks在CVPR上）</p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>&emsp;&emsp;上次亚马逊发了个分类的训练trick在CVPR上，这次是检测的，还没发表。就没什么多说的了，下面直接介绍。先看效果如下，其实摘要声称的5%是单阶段的yolov3的提升，说明：单阶段没有RoIPooling阶段很多性质确实不如两阶段，因此采用trick很有必要；相反，两阶段本身结构优于单阶段所以外加的trick提供的如不变性等网络自身能够学习和适应就不起作用了。 </p>
<p><img src="/img/ch8/8.5.9-1.png" alt=""></p>
<ol>
<li><strong>Bag of Freebies</strong> </li>
</ol>
<p>&emsp;&emsp;提出了一种基于mixup的视觉联系图像混合方法，以及一些数据处理和训练策略。        </p>
<p>2.1  <strong>Visually Coherent Image Mixup for Object Detection</strong><br>&emsp;&emsp;先介绍图像分类中的mixup方法，作用是提供了训练的正则化，应用到图像上如下图，将图像作简单的像素值输入mixup的凸函数中得到合成图；然后将one-hot编码类似处理得到新的label。       </p>
<p><img src="/img/ch8/8.5.9-2.png" alt=""></p>
<p>&emsp;&emsp;技术细节：         </p>
<ul>
<li>相比于分类的resize，为了保证检测图像不畸变影响效果，作者选择直接叠加，取最大的宽高，空白进行灰度填充，不进行缩放。        </li>
<li>选择ab较大（如1.5,1.5）的Beta分布作为系数来混合图像，作者说是相干性视觉图像的更强；loss是两张图像物体的loss之和，loss计算权重分别是beta分布的系数 </li>
</ul>
<p><img src="/img/ch8/8.5.9-3.png" alt=""></p>
<p>2.2  <strong>Classification Head Label Smoothing</strong><br>&emsp;&emsp;标签平滑在检测的分类任务常有用到，最早是Inceptionv2中提出。<br>&emsp;&emsp;如果标签中有的是错的，或者不准，会导致网络过分信任标签而一起错下去。为了提高网络泛化能力，避免这种错误，在one-hot的label进行计算loss时，真实类别位置乘以一个系数（1-e），e很小如0.05，以0.95的概率送进去；非标注的类别原来为0，现在改为e=0.05送进去计算loss。网络的优化方向不变，但是相比0-1label会更加平滑。<br>（标签平滑这个讲的不错：<a href="https://juejin.im/post/5a29fd4051882534af25dc92）" target="_blank" rel="noopener">https://juejin.im/post/5a29fd4051882534af25dc92）</a></p>
<p><img src="/img/ch8/8.5.9-4.png" alt=""></p>
<p>&emsp;&emsp;这里进一步改进了一下label smooth的公式而已，在原来基础上除了个类别数。        </p>
<p>2.3  <strong>Data Preprocessing</strong><br>&emsp;&emsp;就是数据增强，没什么其他的。至于分类也是几何变换和色彩变换。这么分区别其实是是否变换label。但是将真实世界就这么简单地分解过于粗糙了。好不容易谷歌的增强考虑到了如何学习一下检测任务的增强，但是也只是加了bbox_only的增强，就效果而言，一般；而且就实际来说，合理性和有效性有待商榷。<br>&emsp;&emsp;作者认为，两阶段网络的RPN生成就是对输入的任意裁剪，所以这个增强就够了；这老哥膨胀了，two-stage就不用裁剪的增强，虽然两阶段能提供一些不变性，但是用了一般来说都是更好的。 </p>
<p>2.4  <strong>Training Schedule Revamping</strong><br>训练策略上：余弦学习率调整+warmup      </p>
<p>2.5  <strong>Synchronized Batch Normalization</strong><br>跨多卡同步正则化，土豪专区，穷人退避     </p>
<p>2.6  <strong>Random shapes training for single-stage object detection networks</strong><br>多尺度训练，每经过一定的iteration更换一种尺度。举例是yolov3的尺度范围。</p>
<h2 id="8-6-目标检测的常用数据集"><a href="#8-6-目标检测的常用数据集" class="headerlink" title="8.6 目标检测的常用数据集"></a>8.6 目标检测的常用数据集</h2><h3 id="8-6-1-PASCAL-VOC"><a href="#8-6-1-PASCAL-VOC" class="headerlink" title="8.6.1 PASCAL VOC"></a>8.6.1 PASCAL VOC</h3><p>​    VOC数据集是目标检测经常用的一个数据集，自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的train/val 2007和16k的train/val 2012作为训练集，test 2007作为测试集，用10k的train/val 2007+test 2007和16k的train/val 2012作为训练集，test2012作为测试集，分别汇报结果。</p>
<h3 id="8-6-2-MS-COCO"><a href="#8-6-2-MS-COCO" class="headerlink" title="8.6.2 MS COCO"></a>8.6.2 MS COCO</h3><p>​    COCO数据集是微软团队发布的一个可以用来图像recognition+segmentation+captioning 数据集，该数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。</p>
<p>相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。</p>
<p>​    COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p>
<h3 id="8-6-3-Google-Open-Image"><a href="#8-6-3-Google-Open-Image" class="headerlink" title="8.6.3 Google Open Image"></a>8.6.3 Google Open Image</h3><p>​    Open Image是谷歌团队发布的数据集。最新发布的Open Images V4包含190万图像、600个种类，1540万个bounding-box标注，是当前最大的带物体位置标注信息的数据集。这些边界框大部分都是由专业注释人员手动绘制的，确保了它们的准确性和一致性。另外，这些图像是非常多样化的，并且通常包含有多个对象的复杂场景（平均每个图像 8 个）。</p>
<h3 id="8-6-4-ImageNet"><a href="#8-6-4-ImageNet" class="headerlink" title="8.6.4 ImageNet"></a>8.6.4 ImageNet</h3><p>​    ImageNet是一个计算机视觉系统识别项目， 是目前世界上图像识别最大的数据库。ImageNet是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。Imagenet数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。</p>
<h3 id="8-6-5-DOTA"><a href="#8-6-5-DOTA" class="headerlink" title="8.6.5 DOTA"></a>8.6.5 DOTA</h3><p>​    DOTA是遥感航空图像检测的常用数据集，包含2806张航空图像，尺寸大约为4kx4k，包含15个类别共计188282个实例，其中14个主类，small vehicle 和 large vehicle都是vehicle的子类。其标注方式为四点确定的任意形状和方向的四边形。航空图像区别于传统数据集，有其自己的特点，如：尺度变化性更大；密集的小物体检测；检测目标的不确定性。数据划分为1/6验证集，1/3测试集，1/2训练集。目前发布了训练集和验证集，图像尺寸从800x800到4000x4000不等。</p>
<h2 id="8-7-目标检测常用标注工具"><a href="#8-7-目标检测常用标注工具" class="headerlink" title="8.7 目标检测常用标注工具"></a>8.7 目标检测常用标注工具</h2><h3 id="8-7-1-LabelImg"><a href="#8-7-1-LabelImg" class="headerlink" title="8.7.1 LabelImg"></a>8.7.1 LabelImg</h3><p>​    LabelImg 是一款开源的图像标注工具，标签可用于分类和目标检测，它是用 Python 编写的，并使用Qt作为其图形界面，简单好用。注释以 PASCAL VOC 格式保存为 XML 文件，这是 ImageNet 使用的格式。 此外，它还支持 COCO 数据集格式。</p>
<h3 id="8-7-2-labelme"><a href="#8-7-2-labelme" class="headerlink" title="8.7.2 labelme"></a>8.7.2 labelme</h3><p>​    labelme 是一款开源的图像/视频标注工具，标签可用于目标检测、分割和分类。灵感是来自于 MIT 开源的一款标注工具 LabelMe。labelme 具有的特点是：</p>
<ul>
<li>支持图像的标注的组件有：矩形框，多边形，圆，线，点（rectangle, polygons, circle, lines, points）</li>
<li>支持视频标注</li>
<li>GUI 自定义</li>
<li>支持导出 VOC 格式用于 semantic/instance segmentation</li>
<li>支出导出 COCO 格式用于 instance segmentation</li>
</ul>
<h3 id="8-7-3-Labelbox"><a href="#8-7-3-Labelbox" class="headerlink" title="8.7.3 Labelbox"></a>8.7.3 Labelbox</h3><p>​    Labelbox 是一家为机器学习应用程序创建、管理和维护数据集的服务提供商，其中包含一款部分免费的数据标签工具，包含图像分类和分割，文本，音频和视频注释的接口，其中图像视频标注具有的功能如下：</p>
<ul>
<li>可用于标注的组件有：矩形框，多边形，线，点，画笔，超像素等（bounding box, polygons, lines, points，brush, subpixels）</li>
<li>标签可用于分类，分割，目标检测等</li>
<li>以 JSON / CSV / WKT / COCO / Pascal VOC 等格式导出数据</li>
<li>支持 Tiled Imagery (Maps)</li>
<li>支持视频标注 （快要更新）</li>
</ul>
<h3 id="8-7-4-RectLabel"><a href="#8-7-4-RectLabel" class="headerlink" title="8.7.4 RectLabel"></a>8.7.4 RectLabel</h3><p>​    RectLabel 是一款在线免费图像标注工具，标签可用于目标检测、分割和分类。具有的功能或特点：</p>
<ul>
<li>可用的组件：矩形框，多边形，三次贝塞尔曲线，直线和点，画笔，超像素</li>
<li>可只标记整张图像而不绘制</li>
<li>可使用画笔和超像素</li>
<li>导出为YOLO，KITTI，COCO JSON和CSV格式</li>
<li>以PASCAL VOC XML格式读写</li>
<li>使用Core ML模型自动标记图像</li>
<li>将视频转换为图像帧</li>
</ul>
<h3 id="8-7-5-CVAT"><a href="#8-7-5-CVAT" class="headerlink" title="8.7.5 CVAT"></a>8.7.5 CVAT</h3><p>​    CVAT 是一款开源的基于网络的交互式视频/图像标注工具，是对加州视频标注工具（Video Annotation Tool） 项目的重新设计和实现。OpenCV团队正在使用该工具来标注不同属性的数百万个对象，许多 UI 和 UX 的决策都基于专业数据标注团队的反馈。具有的功能</p>
<ul>
<li>关键帧之间的边界框插值</li>
<li>自动标注（使用TensorFlow OD API 和 Intel OpenVINO IR格式的深度学习模型）</li>
</ul>
<h3 id="8-7-6-VIA"><a href="#8-7-6-VIA" class="headerlink" title="8.7.6 VIA"></a>8.7.6 VIA</h3><p>​    VGG Image Annotator（VIA）是一款简单独立的手动注释软件，适用于图像，音频和视频。 VIA 在 Web 浏览器中运行，不需要任何安装或设置。 页面可在大多数现代Web浏览器中作为离线应用程序运行。</p>
<ul>
<li>支持标注的区域组件有：矩形，圆形，椭圆形，多边形，点和折线</li>
</ul>
<h3 id="8-7-6-其他标注工具"><a href="#8-7-6-其他标注工具" class="headerlink" title="8.7.6 其他标注工具"></a>8.7.6 其他标注工具</h3><p>​    liblabel，一个用 MATLAB 写的轻量级 语义/示例(semantic/instance) 标注工具。<br>ImageTagger：一个开源的图像标注平台。<br>Anno-Mage：一个利用深度学习模型半自动图像标注工具，预训练模型是基于MS COCO数据集，用 RetinaNet 训练的。</p>
<p>&lt;/br&gt;<br>​    当然还有一些数据标注公司，可能包含更多标注功能，例如对三维目标检测的标注（3D Bounding box Labelling），激光雷达点云的标注（LIDAR 3D Point Cloud Labeling）等。</p>
<h2 id="8-8-目标检测工具和框架（贡献者：北京理工大学—明奇）"><a href="#8-8-目标检测工具和框架（贡献者：北京理工大学—明奇）" class="headerlink" title="8.8 目标检测工具和框架（贡献者：北京理工大学—明奇）"></a>8.8 目标检测工具和框架（贡献者：北京理工大学—明奇）</h2><p>各种不同的算法虽然部分官方会有公布代码，或者github上有人复现，但是囿于安装环境不一，实现的框架（pytorch、C++、Caffee、tensorflow、MXNet等）不同，每次更换算法都需要重新安装环境，并且代码之间的迁移性差，十分不方便。所以为了方便将不同的算法统一在一个代码库中，不同的大厂都提出了自己的解决方案。如facebook的Detectron、商汤科技的mmdetection、SimpleDet等。其中Detectron最早，所以用户量最大，其次是国内近段时间崛起的mmdetection，下面介绍该目标检测工具箱。</p>
<ol>
<li><strong>Introduction</strong><br>MMdetection的特点：</li>
</ol>
<ul>
<li>模块化设计：将不同网络的部分进行切割，模块之间具有很高的复用性和独立性（十分便利，可以任意组合）</li>
<li>高效的内存使用</li>
<li>SOTA</li>
</ul>
<ol>
<li><strong>Support Frameworks</strong>  </li>
</ol>
<ul>
<li><p>单阶段检测器<br>SSD、RetinaNet、FCOS、FSAF</p>
</li>
<li><p>两阶段检测器<br>Faster R-CNN、R-FCN、Mask R-CNN、Mask Scoring R-CNN、Grid R-CNN</p>
</li>
<li><p>多阶段检测器<br>Cascade R-CNN、Hybrid Task Cascade</p>
</li>
<li><p>通用模块和方法<br>soft-NMS、DCN、OHEN、Train from Scratch 、M2Det 、GN 、HRNet 、Libra R-CNN</p>
</li>
</ul>
<ol>
<li><strong>Architecture</strong></li>
</ol>
<p>模型表征：划分为以下几个模块：<br>Backbone（ResNet等）、Neck（FPN）、DenseHead（AnchorHead）、RoIExtractor、RoIHead（BBoxHead/MaskHead）<br>结构图如下：<br><img src="/img/ch8/mmdetection.png" alt=""></p>
<ol>
<li><strong>Notice</strong></li>
</ol>
<ul>
<li>1x代表12epoch的COCO训练，2x类似推导</li>
<li>由于batch-size一般比较小（1/2这样的量级），所以大多数地方默认冻结BN层。可以使用GN代替。</li>
</ul>
<ol>
<li><strong>参考链接</strong><br>mmdetection代码高度模块化，十分好用和便利，更详细的文档直接参见官方文档：<br><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener">https://github.com/open-mmlab/mmdetection</a></li>
</ol>
<p>注释版的mmdetection代码（更新至v1.0.0）：<a href="https://github.com/ming71/mmdetection-annotated" target="_blank" rel="noopener">https://github.com/ming71/mmdetection-annotated</a></p>
<p>使用方法简介：<br>安装记录（可能过时，以官方文档为准）：<a href="https://ming71.github.io/mmdetection-memo.html" target="_blank" rel="noopener">https://ming71.github.io/mmdetection-memo.html</a><br>使用方法（截止更新日期，如果过时以官方为准）：<a href="https://ming71.github.io/mmdetection-instruction.html" target="_blank" rel="noopener">https://ming71.github.io/mmdetection-instruction.html</a></p>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul>
<li>[ ] 目标检测基础知识：mAP、IoU和NMS等</li>
<li>[ ] 目标检测评测指标</li>
<li>[ ] 目标检测常见标注工具</li>
<li>[ ] 完善目标检测的技巧汇总</li>
<li>[ ] 目标检测的现在难点和未来发展</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">https://github.com/amusi/awesome-object-detection</a></p>
<p><a href="https://github.com/hoya012/deep_learning_object_detection" target="_blank" rel="noopener">https://github.com/hoya012/deep_learning_object_detection</a></p>
<p><a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html" target="_blank" rel="noopener">https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html</a></p>
<p><a href="https://www.zhihu.com/question/272322209/answer/482922713" target="_blank" rel="noopener">https://www.zhihu.com/question/272322209/answer/482922713</a></p>
<p><a href="http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b" target="_blank" rel="noopener">http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b</a></p>
<p><a href="https://blog.csdn.net/hw5226349/article/details/78987385" target="_blank" rel="noopener">https://blog.csdn.net/hw5226349/article/details/78987385</a></p>
<p>[1] Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 580-587.</p>
<p>[2] Girshick R. Fast r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448.</p>
<p>[3] He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2015, 37(9): 1904-1916.</p>
<p>[4] Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99.</p>
<p>[5] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2117-2125.</p>
<p>[6] He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969.</p>
<p>[7] Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector[C]//European conference on computer vision. Springer, Cham, 2016: 21-37.</p>
<p>[8] Fu C Y, Liu W, Ranga A, et al. Dssd: Deconvolutional single shot detector[J]. arXiv preprint arXiv:1701.06659, 2017.</p>
<p>[9] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 779-788.</p>
<p>[10] Redmon J, Farhadi A. YOLO9000: better, faster, stronger[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7263-7271.</p>
<p>[11] Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. arXiv preprint arXiv:1804.02767, 2018.</p>
<p>[12] Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988.</p>
<p>[13] Liu S, Huang D. Receptive field block net for accurate and fast object detection[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 385-400.</p>
<p>[14] Zhao Q, Sheng T, Wang Y, et al. M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network[J]. arXiv preprint arXiv:1811.04533, 2018.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%85%AD%E7%AB%A0_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(RNN)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%85%AD%E7%AB%A0_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(RNN)/" class="post-title-link" itemprop="url">第六章_循环神经网络(RNN)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:25:17 / 修改时间：12:25:36" itemprop="dateCreated datePublished" datetime="2020-03-03T12:25:17+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第六章-循环神经网络-RNN"><a href="#第六章-循环神经网络-RNN" class="headerlink" title="第六章 循环神经网络(RNN)"></a>第六章 循环神经网络(RNN)</h1><h2 id="6-1-为什么需要RNN？"><a href="#6-1-为什么需要RNN？" class="headerlink" title="6.1 为什么需要RNN？"></a>6.1 为什么需要RNN？</h2><p>​    时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的x，就能得到期望y。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如：</p>
<p>​    当我们在理解一句话意思时，孤立的理解这句话的每个词不足以理解整体意思，我们通常需要处理这些词连接起来的整个序列； 当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就由此诞生了。</p>
<h2 id="6-2-图解RNN基本结构"><a href="#6-2-图解RNN基本结构" class="headerlink" title="6.2 图解RNN基本结构"></a>6.2 图解RNN基本结构</h2><h3 id="6-2-1-基本的单层网络结构"><a href="#6-2-1-基本的单层网络结构" class="headerlink" title="6.2.1 基本的单层网络结构"></a>6.2.1 基本的单层网络结构</h3><p>​    在进一步了解RNN之前，先给出最基本的单层网络结构，输入是<code>$x$</code>，经过变换<code>Wx+b</code>和激活函数<code>f</code>得到输出<code>y</code>：</p>
<p><img src="/img/ch6/6.1.jpg" alt=""></p>
<h3 id="6-2-2-图解经典RNN结构"><a href="#6-2-2-图解经典RNN结构" class="headerlink" title="6.2.2 图解经典RNN结构"></a>6.2.2 图解经典RNN结构</h3><p>​    在实际应用中，我们还会遇到很多序列形的数据，如：</p>
<ul>
<li><p>自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。</p>
</li>
<li><p>语音处理。此时，x1、x2、x3……是每帧的声音信号。</p>
</li>
<li><p>时间序列问题。例如每天的股票价格等等。</p>
<p>其单个序列如下图所示：</p>
<p><img src="/img/ch6/6.2.jpg" alt=""></p>
<p>前面介绍了诸如此类的序列数据用原始的神经网络难以建模，基于此，RNN引入了隐状态$h$（hidden state），$h​$可对序列数据提取特征，接着再转换为输出。</p>
<p>为了便于理解，先计算$h_1​$：</p>
<p><img src="/img/ch6/6.3.jpg" alt=""></p>
<p>注：图中的圆圈表示向量，箭头表示对向量做变换。</p>
<p>RNN中，每个步骤使用的参数<code>$U,W,b$</code>​相同，<code>$h_2$</code>的计算方式和<code>$h_1​$</code>类似，其计算结果如下：</p>
<p><img src="/img/ch6/6.4.jpg" alt=""></p>
<p>计算$h_3$,$h_4​$也相似，可得：</p>
<p><img src="/img/ch6/6.5.jpg" alt=""></p>
<p>接下来，计算RNN的输出$y_1$，采用$Softmax$作为激活函数，根据$y_n=f(Wx+b)$，得$y_1​$:</p>
<p><img src="/img/ch6/6.6.jpg" alt=""></p>
<p>使用和$y_1​$相同的参数$V,c​$，得到$y_1,y_2,y_3,y_4​$的输出结构：</p>
<p><img src="/img/ch6/6.7.jpg" alt=""></p>
<p>以上即为最经典的RNN结构，其输入为$x_1,x_2,x_3,x_4$，输出为$y_1,y_2,y_3,y_4$，当然实际中最大值为$y_n$，这里为了便于理解和展示，只计算4个输入和输出。从以上结构可看出，RNN结构的输入和输出等长。</p>
</li>
</ul>
<h3 id="6-2-3-vector-to-sequence结构"><a href="#6-2-3-vector-to-sequence结构" class="headerlink" title="6.2.3 vector-to-sequence结构"></a>6.2.3 vector-to-sequence结构</h3><p>​    有时我们要处理的问题输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>​    方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="/img/ch6/6.9.jpg" alt=""></p>
<p>​    方式二：把输入信息X作为每个阶段的输入，其建模方式如下：</p>
<p><img src="/img/ch6/6.10.jpg" alt=""></p>
<h3 id="6-2-4-sequence-to-vector结构"><a href="#6-2-4-sequence-to-vector结构" class="headerlink" title="6.2.4 sequence-to-vector结构"></a>6.2.4 sequence-to-vector结构</h3><p>​    有时我们要处理的问题输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p>  <img src="/img/ch6/6.8.jpg" alt=""></p>
<h3 id="6-2-5-Encoder-Decoder结构"><a href="#6-2-5-Encoder-Decoder结构" class="headerlink" title="6.2.5 Encoder-Decoder结构"></a>6.2.5 Encoder-Decoder结构</h3><p>​    原始的sequence-to-sequence结构的RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p>
<p>​    其建模步骤如下：</p>
<p>​    <strong>步骤一</strong>：将输入数据编码成一个上下文向量$c$，这部分称为Encoder，得到$c$有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给$c$，还可以对最后的隐状态做一个变换得到$c$，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p>  <img src="/img/ch6/6.12.jpg" alt=""></p>
<p>​    <strong>步骤二</strong>：用另一个RNN网络（我们将其称为Decoder）对其进行编码，方法一是将步骤一中的$c​$作为初始状态输入到Decoder，示意图如下所示：</p>
<p>  <img src="/img/ch6/6.13.jpg" alt=""></p>
<p>方法二是将$c$作为Decoder的每一步输入，示意图如下所示：</p>
<p>  <img src="/img/ch6/6.14.jpg" alt=""></p>
<h3 id="6-2-6-以上三种结构各有怎样的应用场景"><a href="#6-2-6-以上三种结构各有怎样的应用场景" class="headerlink" title="6.2.6  以上三种结构各有怎样的应用场景"></a>6.2.6  以上三种结构各有怎样的应用场景</h3><div class="table-container">
<table>
<thead>
<tr>
<th>网络结构</th>
<th style="text-align:center">结构图示</th>
<th>应用场景举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 vs N</td>
<td style="text-align:center"><img src="/img/ch6/6.9.jpg" alt=""></td>
<td>1、从图像生成文字，输入为图像的特征，输出为一段句子<br />2、根据图像生成语音或音乐，输入为图像特征，输出为一段语音或音乐</td>
</tr>
<tr>
<td>N vs 1</td>
<td style="text-align:center"><img src="/img/ch6/6.8.jpg" alt=""></td>
<td>1、输出一段文字，判断其所属类别<br />2、输入一个句子，判断其情感倾向<br />3、输入一段视频，判断其所属类别</td>
</tr>
<tr>
<td>N vs M</td>
<td style="text-align:center"><img src="/img/ch6/6.13.jpg" alt=""></td>
<td>1、机器翻译，输入一种语言文本序列，输出另外一种语言的文本序列<br />2、文本摘要，输入文本序列，输出这段文本序列摘要<br />3、阅读理解，输入文章，输出问题答案<br />4、语音识别，输入语音序列信息，输出文字序列</td>
</tr>
</tbody>
</table>
</div>
<h3 id="6-2-7-图解RNN中的Attention机制"><a href="#6-2-7-图解RNN中的Attention机制" class="headerlink" title="6.2.7 图解RNN中的Attention机制"></a>6.2.7 图解RNN中的Attention机制</h3><p>​    在上述通用的Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征$c​$再解码，因此，$c​$中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个$c​$可能存不下那么多信息，就会造成翻译精度的下降。Attention机制通过在每个时间输入不同的$c​$来解决此问题。</p>
<p>​    引入了Attention机制的Decoder中，有不同的$c$，每个$c​$会自动选择与当前输出最匹配的上下文信息，其示意图如下所示：</p>
<p><img src="/img/ch6/6.15.jpg" alt=""></p>
<p>​    <strong>举例</strong>，比如输入序列是“我爱中国”，要将此输入翻译成英文：</p>
<p>​    假如用$a_{ij}$衡量Encoder中第$j$阶段的$h_j$和解码时第$i$阶段的相关性，$a_{ij}$从模型中学习得到，和Decoder的第$i-1$阶段的隐状态、Encoder 第$j$个阶段的隐状态有关，比如$a_{3j}​$的计算示意如下所示：</p>
<p><img src="/img/ch6/6.19.jpg" alt=""></p>
<p>最终Decoder中第$i$阶段的输入的上下文信息 $c_i$来自于所有$h_j$对$a_{ij}$的加权和。</p>
<p>其示意图如下图所示：</p>
<p><img src="/img/ch6/6.16.jpg" alt=""></p>
<p>​    在Encoder中，$h_1,h_2,h_3,h_4$分别代表“我”，“爱”，“中”，“国”所代表信息。翻译的过程中，$c_1$会选择和“我”最相关的上下午信息，如上图所示，会优先选择$a_{11}$，以此类推，$c_2$会优先选择相关性较大的$a_{22}$，$c_3$会优先选择相关性较大的$a_{33}，a_{34}$，这就是attention机制。</p>
<h2 id="6-3-RNNs典型特点？"><a href="#6-3-RNNs典型特点？" class="headerlink" title="6.3 RNNs典型特点？"></a>6.3 RNNs典型特点？</h2><ol>
<li>RNNs主要用于处理序列数据。对于传统神经网络模型，从输入层到隐含层再到输出层，层与层之间一般为全连接，每层之间神经元是无连接的。但是传统神经网络无法处理数据间的前后关联问题。例如，为了预测句子的下一个单词，一般需要该词之前的语义信息。这是因为一个句子中前后单词是存在语义联系的。</li>
<li>RNNs中当前单元的输出与之前步骤输出也有关，因此称之为循环神经网络。具体的表现形式为当前单元会对之前步骤信息进行储存并应用于当前输出的计算中。隐藏层之间的节点连接起来，隐藏层当前输出由当前时刻输入向量和之前时刻隐藏层状态共同决定。</li>
<li>标准的RNNs结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。</li>
<li>在标准的RNN结构中，隐层的神经元之间也是带有权值的，且权值共享。</li>
<li>理论上，RNNs能够对任何长度序列数据进行处理。但是在实践中，为了降低复杂度往往假设当前的状态只与之前某几个时刻状态相关，<strong>下图便是一个典型的RNNs</strong>：</li>
</ol>
<p><img src="/img/ch6/figure_6.2_1.png" alt=""></p>
<p><img src="/img/ch6/figure_6.2_2.jpg" alt=""></p>
<p>输入单元(Input units)：输入集$\bigr\{x_0,x_1,…,x_t,x_{t+1},…\bigr\}$，</p>
<p>输出单元(Output units)：输出集$\bigr\{y_0,y_1,…,y_t,y_{y+1},…\bigr\}$，</p>
<p>隐藏单元(Hidden units)：输出集$\bigr\{s_0,s_1,…,s_t,s_{t+1},…\bigr\}$。</p>
<p><strong>图中信息传递特点：</strong></p>
<ol>
<li>一条单向流动的信息流是从输入单元到隐藏单元。</li>
<li>一条单向流动的信息流从隐藏单元到输出单元。</li>
<li>在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”。</li>
<li>在某些情况下，隐藏层的输入还包括上一时刻隐藏层的状态，即隐藏层内的节点可以自连也可以互连。 </li>
<li>当前单元（cell）输出是由当前时刻输入和上一时刻隐藏层状态共同决定。</li>
</ol>
<h2 id="6-4-CNN和RNN的区别-？"><a href="#6-4-CNN和RNN的区别-？" class="headerlink" title="6.4 CNN和RNN的区别 ？"></a>6.4 CNN和RNN的区别 ？</h2><div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>特点描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>相同点</td>
<td>1、传统神经网络的扩展。<br />2、前向计算产生结果，反向计算模型更新。<br />3、每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</td>
</tr>
<tr>
<td>不同点</td>
<td>1、CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算<br />2、RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</td>
</tr>
</tbody>
</table>
</div>
<h2 id="6-5-RNNs和FNNs有什么区别？"><a href="#6-5-RNNs和FNNs有什么区别？" class="headerlink" title="6.5 RNNs和FNNs有什么区别？"></a>6.5 RNNs和FNNs有什么区别？</h2><ol>
<li>不同于传统的前馈神经网络(FNNs)，RNNs引入了定向循环，能够处理输入之间前后关联问题。</li>
<li>RNNs可以记忆之前步骤的训练信息。<br><strong>定向循环结构如下图所示</strong>：</li>
</ol>
<p><img src="/img/ch6/figure_6.1_1.jpg" alt=""></p>
<h2 id="6-6-RNNs训练和传统ANN训练异同点？"><a href="#6-6-RNNs训练和传统ANN训练异同点？" class="headerlink" title="6.6 RNNs训练和传统ANN训练异同点？"></a>6.6 RNNs训练和传统ANN训练异同点？</h2><p><strong>相同点</strong>：</p>
<ol>
<li>RNNs与传统ANN都使用BP（Back Propagation）误差反向传播算法。</li>
</ol>
<p><strong>不同点</strong>：</p>
<ol>
<li>RNNs网络参数W,U,V是共享的(具体在本章6.2节中已介绍)，而传统神经网络各层参数间没有直接联系。</li>
<li>对于RNNs，在使用梯度下降算法中，每一步的输出不仅依赖当前步的网络，还依赖于之前若干步的网络状态。</li>
</ol>
<h2 id="6-7-为什么RNN-训练的时候Loss波动很大"><a href="#6-7-为什么RNN-训练的时候Loss波动很大" class="headerlink" title="6.7 为什么RNN 训练的时候Loss波动很大"></a>6.7 为什么RNN 训练的时候Loss波动很大</h2><p>​    由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏，为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。</p>
<h2 id="6-8-标准RNN前向输出流程"><a href="#6-8-标准RNN前向输出流程" class="headerlink" title="6.8 标准RNN前向输出流程"></a>6.8 标准RNN前向输出流程</h2><p>​    以$x$表示输入，$h$是隐层单元，$o$是输出，$L$为损失函数，$y$为训练集标签。$t$表示$t$时刻的状态，$V,U,W$是权值，同一类型的连接权值相同。以下图为例进行说明标准RNN的前向传播算法：</p>
<p>​    <img src="/img/ch6/rnnbp.png" alt=""></p>
<p>对于$t$时刻：</p>
<script type="math/tex; mode=display">
h^{(t)}=\phi(Ux^{(t)}+Wh^{(t-1)}+b)</script><p>其中$\phi()$为激活函数，一般会选择tanh函数，$b$为偏置。</p>
<p>$t$时刻的输出为：</p>
<script type="math/tex; mode=display">
o^{(t)}=Vh^{(t)}+c</script><p>模型的预测输出为：</p>
<script type="math/tex; mode=display">
\widehat{y}^{(t)}=\sigma(o^{(t)})</script><p>其中$\sigma​$为激活函数，通常RNN用于分类，故这里一般用softmax函数。</p>
<h2 id="6-9-BPTT算法推导"><a href="#6-9-BPTT算法推导" class="headerlink" title="6.9 BPTT算法推导"></a>6.9 BPTT算法推导</h2><p>​    BPTT（back-propagation through time）算法是常用的训练RNN的方法，其本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。需要寻优的参数有三个，分别是U、V、W。与BP算法不同的是，其中W和U两个参数的寻优过程需要追溯之前的历史数据，参数V相对简单只需关注目前，那么我们就来先求解参数V的偏导数。</p>
<script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial V}=\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}</script><p>RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。</p>
<script type="math/tex; mode=display">
L=\sum_{t=1}^{n}L^{(t)}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial V}=\sum_{t=1}^{n}\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}</script><p>​    W和U的偏导的求解由于需要涉及到历史数据，其偏导求起来相对复杂。为了简化推导过程，我们假设只有三个时刻，那么在第三个时刻 L对W，L对U的偏导数分别为：</p>
<script type="math/tex; mode=display">
\frac{\partial L^{(3)}}{\partial W}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial W}</script><script type="math/tex; mode=display">
\frac{\partial L^{(3)}}{\partial U}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial U}</script><p>可以观察到，在某个时刻的对W或是U的偏导数，需要追溯这个时刻之前所有时刻的信息。根据上面两个式子得出L在t时刻对W和U偏导数的通式： </p>
<script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial W}</script><script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial U}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial U}</script><p>整体的偏导公式就是将其按时刻再一一加起来。</p>
<h2 id="6-9-RNN中为什么会出现梯度消失？"><a href="#6-9-RNN中为什么会出现梯度消失？" class="headerlink" title="6.9 RNN中为什么会出现梯度消失？"></a>6.9 RNN中为什么会出现梯度消失？</h2><p>首先来看tanh函数的函数及导数图如下所示：</p>
<p><img src="/img/ch6/tanh.jpg" alt=""></p>
<p>sigmoid函数的函数及导数图如下所示：</p>
<p><img src="/img/ch6/sigmoid.jpg" alt=""></p>
<p>从上图观察可知，sigmoid函数的导数范围是(0,0.25]，tanh函数的导数范围是(0,1]，他们的导数最大都不大于1。</p>
<p>​    基于6.8中式（9-10）中的推导，RNN的激活函数是嵌套在里面的，如果选择激活函数为$tanh$或$sigmoid$，把激活函数放进去，拿出中间累乘的那部分可得：</p>
<script type="math/tex; mode=display">
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{tanh^{'}}\cdot W_{s}</script><script type="math/tex; mode=display">
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{sigmoid^{'}}\cdot W_{s}</script><p>​    <strong>梯度消失现象</strong>：基于上式，会发现累乘会导致激活函数导数的累乘，如果取tanh或sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。</p>
<p>​    实际使用中，会优先选择tanh函数，原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<h2 id="6-10-如何解决RNN中的梯度消失问题？"><a href="#6-10-如何解决RNN中的梯度消失问题？" class="headerlink" title="6.10 如何解决RNN中的梯度消失问题？"></a>6.10 如何解决RNN中的梯度消失问题？</h2><p>​    上节描述的梯度消失是在无限的利用历史数据而造成，但是RNN的特点本来就是能利用历史数据获取更多的可利用信息，解决RNN中的梯度消失方法主要有：</p>
<p>​    1、选取更好的激活函数，如Relu激活函数。ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</p>
<p>​    2、加入BN层，其优点包括可加速收敛、控制过拟合，可以少用或不用Dropout和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</p>
<p>​    2、改变传播结构，LSTM结构可以有效解决这个问题。下面将介绍LSTM相关内容。</p>
<h2 id="6-11-LSTM"><a href="#6-11-LSTM" class="headerlink" title="6.11 LSTM"></a>6.11 LSTM</h2><h3 id="6-11-1-LSTM的产生原因"><a href="#6-11-1-LSTM的产生原因" class="headerlink" title="6.11.1 LSTM的产生原因"></a>6.11.1 LSTM的产生原因</h3><p>​    RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象。为了解决该问题，研究人员提出了许多解决办法，例如ESN（Echo State Network），增加有漏单元（Leaky Units）等等。其中最成功应用最广泛的就是门限RNN（Gated RNN），而LSTM就是门限RNN中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许RNN累积距离较远节点间的长期联系；而门限RNN则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。</p>
<h3 id="6-11-2-图解标准RNN和LSTM的区别"><a href="#6-11-2-图解标准RNN和LSTM的区别" class="headerlink" title="6.11.2 图解标准RNN和LSTM的区别"></a>6.11.2 图解标准RNN和LSTM的区别</h3><p>​    所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：</p>
<p><img src="/img/ch6/LSTM1.png" alt=""></p>
<p>​    LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p><img src="/img/ch6/LSTM2.png" alt=""></p>
<p>注：上图图标具体含义如下所示：</p>
<p><img src="/img/ch6/LSTM3.png" alt=""></p>
<p>​    上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
<h3 id="6-11-3-LSTM核心思想图解"><a href="#6-11-3-LSTM核心思想图解" class="headerlink" title="6.11.3 LSTM核心思想图解"></a>6.11.3 LSTM核心思想图解</h3><p>​    LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。示意图如下所示：</p>
<p><img src="/img/ch6/LSTM4.png" alt=""></p>
<p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。示意图如下：</p>
<p><img src="/img/ch6/LSTM5.png" alt=""></p>
<p>LSTM 拥有三个门，分别是忘记层门，输入层门和输出层门，来保护和控制细胞状态。</p>
<p><strong>忘记层门</strong></p>
<p>​    作用对象：细胞状态 。</p>
<p>​    作用：将细胞状态中的信息选择性的遗忘。</p>
<p>​    操作步骤：该门会读取$h_{t-1}$和$x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C_{t-1}​$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。示意图如下：</p>
<p><img src="/img/ch6/LSTM6.png" alt=""></p>
<p><strong>输入层门</strong></p>
<p>​    作用对象：细胞状态 </p>
<p>​    作用：将新的信息选择性的记录到细胞状态中。</p>
<p>​    操作步骤：</p>
<p>​    步骤一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。</p>
<p>​    步骤二，tanh 层创建一个新的候选值向量$\tilde{C}_t$加入到状态中。其示意图如下：</p>
<p><img src="/img/ch6/LSTM7.png" alt=""></p>
<p>​    步骤三：将$c_{t-1}$更新为$c_{t}$。将旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t * \tilde{C}_t$得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</p>
<p><img src="/img/ch6/LSTM8.png" alt=""></p>
<p><strong>输出层门</strong><br>    作用对象：隐层$h_t$ </p>
<p>​    作用：确定输出什么值。</p>
<p>​    操作步骤：</p>
<p>​    步骤一：通过sigmoid 层来确定细胞状态的哪个部分将输出。</p>
<p>​    步骤二：把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p>其示意图如下所示：</p>
<p><img src="/img/ch6/LSTM9.png" alt=""></p>
<h3 id="6-11-4-LSTM流行的变体"><a href="#6-11-4-LSTM流行的变体" class="headerlink" title="6.11.4 LSTM流行的变体"></a>6.11.4 LSTM流行的变体</h3><p><strong>增加peephole 连接</strong></p>
<p>​    在正常的LSTM结构中，Gers F A 等人提出增加peephole 连接，可以门层接受细胞状态的输入。示意图如下所示：</p>
<p><img src="/img/ch6/LSTM10.png" alt=""></p>
<p><strong>对忘记门和输入门进行同时确定</strong></p>
<p>​    不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。示意图如下所示：</p>
<p><img src="/img/ch6/LSTM11.png" alt=""></p>
<p><strong>Gated Recurrent Unit</strong></p>
<p>​     由Kyunghyun Cho等人提出的Gated Recurrent Unit (GRU)，其将忘记门和输入门合成了一个单一的更新门，同样还混合了细胞状态和隐藏状态，和其他一些改动。其示意图如下：</p>
<p><img src="/img/ch6/LSTM12.png" alt=""></p>
<p>最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<h2 id="6-12-LSTMs与GRUs的区别"><a href="#6-12-LSTMs与GRUs的区别" class="headerlink" title="6.12 LSTMs与GRUs的区别"></a>6.12 LSTMs与GRUs的区别</h2><p>LSTMs与GRUs的区别如图所示：</p>
<p><img src="/img/ch6/figure_6.6.6_2.png" alt=""></p>
<p>从上图可以看出，二者结构十分相似，<strong>不同在于</strong>：</p>
<ol>
<li>new memory都是根据之前state及input进行计算，但是GRUs中有一个reset gate控制之前state的进入量，而在LSTMs里没有类似gate；</li>
<li>产生新的state的方式不同，LSTMs有两个不同的gate，分别是forget gate (f gate)和input gate(i gate)，而GRUs只有一种update gate(z gate)；</li>
<li>LSTMs对新产生的state可以通过output gate(o gate)进行调节，而GRUs对输出无任何调节。</li>
</ol>
<h2 id="6-13-RNNs在NLP中典型应用？"><a href="#6-13-RNNs在NLP中典型应用？" class="headerlink" title="6.13 RNNs在NLP中典型应用？"></a>6.13 RNNs在NLP中典型应用？</h2><p><strong>（1）语言模型与文本生成(Language Modeling and Generating Text)</strong></p>
<p>​    给定一组单词序列，需要根据前面单词预测每个单词出现的可能性。语言模型能够评估某个语句正确的可能性，可能性越大，语句越正确。另一种应用便是使用生成模型预测下一个单词的出现概率，从而利用输出概率的采样生成新的文本。</p>
<p><strong>（2）机器翻译(Machine Translation)</strong></p>
<p>​    机器翻译是将一种源语言语句变成意思相同的另一种源语言语句，如将英语语句变成同样意思的中文语句。与语言模型关键的区别在于，需要将源语言语句序列输入后，才进行输出，即输出第一个单词时，便需要从完整的输入序列中进行获取。</p>
<p><strong>（3）语音识别(Speech Recognition)</strong></p>
<p>​    语音识别是指给定一段声波的声音信号，预测该声波对应的某种指定源语言语句以及计算该语句的概率值。 </p>
<p><strong>（4）图像描述生成 (Generating Image Descriptions)</strong></p>
<p>​    同卷积神经网络一样，RNNs已经在对无标图像描述自动生成中得到应用。CNNs与RNNs结合也被应用于图像描述自动生成。<br><img src="/img/ch6/figure_6.4_1.png" alt=""></p>
<h2 id="6-13-常见的RNNs扩展和改进模型"><a href="#6-13-常见的RNNs扩展和改进模型" class="headerlink" title="6.13 常见的RNNs扩展和改进模型"></a>6.13 常见的RNNs扩展和改进模型</h2><h3 id="6-13-1-Simple-RNNs-SRNs"><a href="#6-13-1-Simple-RNNs-SRNs" class="headerlink" title="6.13.1 Simple RNNs(SRNs)"></a>6.13.1 Simple RNNs(SRNs)</h3><ol>
<li>SRNs是一个三层网络，其在隐藏层增加了上下文单元。下图中的y是隐藏层，u是上下文单元。上下文单元节点与隐藏层中节点的连接是固定的，并且权值也是固定的。上下文节点与隐藏层节点一一对应，并且值是确定的。</li>
<li>在每一步中，使用标准的前向反馈进行传播，然后使用学习算法进行学习。上下文每一个节点保存其连接隐藏层节点上一步输出，即保存上文，并作用于当前步对应的隐藏层节点状态，即隐藏层的输入由输入层的输出与上一步的自身状态所决定。因此SRNs能够解决标准多层感知机(MLP)无法解决的对序列数据进行预测的问题。<br>SRNs网络结构如下图所示：</li>
</ol>
<p><img src="/img/ch6/figure_6.6.1_1.png" alt=""></p>
<h3 id="6-13-2-Bidirectional-RNNs"><a href="#6-13-2-Bidirectional-RNNs" class="headerlink" title="6.13.2 Bidirectional RNNs"></a>6.13.2 Bidirectional RNNs</h3><p>​    Bidirectional RNNs(双向网络)将两层RNNs叠加在一起，当前时刻输出(第t步的输出)不仅仅与之前序列有关，还与之后序列有关。例如：为了预测一个语句中的缺失词语，就需要该词汇的上下文信息。Bidirectional RNNs是一个相对较简单的RNNs，是由两个RNNs上下叠加在一起组成的。输出由前向RNNs和后向RNNs共同决定。如下图所示：</p>
<p><img src="/img/ch6/figure_6.6.2_1.png" alt=""></p>
<h3 id="6-13-3-Deep-RNNs"><a href="#6-13-3-Deep-RNNs" class="headerlink" title="6.13.3 Deep RNNs"></a>6.13.3 Deep RNNs</h3><p>​    Deep RNNs与Bidirectional RNNs相似，其也是又多层RNNs叠加，因此每一步的输入有了多层网络。该网络具有更强大的表达与学习能力，但是复杂性也随之提高，同时需要更多的训练数据。Deep RNNs的结构如下图所示：<br><img src="/img/ch6/figure_6.6.3_1.png" alt=""></p>
<h3 id="6-13-4-Echo-State-Networks（ESNs）"><a href="#6-13-4-Echo-State-Networks（ESNs）" class="headerlink" title="6.13.4 Echo State Networks（ESNs）"></a>6.13.4 Echo State Networks（ESNs）</h3><p><strong>ESNs特点</strong>：</p>
<ol>
<li>它的核心结构为一个随机生成、且保持不变的储备池(Reservoir)。储备池是大规模随机生成稀疏连接(SD通常保持1%～5%，SD表示储备池中互相连接的神经元占总神经元个数N的比例)的循环结构；</li>
<li>从储备池到输出层的权值矩阵是唯一需要调整的部分；</li>
<li>简单的线性回归便能够完成网络训练；</li>
</ol>
<p><strong>ESNs基本思想</strong>：</p>
<p>​    使用大规模随机连接的循环网络取代经典神经网络中的中间层，从而简化网络的训练过程。<br>网络中的参数包括：<br>（1）W - 储备池中节点间连接权值矩阵；<br>（2）Win - 输入层到储备池之间连接权值矩阵，表明储备池中的神经元之间是相互连接；<br>（3）Wback - 输出层到储备池之间的反馈连接权值矩阵，表明储备池会有输出层来的反馈；<br>（4）Wout - 输入层、储备池、输出层到输出层的连接权值矩阵，表明输出层不仅与储备池连接，还与输入层和自己连接。<br>（5）Woutbias - 输出层的偏置项。 </p>
<p>​    ESNs的结构如下图所示：</p>
<p><img src="/img/ch6/figure_6.6.4_2.png" alt=""></p>
<h3 id="6-13-4-Gated-Recurrent-Unit-Recurrent-Neural-Networks"><a href="#6-13-4-Gated-Recurrent-Unit-Recurrent-Neural-Networks" class="headerlink" title="6.13.4 Gated Recurrent Unit Recurrent Neural Networks"></a>6.13.4 Gated Recurrent Unit Recurrent Neural Networks</h3><p>GRUs是一般的RNNs的变型版本，其主要是从以下两个方面进行改进。</p>
<ol>
<li><p>以语句为例，序列中不同单词处的数据对当前隐藏层状态的影响不同，越前面的影响越小，即每个之前状态对当前的影响进行了距离加权，距离越远，权值越小。</p>
</li>
<li><p>在产生误差error时，其可能是由之前某一个或者几个单词共同造成，所以应当对对应的单词weight进行更新。GRUs的结构如下图所示。GRUs首先根据当前输入单词向量word vector以及前一个隐藏层状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前所有memory content，最终的memory是由之前的hidden state与new memory content一起决定。</p>
</li>
</ol>
<p><img src="/img/ch6/figure_6.6.5_1.png" alt=""></p>
<h3 id="6-13-5-Bidirectional-LSTMs"><a href="#6-13-5-Bidirectional-LSTMs" class="headerlink" title="6.13.5 Bidirectional LSTMs"></a>6.13.5 Bidirectional LSTMs</h3><ol>
<li>与bidirectional RNNs 类似，bidirectional LSTMs有两层LSTMs。一层处理过去的训练信息，另一层处理将来的训练信息。</li>
<li>在bidirectional LSTMs中，通过前向LSTMs获得前向隐藏状态，后向LSTMs获得后向隐藏状态，当前隐藏状态是前向隐藏状态与后向隐藏状态的组合。</li>
</ol>
<h3 id="6-13-6-Stacked-LSTMs"><a href="#6-13-6-Stacked-LSTMs" class="headerlink" title="6.13.6 Stacked LSTMs"></a>6.13.6 Stacked LSTMs</h3><ol>
<li>与deep rnns 类似，stacked LSTMs 通过将多层LSTMs叠加起来得到一个更加复杂的模型。</li>
<li>不同于bidirectional LSTMs，stacked LSTMs只利用之前步骤的训练信息。 </li>
</ol>
<h3 id="6-13-7-Clockwork-RNNs-CW-RNNs"><a href="#6-13-7-Clockwork-RNNs-CW-RNNs" class="headerlink" title="6.13.7 Clockwork RNNs(CW-RNNs)"></a>6.13.7 Clockwork RNNs(CW-RNNs)</h3><p>​    CW-RNNs是RNNs的改良版本，其使用时钟频率来驱动。它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。为了降低RNNs的复杂度，CW-RNNs减少了参数数量，并且提高了网络性能，加速网络训练。CW-RNNs通过不同隐藏层模块在不同时钟频率下工作来解决长时依赖问题。将时钟时间进行离散化，不同的隐藏层组将在不同时刻进行工作。因此，所有的隐藏层组在每一步不会全部同时工作，这样便会加快网络的训练。并且，时钟周期小组的神经元不会连接到时钟周期大组的神经元，只允许周期大的神经元连接到周期小的(组与组之间的连接以及信息传递是有向的)。周期大的速度慢，周期小的速度快，因此是速度慢的神经元连速度快的神经元，反之则不成立。</p>
<p>​    CW-RNNs与SRNs网络结构类似，也包括输入层(Input)、隐藏层(Hidden)、输出层(Output)，它们之间存在前向连接，输入层到隐藏层连接，隐藏层到输出层连接。但是与SRN不同的是，隐藏层中的神经元会被划分为若干个组，设为$g​$，每一组中的神经元个数相同，设为$k​$，并为每一个组分配一个时钟周期$T_i\epsilon\{T_1,T_2,…,T_g\}​$，每一组中的所有神经元都是全连接，但是组$j​$到组$i​$的循环连接则需要满足$T_j​$大于$T_i​$。如下图所示，将这些组按照时钟周期递增从左到右进行排序，即$T_1&lt;T_2&lt;…&lt;T_g​$，那么连接便是从右到左。例如：隐藏层共有256个节点，分为四组，周期分别是[1,2,4,8]，那么每个隐藏层组256/4=64个节点，第一组隐藏层与隐藏层的连接矩阵为64$\times​$64的矩阵，第二层的矩阵则为64$\times​$128矩阵，第三组为64$\times​$(3$\times​$64)=64$\times​$192矩阵，第四组为64$\times​$(4$\times​$64)=64$\times​$256矩阵。这就解释了上一段中速度慢的组连接到速度快的组，反之则不成立。</p>
<p><strong>CW-RNNs的网络结构如下图所示</strong>：</p>
<p><img src="/img/ch6/figure_6.6.7_1.png" alt=""></p>
<h3 id="6-13-8-CNN-LSTMs"><a href="#6-13-8-CNN-LSTMs" class="headerlink" title="6.13.8 CNN-LSTMs"></a>6.13.8 CNN-LSTMs</h3><ol>
<li>为了同时利用CNN以及LSTMs的优点，CNN-LSTMs被提出。在该模型中，CNN用于提取对象特征，LSTMs用于预测。CNN由于卷积特性，其能够快速而且准确地捕捉对象特征。LSTMs的优点在于能够捕捉数据间的长时依赖性。</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 何之源.<a href="https://zhuanlan.zhihu.com/p/28054589" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28054589</a>.</p>
<p>[2] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>[3] <a href="https://blog.csdn.net/zhaojc1995/article/details/80572098" target="_blank" rel="noopener">https://blog.csdn.net/zhaojc1995/article/details/80572098</a></p>
<p>[4] Graves A. Supervised Sequence Labelling with Recurrent Neural Networks[J]. Studies in Computational Intelligence, 2008, 385.</p>
<p>[5] Graves A. Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[6]  Greff K ,  Srivastava R K , Koutník, Jan, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp; Learning Systems, 2015, 28(10):2222-2232.</p>
<p>[7] Lanchantin J, Singh R, Wang B, et al. DEEP MOTIF DASHBOARD: VISUALIZING AND UNDERSTANDING GENOMIC SEQUENCES USING DEEP NEURAL NETWORKS.[J]. Pacific Symposium on Biocomputing Pacific Symposium on Biocomputing, 2016, 22:254.</p>
<p>[8]  Pascanu R ,  Mikolov T ,  Bengio Y . On the difficulty of training Recurrent Neural Networks[J].  2012.</p>
<p>[9]  Hochreiter S. The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions[J]. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 1998, 06(02):-.</p>
<p>[10] Dyer C, Kuncoro A, Ballesteros M, et al. Recurrent Neural Network Grammars[C]// Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.</p>
<p>[11]  Mulder W D ,  Bethard S ,  Moens M F . A survey on the application of recurrent neural networks to statistical language modeling.[M]. Academic Press Ltd.  2015.</p>
<p>[12] Graves A. Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[13] Zhang B, Xiong D, Su J. Neural Machine Translation with Deep Attention[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, PP(99):1-1.</p>
<p>[14] <a href="https://github.com/xuanyuansen/scalaLSTM" target="_blank" rel="noopener">https://github.com/xuanyuansen/scalaLSTM</a></p>
<p>[15] Deep Learning，Ian Goodfellow Yoshua Bengio and Aaron Courville，Book in preparation for MIT Press，2016；</p>
<p>[16] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>[17] Greff K, Srivastava R K, Koutník J, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp; Learning Systems, 2016, 28(10):2222-2232.</p>
<p>[18] Yao K ,  Cohn T ,  Vylomova K , et al. Depth-Gated Recurrent Neural Networks[J].  2015.</p>
<p>[19] Koutník J, Greff K, Gomez F, et al. A Clockwork RNN[J]. Computer Science, 2014:1863-1871.</p>
<p>[20]  Gers F A ,  Schmidhuber J . Recurrent nets that time and count[C]// Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on. IEEE, 2000.</p>
<p>[21] Li S, Wu C, Hai L, et al. FPGA Acceleration of Recurrent Neural Network Based Language Model[C]// IEEE International Symposium on Field-programmable Custom Computing Machines. 2015.</p>
<p>[22]  Mikolov T ,  Kombrink S ,  Burget L , et al. Extensions of recurrent neural network language model[C]// Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011.</p>
<p>[23]  Graves A . Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[24]  Sutskever I ,  Vinyals O ,  Le Q V . Sequence to Sequence Learning with Neural Networks[J].  2014.</p>
<p>[25] Liu B, Lane I. Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks[J].  2016.</p>
<p>[26] Graves A, Mohamed A R, Hinton G. Speech recognition with deep recurrent neural networks[C]// IEEE International Conference on Acoustics. 2013.</p>
<p>[27] <a href="https://cs.stanford.edu/people/karpathy/deepimagesent/" target="_blank" rel="noopener">https://cs.stanford.edu/people/karpathy/deepimagesent/</a></p>
<p>[28] Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%9B%9B%E7%AB%A0_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%9B%9B%E7%AB%A0_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">第四章_经典网络</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:23:28 / 修改时间：12:24:03" itemprop="dateCreated datePublished" datetime="2020-03-03T12:23:28+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第四章-经典网络解读"><a href="#第四章-经典网络解读" class="headerlink" title="第四章 经典网络解读"></a>第四章 经典网络解读</h1><h2 id="4-1-LeNet-5"><a href="#4-1-LeNet-5" class="headerlink" title="4.1 LeNet-5"></a>4.1 LeNet-5</h2><h3 id="4-1-1-模型介绍"><a href="#4-1-1-模型介绍" class="headerlink" title="4.1.1 模型介绍"></a>4.1.1 模型介绍</h3><p>​    LeNet-5是由$LeCun$ 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络（Convolutional Neural Network，CNN）$^{[1]}$，其命名来源于作者$LeCun$的名字，5则是其研究成果的代号，在LeNet-5之前还有LeNet-4和LeNet-1鲜为人知。LeNet-5阐述了图像中像素特征之间的相关性能够由参数共享的卷积操作所提取，同时使用卷积、下采样（池化）和非线性映射这样的组合结构，是当前流行的大多数深度图像识别网络的基础。</p>
<h3 id="4-1-2-模型结构"><a href="#4-1-2-模型结构" class="headerlink" title="4.1.2 模型结构"></a>4.1.2 模型结构</h3><p><img src="/img/ch4/image1.png" alt=""></p>
<p>​                                                                 图4.1 LeNet-5网络结构图</p>
<p>​    如图4.1所示，LeNet-5一共包含7层（输入层不作为网络结构），分别由2个卷积层、2个下采样层和3个连接层组成，网络的参数配置如表4.1所示，其中下采样层和全连接层的核尺寸分别代表采样范围和连接矩阵的尺寸（如卷积核尺寸中的$“5\times5\times1/1,6”$表示核大小为$5\times5\times1$、步长为$1​$且核个数为6的卷积核）。</p>
<p>​                                                                 表4.1 LeNet-5网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">可训练参数量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_1$</td>
<td style="text-align:center">$32\times32\times1$</td>
<td style="text-align:center">$5\times5\times1/1,6$</td>
<td style="text-align:center">$28\times28\times6$</td>
<td style="text-align:center">$(5\times5\times1+1)\times6$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_2$</td>
<td style="text-align:center">$28\times28\times6$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$14\times14\times6$</td>
<td style="text-align:center">$(1+1)\times6$ $^*$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_3$</td>
<td style="text-align:center">$14\times14\times6$</td>
<td style="text-align:center">$5\times5\times6/1,16$</td>
<td style="text-align:center">$10\times10\times16$</td>
<td style="text-align:center">$1516^*$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_4$</td>
<td style="text-align:center">$10\times10\times16$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$5\times5\times16$</td>
<td style="text-align:center">$(1+1)\times16$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_5$$^*$</td>
<td style="text-align:center">$5\times5\times16$</td>
<td style="text-align:center">$5\times5\times16/1,120$</td>
<td style="text-align:center">$1\times1\times120$</td>
<td style="text-align:center">$(5\times5\times16+1)\times120$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_6$</td>
<td style="text-align:center">$1\times1\times120$</td>
<td style="text-align:center">$120\times84$</td>
<td style="text-align:center">$1\times1\times84$</td>
<td style="text-align:center">$(120+1)\times84$</td>
</tr>
<tr>
<td style="text-align:center">输出层</td>
<td style="text-align:center">$1\times1\times84$</td>
<td style="text-align:center">$84\times10$</td>
<td style="text-align:center">$1\times1\times10$</td>
<td style="text-align:center">$(84+1)\times10$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>​    $^*$ 在LeNet中，下采样操作和池化操作类似，但是在得到采样结果后会乘以一个系数和加上一个偏置项，所以下采样的参数个数是$(1+1)\times6​$而不是零。</p>
<p>​    $^*$ $C_3$卷积层可训练参数并未直接连接$S_2$中所有的特征图（Feature Map），而是采用如图4.2所示的采样特征方式进行连接（稀疏连接），生成的16个通道特征图中分别按照相邻3个特征图、相邻4个特征图、非相邻4个特征图和全部6个特征图进行映射，得到的参数个数计算公式为$6\times(25\times3+1)+6\times(25\times4+1)+3\times(25\times4+1)+1\times(25\times6+1)=1516$，在原论文中解释了使用这种采样方式原因包含两点：限制了连接数不至于过大（当年的计算能力比较弱）;强制限定不同特征图的组合可以使映射得到的特征图学习到不同的特征模式。</p>
</blockquote>
<p><img src="/img/ch4/featureMap.jpg" alt="FeatureMap"></p>
<p>​                                                                图4.2 $S_2$与$C_3$之间的特征图稀疏连接</p>
<blockquote>
<p>​    $^*$ $C_5$卷积层在图4.1中显示为全连接层，原论文中解释这里实际采用的是卷积操作，只是刚好在$5\times5$卷积后尺寸被压缩为$1\times1​$，输出结果看起来和全连接很相似。</p>
</blockquote>
<h3 id="4-1-3-模型特性"><a href="#4-1-3-模型特性" class="headerlink" title="4.1.3 模型特性"></a>4.1.3 模型特性</h3><ul>
<li>卷积网络使用一个3层的序列组合：卷积、下采样（池化）、非线性映射（LeNet-5最重要的特性，奠定了目前深层卷积网络的基础）</li>
<li>使用卷积提取空间特征</li>
<li>使用映射的空间均值进行下采样</li>
<li>使用$tanh$或$sigmoid$进行非线性映射</li>
<li>多层神经网络（MLP）作为最终的分类器</li>
<li>层间的稀疏连接矩阵以避免巨大的计算开销</li>
</ul>
<h2 id="4-2-AlexNet"><a href="#4-2-AlexNet" class="headerlink" title="4.2 AlexNet"></a>4.2 AlexNet</h2><h3 id="4-2-1-模型介绍"><a href="#4-2-1-模型介绍" class="headerlink" title="4.2.1 模型介绍"></a>4.2.1 模型介绍</h3><p>​    AlexNet是由$Alex$ $Krizhevsky $提出的首个应用于图像分类的深层卷积神经网络，该网络在2012年ILSVRC（ImageNet Large Scale Visual Recognition Competition）图像分类竞赛中以15.3%的top-5测试错误率赢得第一名$^{[2]}$。AlexNet使用GPU代替CPU进行运算，使得在可接受的时间范围内模型结构能够更加复杂，它的出现证明了深层卷积神经网络在复杂模型下的有效性，使CNN在计算机视觉中流行开来，直接或间接地引发了深度学习的热潮。</p>
<h3 id="4-2-2-模型结构"><a href="#4-2-2-模型结构" class="headerlink" title="4.2.2 模型结构"></a>4.2.2 模型结构</h3><p><img src="/img/ch4/alexnet.png" alt=""></p>
<p>​                                                                         图4.3 AlexNet网络结构图</p>
<p>​    如图4.3所示，除去下采样（池化层）和局部响应规范化操作（Local Responsible Normalization, LRN），AlexNet一共包含8层，前5层由卷积层组成，而剩下的3层为全连接层。网络结构分为上下两层，分别对应两个GPU的操作过程，除了中间某些层（$C_3$卷积层和$F_{6-8}$全连接层会有GPU间的交互），其他层两个GPU分别计算结 果。最后一层全连接层的输出作为$softmax$的输入，得到1000个图像分类标签对应的概率值。除去GPU并行结构的设计，AlexNet网络结构与LeNet十分相似，其网络的参数配置如表4.2所示。</p>
<p>​                                    表4.2 AlexNet网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">可训练参数量</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_1$ $^*$</td>
<td style="text-align:center">$224\times224\times3$</td>
<td style="text-align:center">$11\times11\times3/4,48(\times2_{GPU})$</td>
<td style="text-align:center">$55\times55\times48(\times2_{GPU})$</td>
<td style="text-align:center">$(11\times11\times3+1)\times48\times2$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max}$$^*$</td>
<td style="text-align:center">$55\times55\times48(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3/2(\times2_{GPU})$</td>
<td style="text-align:center">$27\times27\times48(\times2_{GPU})$</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_2$</td>
<td style="text-align:center">$27\times27\times48(\times2_{GPU})$</td>
<td style="text-align:center">$5\times5\times48/1,128(\times2_{GPU})$</td>
<td style="text-align:center">$27\times27\times128(\times2_{GPU})$</td>
<td style="text-align:center">$(5\times5\times48+1)\times128\times2$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max}$</td>
<td style="text-align:center">$27\times27\times128(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3/2(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times128(\times2_{GPU})$</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_3$ $^*$</td>
<td style="text-align:center">$13\times13\times128\times2_{GPU}$</td>
<td style="text-align:center">$3\times3\times256/1,192(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$(3\times3\times256+1)\times192\times2$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_4$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3\times192/1,192(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$(3\times3\times192+1)\times192\times2$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_5$</td>
<td style="text-align:center">$13\times13\times192(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3\times192/1,128(\times2_{GPU})$</td>
<td style="text-align:center">$13\times13\times128(\times2_{GPU})$</td>
<td style="text-align:center">$(3\times3\times192+1)\times128\times2$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max}$</td>
<td style="text-align:center">$13\times13\times128(\times2_{GPU})$</td>
<td style="text-align:center">$3\times3/2(\times2_{GPU})$</td>
<td style="text-align:center">$6\times6\times128(\times2_{GPU})$</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_6$  $^*$</td>
<td style="text-align:center">$6\times6\times128\times2_{GPU}$</td>
<td style="text-align:center">$9216\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$1\times1\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$(9216+1)\times2048\times2$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_7$</td>
<td style="text-align:center">$1\times1\times2048\times2_{GPU}$</td>
<td style="text-align:center">$4096\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$1\times1\times2048(\times2_{GPU})$</td>
<td style="text-align:center">$(4096+1)\times2048\times2$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$F_8$</td>
<td style="text-align:center">$1\times1\times2048\times2_{GPU}$</td>
<td style="text-align:center">$4096\times1000$</td>
<td style="text-align:center">$1\times1\times1000$</td>
<td style="text-align:center">$(4096+1)\times1000\times2$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>卷积层$C_1$输入为$224\times224\times3$的图片数据，分别在两个GPU中经过核为$11\times11\times3$、步长（stride）为4的卷积卷积后，分别得到两条独立的$55\times55\times48$的输出数据。</p>
<p>下采样层$S_{max}$实际上是嵌套在卷积中的最大池化操作，但是为了区分没有采用最大池化的卷积层单独列出来。在$C_{1-2}$卷积层中的池化操作之后（ReLU激活操作之前），还有一个LRN操作，用作对相邻特征点的归一化处理。</p>
<p>卷积层$C_3$ 的输入与其他卷积层不同，$13\times13\times192\times2_{GPU}$表示汇聚了上一层网络在两个GPU上的输出结果作为输入，所以在进行卷积操作时通道上的卷积核维度为384。</p>
<p>全连接层$F_{6-8}$中输入数据尺寸也和$C_3$类似，都是融合了两个GPU流向的输出结果作为输入。</p>
</blockquote>
<h3 id="4-2-3-模型特性"><a href="#4-2-3-模型特性" class="headerlink" title="4.2.3 模型特性"></a>4.2.3 模型特性</h3><ul>
<li>所有卷积层都使用ReLU作为非线性映射函数，使模型收敛速度更快</li>
<li>在多个GPU上进行模型的训练，不但可以提高模型的训练速度，还能提升数据的使用规模</li>
<li>使用LRN对局部的特征进行归一化，结果作为ReLU激活函数的输入能有效降低错误率</li>
<li>重叠最大池化（overlapping max pooling），即池化范围z与步长s存在关系$z&gt;s$（如$S_{max}$中核尺度为$3\times3/2$），避免平均池化（average pooling）的平均效应</li>
<li>使用随机丢弃技术（dropout）选择性地忽略训练中的单个神经元，避免模型的过拟合</li>
</ul>
<h2 id="4-3-ZFNet"><a href="#4-3-ZFNet" class="headerlink" title="4.3 ZFNet"></a>4.3 ZFNet</h2><h3 id="4-3-1-模型介绍"><a href="#4-3-1-模型介绍" class="headerlink" title="4.3.1 模型介绍"></a>4.3.1 模型介绍</h3><p>​    ZFNet是由$Matthew$ $D. Zeiler$和$Rob$ $Fergus$在AlexNet基础上提出的大型卷积网络，在2013年ILSVRC图像分类竞赛中以11.19%的错误率获得冠军（实际上原ZFNet所在的队伍并不是真正的冠军，原ZFNet以13.51%错误率排在第8，真正的冠军是$Clarifai$这个队伍，而$Clarifai$这个队伍所对应的一家初创公司的CEO又是$Zeiler$，而且$Clarifai$对ZFNet的改动比较小，所以通常认为是ZFNet获得了冠军）$^{[3-4]}​$。ZFNet实际上是微调（fine-tuning）了的AlexNet，并通过反卷积（Deconvolution）的方式可视化各层的输出特征图，进一步解释了卷积操作在大型网络中效果显著的原因。</p>
<h3 id="4-3-2-模型结构"><a href="#4-3-2-模型结构" class="headerlink" title="4.3.2 模型结构"></a>4.3.2 模型结构</h3><p><img src="/img/ch4/image21.png" alt=""></p>
<p><img src="/img/ch4/image21.jpeg" alt=""></p>
<p>​                        图4.4 ZFNet网络结构图（原始结构图与AlexNet风格结构图）</p>
<p>​    如图4.4所示，ZFNet与AlexNet类似，都是由8层网络组成的卷积神经网络，其中包含5层卷积层和3层全连接层。两个网络结构最大的不同在于，ZFNet第一层卷积采用了$7\times7\times3/2$的卷积核替代了AlexNet中第一层卷积核$11\times11\times3/4$的卷积核。图4.5中ZFNet相比于AlexNet在第一层输出的特征图中包含更多中间频率的信息，而AlexNet第一层输出的特征图大多是低频或高频的信息，对中间频率特征的缺失导致后续网络层次如图4.5（c）能够学习到的特征不够细致，而导致这个问题的根本原因在于AlexNet在第一层中采用的卷积核和步长过大。</p>
<p><img src="/img/ch4/zfnet-layer1.png" alt=""></p>
<p><img src="/img/ch4/zfnet-layer2.png" alt=""></p>
<p>​    图4.5 （a）ZFNet第一层输出的特征图（b）AlexNet第一层输出的特征图（c）AlexNet第二层输出的特征图（d）ZFNet第二层输出的特征图</p>
<p>​                                    表4.3 ZFNet网络参数配置<br>|        网络层         |               输入尺寸               |                  核尺寸                  |               输出尺寸               |              可训练参数量               |<br>| :—————————-: | :—————————————————: | :———————————————————: | :—————————————————: | :——————————————————-: |<br>|   卷积层$C_1$ $^<em>$  |        $224\times224\times3$         |          $7\times7\times3/2,96$          |        $110\times110\times96$        |      $(7\times7\times3+1)\times96$      |<br>| 下采样层$S_{max}$ |        $110\times110\times96$        |               $3\times3/2$               |         $55\times55\times96$         |                    0                    |<br>|      卷积层$C_2$ $^</em>$      |         $55\times55\times96$         |         $5\times5\times96/2,256$         |        $26\times26\times256$        | $(5\times5\times96+1)\times256$ |<br>|   下采样层$S_{max}$   | $26\times26\times256$ |       $3\times3/2$       | $13\times13\times256$ |                    0                    |<br>|   卷积层$C_3$  |  $13\times13\times256$  | $3\times3\times256/1,384$ | $13\times13\times384$ | $(3\times3\times256+1)\times384$ |<br>|      卷积层$C_4$      | $13\times13\times384$ | $3\times3\times384/1,384$ | $13\times13\times384$ | $(3\times3\times384+1)\times384$ |<br>|      卷积层$C_5$      | $13\times13\times384$ | $3\times3\times384/1,256$ | $13\times13\times256$ | $(3\times3\times384+1)\times256$ |<br>|   下采样层$S_{max}$   | $13\times13\times256$ |       $3\times3/2$       |  $6\times6\times256$  |                    0                    |<br>|  全连接层$F_6$  |   $6\times6\times256$   |     $9216\times4096$     | $1\times1\times4096$ |       $(9216+1)\times4096$       |<br>|     全连接层$F_7$     |  $1\times1\times4096$  |     $4096\times4096$     | $1\times1\times4096$ |       $(4096+1)\times4096$       |<br>|     全连接层$F_8$     | $1\times1\times4096$ |             $4096\times1000$             |         $1\times1\times1000$         |       $(4096+1)\times1000$       |</p>
<blockquote>
<p>卷积层$C_1$与AlexNet中的$C_1$有所不同，采用$7\times7\times3/2$的卷积核代替$11\times11\times3/4​$，使第一层卷积输出的结果可以包含更多的中频率特征，对后续网络层中多样化的特征组合提供更多选择，有利于捕捉更细致的特征。</p>
<p>卷积层$C_2$采用了步长2的卷积核，区别于AlexNet中$C_2$的卷积核步长，所以输出的维度有所差异。</p>
</blockquote>
<h3 id="4-3-3-模型特性"><a href="#4-3-3-模型特性" class="headerlink" title="4.3.3 模型特性"></a>4.3.3 模型特性</h3><p>​    ZFNet与AlexNet在结构上几乎相同，此部分虽属于模型特性，但准确地说应该是ZFNet原论文中可视化技术的贡献。</p>
<ul>
<li>可视化技术揭露了激发模型中每层单独的特征图。</li>
<li>可视化技术允许观察在训练阶段特征的演变过程且诊断出模型的潜在问题。</li>
<li>可视化技术用到了多层解卷积网络，即由特征激活返回到输入像素空间。</li>
<li>可视化技术进行了分类器输出的敏感性分析，即通过阻止部分输入图像来揭示那部分对于分类是重要的。</li>
<li>可视化技术提供了一个非参数的不变性来展示来自训练集的哪一块激活哪个特征图，不仅需要裁剪输入图片，而且自上而下的投影来揭露来自每块的结构激活一个特征图。</li>
<li>可视化技术依赖于解卷积操作，即卷积操作的逆过程，将特征映射到像素上。</li>
</ul>
<h2 id="4-4-Network-in-Network"><a href="#4-4-Network-in-Network" class="headerlink" title="4.4 Network in Network"></a>4.4 Network in Network</h2><h3 id="4-4-1-模型介绍"><a href="#4-4-1-模型介绍" class="headerlink" title="4.4.1 模型介绍"></a>4.4.1 模型介绍</h3><p>​    Network In Network (NIN)是由$Min Lin$等人提出，在CIFAR-10和CIFAR-100分类任务中达到当时的最好水平，因其网络结构是由三个多层感知机堆叠而被成为NIN$^{[5]}$。NIN以一种全新的角度审视了卷积神经网络中的卷积核设计，通过引入子网络结构代替纯卷积中的线性映射部分，这种形式的网络结构激发了更复杂的卷积神经网络的结构设计，其中下一节中介绍的GoogLeNet的Inception结构就是来源于这个思想。</p>
<h3 id="4-4-2-模型结构"><a href="#4-4-2-模型结构" class="headerlink" title="4.4.2 模型结构"></a>4.4.2 模型结构</h3><p><img src="/img/ch4/image23.png" alt=""><br>​                                    图 4.6 NIN网络结构图</p>
<p>​    NIN由三层的多层感知卷积层（MLPConv Layer）构成，每一层多层感知卷积层内部由若干层的局部全连接层和非线性激活函数组成，代替了传统卷积层中采用的线性卷积核。在网络推理（inference）时，这个多层感知器会对输入特征图的局部特征进行划窗计算，并且每个划窗的局部特征图对应的乘积的权重是共享的，这两点是和传统卷积操作完全一致的，最大的不同在于多层感知器对局部特征进行了非线性的映射，而传统卷积的方式是线性的。NIN的网络参数配置表4.4所示（原论文并未给出网络参数，表中参数为编者结合网络结构图和CIFAR-100数据集以$3\times3$卷积为例给出）。</p>
<p>​                    表4.4 NIN网络参数配置（结合原论文NIN结构和CIFAR-100数据给出）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">局部全连接层$L_{11}$ $^*$</td>
<td style="text-align:center">$32\times32\times3$</td>
<td style="text-align:center">$(3\times3)\times16/1$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$(3\times3\times3+1)\times16$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$L_{12}$ $^*$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$16\times16$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$((16+1)\times16)$</td>
</tr>
<tr>
<td style="text-align:center">局部全连接层$L_{21}$</td>
<td style="text-align:center">$30\times30\times16$</td>
<td style="text-align:center">$(3\times3)\times64/1$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$(3\times3\times16+1)\times64$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$L_{22}$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$64\times64$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$((64+1)\times64)$</td>
</tr>
<tr>
<td style="text-align:center">局部全连接层$L_{31}$</td>
<td style="text-align:center">$28\times28\times64$</td>
<td style="text-align:center">$(3\times3)\times100/1$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$(3\times3\times64+1)\times100$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$L_{32}$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$100\times100$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$((100+1)\times100)$</td>
</tr>
<tr>
<td style="text-align:center">全局平均采样$GAP$ $^*$</td>
<td style="text-align:center">$26\times26\times100$</td>
<td style="text-align:center">$26\times26\times100/1$</td>
<td style="text-align:center">$1\times1\times100$</td>
<td style="text-align:center">$0$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>局部全连接层$L_{11}$实际上是对原始输入图像进行划窗式的全连接操作，因此划窗得到的输出特征尺寸为$30\times30$（$\frac{32-3_k+1}{1_{stride}}=30$）<br>全连接层$L_{12}$是紧跟$L_{11}$后的全连接操作，输入的特征是划窗后经过激活的局部响应特征，因此仅需连接$L_{11}$和$L_{12}$的节点即可，而每个局部全连接层和紧接的全连接层构成代替卷积操作的多层感知卷积层（MLPConv）。<br>全局平均采样层或全局平均池化层$GAP$（Global Average Pooling）将$L_{32}$输出的每一个特征图进行全局的平均池化操作，直接得到最后的类别数，可以有效地减少参数量。</p>
</blockquote>
<h3 id="4-4-3-模型特点"><a href="#4-4-3-模型特点" class="headerlink" title="4.4.3 模型特点"></a>4.4.3 模型特点</h3><ul>
<li>使用多层感知机结构来代替卷积的滤波操作，不但有效减少卷积核数过多而导致的参数量暴涨问题，还能通过引入非线性的映射来提高模型对特征的抽象能力。</li>
<li>使用全局平均池化来代替最后一个全连接层，能够有效地减少参数量（没有可训练参数），同时池化用到了整个特征图的信息，对空间信息的转换更加鲁棒，最后得到的输出结果可直接作为对应类别的置信度。</li>
</ul>
<h2 id="4-5-VGGNet"><a href="#4-5-VGGNet" class="headerlink" title="4.5 VGGNet"></a>4.5 VGGNet</h2><h3 id="4-5-1-模型介绍"><a href="#4-5-1-模型介绍" class="headerlink" title="4.5.1 模型介绍"></a>4.5.1 模型介绍</h3><p>​    VGGNet是由牛津大学视觉几何小组（Visual Geometry Group, VGG）提出的一种深层卷积网络结构，他们以7.32%的错误率赢得了2014年ILSVRC分类任务的亚军（冠军由GoogLeNet以6.65%的错误率夺得）和25.32%的错误率夺得定位任务（Localization）的第一名（GoogLeNet错误率为26.44%）$^{[5]}$，网络名称VGGNet取自该小组名缩写。VGGNet是首批把图像分类的错误率降低到10%以内模型，同时该网络所采用的$3\times3$卷积核的思想是后来许多模型的基础，该模型发表在2015年国际学习表征会议（International Conference On Learning Representations, ICLR）后至今被引用的次数已经超过1万4千余次。</p>
<h3 id="4-5-2-模型结构"><a href="#4-5-2-模型结构" class="headerlink" title="4.5.2 模型结构"></a>4.5.2 模型结构</h3><p><img src="/img/ch4/vgg16.png" alt=""></p>
<p>​                                图 4.7 VGG16网络结构图</p>
<p>​    在原论文中的VGGNet包含了6个版本的演进，分别对应VGG11、VGG11-LRN、VGG13、VGG16-1、VGG16-3和VGG19，不同的后缀数值表示不同的网络层数（VGG11-LRN表示在第一层中采用了LRN的VGG11，VGG16-1表示后三组卷积块中最后一层卷积采用卷积核尺寸为$1\times1$，相应的VGG16-3表示卷积核尺寸为$3\times3$），本节介绍的VGG16为VGG16-3。图4.7中的VGG16体现了VGGNet的核心思路，使用$3\times3$的卷积组合代替大尺寸的卷积（2个$3\times3卷积即可与$$5\times5$卷积拥有相同的感受视野），网络参数设置如表4.5所示。</p>
<p>​                                表4.5 VGG16网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_{11}$</td>
<td style="text-align:center">$224\times224\times3$</td>
<td style="text-align:center">$3\times3\times64/1$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$(3\times3\times3+1)\times64$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{12}$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$3\times3\times64/1$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$(3\times3\times64+1)\times64$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max1}$</td>
<td style="text-align:center">$224\times224\times64$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$112\times112\times64$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{21}$</td>
<td style="text-align:center">$112\times112\times64$</td>
<td style="text-align:center">$3\times3\times128/1$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$(3\times3\times64+1)\times128$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{22}$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$3\times3\times128/1$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$(3\times3\times128+1)\times128$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max2}$</td>
<td style="text-align:center">$112\times112\times128$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$56\times56\times128$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{31}$</td>
<td style="text-align:center">$56\times56\times128$</td>
<td style="text-align:center">$3\times3\times256/1$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$(3\times3\times128+1)\times256$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{32}$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$3\times3\times256/1$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$(3\times3\times256+1)\times256$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{33}$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$3\times3\times256/1$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$(3\times3\times256+1)\times256$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max3}$</td>
<td style="text-align:center">$56\times56\times256$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$28\times28\times256$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{41}$</td>
<td style="text-align:center">$28\times28\times256$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$(3\times3\times256+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{42}$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{43}$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max4}$</td>
<td style="text-align:center">$28\times28\times512$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{51}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{52}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{53}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$3\times3\times512/1$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$(3\times3\times512+1)\times512$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{max5}$</td>
<td style="text-align:center">$14\times14\times512$</td>
<td style="text-align:center">$2\times2/2$</td>
<td style="text-align:center">$7\times7\times512$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$FC_{1}$</td>
<td style="text-align:center">$7\times7\times512$</td>
<td style="text-align:center">$(7\times7\times512)\times4096$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$(7\times7\times512+1)\times4096$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$FC_{2}$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$4096\times4096$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$(4096+1)\times4096$</td>
</tr>
<tr>
<td style="text-align:center">全连接层$FC_{3}$</td>
<td style="text-align:center">$1\times4096$</td>
<td style="text-align:center">$4096\times1000$</td>
<td style="text-align:center">$1\times1000$</td>
<td style="text-align:center">$(4096+1)\times1000$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="4-5-3-模型特性"><a href="#4-5-3-模型特性" class="headerlink" title="4.5.3 模型特性"></a>4.5.3 模型特性</h3><ul>
<li>整个网络都使用了同样大小的卷积核尺寸$3\times3$和最大池化尺寸$2\times2$。</li>
<li>$1\times1$卷积的意义主要在于线性变换，而输入通道数和输出通道数不变，没有发生降维。</li>
<li>两个$3\times3$的卷积层串联相当于1个$5\times5$的卷积层，感受野大小为$5\times5$。同样地，3个$3\times3$的卷积层串联的效果则相当于1个$7\times7$的卷积层。这样的连接方式使得网络参数量更小，而且多层的激活函数令网络对特征的学习能力更强。</li>
<li>VGGNet在训练时有一个小技巧，先训练浅层的的简单网络VGG11，再复用VGG11的权重来初始化VGG13，如此反复训练并初始化VGG19，能够使训练时收敛的速度更快。</li>
<li>在训练过程中使用多尺度的变换对原始数据做数据增强，使得模型不易过拟合。</li>
</ul>
<h2 id="4-6-GoogLeNet"><a href="#4-6-GoogLeNet" class="headerlink" title="4.6 GoogLeNet"></a>4.6 GoogLeNet</h2><h3 id="4-6-1-模型介绍"><a href="#4-6-1-模型介绍" class="headerlink" title="4.6.1 模型介绍"></a>4.6.1 模型介绍</h3><p>​    GoogLeNet作为2014年ILSVRC在分类任务上的冠军，以6.65%的错误率力压VGGNet等模型，在分类的准确率上面相比过去两届冠军ZFNet和AlexNet都有很大的提升。从名字<strong>GoogLe</strong>Net可以知道这是来自谷歌工程师所设计的网络结构，而名字中Goog<strong>LeNet</strong>更是致敬了LeNet$^{[0]}$。GoogLeNet中最核心的部分是其内部子网络结构Inception，该结构灵感来源于NIN，至今已经经历了四次版本迭代（Inception$_{v1-4}$）。</p>
<p><img src="/img/ch4/img_inception_01.png" alt=""><br>​                    图 4.8 Inception性能比较图</p>
<h3 id="4-6-2-模型结构"><a href="#4-6-2-模型结构" class="headerlink" title="4.6.2 模型结构"></a>4.6.2 模型结构</h3><p><img src="/img/ch4/image25.jpeg" alt=""><br>​                    图 4.9 GoogLeNet网络结构图<br>​    如图4.9中所示，GoogLeNet相比于以前的卷积神经网络结构，除了在深度上进行了延伸，还对网络的宽度进行了扩展，整个网络由许多块状子网络的堆叠而成，这个子网络构成了Inception结构。图4.9为Inception的四个版本：$Inception_{v1}​$在同一层中采用不同的卷积核，并对卷积结果进行合并;$Inception_{v2}​$组合不同卷积核的堆叠形式，并对卷积结果进行合并;$Inception_{v3}​$则在$v_2​$基础上进行深度组合的尝试;$Inception_{v4}​$结构相比于前面的版本更加复杂，子网络中嵌套着子网络。</p>
<p>$Inception_{v1}$</p>
<p><img src="/img/ch4/image27.png" alt=""></p>
<p><img src="/img/ch4/image28.png" alt=""></p>
<p>$Inception_{v2}$</p>
<p><img src="/img/ch4/image34.png" alt=""></p>
<p><img src="/img/ch4/image36.png" alt=""></p>
<p><img src="/img/ch4/image38.png" alt=""></p>
<p>$Inception_{v3}$</p>
<p><img src="/img/ch4/image37.png" alt=""></p>
<p>$Inception_{v4}$</p>
<p><img src="/img/ch4/image46.png" alt=""></p>
<p><img src="/img/ch4/image47.png" alt=""></p>
<p><img src="/img/ch4/image63.png" alt=""></p>
<p>​                    图 4.10 Inception$_{v1-4}$结构图</p>
<p>​                    表 4.6 GoogLeNet中Inception$_{v1}$网络参数配置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络层</th>
<th style="text-align:center">输入尺寸</th>
<th style="text-align:center">核尺寸</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:center">参数个数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积层$C_{11}$</td>
<td style="text-align:center">$H\times{W}\times{C_1}$</td>
<td style="text-align:center">$1\times1\times{C_2}/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(1\times1\times{C_1}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{21}$</td>
<td style="text-align:center">$H\times{W}\times{C_2}$</td>
<td style="text-align:center">$1\times1\times{C_2}/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(1\times1\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{22}$</td>
<td style="text-align:center">$H\times{W}\times{C_2}$</td>
<td style="text-align:center">$3\times3\times{C_2}/1$</td>
<td style="text-align:center">$H\times{W}\times{C_2}/1$</td>
<td style="text-align:center">$(3\times3\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{31}$</td>
<td style="text-align:center">$H\times{W}\times{C_1}$</td>
<td style="text-align:center">$1\times1\times{C_2}/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(1\times1\times{C_1}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{32}$</td>
<td style="text-align:center">$H\times{W}\times{C_2}$</td>
<td style="text-align:center">$5\times5\times{C_2}/1$</td>
<td style="text-align:center">$H\times{W}\times{C_2}/1$</td>
<td style="text-align:center">$(5\times5\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">下采样层$S_{41}$</td>
<td style="text-align:center">$H\times{W}\times{C_1}$</td>
<td style="text-align:center">$3\times3/2$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$0$</td>
</tr>
<tr>
<td style="text-align:center">卷积层$C_{42}$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$1\times1\times{C_2}/1$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}$</td>
<td style="text-align:center">$(3\times3\times{C_2}+1)\times{C_2}$</td>
</tr>
<tr>
<td style="text-align:center">合并层$M$</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times{C_2}(\times4)$</td>
<td style="text-align:center">拼接</td>
<td style="text-align:center">$\frac{H}{2}\times\frac{W}{2}\times({C_2}\times4)$</td>
<td style="text-align:center">$0$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="4-6-3-模型特性"><a href="#4-6-3-模型特性" class="headerlink" title="4.6.3 模型特性"></a>4.6.3 模型特性</h3><ul>
<li><p>采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； </p>
</li>
<li><p>之所以卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定pad=0、1、2，那么卷积之后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；</p>
</li>
<li><p>网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。但是，使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN2，采用1x1卷积核来进行降维。</p>
<h1 id=""><a href="#" class="headerlink" title=" "></a> </h1></li>
</ul>
<h2 id="Restnet"><a href="#Restnet" class="headerlink" title="Restnet"></a>Restnet</h2><h2 id="Densenet"><a href="#Densenet" class="headerlink" title="Densenet"></a>Densenet</h2><h2 id="4-7-为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？"><a href="#4-7-为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？" class="headerlink" title="4.7 为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？"></a>4.7 为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？</h2><ul>
<li>评测对比：为了让自己的结果更有说服力，在发表自己成果的时候会同一个标准的baseline及在baseline上改进而进行比较，常见的比如各种检测分割的问题都会基于VGG或者Resnet101这样的基础网络。</li>
<li>时间和精力有限：在科研压力和工作压力中，时间和精力只允许大家在有限的范围探索。</li>
<li>模型创新难度大：进行基本模型的改进需要大量的实验和尝试，并且需要大量的实验积累和强大灵感，很有可能投入产出比比较小。</li>
<li>资源限制：创造一个新的模型需要大量的时间和计算资源，往往在学校和小型商业团队不可行。</li>
<li>在实际的应用场景中，其实是有大量的非标准模型的配置。</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. <em>Proceedings of the IEEE</em>, november 1998.</p>
<p>[2] A. Krizhevsky, I. Sutskever and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. <em>Advances in Neural Information Processing Systems 25</em>. Curran Associates, Inc. 1097–1105.</p>
<p>[3] LSVRC-2013. <a href="http://www.image-net.org/challenges/LSVRC/2013/results.php" target="_blank" rel="noopener">http://www.image-net.org/challenges/LSVRC/2013/results.php</a></p>
<p>[4] M. D. Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks. <em>European Conference on Computer Vision</em>. </p>
<p>[5] M. Lin,  Q. Chen,  and S. Yan.   Network in network. <em>Computing Research Repository</em>, abs/1312.4400, 2013.</p>
<p>[6] K. Simonyan and A. Zisserman.  Very Deep Convolutional Networks for Large-Scale Image Recognition. <em>International Conference on Machine Learning</em>, 2015.</p>
<p>[7] Bharath Raj. <a href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202" target="_blank" rel="noopener">a-simple-guide-to-the-versions-of-the-inception-network</a>, 2018.</p>
<p>[8] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi. <a href="https://arxiv.org/pdf/1602.07261.pdf" target="_blank" rel="noopener">Inception-v4, Inception-ResNet and<br>the Impact of Residual Connections on Learning</a>, 2016.</p>
<p>[9] Sik-Ho Tsang. <a href="https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc" target="_blank" rel="noopener">review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification</a>, 2018.</p>
<p>[10] Zbigniew Wojna, Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens. <a href="https://arxiv.org/pdf/1512.00567v3.pdf" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision</a>, 2015.</p>
<p>[11] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich. <a href="https://arxiv.org/pdf/1409.4842v1.pdf" target="_blank" rel="noopener">Going deeper with convolutions</a>, 2014.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">第一章_数学基础</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:21:02 / 修改时间：12:22:07" itemprop="dateCreated datePublished" datetime="2020-03-03T12:21:02+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第一章-数学基础"><a href="#第一章-数学基础" class="headerlink" title="第一章 数学基础"></a>第一章 数学基础</h1><p>深度学习通常又需要哪些数学基础？深度学习里的数学到底难在哪里？通常初学者都会有这些问题，在网络推荐及书本推荐里，经常看到会列出一系列数学科目，比如微积分、线性代数、概率论、复变函数、数值计算、优化理论、信息论等等。这些数学知识有相关性，但实际上按照这样的知识范围来学习，学习成本会很久，而且会很枯燥，本章我们通过选举一些数学基础里容易混淆的一些概念做以介绍，帮助大家更好的理清这些易混淆概念之间的关系。</p>
<h2 id="1-1-向量和矩阵"><a href="#1-1-向量和矩阵" class="headerlink" title="1.1 向量和矩阵"></a>1.1 向量和矩阵</h2><h3 id="1-1-1-标量、向量、矩阵、张量之间的联系"><a href="#1-1-1-标量、向量、矩阵、张量之间的联系" class="headerlink" title="1.1.1 标量、向量、矩阵、张量之间的联系"></a>1.1.1 标量、向量、矩阵、张量之间的联系</h3><p><strong>标量（scalar）</strong><br>一个标量表示一个单独的数，它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小写的变量名称。 </p>
<p><strong>向量（vector）</strong><br>​一个向量表示一组有序排列的数。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比如xx。向量中的元素可以通过带脚标的斜体表示。向量$X$的第一个元素是$X_1$，第二个元素是$X_2$，以此类推。我们也会注明存储在向量中的元素的类型（实数、虚数等）。</p>
<p><strong>矩阵（matrix）</strong><br>​矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。其意义是一个对象表示为矩阵中的一行，一个特征表示为矩阵中的一列，每个特征都有数值型的取值。通常会赋予矩阵粗体的大写变量名称，比如$A$。</p>
<p><strong>张量（tensor）</strong><br>​在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用 $A$ 来表示张量“A”。张量$A$中坐标为$(i,j,k)$的元素记作$A_{(i,j,k)}$。 </p>
<p><strong>四者之间关系</strong>  </p>
<blockquote>
<p>标量是0阶张量，向量是一阶张量。举例：<br>​标量就是知道棍子的长度，但是你不会知道棍子指向哪儿。<br>​向量就是不但知道棍子的长度，还知道棍子指向前面还是后面。<br>​张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子又向上/下和左/右偏转了多少。</p>
</blockquote>
<h3 id="1-1-2-张量与矩阵的区别"><a href="#1-1-2-张量与矩阵的区别" class="headerlink" title="1.1.2 张量与矩阵的区别"></a>1.1.2 张量与矩阵的区别</h3><ul>
<li>从代数角度讲， 矩阵它是向量的推广。向量可以看成一维的“表格”（即分量按照顺序排成一排）， 矩阵是二维的“表格”（分量按照纵横位置排列）， 那么$n$阶张量就是所谓的$n$维的“表格”。 张量的严格定义是利用线性映射来描述。</li>
<li>从几何角度讲， 矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。</li>
<li>张量可以用3×3矩阵形式来表达。 </li>
<li>表示标量的数和表示向量的三维数组也可分别看作1×1，1×3的矩阵。 </li>
</ul>
<h3 id="1-1-3-矩阵和向量相乘结果"><a href="#1-1-3-矩阵和向量相乘结果" class="headerlink" title="1.1.3 矩阵和向量相乘结果"></a>1.1.3 矩阵和向量相乘结果</h3><p>若使用爱因斯坦求和约定（Einstein summation convention），矩阵$A$, $B$相乘得到矩阵$C$可以用下式表示：</p>
<script type="math/tex; mode=display">a_{ik}*b_{kj}=c_{ij} \tag{1.3-1}</script><p>其中，$a_{ik}$, $b_{kj}$, $c_{ij}$分别表示矩阵$A, B, C$的元素，$k$出现两次，是一个哑变量（Dummy Variables）表示对该参数进行遍历求和。<br>而矩阵和向量相乘可以看成是矩阵相乘的一个特殊情况，例如：矩阵$B$是一个$n \times 1$的矩阵。</p>
<h3 id="1-1-4-向量和矩阵的范数归纳"><a href="#1-1-4-向量和矩阵的范数归纳" class="headerlink" title="1.1.4 向量和矩阵的范数归纳"></a>1.1.4 向量和矩阵的范数归纳</h3><p><strong>向量的范数(norm)</strong><br>​    定义一个向量为：$\vec{a}=[-5, 6, 8, -10]$。任意一组向量设为$\vec{x}=(x_1,x_2,…,x_N)$。其不同范数求解如下：</p>
<ul>
<li>向量的1范数：向量的各个元素的绝对值之和，上述向量$\vec{a}$的1范数结果就是：29。</li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert</script><ul>
<li>向量的2范数：向量的每个元素的平方和再开平方根，上述$\vec{a}$的2范数结果就是：15。</li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2}</script><ul>
<li>向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量$\vec{a}$的负无穷范数结果就是：5。  </li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_{-\infty}=\min{|{x_i}|}</script><ul>
<li>向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量$\vec{a}$的正无穷范数结果就是：10。 </li>
</ul>
<script type="math/tex; mode=display">
\Vert\vec{x}\Vert_{+\infty}=\max{|{x_i}|}</script><ul>
<li>向量的p范数：</li>
</ul>
<script type="math/tex; mode=display">
L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p}</script><p><strong>矩阵的范数</strong>  </p>
<p>定义一个矩阵$A=[-1, 2, -3; 4, -6, 6]$。 任意矩阵定义为：$A_{m\times n}$，其元素为 $a_{ij}$。</p>
<p>矩阵的范数定义为</p>
<script type="math/tex; mode=display">
\Vert{A}\Vert_p :=\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}</script><p>当向量取不同范数时, 相应得到了不同的矩阵范数。</p>
<ul>
<li><strong>矩阵的1范数（列范数）</strong>：矩阵的每一列上的元</li>
</ul>
<p>  素绝对值先求和，再从中取个最大的,（列和最大），上述矩阵$A$的1范数先得到$[5,8,9]$，再取最大的最终结果就是：9。</p>
<script type="math/tex; mode=display">
\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}|</script><ul>
<li><strong>矩阵的2范数</strong>：矩阵$A^TA$的最大特征值开平方根，上述矩阵$A$的2范数得到的最终结果是：10.0623。 </li>
</ul>
<script type="math/tex; mode=display">
\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}</script><p>其中， $\lambda_{max}(A^T A)$ 为 $A^T A​$ 的特征值绝对值的最大值。</p>
<ul>
<li><p><strong>矩阵的无穷范数（行范数）</strong>：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大），上述矩阵$A$的行范数先得到$[6；16]$，再取最大的最终结果就是：16。 </p>
<script type="math/tex; mode=display">
\Vert A\Vert_{\infty}=\max_{1\le i \le m}\sum_{j=1}^n |{a_{ij}}|</script></li>
<li><p><strong>矩阵的核范数</strong>：矩阵的奇异值（将矩阵svd分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287。  </p>
</li>
<li><p><strong>矩阵的L0范数</strong>：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵$A$最终结果就是：6。</p>
</li>
<li><strong>矩阵的L1范数</strong>：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵$A$最终结果就是：22。  </li>
<li><strong>矩阵的F范数</strong>：矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，它的优点在于它是一个凸函数，可以求导求解，易于计算，上述矩阵A最终结果就是：10.0995。  </li>
</ul>
<script type="math/tex; mode=display">
\Vert A\Vert_F=\sqrt{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^2)}</script><ul>
<li><strong>矩阵的L21范数</strong>：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵$A$最终结果就是：17.1559。 </li>
<li><strong>矩阵的 p范数</strong> </li>
</ul>
<script type="math/tex; mode=display">
\Vert A\Vert_p=\sqrt[p]{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^p)}</script><h3 id="1-1-5-如何判断一个矩阵为正定"><a href="#1-1-5-如何判断一个矩阵为正定" class="headerlink" title="1.1.5 如何判断一个矩阵为正定"></a>1.1.5 如何判断一个矩阵为正定</h3><p>判定一个矩阵是否为正定，通常有以下几个方面：  </p>
<ul>
<li>顺序主子式全大于0；  </li>
<li>存在可逆矩阵$C$使$C^TC$等于该矩阵；</li>
<li>正惯性指数等于$n$；</li>
<li>合同于单位矩阵$E$（即：规范形为$E$）</li>
<li>标准形中主对角元素全为正；</li>
<li>特征值全为正；</li>
<li>是某基的度量矩阵。</li>
</ul>
<h2 id="1-2-导数和偏导数"><a href="#1-2-导数和偏导数" class="headerlink" title="1.2 导数和偏导数"></a>1.2 导数和偏导数</h2><h3 id="1-2-1-导数偏导计算"><a href="#1-2-1-导数偏导计算" class="headerlink" title="1.2.1 导数偏导计算"></a>1.2.1 导数偏导计算</h3><p><strong>导数定义</strong>:</p>
<p>导数(derivative)代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量的变化的比值。几何意义是这个点的切线。物理意义是该时刻的（瞬时）变化率。<br>​</p>
<p><em>注意</em>：在一元函数中，只有一个自变量变动，也就是说只存在一个方向的变化率，这也就是为什么一元函数没有偏导数的原因。在物理学中有平均速度和瞬时速度之说。平均速度有</p>
<script type="math/tex; mode=display">
v=\frac{s}{t}</script><p>其中$v$表示平均速度，$s$表示路程，$t$表示时间。这个公式可以改写为</p>
<script type="math/tex; mode=display">
\bar{v}=\frac{\Delta s}{\Delta t}=\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}</script><p>其中$\Delta s$表示两点之间的距离，而$\Delta t$表示走过这段距离需要花费的时间。当$\Delta t$趋向于0（$\Delta t \to 0$）时，也就是时间变得很短时，平均速度也就变成了在$t_0$时刻的瞬时速度，表示成如下形式：</p>
<script type="math/tex; mode=display">
v(t_0)=\lim_{\Delta t \to 0}{\bar{v}}=\lim_{\Delta t \to 0}{\frac{\Delta s}{\Delta t}}=\lim_{\Delta t \to 0}{\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}}</script><p>实际上，上式表示的是路程$s$关于时间$t$的函数在$t=t_0$处的导数。一般的，这样定义导数：如果平均变化率的极限存在，即有</p>
<script type="math/tex; mode=display">
\lim_{\Delta x \to 0}{\frac{\Delta y}{\Delta x}}=\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}}</script><p>则称此极限为函数 $y=f(x)$ 在点 $x_0$ 处的导数。记作 $f’(x_0)$ 或 $y’\vert_{x=x_0}$ 或 $\frac{dy}{dx}\vert_{x=x_0}$ 或 $\frac{df(x)}{dx}\vert_{x=x_0}$。</p>
<p>通俗地说，导数就是曲线在某一点切线的斜率。</p>
<p><strong>偏导数</strong>:</p>
<p>既然谈到偏导数(partial derivative)，那就至少涉及到两个自变量。以两个自变量为例，$z=f(x,y)​$，从导数到偏导数，也就是从曲线来到了曲面。曲线上的一点，其切线只有一条。但是曲面上的一点，切线有无数条。而偏导数就是指多元函数沿着坐标轴的变化率。 </p>
<p><em>注意</em>：直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。</p>
<p>设函数$z=f(x,y)​$在点$(x_0,y_0)​$的领域内有定义，当$y=y_0​$时，$z​$可以看作关于$x​$的一元函数$f(x,y_0)​$，若该一元函数在$x=x_0​$处可导，即有</p>
<script type="math/tex; mode=display">
\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x,y_0)-f(x_0,y_0)}{\Delta x}}=A</script><p>函数的极限$A$存在。那么称$A$为函数$z=f(x,y)$在点$(x_0,y_0)$处关于自变量$x$的偏导数，记作$f_x(x_0,y_0)$或$\frac{\partial z}{\partial x}\vert_{y=y_0}^{x=x_0}$或$\frac{\partial f}{\partial x}\vert_{y=y_0}^{x=x_0}$或$z_x\vert_{y=y_0}^{x=x_0}$。</p>
<p>偏导数在求解时可以将另外一个变量看做常数，利用普通的求导方式求解，比如$z=3x^2+xy$关于$x$的偏导数就为$z_x=6x+y$，这个时候$y$相当于$x$的系数。</p>
<p>某点$(x_0,y_0)$处的偏导数的几何意义为曲面$z=f(x,y)$与面$x=x_0$或面$y=y_0$交线在$y=y_0$或$x=x_0$处切线的斜率。  </p>
<h3 id="1-2-2-导数和偏导数有什么区别？"><a href="#1-2-2-导数和偏导数有什么区别？" class="headerlink" title="1.2.2 导数和偏导数有什么区别？"></a>1.2.2 导数和偏导数有什么区别？</h3><p>导数和偏导没有本质区别，如果极限存在，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。  </p>
<blockquote>
<ul>
<li>一元函数，一个$y$对应一个$x$，导数只有一个。  </li>
<li>二元函数，一个$z$对应一个$x$和一个$y$，有两个导数：一个是$z$对$x$的导数，一个是$z$对$y$的导数，称之为偏导。  </li>
<li>求偏导时要注意，对一个变量求导，则视另一个变量为常数，只对改变量求导，从而将偏导的求解转化成了一元函数的求导。</li>
</ul>
</blockquote>
<h2 id="1-3-特征值和特征向量"><a href="#1-3-特征值和特征向量" class="headerlink" title="1.3 特征值和特征向量"></a>1.3 特征值和特征向量</h2><h3 id="1-3-1-特征值分解与特征向量"><a href="#1-3-1-特征值分解与特征向量" class="headerlink" title="1.3.1 特征值分解与特征向量"></a>1.3.1 特征值分解与特征向量</h3><ul>
<li><p>特征值分解可以得到特征值(eigenvalues)与特征向量(eigenvectors)；</p>
</li>
<li><p>特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。  </p>
<p>如果说一个向量$\vec{v}$是方阵$A$的特征向量，将一定可以表示成下面的形式：</p>
</li>
</ul>
<script type="math/tex; mode=display">
A\nu = \lambda \nu</script><p>$\lambda$为特征向量$\vec{v}$对应的特征值。特征值分解是将一个矩阵分解为如下形式： </p>
<script type="math/tex; mode=display">
A=Q\sum Q^{-1}</script><p>其中，$Q$是这个矩阵$A$的特征向量组成的矩阵，$\sum$是一个对角矩阵，每一个对角线元素就是一个特征值，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵$A$的信息可以由其特征值和特征向量表示。</p>
<h3 id="1-3-2-奇异值与特征值有什么关系"><a href="#1-3-2-奇异值与特征值有什么关系" class="headerlink" title="1.3.2 奇异值与特征值有什么关系"></a>1.3.2 奇异值与特征值有什么关系</h3><p>那么奇异值和特征值是怎么对应起来的呢？我们将一个矩阵$A$的转置乘以$A$，并对$A^TA​$求特征值，则有下面的形式：</p>
<script type="math/tex; mode=display">
(A^TA)V = \lambda V</script><p>这里$V​$就是上面的右奇异向量，另外还有：</p>
<script type="math/tex; mode=display">
\sigma_i = \sqrt{\lambda_i}, u_i=\frac{1}{\sigma_i}A\mu_i</script><p>这里的$\sigma​$就是奇异值，$u​$就是上面说的左奇异向量。【证明那个哥们也没给】<br>​奇异值$\sigma​$跟特征值类似，在矩阵$\sum​$中也是从大到小排列，而且$\sigma​$的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前$r​$（$r​$远小于$m、n​$）个的奇异值来近似描述矩阵，即部分奇异值分解：</p>
<script type="math/tex; mode=display">
A_{m\times n}\approx U_{m \times r}\sum_{r\times r}V_{r \times n}^T</script><p>右边的三个矩阵相乘的结果将会是一个接近于$A$的矩阵，在这儿，$r$越接近于$n$，则相乘的结果越接近于$A$。</p>
<h2 id="1-4-概率分布与随机变量"><a href="#1-4-概率分布与随机变量" class="headerlink" title="1.4 概率分布与随机变量"></a>1.4 概率分布与随机变量</h2><h3 id="1-4-1-机器学习为什么要使用概率"><a href="#1-4-1-机器学习为什么要使用概率" class="headerlink" title="1.4.1 机器学习为什么要使用概率"></a>1.4.1 机器学习为什么要使用概率</h3><p>事件的概率是衡量该事件发生的可能性的量度。虽然在一次随机试验中某个事件的发生是带有偶然性的，但那些可在相同条件下大量重复的随机试验却往往呈现出明显的数量规律。<br>​机器学习除了处理不确定量，也需处理随机量。不确定性和随机性可能来自多个方面，使用概率论来量化不确定性。<br>​概率论在机器学习中扮演着一个核心角色，因为机器学习算法的设计通常依赖于对数据的概率假设。  </p>
<blockquote>
<p>​    例如在机器学习（Andrew Ng）的课中，会有一个朴素贝叶斯假设就是条件独立的一个例子。该学习算法对内容做出假设，用来分辨电子邮件是否为垃圾邮件。假设无论邮件是否为垃圾邮件，单词x出现在邮件中的概率条件独立于单词y。很明显这个假设不是不失一般性的，因为某些单词几乎总是同时出现。然而，最终结果是，这个简单的假设对结果的影响并不大，且无论如何都可以让我们快速判别垃圾邮件。</p>
</blockquote>
<h3 id="1-4-2-变量与随机变量有什么区别"><a href="#1-4-2-变量与随机变量有什么区别" class="headerlink" title="1.4.2 变量与随机变量有什么区别"></a>1.4.2 变量与随机变量有什么区别</h3><p><strong>随机变量</strong>（random variable）</p>
<p>表示随机现象（在一定条件下，并不总是出现相同结果的现象称为随机现象）中各种结果的实值函数（一切可能的样本点）。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数等，都是随机变量的实例。<br>​随机变量与模糊变量的不确定性的本质差别在于，后者的测定结果仍具有不确定性，即模糊性。</p>
<p><strong>变量与随机变量的区别：</strong><br>​当变量的取值的概率不是1时,变量就变成了随机变量；当随机变量取值的概率为1时,随机变量就变成了变量。</p>
<blockquote>
<p>比如：<br>​    当变量$x$值为100的概率为1的话,那么$x=100$就是确定了的,不会再有变化,除非有进一步运算.<br>​    当变量$x$的值为100的概率不为1,比如为50的概率是0.5,为100的概率是0.5,那么这个变量就是会随不同条件而变化的,是随机变量,取到50或者100的概率都是0.5,即50%。  </p>
</blockquote>
<h3 id="1-4-3-随机变量与概率分布的联系"><a href="#1-4-3-随机变量与概率分布的联系" class="headerlink" title="1.4.3 随机变量与概率分布的联系"></a>1.4.3 随机变量与概率分布的联系</h3><p>一个随机变量仅仅表示一个可能取得的状态，还必须给定与之相伴的概率分布来制定每个状态的可能性。用来描述随机变量或一簇随机变量的每一个可能的状态的可能性大小的方法，就是 <strong>概率分布(probability distribution)</strong>.</p>
<p>随机变量可以分为离散型随机变量和连续型随机变量。</p>
<p>相应的描述其概率分布的函数是 </p>
<p>概率质量函数(Probability Mass Function, PMF):描述离散型随机变量的概率分布，通常用大写字母 $P$表示。</p>
<p>概率密度函数(Probability Density Function, PDF):描述连续型随机变量的概率分布，通常用小写字母$p$表示。</p>
<h3 id="1-4-4-离散型随机变量和概率质量函数"><a href="#1-4-4-离散型随机变量和概率质量函数" class="headerlink" title="1.4.4 离散型随机变量和概率质量函数"></a>1.4.4 离散型随机变量和概率质量函数</h3><p>PMF 将随机变量能够取得的每个状态映射到随机变量取得该状态的概率。</p>
<ul>
<li>一般而言，$P(x)​$ 表示时$X=x​$的概率.</li>
<li>有时候为了防止混淆，要明确写出随机变量的名称$P(​$x$=x)​$ </li>
<li>有时候需要先定义一个随机变量，然后制定它遵循的概率分布x服从$P(​$x​$)​$ </li>
</ul>
<p>PMF 可以同时作用于多个随机变量，即联合概率分布(joint probability distribution) $P(X=x,Y=y)$*表示 $X=x$和$Y=y$同时发生的概率，也可以简写成 $P(x,y)$.</p>
<p>如果一个函数$P​$是随机变量 $X​$ 的 PMF， 那么它必须满足如下三个条件</p>
<ul>
<li>$P​$的定义域必须是的所有可能状态的集合</li>
<li>$∀x∈​$x, $0 \leq P(x) \leq 1 ​$. </li>
<li>$∑_{x∈X} P(x)=1$. 我们把这一条性质称之为 归一化的(normalized)</li>
</ul>
<h3 id="1-4-5-连续型随机变量和概率密度函数"><a href="#1-4-5-连续型随机变量和概率密度函数" class="headerlink" title="1.4.5 连续型随机变量和概率密度函数"></a>1.4.5 连续型随机变量和概率密度函数</h3><p>如果一个函数$p​$是x的PDF，那么它必须满足如下几个条件</p>
<ul>
<li>$p$的定义域必须是 xx 的所有可能状态的集合。</li>
<li>$∀x∈X,p(x)≥0$. 注意，我们并不要求$ p(x)≤1$，因为此处 $p(x)$不是表示的对应此状态具体的概率，而是概率的一个相对大小(密度)。具体的概率，需要积分去求。</li>
<li>$∫p(x)dx=1$, 积分下来，总和还是1，概率之和还是1.</li>
</ul>
<p>注：PDF$p(x)$并没有直接对特定的状态给出概率，给出的是密度，相对的，它给出了落在面积为 $δx$的无线小的区域内的概率为$ p(x)δx$. 由此，我们无法求得具体某个状态的概率，我们可以求得的是 某个状态 $x$ 落在 某个区间$[a,b]$内的概率为$ \int_{a}^{b}p(x)dx$.</p>
<h3 id="1-4-6-举例理解条件概率"><a href="#1-4-6-举例理解条件概率" class="headerlink" title="1.4.6 举例理解条件概率"></a>1.4.6 举例理解条件概率</h3><p>条件概率公式如下：</p>
<script type="math/tex; mode=display">
P(A|B) = P(A\cap B) / P(B)</script><p>说明：在同一个样本空间$\Omega$中的事件或者子集$A$与$B$，如果随机从$\Omega$中选出的一个元素属于$B$，那么下一个随机选择的元素属于$A$ 的概率就定义为在$B$的前提下$A$的条件概率。条件概率文氏图示意如图1.1所示。<br><img src="/img/ch1/conditional_probability.jpg" alt="条件概率"></p>
<p>图1.1 条件概率文氏图示意</p>
<p>根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是$P(A\bigcap B)$除以$P(B)$。<br>​举例：一对夫妻有两个小孩，已知其中一个是女孩，则另一个是女孩子的概率是多少？（面试、笔试都碰到过）<br>​<strong>穷举法</strong>：已知其中一个是女孩，那么样本空间为男女，女女，女男，则另外一个仍然是女生的概率就是1/3。<br>​<strong>条件概率法</strong>：$P(女|女)=P(女女)/P(女)$,夫妻有两个小孩，那么它的样本空间为女女，男女，女男，男男，则$P(女女)$为1/4，$P（女）= 1-P(男男)=3/4$,所以最后$1/3$。<br>这里大家可能会误解，男女和女男是同一种情况，但实际上类似姐弟和兄妹是不同情况。 </p>
<h3 id="1-4-7-联合概率与边缘概率联系区别"><a href="#1-4-7-联合概率与边缘概率联系区别" class="headerlink" title="1.4.7 联合概率与边缘概率联系区别"></a>1.4.7 联合概率与边缘概率联系区别</h3><p><strong>区别：</strong><br>​联合概率：联合概率指类似于$P(X=a,Y=b)$这样，包含多个条件，且所有条件同时成立的概率。联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率。<br>​边缘概率：边缘概率是某个事件发生的概率，而与其它事件无关。边缘概率指类似于$P(X=a)$，$P(Y=b)$这样，仅与单个随机变量有关的概率。</p>
<p><strong>联系：</strong><br>​联合分布可求边缘分布，但若只知道边缘分布，无法求得联合分布。  </p>
<h3 id="1-4-8-条件概率的链式法则"><a href="#1-4-8-条件概率的链式法则" class="headerlink" title="1.4.8 条件概率的链式法则"></a>1.4.8 条件概率的链式法则</h3><p>由条件概率的定义，可直接得出下面的乘法公式：<br>​乘法公式 设$A, B$是两个事件，并且$P(A) &gt; 0$, 则有 </p>
<script type="math/tex; mode=display">
P(AB) = P(B|A)P(A)</script><p>推广 </p>
<script type="math/tex; mode=display">
P(ABC)=P(C|AB)P(B|A)P(A)</script><p>一般地，用归纳法可证：若$P(A_1A_2…A_n)&gt;0$，则有</p>
<script type="math/tex; mode=display">
P(A_1A_2...A_n)=P(A_n|A_1A_2...A_{n-1})P(A_{n-1}|A_1A_2...A_{n-2})...P(A_2|A_1)P(A_1)
=P(A_1)\prod_{i=2}^{n}P(A_i|A_1A_2...A_{i-1})</script><p>任何多维随机变量联合概率分布，都可以分解成只有一个变量的条件概率相乘形式。 </p>
<h3 id="1-4-9-独立性和条件独立性"><a href="#1-4-9-独立性和条件独立性" class="headerlink" title="1.4.9 独立性和条件独立性"></a>1.4.9 独立性和条件独立性</h3><p><strong>独立性</strong><br>​两个随机变量$x$和$y$，概率分布表示成两个因子乘积形式，一个因子只包含$x$，另一个因子只包含$y$，两个随机变量相互独立(independent)。<br>​条件有时为不独立的事件之间带来独立，有时也会把本来独立的事件，因为此条件的存在，而失去独立性。<br>​举例：$P(XY)=P(X)P(Y)$, 事件$X$和事件$Y$独立。此时给定$Z$，</p>
<script type="math/tex; mode=display">
P(X,Y|Z) \not = P(X|Z)P(Y|Z)</script><p>事件独立时，联合概率等于概率的乘积。这是一个非常好的数学性质，然而不幸的是，无条件的独立是十分稀少的，因为大部分情况下，事件之间都是互相影响的。 </p>
<p><strong>条件独立性</strong><br>​给定$Z$的情况下,$X$和$Y$条件独立，当且仅当</p>
<script type="math/tex; mode=display">
X\bot Y|Z \iff P(X,Y|Z) = P(X|Z)P(Y|Z)</script><p>$X$和$Y$的关系依赖于$Z$，而不是直接产生。  </p>
<blockquote>
<p><strong>举例</strong>定义如下事件：<br>$X$：明天下雨；<br>$Y$：今天的地面是湿的；<br>$Z$：今天是否下雨；<br>$Z$事件的成立，对$X$和$Y$均有影响，然而，在$Z$事件成立的前提下，今天的地面情况对明天是否下雨没有影响。 </p>
</blockquote>
<h2 id="1-5-常见概率分布"><a href="#1-5-常见概率分布" class="headerlink" title="1.5 常见概率分布"></a>1.5 常见概率分布</h2><h3 id="1-5-1-Bernoulli分布"><a href="#1-5-1-Bernoulli分布" class="headerlink" title="1.5.1 Bernoulli分布"></a>1.5.1 Bernoulli分布</h3><p><strong>Bernoulli分布</strong>是单个二值随机变量分布, 单参数$\phi​$∈[0,1]控制,$\phi​$给出随机变量等于1的概率. 主要性质有: </p>
<script type="math/tex; mode=display">
\begin{align*}
P(x=1) &= \phi \\
P(x=0) &= 1-\phi  \\
P(x=x) &= \phi^x(1-\phi)^{1-x} \\
\end{align*}</script><p>其期望和方差为：</p>
<script type="math/tex; mode=display">
\begin{align*}
E_x[x] &= \phi \\
Var_x(x) &= \phi{(1-\phi)}
\end{align*}</script><p><strong>Multinoulli分布</strong>也叫<strong>范畴分布</strong>, 是单个<em>k</em>值随机分布,经常用来表示<strong>对象分类的分布</strong>. 其中$k$是有限值.Multinoulli分布由向量$\vec{p}\in[0,1]^{k-1}$参数化,每个分量$p_i$表示第$i$个状态的概率, 且$p_k=1-1^Tp​$.</p>
<p><strong>适用范围</strong>: <strong>伯努利分布</strong>适合对<strong>离散型</strong>随机变量建模.</p>
<h3 id="1-5-2-高斯分布"><a href="#1-5-2-高斯分布" class="headerlink" title="1.5.2 高斯分布"></a>1.5.2 高斯分布</h3><p>高斯也叫正态分布(Normal Distribution), 概率度函数如下:  </p>
<script type="math/tex; mode=display">
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}exp\left ( -\frac{1}{2\sigma^2}(x-\mu)^2 \right )</script><p>其中, $\mu​$和$\sigma​$分别是均值和方差, 中心峰值x坐标由$\mu​$给出, 峰的宽度受$\sigma​$控制, 最大点在$x=\mu​$处取得, 拐点为$x=\mu\pm\sigma​$</p>
<p>正态分布中，±1$\sigma$、±2$\sigma$、±3$\sigma$下的概率分别是68.3%、95.5%、99.73%，这3个数最好记住。 </p>
<p>此外, 令$\mu=0,\sigma=1​$高斯分布即简化为标准正态分布: </p>
<script type="math/tex; mode=display">
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi}}exp\left ( -\frac{1}{2}x^2 \right )</script><p>对概率密度函数高效求值: </p>
<script type="math/tex; mode=display">
N(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2\pi}}exp\left(-\frac{1}{2}\beta(x-\mu)^2\right)</script><p>其中，$\beta=\frac{1}{\sigma^2}$通过参数$\beta∈（0，\infty）​$来控制分布精度。</p>
<h3 id="1-5-3-何时采用正态分布"><a href="#1-5-3-何时采用正态分布" class="headerlink" title="1.5.3 何时采用正态分布"></a>1.5.3 何时采用正态分布</h3><p>问: 何时采用正态分布?<br>答: 缺乏实数上分布的先验知识, 不知选择何种形式时, 默认选择正态分布总是不会错的, 理由如下: </p>
<ol>
<li>中心极限定理告诉我们, 很多独立随机变量均近似服从正态分布, 现实中很多复杂系统都可以被建模成正态分布的噪声, 即使该系统可以被结构化分解. </li>
<li>正态分布是具有相同方差的所有概率分布中, 不确定性最大的分布, 换句话说, 正态分布是对模型加入先验知识最少的分布.</li>
</ol>
<p>正态分布的推广:<br>正态分布可以推广到$R^n$空间, 此时称为<strong>多位正态分布</strong>, 其参数是一个正定对称矩阵$\Sigma​$: </p>
<script type="math/tex; mode=display">
N(x;\vec\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right)</script><p>对多为正态分布概率密度高效求值: </p>
<script type="math/tex; mode=display">
N(x;\vec{\mu},\vec\beta^{-1}) = \sqrt{det(\vec\beta)}{(2\pi)^n}exp\left(-\frac{1}{2}(\vec{x}-\vec\mu)^T\beta(\vec{x}-\vec\mu)\right)</script><p>此处，$\vec\beta$是一个精度矩阵。</p>
<h3 id="1-5-4-指数分布"><a href="#1-5-4-指数分布" class="headerlink" title="1.5.4 指数分布"></a>1.5.4 指数分布</h3><p>深度学习中, 指数分布用来描述在$x=0​$点处取得边界点的分布, 指数分布定义如下:</p>
<script type="math/tex; mode=display">
p(x;\lambda)=\lambda I_{x\geq 0}exp(-\lambda{x})</script><p>指数分布用指示函数$I_{x\geq 0}​$来使$x​$取负值时的概率为零。</p>
<h3 id="1-5-5-Laplace-分布"><a href="#1-5-5-Laplace-分布" class="headerlink" title="1.5.5 Laplace 分布"></a>1.5.5 Laplace 分布</h3><p>一个联系紧密的概率分布是 Laplace 分布（Laplace distribution），它允许我们在任意一点 $\mu$处设置概率质量的峰值</p>
<script type="math/tex; mode=display">
Laplace(x;\mu;\gamma)=\frac{1}{2\gamma}exp\left(-\frac{|x-\mu|}{\gamma}\right)</script><h3 id="1-5-6-Dirac分布和经验分布"><a href="#1-5-6-Dirac分布和经验分布" class="headerlink" title="1.5.6 Dirac分布和经验分布"></a>1.5.6 Dirac分布和经验分布</h3><p>Dirac分布可保证概率分布中所有质量都集中在一个点上. Diract分布的狄拉克$\delta​$函数(也称为<strong>单位脉冲函数</strong>)定义如下: </p>
<script type="math/tex; mode=display">
p(x)=\delta(x-\mu), x\neq \mu</script><script type="math/tex; mode=display">
\int_{a}^{b}\delta(x-\mu)dx = 1, a < \mu < b</script><p>Dirac 分布经常作为 经验分布（empirical distribution）的一个组成部分出现</p>
<script type="math/tex; mode=display">
\hat{p}(\vec{x})=\frac{1}{m}\sum_{i=1}^{m}\delta(\vec{x}-{\vec{x}}^{(i)})</script><p>, 其中, m个点$x^{1},…,x^{m}$是给定的数据集, <strong>经验分布</strong>将概率密度$\frac{1}{m}​$赋给了这些点.</p>
<p>当我们在训练集上训练模型时, 可以认为从这个训练集上得到的经验分布指明了<strong>采样来源</strong>.</p>
<p><strong>适用范围</strong>: 狄拉克δ函数适合对<strong>连续型</strong>随机变量的经验分布.</p>
<p>&gt;</p>
<h2 id="1-6-期望、方差、协方差、相关系数"><a href="#1-6-期望、方差、协方差、相关系数" class="headerlink" title="1.6 期望、方差、协方差、相关系数"></a>1.6 期望、方差、协方差、相关系数</h2><h3 id="1-6-1-期望"><a href="#1-6-1-期望" class="headerlink" title="1.6.1 期望"></a>1.6.1 期望</h3><p>在概率论和统计学中，数学期望（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和。它反映随机变量平均取值的大小。</p>
<ul>
<li>线性运算： $E(ax+by+c) = aE(x)+bE(y)+c$  </li>
<li>推广形式： $E(\sum_{k=1}^{n}{a_ix_i+c}) = \sum_{k=1}^{n}{a_iE(x_i)+c}$ </li>
<li>函数期望：设$f(x)$为$x$的函数，则$f(x)$的期望为<ul>
<li>离散函数： $E(f(x))=\sum_{k=1}^{n}{f(x_k)P(x_k)}$</li>
<li>连续函数： $E(f(x))=\int_{-\infty}^{+\infty}{f(x)p(x)dx}$</li>
</ul>
</li>
</ul>
<blockquote>
<p>注意：</p>
<ul>
<li>函数的期望大于等于期望的函数（Jensen不等式），即$E(f(x))\geqslant f(E(x))$  </li>
<li>一般情况下，乘积的期望不等于期望的乘积。  </li>
<li>如果$X$和$Y$相互独立，则$E(xy)=E(x)E(y)​$。  </li>
</ul>
</blockquote>
<h3 id="1-6-2-方差"><a href="#1-6-2-方差" class="headerlink" title="1.6.2 方差"></a>1.6.2 方差</h3><p>概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。方差是一种特殊的期望。定义为：</p>
<script type="math/tex; mode=display">
Var(x) = E((x-E(x))^2)</script><blockquote>
<p>方差性质：  </p>
<p>1）$Var(x) = E(x^2) -E(x)^2$<br>2）常数的方差为0;<br>3）方差不满足线性性质;<br>4）如果$X$和$Y$相互独立, $Var(ax+by)=a^2Var(x)+b^2Var(y)$   </p>
</blockquote>
<h3 id="1-6-3-协方差"><a href="#1-6-3-协方差" class="headerlink" title="1.6.3 协方差"></a>1.6.3 协方差</h3><p>协方差是衡量两个变量线性相关性强度及变量尺度。  两个随机变量的协方差定义为：</p>
<script type="math/tex; mode=display">
Cov(x,y)=E((x-E(x))(y-E(y)))</script><p>方差是一种特殊的协方差。当$X=Y$时，$Cov(x,y)=Var(x)=Var(y)$。</p>
<blockquote>
<p>协方差性质：  </p>
<p>1）独立变量的协方差为0。<br>2）协方差计算公式：</p>
</blockquote>
<script type="math/tex; mode=display">
Cov(\sum_{i=1}^{m}{a_ix_i}, \sum_{j=1}^{m}{b_jy_j}) = \sum_{i=1}^{m} \sum_{j=1}^{m}{a_ib_jCov(x_iy_i)}</script><p>&gt;</p>
<blockquote>
<p>3）特殊情况：</p>
</blockquote>
<script type="math/tex; mode=display">
Cov(a+bx, c+dy) = bdCov(x, y)</script><h3 id="1-6-4-相关系数"><a href="#1-6-4-相关系数" class="headerlink" title="1.6.4 相关系数"></a>1.6.4 相关系数</h3><p>相关系数是研究变量之间线性相关程度的量。两个随机变量的相关系数定义为：</p>
<script type="math/tex; mode=display">
Corr(x,y) = \frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}</script><blockquote>
<p>相关系数的性质：<br>1）有界性。相关系数的取值范围是 [-1,1]，可以看成无量纲的协方差。<br>2）值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性。  </p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]Ian，Goodfellow，Yoshua，Bengio，Aaron…深度学习[M]，人民邮电出版，2017</p>
<p>[2]周志华.机器学习[M].清华大学出版社，2016.</p>
<p>[3]同济大学数学系.高等数学（第七版）[M]，高等教育出版社，2014.</p>
<p>[4]盛骤，试式千，潘承毅等编. 概率论与数理统计（第4版）[M]，高等教育出版社，2008</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E4%BA%8C%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E4%BA%8C%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">第二章_机器学习基础知识</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:13:21 / 修改时间：14:38:56" itemprop="dateCreated datePublished" datetime="2020-03-03T12:13:21+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第二章-机器学习基础"><a href="#第二章-机器学习基础" class="headerlink" title="第二章 机器学习基础"></a>第二章 机器学习基础</h1><p>​    机器学习起源于上世纪50年代，1959年在IBM工作的Arthur Samuel设计了一个下棋程序，这个程序具有学习的能力，它可以在不断的对弈中提高自己。由此提出了“机器学习”这个概念，它是一个结合了多个学科如概率论，优化理论，统计等，最终在计算机上实现自我获取新知识，学习改善自己的这样一个研究领域。机器学习是人工智能的一个子集，目前已经发展出许多有用的方法，比如支持向量机，回归，决策树，随机森林，强化方法，集成学习，深度学习等等，一定程度上可以帮助人们完成一些数据预测，自动化，自动决策，最优化等初步替代脑力的任务。本章我们主要介绍下机器学习的基本概念、监督学习、分类算法、逻辑回归、代价函数、损失函数、LDA、PCA、决策树、支持向量机、EM算法、聚类和降维以及模型评估有哪些方法、指标等等。</p>
<h2 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h2><h3 id="2-1-1-大话理解机器学习本质"><a href="#2-1-1-大话理解机器学习本质" class="headerlink" title="2.1.1 大话理解机器学习本质"></a>2.1.1 大话理解机器学习本质</h3><p>​    机器学习(Machine Learning, ML)，顾名思义，让机器去学习。这里，机器指的是计算机，是算法运行的物理载体，你也可以把各种算法本身当做一个有输入和输出的机器。那么到底让计算机去学习什么呢？对于一个任务及其表现的度量方法，设计一种算法，让算法能够提取中数据所蕴含的规律，这就叫机器学习。如果输入机器的数据是带有标签的，就称作有监督学习。如果数据是无标签的，就是无监督学习。</p>
<h3 id="2-1-2-什么是神经网络"><a href="#2-1-2-什么是神经网络" class="headerlink" title="2.1.2 什么是神经网络"></a>2.1.2 什么是神经网络</h3><p>​    神经网络就是按照一定规则将多个神经元连接起来的网络。不同的神经网络，具有不同的连接规则。例如全连接(Full Connected, FC)神经网络，它的规则包括：</p>
<p>（1）有三种层：输入层，输出层，隐藏层。</p>
<p>（2）同一层的神经元之间没有连接。</p>
<p>（3）fully connected的含义：第 N 层的每个神经元和第 N-1 层的所有神经元相连，第 N-1 层神经元的输出就是第 N 层神经元的输入。</p>
<p>（4）每个连接都有一个权值。</p>
<p><strong>神经网络架构</strong><br>​    图2-1就是一个神经网络系统，它由很多层组成。输入层负责接收信息，比如一只猫的图片。输出层是计算机对这个输入信息的判断结果，它是不是猫。隐藏层就是对输入信息的传递和加工处理。<br><img src="/img/ch2/2.5.1.png" alt="图2-2 神经网络系统"></p>
<p>​                                图2-1 神经网络系统</p>
<h3 id="2-1-3-各种常见算法图示"><a href="#2-1-3-各种常见算法图示" class="headerlink" title="2.1.3 各种常见算法图示"></a>2.1.3 各种常见算法图示</h3><p>​    日常使用机器学习的任务中，我们经常会遇见各种算法，图2-2是各种常见算法的图示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">回归算法</th>
<th style="text-align:center">聚类算法</th>
<th style="text-align:center">正则化方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/ch2/2.1/1.jpg" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.1/2.jpg" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.1/3.jpg" alt=""></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">决策树学习</th>
<th style="text-align:center">贝叶斯方法</th>
<th style="text-align:center">基于核的算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/ch2/2.2.4.png" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.1/5.jpg" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.1/6.jpg" alt=""></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">聚类算法</th>
<th style="text-align:center">关联规则学习</th>
<th style="text-align:center">人工神经网络</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/ch2/2.1/7.jpg" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.2.8.png" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.2.09.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">深度学习</th>
<th style="text-align:center">降低维度算法</th>
<th style="text-align:center">集成算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/img/ch2/2.2.10.png" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.2.11.png" alt=""></td>
<td style="text-align:center"><img src="/img/ch2/2.2.12.png" alt=""></td>
</tr>
</tbody>
</table>
</div>
<p>​                                        图2-2 各种常见算法图示</p>
<h3 id="2-1-4-计算图的导数计算"><a href="#2-1-4-计算图的导数计算" class="headerlink" title="2.1.4 计算图的导数计算"></a>2.1.4 计算图的导数计算</h3><p>​    计算图导数计算是反向传播，利用链式法则和隐式函数求导。</p>
<p>​    假设 $z = f(u,v)$ 在点 $(u,v)$ 处偏导连续，$(u,v)$是关于 $t$ 的函数，在 $t$ 点可导，求 $z$ 在 $t$ 点的导数。</p>
<p>根据链式法则有</p>
<script type="math/tex; mode=display">
\frac{dz}{dt}=\frac{\partial z}{\partial u}.\frac{du}{dt}+\frac{\partial z}{\partial v}
                .\frac{dv}{dt}</script><p>​    链式法则用文字描述:“由两个函数凑起来的复合函数，其导数等于里边函数代入外边函数的值之导数，乘以里边函数的导数。<br>​    为了便于理解，下面举例说明：</p>
<script type="math/tex; mode=display">
f(x)=x^2,g(x)=2x+1</script><p>​    则:</p>
<script type="math/tex; mode=display">
{f[g(x)]}'=2[g(x)] \times g'(x)=2[2x+1] \times 2=8x+4</script><h3 id="2-1-5-理解局部最优与全局最优"><a href="#2-1-5-理解局部最优与全局最优" class="headerlink" title="2.1.5 理解局部最优与全局最优"></a>2.1.5 理解局部最优与全局最优</h3><p>​    笑谈局部最优和全局最优</p>
<blockquote>
<p>​    柏拉图有一天问老师苏格拉底什么是爱情？苏格拉底叫他到麦田走一次，摘一颗最大的麦穗回来，不许回头，只可摘一次。柏拉图空着手出来了，他的理由是，看见不错的，却不知道是不是最好的，一次次侥幸，走到尽头时，才发现还不如前面的，于是放弃。苏格拉底告诉他：“这就是爱情。”这故事让我们明白了一个道理，因为生命的一些不确定性，所以全局最优解是很难寻找到的，或者说根本就不存在，我们应该设置一些限定条件，然后在这个范围内寻找最优解，也就是局部最优解——有所斩获总比空手而归强，哪怕这种斩获只是一次有趣的经历。<br>​    柏拉图有一天又问什么是婚姻？苏格拉底叫他到树林走一次,选一棵最好的树做圣诞树，也是不许回头，只许选一次。这次他一身疲惫地拖了一棵看起来直挺、翠绿，却有点稀疏的杉树回来，他的理由是，有了上回的教训，好不容易看见一棵看似不错的，又发现时间、体力已经快不够用了，也不管是不是最好的，就拿回来了。苏格拉底告诉他：“这就是婚姻。”</p>
</blockquote>
<p>​    优化问题一般分为局部最优和全局最优。其中，</p>
<p>（1）局部最优，就是在函数值空间的一个有限区域内寻找最小值；而全局最优，是在函数值空间整个区域寻找最小值问题。</p>
<p>（2）函数局部最小点是它的函数值小于或等于附近点的点，但是有可能大于较远距离的点。</p>
<p>（3）全局最小点是那种它的函数值小于或等于所有的可行点。</p>
<h3 id="2-1-5-大数据与深度学习之间的关系"><a href="#2-1-5-大数据与深度学习之间的关系" class="headerlink" title="2.1.5 大数据与深度学习之间的关系"></a>2.1.5 大数据与深度学习之间的关系</h3><p>首先来看大数据、机器学习及数据挖掘三者简单的定义：</p>
<p><strong>大数据</strong>通常被定义为“超出常用软件工具捕获，管理和处理能力”的数据集。<br><strong>机器学习</strong>关心的问题是如何构建计算机程序使用经验自动改进。<br><strong>数据挖掘</strong>是从数据中提取模式的特定算法的应用，在数据挖掘中，重点在于算法的应用，而不是算法本身。</p>
<p><strong>机器学习和数据挖掘</strong>之间的关系如下：<br>数据挖掘是一个过程，在此过程中机器学习算法被用作提取数据集中的潜在有价值模式的工具。<br>大数据与深度学习关系总结如下：</p>
<p>（1）深度学习是一种模拟大脑的行为。可以从所学习对象的机制以及行为等等很多相关联的方面进行学习，模仿类型行为以及思维。</p>
<p>（2）深度学习对于大数据的发展有帮助。深度学习对于大数据技术开发的每一个阶段均有帮助，不管是数据的分析还是挖掘还是建模，只有深度学习，这些工作才会有可能一一得到实现。</p>
<p>（3）深度学习转变了解决问题的思维。很多时候发现问题到解决问题，走一步看一步不是一个主要的解决问题的方式了，在深度学习的基础上，要求我们从开始到最后都要基于一个目标，为了需要优化的那个最终目标去进行处理数据以及将数据放入到数据应用平台上去，这就是端到端（End to End）。</p>
<p>（4）大数据的深度学习需要一个框架。在大数据方面的深度学习都是从基础的角度出发的，深度学习需要一个框架或者一个系统。总而言之，将你的大数据通过深度分析变为现实，这就是深度学习和大数据的最直接关系。</p>
<h2 id="2-2-机器学习学习方式"><a href="#2-2-机器学习学习方式" class="headerlink" title="2.2 机器学习学习方式"></a>2.2 机器学习学习方式</h2><p>​    根据数据类型的不同，对一个问题的建模有不同的方式。依据不同的学习方式和输入数据，机器学习主要分为以下四种学习方式。</p>
<h3 id="2-2-1-监督学习"><a href="#2-2-1-监督学习" class="headerlink" title="2.2.1 监督学习"></a>2.2.1 监督学习</h3><p>​    特点：监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。</p>
<p>​    常见应用场景：监督式学习的常见应用场景如分类问题和回归问题。</p>
<p>​    算法举例：常见的有监督机器学习算法包括支持向量机(Support Vector Machine, SVM)，朴素贝叶斯(Naive Bayes)，逻辑回归(Logistic Regression)，K近邻(K-Nearest Neighborhood, KNN)，决策树(Decision Tree)，随机森林(Random Forest)，AdaBoost以及线性判别分析(Linear Discriminant Analysis, LDA)等。深度学习(Deep Learning)也是大多数以监督学习的方式呈现。</p>
<h3 id="2-2-2-非监督式学习"><a href="#2-2-2-非监督式学习" class="headerlink" title="2.2.2 非监督式学习"></a>2.2.2 非监督式学习</h3><p>​    定义：在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些内在结构。</p>
<p>​    常见应用场景：常见的应用场景包括关联规则的学习以及聚类等。</p>
<p>​    算法举例：常见算法包括Apriori算法以及k-Means算法。</p>
<h3 id="2-2-3-半监督式学习"><a href="#2-2-3-半监督式学习" class="headerlink" title="2.2.3 半监督式学习"></a>2.2.3 半监督式学习</h3><p>​    特点：在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。</p>
<p>​    常见应用场景：应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。</p>
<p>​    算法举例：常见算法如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。</p>
<h3 id="2-2-4-弱监督学习"><a href="#2-2-4-弱监督学习" class="headerlink" title="2.2.4 弱监督学习"></a>2.2.4 弱监督学习</h3><p>​    特点：弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。</p>
<p>​    算法举例：举例，给出一张包含气球的图片，需要得出气球在图片中的位置及气球和背景的分割线，这就是已知弱标签学习强标签的问题。</p>
<p>​    在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。</p>
<h3 id="2-2-5-监督学习有哪些步骤"><a href="#2-2-5-监督学习有哪些步骤" class="headerlink" title="2.2.5 监督学习有哪些步骤"></a>2.2.5 监督学习有哪些步骤</h3><p>​    监督学习是使用已知正确答案的示例来训练网络，每组训练数据有一个明确的标识或结果。想象一下，我们可以训练一个网络，让其从照片库中（其中包含气球的照片）识别出气球的照片。以下就是我们在这个假设场景中所要采取的步骤。</p>
<p><strong>步骤1：数据集的创建和分类</strong><br>​    首先，浏览你的照片（数据集），确定所有包含气球的照片，并对其进行标注。然后，将所有照片分为训练集和验证集。目标就是在深度网络中找一函数，这个函数输入是任意一张照片，当照片中包含气球时，输出1，否则输出0。</p>
<p><strong>步骤2：数据增强（Data Augmentation）</strong><br>​    当原始数据搜集和标注完毕，一般搜集的数据并不一定包含目标在各种扰动下的信息。数据的好坏对于机器学习模型的预测能力至关重要，因此一般会进行数据增强。对于图像数据来说，数据增强一般包括，图像旋转，平移，颜色变换，裁剪，仿射变换等。</p>
<p><strong>步骤3：特征工程（Feature Engineering）</strong><br>​    一般来讲，特征工程包含特征提取和特征选择。常见的手工特征(Hand-Crafted Feature)有尺度不变特征变换(Scale-Invariant Feature Transform, SIFT)，方向梯度直方图(Histogram of Oriented Gradient, HOG)等。由于手工特征是启发式的，其算法设计背后的出发点不同，将这些特征组合在一起的时候有可能会产生冲突，如何将组合特征的效能发挥出来，使原始数据在特征空间中的判别性最大化，就需要用到特征选择的方法。在深度学习方法大获成功之后，人们很大一部分不再关注特征工程本身。因为，最常用到的卷积神经网络(Convolutional Neural Networks, CNNs)本身就是一种特征提取和选择的引擎。研究者提出的不同的网络结构、正则化、归一化方法实际上就是深度学习背景下的特征工程。</p>
<p><strong>步骤4：构建预测模型和损失</strong><br>​    将原始数据映射到特征空间之后，也就意味着我们得到了比较合理的输入。下一步就是构建合适的预测模型得到对应输入的输出。而如何保证模型的输出和输入标签的一致性，就需要构建模型预测和标签之间的损失函数，常见的损失函数(Loss Function)有交叉熵、均方差等。通过优化方法不断迭代，使模型从最初的初始化状态一步步变化为有预测能力的模型的过程，实际上就是学习的过程。</p>
<p><strong>步骤5：训练</strong><br>​    选择合适的模型和超参数进行初始化，其中超参数比如支持向量机中核函数、误差项惩罚权重等。当模型初始化参数设定好后，将制作好的特征数据输入到模型，通过合适的优化方法不断缩小输出与标签之间的差距，当迭代过程到了截止条件，就可以得到训练好的模型。优化方法最常见的就是梯度下降法及其变种，使用梯度下降法的前提是优化目标函数对于模型是可导的。</p>
<p><strong>步骤6：验证和模型选择</strong><br>​    训练完训练集图片后，需要进行模型测试。利用验证集来验证模型是否可以准确地挑选出含有气球在内的照片。<br>​    在此过程中，通常会通过调整和模型相关的各种事物（超参数）来重复步骤2和3，诸如里面有多少个节点，有多少层，使用怎样的激活函数和损失函数，如何在反向传播阶段积极有效地训练权值等等。</p>
<p><strong>步骤7：测试及应用</strong><br>​    当有了一个准确的模型，就可以将该模型部署到你的应用程序中。你可以将预测功能发布为API（Application Programming Interface, 应用程序编程接口）调用，并且你可以从软件中调用该API，从而进行推理并给出相应的结果。</p>
<h2 id="2-8-分类算法"><a href="#2-8-分类算法" class="headerlink" title="2.8 分类算法"></a>2.8 分类算法</h2><p>​    分类算法和回归算法是对真实世界不同建模的方法。分类模型是认为模型的输出是离散的，例如大自然的生物被划分为不同的种类，是离散的。回归模型的输出是连续的，例如人的身高变化过程是一个连续过程，而不是离散的。</p>
<p>​    因此，在实际建模过程时，采用分类模型还是回归模型，取决于你对任务（真实世界）的分析和理解。</p>
<h3 id="2-8-1-常用分类算法的优缺点？"><a href="#2-8-1-常用分类算法的优缺点？" class="headerlink" title="2.8.1 常用分类算法的优缺点？"></a>2.8.1 常用分类算法的优缺点？</h3><p>​    接下来我们介绍常用分类算法的优缺点，如表2-1所示。</p>
<p>​                                    表2-1 常用分类算法的优缺点</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">算法</th>
<th style="text-align:left">优点</th>
<th style="text-align:left">缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Bayes 贝叶斯分类法</td>
<td style="text-align:left">1）所需估计的参数少，对于缺失数据不敏感。<br />2）有着坚实的数学基础，以及稳定的分类效率。</td>
<td style="text-align:left">1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br />2）需要知道先验概率。<br />3）分类决策存在错误率。</td>
</tr>
<tr>
<td style="text-align:left">Decision Tree决策树</td>
<td style="text-align:left">1）不需要任何领域知识或参数假设。<br />2）适合高维数据。<br />3）简单易于理解。<br />4）短时间内处理大量数据，得到可行且效果较好的结果。<br />5）能够同时处理数据型和常规性属性。</td>
<td style="text-align:left">1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br />2）易于过拟合。<br />3）忽略属性之间的相关性。<br />4）不支持在线学习。</td>
</tr>
<tr>
<td style="text-align:left">SVM支持向量机</td>
<td style="text-align:left">1）可以解决小样本下机器学习的问题。<br />2）提高泛化性能。<br />3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。<br />4）避免神经网络结构选择和局部极小的问题。</td>
<td style="text-align:left">1）对缺失数据敏感。<br />2）内存消耗大，难以解释。<br />3）运行和调参略烦人。</td>
</tr>
<tr>
<td style="text-align:left">KNN K近邻</td>
<td style="text-align:left">1）思想简单，理论成熟，既可以用来做分类也可以用来做回归； <br />2）可用于非线性分类；<br /> 3）训练时间复杂度为O(n)； <br />4）准确度高，对数据没有假设，对outlier不敏感；</td>
<td style="text-align:left">1）计算量太大。<br />2）对于样本分类不均衡的问题，会产生误判。<br />3）需要大量的内存。<br />4）输出的可解释性不强。</td>
</tr>
<tr>
<td style="text-align:left">Logistic Regression逻辑回归</td>
<td style="text-align:left">1）速度快。<br />2）简单易于理解，直接看到各个特征的权重。<br />3）能容易地更新模型吸收新的数据。<br />4）如果想要一个概率框架，动态调整分类阀值。</td>
<td style="text-align:left">特征处理复杂。需要归一化和较多的特征工程。</td>
</tr>
<tr>
<td style="text-align:left">Neural Network 神经网络</td>
<td style="text-align:left">1）分类准确率高。<br />2）并行处理能力强。<br />3）分布式存储和学习能力强。<br />4）鲁棒性较强，不易受噪声影响。</td>
<td style="text-align:left">1）需要大量参数（网络拓扑、阀值、阈值）。<br />2）结果难以解释。<br />3）训练时间过长。</td>
</tr>
<tr>
<td style="text-align:left">Adaboosting</td>
<td style="text-align:left">1）adaboost是一种有很高精度的分类器。<br />2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br />3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br />4）简单，不用做特征筛选。<br />5）不用担心overfitting。</td>
<td style="text-align:left">对outlier比较敏感</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-8-2-分类算法的评估方法"><a href="#2-8-2-分类算法的评估方法" class="headerlink" title="2.8.2 分类算法的评估方法"></a>2.8.2 分类算法的评估方法</h3><p>​    分类评估方法主要功能是用来评估分类算法的好坏，而评估一个分类器算法的好坏又包括许多项指标。了解各种评估方法，在实际应用中选择正确的评估方法是十分重要的。</p>
<ul>
<li><p><strong>几个常用术语</strong><br>​    这里首先介绍几个常见的模型评价术语，现在假设我们的分类目标只有两类，计为正例（positive）和负例（negative）分别是：<br> 1) True positives(TP):  被正确地划分为正例的个数，即实际为正例且被分类器划分为正例的实例数；<br> 2) False positives(FP): 被错误地划分为正例的个数，即实际为负例但被分类器划分为正例的实例数；<br> 3) False negatives(FN):被错误地划分为负例的个数，即实际为正例但被分类器划分为负例的实例数；<br> 4) True negatives(TN): 被正确地划分为负例的个数，即实际为负例且被分类器划分为负例的实例数。　</p>
<p>​                                    表2-2 四个术语的混淆矩阵</p>
</li>
</ul>
<p><img src="/img/ch2/2.9/1.png" alt="图2-3 术语的混淆矩阵"></p>
<p>表2-2是这四个术语的混淆矩阵，做以下说明：<br>    1）P=TP+FN表示实际为正例的样本个数。<br>    2）True、False描述的是分类器是否判断正确。<br>    3）Positive、Negative是分类器的分类结果，如果正例计为1、负例计为-1，即positive=1、negative=-1。用1表示True，-1表示False，那么实际的类标=TF*PN，TF为true或false，PN为positive或negative。<br>    4）例如True positives(TP)的实际类标=1*1=1为正例，False positives(FP)的实际类标=(-1)*1=-1为负例，False negatives(FN)的实际类标=(-1)*(-1)=1为正例，True negatives(TN)的实际类标=1*(-1)=-1为负例。</p>
<ul>
<li><p><strong>评价指标</strong><br>1) 正确率（accuracy）</p>
<pre><code>正确率是我们最常见的评价指标，accuracy = (TP+TN)/(P+N)，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好。
</code></pre><p>2) 错误率（error rate)</p>
<pre><code>错误率则与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(P+N)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 -  error rate。
</code></pre><p>3) 灵敏度（sensitivity）</p>
<pre><code>sensitivity = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。
</code></pre><p>4) 特异性（specificity)</p>
<pre><code>specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。
</code></pre><p>5) 精度（precision）</p>
<pre><code>precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。
</code></pre><p>6) 召回率（recall）</p>
<pre><code>召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitivity，可以看到召回率与灵敏度是一样的。
</code></pre><p>7) 其他评价指标</p>
<pre><code>计算速度：分类器训练和预测需要的时间；
鲁棒性：处理缺失值和异常值的能力；
可扩展性：处理大数据集的能力；
可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。
</code></pre><p>8) 精度和召回率反映了分类器分类性能的两个方面。如果综合考虑查准率与查全率，可以得到新的评价指标F1-score，也称为综合分类率：$F1=\frac{2 \times precision \times recall}{precision + recall}​$。</p>
<pre><code>  为了综合多个类别的分类情况，评测系统整体性能，经常采用的还有微平均F1（micro-averaging）和宏平均F1（macro-averaging ）两种指标。

  （1）宏平均F1与微平均F1是以两种不同的平均方式求的全局F1指标。

  （2）宏平均F1的计算方法先对每个类别单独计算F1值，再取这些F1值的算术平均值作为全局指标。

  （3）微平均F1的计算方法是先累加计算各个类别的a、b、c、d的值，再由这些值求出F1值。

  （4）由两种平均F1的计算方式不难看出，宏平均F1平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均F1平等考虑文档集中的每一个文档，所以它的值受到常见类别的影响比较大。
</code></pre></li>
</ul>
<ul>
<li><p><strong>ROC曲线和PR曲线</strong></p>
<pre><code>  如图2-3，ROC曲线是（Receiver Operating Characteristic Curve，受试者工作特征曲线）的简称，是以灵敏度（真阳性率）为纵坐标，以1减去特异性（假阳性率）为横坐标绘制的性能评价曲线。可以将不同模型对同一数据集的ROC曲线绘制在同一笛卡尔坐标系中，ROC曲线越靠近左上角，说明其对应模型越可靠。也可以通过ROC曲线下面的面积（Area Under Curve, AUC）来评价模型，AUC越大，模型越可靠。
</code></pre></li>
</ul>
<p><img src="/img/ch2/2.7.3.png" alt=""></p>
<p>​                                                                             图2-3 ROC曲线</p>
<p>​    PR曲线是Precision Recall Curve的简称，描述的是precision和recall之间的关系，以recall为横坐标，precision为纵坐标绘制的曲线。该曲线的所对应的面积AUC实际上是目标检测中常用的评价指标平均精度（Average Precision, AP）。AP越高，说明模型性能越好。</p>
<h3 id="2-8-3-正确率能很好的评估分类算法吗"><a href="#2-8-3-正确率能很好的评估分类算法吗" class="headerlink" title="2.8.3 正确率能很好的评估分类算法吗"></a>2.8.3 正确率能很好的评估分类算法吗</h3><p>​    不同算法有不同特点，在不同数据集上有不同的表现效果，根据特定的任务选择不同的算法。如何评价分类算法的好坏，要做具体任务具体分析。对于决策树，主要用正确率去评估，但是其他算法，只用正确率能很好的评估吗？<br>​    答案是否定的。<br>​    正确率确实是一个很直观很好的评价指标，但是有时候正确率高并不能完全代表一个算法就好。比如对某个地区进行地震预测，地震分类属性分为0：不发生地震、1发生地震。我们都知道，不发生的概率是极大的，对于分类器而言，如果分类器不加思考，对每一个测试样例的类别都划分为0，达到99%的正确率，但是，问题来了，如果真的发生地震时，这个分类器毫无察觉，那带来的后果将是巨大的。很显然，99%正确率的分类器并不是我们想要的。出现这种现象的原因主要是数据分布不均衡，类别为1的数据太少，错分了类别1但达到了很高的正确率缺忽视了研究者本身最为关注的情况。</p>
<h3 id="2-8-4-什么样的分类器是最好的"><a href="#2-8-4-什么样的分类器是最好的" class="headerlink" title="2.8.4 什么样的分类器是最好的"></a>2.8.4 什么样的分类器是最好的</h3><p>​    对某一个任务，某个具体的分类器不可能同时满足或提高所有上面介绍的指标。<br>​    如果一个分类器能正确分对所有的实例，那么各项指标都已经达到最优，但这样的分类器往往不存在。比如之前说的地震预测，既然不能百分百预测地震的发生，但实际情况中能容忍一定程度的误报。假设在1000次预测中，共有5次预测发生了地震，真实情况中有一次发生了地震，其他4次则为误报。正确率由原来的999/1000=99.9下降为996/1000=99.6。召回率由0/1=0%上升为1/1=100%。对此解释为，虽然预测失误了4次，但真的地震发生前，分类器能预测对，没有错过，这样的分类器实际意义更为重大，正是我们想要的。在这种情况下，在一定正确率前提下，要求分类器的召回率尽量高。</p>
<h2 id="2-9-逻辑回归"><a href="#2-9-逻辑回归" class="headerlink" title="2.9 逻辑回归"></a>2.9 逻辑回归</h2><h3 id="2-9-1-回归划分"><a href="#2-9-1-回归划分" class="headerlink" title="2.9.1 回归划分"></a>2.9.1 回归划分</h3><p>广义线性模型家族里，依据因变量不同，可以有如下划分：</p>
<p>（1）如果是连续的，就是多重线性回归。</p>
<p>（2）如果是二项分布，就是逻辑回归。</p>
<p>（3）如果是泊松（Poisson）分布，就是泊松回归。</p>
<p>（4）如果是负二项分布，就是负二项回归。</p>
<p>（5）逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的逻辑回归。</p>
<h3 id="2-9-2-逻辑回归适用性"><a href="#2-9-2-逻辑回归适用性" class="headerlink" title="2.9.2 逻辑回归适用性"></a>2.9.2 逻辑回归适用性</h3><p>逻辑回归可用于以下几个方面：</p>
<p>（1）用于概率预测。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。</p>
<p>（2）用于分类。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。</p>
<p>（3）寻找危险因素。寻找某一疾病的危险因素等。</p>
<p>（4）仅能用于线性问题。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。</p>
<p>（5）各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算。</p>
<h3 id="2-9-3-逻辑回归与朴素贝叶斯有什么区别"><a href="#2-9-3-逻辑回归与朴素贝叶斯有什么区别" class="headerlink" title="2.9.3 逻辑回归与朴素贝叶斯有什么区别"></a>2.9.3 逻辑回归与朴素贝叶斯有什么区别</h3><p>逻辑回归与朴素贝叶斯区别有以下几个方面：</p>
<p>（1）逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。</p>
<p>（2）朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。</p>
<p>（3）朴素贝叶斯需要条件独立假设。</p>
<p>（4）逻辑回归需要求特征参数间是线性的。</p>
<h3 id="2-9-4-线性回归与逻辑回归的区别"><a href="#2-9-4-线性回归与逻辑回归的区别" class="headerlink" title="2.9.4 线性回归与逻辑回归的区别"></a>2.9.4 线性回归与逻辑回归的区别</h3><p>线性回归与逻辑回归的区别如下描述：</p>
<p>（1）线性回归的样本的输出，都是连续值，$ y\in (-\infty ,+\infty )$，而逻辑回归中$y\in (0,1)$，只能取0和1。</p>
<p>（2）对于拟合函数也有本质上的差别： </p>
<p>​    线性回归：$f(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+…+\theta _{n}x _{n}$</p>
<p>​    逻辑回归：$f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$，其中，$g(z)=\frac{1}{1+e^{-z}}$</p>
<p>​    可以看出，线性回归的拟合函数，是对f(x)的输出变量y的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。</p>
<p>​    那么，为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？ </p>
<p>​    $\theta ^{T}x=0$就相当于是1类和0类的决策边界： </p>
<p>​    当$\theta ^{T}x&gt;0$，则y&gt;0.5；若$\theta ^{T}x\rightarrow +\infty $，则$y \rightarrow  1 $，即y为1类; </p>
<p>​    当$\theta ^{T}x&lt;0$，则y&lt;0.5；若$\theta ^{T}x\rightarrow -\infty $，则$y \rightarrow  0 $，即y为0类; </p>
<p>这个时候就能看出区别，在线性回归中$\theta ^{T}x$为预测值的拟合函数；而在逻辑回归中$\theta ^{T}x$为决策边界。下表2-3为线性回归和逻辑回归的区别。</p>
<p>​                                    表2-3 线性回归和逻辑回归的区别</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">线性回归</th>
<th style="text-align:center">逻辑回归</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">目的</td>
<td style="text-align:center">预测</td>
<td style="text-align:center">分类</td>
</tr>
<tr>
<td style="text-align:center">$y^{(i)}$</td>
<td style="text-align:center">未知</td>
<td style="text-align:center">（0,1）</td>
</tr>
<tr>
<td style="text-align:center">函数</td>
<td style="text-align:center">拟合函数</td>
<td style="text-align:center">预测函数</td>
</tr>
<tr>
<td style="text-align:center">参数计算方式</td>
<td style="text-align:center">最小二乘法</td>
<td style="text-align:center">极大似然估计</td>
</tr>
</tbody>
</table>
</div>
<p>下面具体解释一下： </p>
<ol>
<li>拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得$y^{(i)} \in (0,1)$;</li>
<li>最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：最大似然估计是计算使得数据出现的可能性最大的参数，依仗的自然是Probability。而最小二乘是计算误差损失。</li>
</ol>
<h2 id="2-10-代价函数"><a href="#2-10-代价函数" class="headerlink" title="2.10 代价函数"></a>2.10 代价函数</h2><h3 id="2-10-1-为什么需要代价函数"><a href="#2-10-1-为什么需要代价函数" class="headerlink" title="2.10.1 为什么需要代价函数"></a>2.10.1 为什么需要代价函数</h3><ol>
<li>为了得到训练逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。</li>
<li>用于找到最优解的目的函数。</li>
</ol>
<h3 id="2-10-2-代价函数作用原理"><a href="#2-10-2-代价函数作用原理" class="headerlink" title="2.10.2 代价函数作用原理"></a>2.10.2 代价函数作用原理</h3><p>​    在回归问题中，通过代价函数来求解最优解，常用的是平方误差代价函数。假设函数图像如图2-4所示，当参数发生变化时，假设函数状态也会随着变化。</p>
<p><img src="/img/ch2/2.16/1.jpg" alt=""></p>
<p>​                                        图2-4  $h(x) = A + Bx$函数示意图</p>
<p>​    想要拟合图中的离散点，我们需要尽可能找到最优的$A$和$B$来使这条直线更能代表所有数据。如何找到最优解呢，这就需要使用代价函数来求解，以平方误差代价函数为例，假设函数为$h(x)=\theta_0x$。<br>​    <strong>平方误差代价函数的主要思想</strong>就是将实际数据给出的值与拟合出的线的对应值做差，求出拟合出的直线与实际的差距。在实际应用中，为了避免因个别极端数据产生的影响，采用类似方差再取二分之一的方式来减小个别数据的影响。因此，引出代价函数：</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2</script><p>​    <strong>最优解即为代价函数的最小值</strong>$\min J(\theta_0, \theta_1)$。如果是1个参数，代价函数一般通过二维曲线便可直观看出。如果是2个参数，代价函数通过三维图像可看出效果，参数越多，越复杂。<br>当参数为2个时，代价函数是三维图像，如下图2-5所示。</p>
<p><img src="/img/ch2/2.16/2.jpg" alt=""></p>
<p>​                                        图2-5  代价函数三维图像</p>
<h3 id="2-10-3-为什么代价函数要非负"><a href="#2-10-3-为什么代价函数要非负" class="headerlink" title="2.10.3 为什么代价函数要非负"></a>2.10.3 为什么代价函数要非负</h3><p>​    目标函数存在一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。<br>​    只要设计的目标函数有下界，基本上都可以，代价函数非负更为方便。</p>
<h3 id="2-10-4-常见代价函数"><a href="#2-10-4-常见代价函数" class="headerlink" title="2.10.4 常见代价函数"></a>2.10.4 常见代价函数</h3><p>（1）<strong>二次代价函数（quadratic cost）</strong>：</p>
<script type="math/tex; mode=display">
J = \frac{1}{2n}\sum_x\Vert y(x)-a^L(x)\Vert^2</script><p>​    其中，$J$表示代价函数，$x$表示样本，$y$表示实际值，$a$表示输出值，$n$表示样本的总数。使用一个样本为例简单说明，此时二次代价函数为：</p>
<script type="math/tex; mode=display">
J = \frac{(y-a)^2}{2}</script><p>​    假如使用梯度下降法（Gradient descent）来调整权值参数的大小，权值$w$和偏置$b$的梯度推导如下：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial b}=(a-y)\sigma'(z)</script><p>其中，$z​$表示神经元的输入，$\sigma​$表示激活函数。权值$w​$和偏置$b​$的梯度跟激活函数的梯度成正比，激活函数的梯度越大，权值$w​$和偏置$b​$的大小调整得越快，训练收敛得就越快。</p>
<p><em>注</em>：神经网络常用的激活函数为sigmoid函数，该函数的曲线如下图2-6所示：</p>
<p><img src="/img/ch2/2.18/1.jpg" alt=""></p>
<p>​                                                图2-6 sigmoid函数曲线</p>
<p>如上图所示，对0.88和0.98两个点进行比较：<br>​    假设目标是收敛到1.0。0.88离目标1.0比较远，梯度比较大，权值调整比较大。0.98离目标1.0比较近，梯度比较小，权值调整比较小。调整方案合理。<br>​    假如目标是收敛到0。0.88离目标0比较近，梯度比较大，权值调整比较大。0.98离目标0比较远，梯度比较小，权值调整比较小。调整方案不合理。<br>​    原因：在使用sigmoid函数的情况下, 初始的代价（误差）越大，导致训练越慢。</p>
<p>（2）<strong>交叉熵代价函数（cross-entropy）</strong>：</p>
<script type="math/tex; mode=display">
J = -\frac{1}{n}\sum_x[y\ln a + (1-y)\ln{(1-a)}]</script><p>其中，$J$表示代价函数，$x$表示样本，$y$表示实际值，$a$表示输出值，$n$表示样本的总数。<br>    权值$w$和偏置$b​$的梯度推导如下：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)</script><p>当误差越大时，梯度就越大，权值$w$和偏置$b$调整就越快，训练的速度也就越快。<br><strong>二次代价函数适合输出神经元是线性的情况，交叉熵代价函数适合输出神经元是S型函数的情况。</strong></p>
<p>（3）<strong>对数似然代价函数（log-likelihood cost）</strong>：<br>对数似然函数常用来作为softmax回归的代价函数。深度学习中普遍的做法是将softmax作为最后一层，此时常用的代价函数是对数似然代价函数。<br>    对数似然代价函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似。对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式。<br>在tensorflow中：<br>    与sigmoid搭配使用的交叉熵函数：<code>tf.nn.sigmoid_cross_entropy_with_logits()</code>。<br>    与softmax搭配使用的交叉熵函数：<code>tf.nn.softmax_cross_entropy_with_logits()</code>。<br>在pytorch中：<br>        与sigmoid搭配使用的交叉熵函数：<code>torch.nn.BCEWithLogitsLoss()</code>。<br>    与softmax搭配使用的交叉熵函数：<code>torch.nn.CrossEntropyLoss()</code>。</p>
<h3 id="2-10-5-为什么用交叉熵代替二次代价函数"><a href="#2-10-5-为什么用交叉熵代替二次代价函数" class="headerlink" title="2.10.5 为什么用交叉熵代替二次代价函数"></a>2.10.5 为什么用交叉熵代替二次代价函数</h3><p>（1）<strong>为什么不用二次方代价函数</strong><br>由上一节可知，权值$w$和偏置$b$的偏导数为$\frac{\partial J}{\partial w}=(a-y)\sigma’(z)x$，$\frac{\partial J}{\partial b}=(a-y)\sigma’(z)$， 偏导数受激活函数的导数影响，sigmoid函数导数在输出接近0和1时非常小，会导致一些实例在刚开始训练时学习得非常慢。</p>
<p>（2）<strong>为什么要用交叉熵</strong><br>交叉熵函数权值$w$和偏置$b$的梯度推导为：</p>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial w_j}=\frac{1}{n}\sum_{x}x_j(\sigma{(z)}-y)\;，
\frac{\partial J}{\partial b}=\frac{1}{n}\sum_{x}(\sigma{(z)}-y)</script><p>由以上公式可知，权重学习的速度受到$\sigma{(z)}-y$影响，更大的误差，就有更快的学习速度，避免了二次代价函数方程中因$\sigma’{(z)}$导致的学习缓慢的情况。</p>
<h2 id="2-11-损失函数"><a href="#2-11-损失函数" class="headerlink" title="2.11 损失函数"></a>2.11 损失函数</h2><h3 id="2-11-1-什么是损失函数"><a href="#2-11-1-什么是损失函数" class="headerlink" title="2.11.1 什么是损失函数"></a>2.11.1 什么是损失函数</h3><p>​    损失函数（Loss Function）又叫做误差函数，用来衡量算法的运行情况，估量模型的预测值与真实值的不一致程度，是一个非负实值函数，通常使用$<br>L(Y, f(x))​$来表示。损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。</p>
<h3 id="2-11-2-常见的损失函数"><a href="#2-11-2-常见的损失函数" class="headerlink" title="2.11.2 常见的损失函数"></a>2.11.2 常见的损失函数</h3><p>​    机器学习通过对算法中的目标函数进行不断求解优化，得到最终想要的结果。分类和回归问题中，通常使用损失函数或代价函数作为目标函数。<br>​    损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。<br>​    损失函数可分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是在经验风险损失函数上加上正则项。<br>​    下面介绍常用的损失函数：</p>
<p>（1）<strong>0-1损失函数</strong><br>如果预测值和目标值相等，值为0，如果不相等，值为1。</p>
<script type="math/tex; mode=display">
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}</script><p>一般的在实际使用中，相等的条件过于严格，可适当放宽条件：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) =
\begin{cases}
1,& |Y-f(x)|\geqslant T\\
0,& |Y-f(x)|< T
\end{cases}</script><p>（2）<strong>绝对值损失函数</strong><br>和0-1损失函数相似，绝对值损失函数表示为：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) = |Y-f(x)|​</script><p>（3）<strong>平方损失函数</strong></p>
<script type="math/tex; mode=display">
L(Y, f(x)) = \sum_N{(Y-f(x))}^2</script><p>这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该使所有点到回归直线的距离和最小。</p>
<p>（4）<strong>对数损失函数</strong></p>
<script type="math/tex; mode=display">
L(Y, P(Y|X)) = -\log{P(Y|X)}</script><p>​    常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数是平方损失，其实不然。逻辑回归它假设样本服从伯努利分布（0-1分布），进而求得满足该分布的似然函数，接着取对数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看，就是对数损失函数。</p>
<p>（6）<strong>指数损失函数</strong><br>指数损失函数的标准形式为：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) = \exp(-Yf(x))</script><p>例如AdaBoost就是以指数损失函数为损失函数。</p>
<p>（7）<strong>Hinge损失函数</strong><br>Hinge损失函数的标准形式如下：</p>
<script type="math/tex; mode=display">
L(y) = \max{(0, 1-ty)}</script><p>统一的形式：</p>
<script type="math/tex; mode=display">
L(Y, f(x)) = \max{(0, Yf(x))}</script><p>其中y是预测值，范围为(-1,1)，t为目标值，其为-1或1。</p>
<p>在线性支持向量机中，最优化问题可等价于</p>
<script type="math/tex; mode=display">
\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w\Vert ^2</script><p>上式相似于下式</p>
<script type="math/tex; mode=display">
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w\Vert ^2</script><p>其中$l(wx_i+by_i)$是Hinge损失函数，$\Vert w\Vert ^2$可看做为正则化项。</p>
<h3 id="2-11-3-逻辑回归为什么使用对数损失函数"><a href="#2-11-3-逻辑回归为什么使用对数损失函数" class="headerlink" title="2.11.3 逻辑回归为什么使用对数损失函数"></a>2.11.3 逻辑回归为什么使用对数损失函数</h3><p>假设逻辑回归模型</p>
<script type="math/tex; mode=display">
P(y=1|x;\theta)=\frac{1}{1+e^{-\theta^{T}x}}</script><p>假设逻辑回归模型的概率分布是伯努利分布，其概率质量函数为：</p>
<script type="math/tex; mode=display">
P(X=n)=
\begin{cases}
1-p, n=0\\
 p,n=1
\end{cases}</script><p>其似然函数为：</p>
<script type="math/tex; mode=display">
L(\theta)=\prod_{i=1}^{m}
P(y=1|x_i)^{y_i}P(y=0|x_i)^{1-y_i}</script><p>对数似然函数为：</p>
<script type="math/tex; mode=display">
\ln L(\theta)=\sum_{i=1}^{m}[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln{P(y=0|x_i)}]\\
  =\sum_{i=1}^m[y_i\ln{P(y=1|x_i)}+(1-y_i)\ln(1-P(y=1|x_i))]</script><p>对数函数在单个数据点上的定义为：</p>
<script type="math/tex; mode=display">
cost(y,p(y|x))=-y\ln{p(y|x)-(1-y)\ln(1-p(y|x))}</script><p>则全局样本损失函数为：</p>
<script type="math/tex; mode=display">
cost(y,p(y|x)) = -\sum_{i=1}^m[y_i\ln p(y_i|x_i)+(1-y_i)\ln(1-p(y_i|x_i))]</script><p>由此可看出，对数损失函数与极大似然估计的对数似然函数本质上是相同的。所以逻辑回归直接采用对数损失函数。</p>
<h3 id="2-11-4-对数损失函数是如何度量损失的"><a href="#2-11-4-对数损失函数是如何度量损失的" class="headerlink" title="2.11.4 对数损失函数是如何度量损失的"></a>2.11.4 对数损失函数是如何度量损失的</h3><p>​    例如，在高斯分布中，我们需要确定均值和标准差。<br>​    如何确定这两个参数？最大似然估计是比较常用的方法。最大似然的目标是找到一些参数值，这些参数值对应的分布可以最大化观测到数据的概率。<br>​    因为需要计算观测到所有数据的全概率，即所有观测到的数据点的联合概率。现考虑如下简化情况：</p>
<p>（1）假设观测到每个数据点的概率和其他数据点的概率是独立的。</p>
<p>（2）取自然对数。<br>假设观测到单个数据点$x_i(i=1,2,…n)$的概率为：</p>
<script type="math/tex; mode=display">
P(x_i;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_i-\mu)^2}{2\sigma^2} \right)</script><p>（3）其联合概率为：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,...,x_n;\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_1-\mu)^2}{2\sigma^2} \right) \\ \times
         \frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_2-\mu)^2}{2\sigma^2} \right) \times ... \times
        \frac{1}{\sigma \sqrt{2\pi}}\exp 
        \left( - \frac{(x_n-\mu)^2}{2\sigma^2} \right)</script><p>​    对上式取自然对数，可得：</p>
<script type="math/tex; mode=display">
 \ln(P(x_1,x_2,...x_n;\mu,\sigma))=
         \ln \left(\frac{1}{\sigma \sqrt{2\pi}} \right) 
          - \frac{(x_1-\mu)^2}{2\sigma^2}  \\ +
          \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right) 
          - \frac{(x_2-\mu)^2}{2\sigma^2} +...+
          \ln \left( \frac{1}{\sigma \sqrt{2\pi}} \right) 
          - \frac{(x_n-\mu)^2}{2\sigma^2}</script><p>根据对数定律，上式可以化简为：</p>
<script type="math/tex; mode=display">
\ln(P(x_1,x_2,...x_n;\mu,\sigma))=-n\ln(\sigma)-\frac{n}{2} \ln(2\pi)\\
         -\frac{1}{2\sigma^2}[(x_1-\mu)^2+(x_2-\mu)^2+...+(x_n-\mu)^2]</script><p>然后求导为：</p>
<script type="math/tex; mode=display">
\frac{\partial\ln(P(x_1,x_2,...,x_n;\mu,\sigma))}{\partial\mu}=
                 \frac{n}{\sigma^2}[\mu - (x_1+x_2+...+x_n)]</script><p>​     上式左半部分为对数损失函数。损失函数越小越好，因此我们令等式左半的对数损失函数为0，可得：</p>
<script type="math/tex; mode=display">
\mu=\frac{x_1+x_2+...+x_n}{n}</script><p>同理，可计算$\sigma ​$。</p>
<h2 id="2-12-梯度下降"><a href="#2-12-梯度下降" class="headerlink" title="2.12 梯度下降"></a>2.12 梯度下降</h2><h3 id="2-12-1-机器学习中为什么需要梯度下降"><a href="#2-12-1-机器学习中为什么需要梯度下降" class="headerlink" title="2.12.1 机器学习中为什么需要梯度下降"></a>2.12.1 机器学习中为什么需要梯度下降</h3><p>梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：</p>
<p>（1）梯度下降是迭代法的一种，可以用于求解最小二乘问题。</p>
<p>（2）在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。</p>
<p>（3）在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。</p>
<p>（4）如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。</p>
<p>（5）在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。</p>
<h3 id="2-12-2-梯度下降法缺点"><a href="#2-12-2-梯度下降法缺点" class="headerlink" title="2.12.2 梯度下降法缺点"></a>2.12.2 梯度下降法缺点</h3><p>梯度下降法缺点有以下几点：</p>
<p>（1）靠近极小值时收敛速度减慢。</p>
<p>（2）直线搜索时可能会产生一些问题。</p>
<p>（3）可能会“之字形”地下降。</p>
<p>梯度概念也有需注意的地方：</p>
<p>（1）梯度是一个向量，即有方向有大小。 </p>
<p>（2）梯度的方向是最大方向导数的方向。 </p>
<p>（3）梯度的值是最大方向导数的值。</p>
<h3 id="2-12-3-梯度下降法直观理解"><a href="#2-12-3-梯度下降法直观理解" class="headerlink" title="2.12.3 梯度下降法直观理解"></a>2.12.3 梯度下降法直观理解</h3><p>梯度下降法经典图示如下图2.7所示：</p>
<p><img src="/img/ch2/2.25/1.png" alt=""></p>
<p>​                                    图2.7 梯度下降法经典图示</p>
<p>​    形象化举例，由上图2.7所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。<br>​    由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。</p>
<p><strong>核心思想归纳</strong>：</p>
<p>（1）初始化参数，随机选取取值范围内的任意数；</p>
<p>（2）迭代操作：<br>    a）计算当前梯度；<br>    b）修改新的变量；<br>    c）计算朝最陡的下坡方向走一步；<br>    d）判断是否需要终止，如否，返回a）；</p>
<p>（3）得到全局最优解或者接近全局最优解。</p>
<h3 id="2-12-4-梯度下降法算法描述"><a href="#2-12-4-梯度下降法算法描述" class="headerlink" title="2.12.4 梯度下降法算法描述"></a>2.12.4 梯度下降法算法描述</h3><p>梯度下降法算法步骤如下：</p>
<p>（1）确定优化模型的假设函数及损失函数。<br>​    举例，对于线性回归，假设函数为：</p>
<script type="math/tex; mode=display">
  h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n</script><p>  其中，$\theta_i,x_i(i=0,1,2,…,n)$分别为模型参数、每个样本的特征值。<br>  对于假设函数，损失函数为：</p>
<script type="math/tex; mode=display">
  J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
      ,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2</script><p>（2）相关参数初始化。<br>​    主要初始化${\theta}_i$、算法迭代步长${\alpha} $、终止距离${\zeta} $。初始化时可以根据经验初始化，即${\theta} $初始化为0，步长${\alpha} $初始化为1。当前步长记为${\varphi}_i $。当然，也可随机初始化。</p>
<p>（3）迭代计算。</p>
<p>​    1）计算当前位置时损失函数的梯度，对${\theta}_i $，其梯度表示为：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
    ,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2</script><p>​    2）计算当前位置下降的距离。</p>
<script type="math/tex; mode=display">
{\varphi}_i={\alpha} \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)</script><p>​    3）判断是否终止。<br>​    确定是否所有${\theta}_i$梯度下降的距离${\varphi}_i$都小于终止距离${\zeta}$，如果都小于${\zeta}$，则算法终止，当然的值即为最终结果，否则进入下一步。<br>​    4）更新所有的${\theta}_i$，更新后的表达式为：</p>
<script type="math/tex; mode=display">
{\theta}_i={\theta}_i-\alpha \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)</script><script type="math/tex; mode=display">
\theta_i=\theta_i - \alpha \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0
    ,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i</script><p>​    5）令上式$x^{(j)}_0=1$，更新完毕后转入1)。<br>​    由此，可看出，当前位置的梯度方向由所有样本决定，上式中 $\frac{1}{m}​$、$\alpha \frac{1}{m}​$ 的目的是为了便于理解。</p>
<h3 id="2-12-5-如何对梯度下降法进行调优"><a href="#2-12-5-如何对梯度下降法进行调优" class="headerlink" title="2.12.5 如何对梯度下降法进行调优"></a>2.12.5 如何对梯度下降法进行调优</h3><p>实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：</p>
<p>（1）<strong>算法迭代步长$\alpha$选择。</strong><br>    在算法参数初始化时，有时根据经验将步长初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。</p>
<p>（2）<strong>参数的初始值选择。</strong><br>    初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。如果损失函数是凸函数，则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p>
<p>（3）<strong>标准化处理。</strong><br>    由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，可节省算法运行时间。</p>
<h3 id="2-12-6-随机梯度和批量梯度区别"><a href="#2-12-6-随机梯度和批量梯度区别" class="headerlink" title="2.12.6 随机梯度和批量梯度区别"></a>2.12.6 随机梯度和批量梯度区别</h3><p>​    随机梯度下降（SDG）和批量梯度下降（BDG）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。<br>下面通过介绍两种梯度下降法的求解思路，对其进行比较。<br>假设函数为：</p>
<script type="math/tex; mode=display">
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n</script><p>损失函数为：</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)^2</script><p>其中，$m​$为样本个数，$j​$为参数个数。</p>
<p>1、 <strong>批量梯度下降的求解思路如下：</strong><br>a) 得到每个$ \theta ​$对应的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>b) 由于是求最小化风险函数，所以按每个参数 $ \theta ​$ 的梯度负方向更新 $ \theta_i ​$ ：</p>
<script type="math/tex; mode=display">
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
    ,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i</script><p>c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，如果样本数据很大，这种方法迭代速度就很慢。<br>相比而言，随机梯度下降可避免这种问题。</p>
<p>2、<strong>随机梯度下降的求解思路如下：</strong><br>a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是训练集中每个样本的粒度。<br>损失函数可以写成如下这种形式，</p>
<script type="math/tex; mode=display">
J(\theta_0, \theta_1, ... , \theta_n) = 
            \frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
            ,x^{j}_1,...,x^{j}_n))^2 = 
            \frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))</script><p>b）对每个参数 $ \theta​$ 按梯度方向更新 $ \theta​$：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))</script><p>c) 随机梯度下降是通过每个样本来迭代更新一次。<br>随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。</p>
<p><strong>小结：</strong><br>随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">批量梯度下降</td>
<td style="text-align:left">a）采用所有数据来梯度下降。<br/>b）批量梯度下降法在样本量很大的时候，训练速度慢。</td>
</tr>
<tr>
<td style="text-align:center">随机梯度下降</td>
<td style="text-align:left">a）随机梯度下降用一个样本来梯度下降。<br/>b）训练速度很快。<br />c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</td>
</tr>
</tbody>
</table>
</div>
<p>下面介绍能结合两种方法优点的小批量梯度下降法。</p>
<p>3、 <strong>小批量（Mini-Batch）梯度下降的求解思路如下</strong><br>对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1&lt; n&lt; m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：</p>
<script type="math/tex; mode=display">
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
        ( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}</script><h3 id="2-12-7-各种梯度下降法性能比较"><a href="#2-12-7-各种梯度下降法性能比较" class="headerlink" title="2.12.7 各种梯度下降法性能比较"></a>2.12.7 各种梯度下降法性能比较</h3><p>​    下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">BGD</th>
<th style="text-align:center">SGD</th>
<th style="text-align:center">Mini-batch GD</th>
<th style="text-align:center">Online GD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">训练集</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">固定</td>
<td style="text-align:center">实时更新</td>
</tr>
<tr>
<td style="text-align:center">单次迭代样本数</td>
<td style="text-align:center">整个训练集</td>
<td style="text-align:center">单个样本</td>
<td style="text-align:center">训练集的子集</td>
<td style="text-align:center">根据具体算法定</td>
</tr>
<tr>
<td style="text-align:center">算法复杂度</td>
<td style="text-align:center">高</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">低</td>
</tr>
<tr>
<td style="text-align:center">时效性</td>
<td style="text-align:center">低</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">高</td>
</tr>
<tr>
<td style="text-align:center">收敛性</td>
<td style="text-align:center">稳定</td>
<td style="text-align:center">不稳定</td>
<td style="text-align:center">较稳定</td>
<td style="text-align:center">不稳定</td>
</tr>
</tbody>
</table>
</div>
<p>BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD。</p>
<p>​    Online GD于Mini-batch GD/SGD的区别在于，所有训练数据只用一次，然后丢弃。这样做的优点在于可预测最终模型的变化趋势。</p>
<p>​    Online GD在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而Online GD算法可以实时的依据网民的点击行为进行迁移。</p>
<h2 id="2-14-线性判别分析（LDA）"><a href="#2-14-线性判别分析（LDA）" class="headerlink" title="2.14 线性判别分析（LDA）"></a>2.14 线性判别分析（LDA）</h2><h3 id="2-14-1-LDA思想总结"><a href="#2-14-1-LDA思想总结" class="headerlink" title="2.14.1 LDA思想总结"></a>2.14.1 LDA思想总结</h3><p>​    线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。  </p>
<p>LDA分类思想简单总结如下：  </p>
<ol>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。  </li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。  </li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  </li>
</ol>
<p>如果用一句话概括LDA思想，即“投影后类内方差最小，类间方差最大”。</p>
<h3 id="2-14-2-图解LDA核心思想"><a href="#2-14-2-图解LDA核心思想" class="headerlink" title="2.14.2 图解LDA核心思想"></a>2.14.2 图解LDA核心思想</h3><p>​    假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="/img/ch2/2.29/1.png" alt=""></p>
<p>左图和右图是两种不同的投影方式。</p>
<p>​    左图思路：让不同类别的平均点距离最远的投影方式。</p>
<p>​    右图思路：让同类别的数据挨得最近的投影方式。</p>
<p>​    从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。</p>
<p>​    以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<h3 id="2-14-3-二类LDA算法原理"><a href="#2-14-3-二类LDA算法原理" class="headerlink" title="2.14.3 二类LDA算法原理"></a>2.14.3 二类LDA算法原理</h3><p>​    输入：数据集 $D=\{(\boldsymbol x_1,\boldsymbol y_1),(\boldsymbol x_2,\boldsymbol y_2),…,(\boldsymbol x_m,\boldsymbol y_m)\}​$，其中样本 $\boldsymbol x_i ​$ 是n维向量，$\boldsymbol y_i  \epsilon \{0, 1\}​$，降维后的目标维度 $d​$。定义</p>
<p>​    $N_j(j=0,1)$ 为第 $j$ 类样本个数；</p>
<p>​    $X_j(j=0,1)$ 为第 $j$ 类样本的集合；</p>
<p>​    $u_j(j=0,1)​$ 为第 $j​$ 类样本的均值向量；</p>
<p>​    $\sum_j(j=0,1)$ 为第 $j$ 类样本的协方差矩阵。</p>
<p>​    其中</p>
<script type="math/tex; mode=display">
u_j = \frac{1}{N_j} \sum_{\boldsymbol x\epsilon X_j}\boldsymbol x(j=0,1)， 
\sum_j = \sum_{\boldsymbol x\epsilon X_j}(\boldsymbol x-u_j)(\boldsymbol x-u_j)^T(j=0,1)</script><p>​    假设投影直线是向量 $\boldsymbol w$，对任意样本 $\boldsymbol x_i$，它在直线 $w$上的投影为 $\boldsymbol w^Tx_i$，两个类别的中心点 $u_0$, $u_1 $在直线 $w$ 的投影分别为 $\boldsymbol w^Tu_0$ 、$\boldsymbol w^Tu_1$。</p>
<p>​    LDA的目标是让两类别的数据中心间的距离 $| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 |^2_2$ 尽量大，与此同时，希望同类样本投影点的协方差$\boldsymbol w^T \sum_0 \boldsymbol w$、$\boldsymbol w^T \sum_1 \boldsymbol w$ 尽量小，最小化 $\boldsymbol w^T \sum_0 \boldsymbol w + \boldsymbol w^T \sum_1 \boldsymbol w​$ 。<br>​    定义<br>​    类内散度矩阵</p>
<script type="math/tex; mode=display">
S_w = \sum_0 + \sum_1 = 
    \sum_{\boldsymbol x\epsilon X_0}(\boldsymbol x-u_0)(\boldsymbol x-u_0)^T + 
    \sum_{\boldsymbol x\epsilon X_1}(\boldsymbol x-u_1)(\boldsymbol x-u_1)^T</script><p>​    类间散度矩阵 $S_b = (u_0 - u_1)(u_0 - u_1)^T$</p>
<p>​    据上分析，优化目标为</p>
<script type="math/tex; mode=display">
\mathop{\arg\max}_\boldsymbol w J(\boldsymbol w) = \frac{\| \boldsymbol w^Tu_0 - \boldsymbol w^Tu_1 \|^2_2}{\boldsymbol w^T \sum_0\boldsymbol w + \boldsymbol w^T \sum_1\boldsymbol w} = 
\frac{\boldsymbol w^T(u_0-u_1)(u_0-u_1)^T\boldsymbol w}{\boldsymbol w^T(\sum_0 + \sum_1)\boldsymbol w} =
\frac{\boldsymbol w^TS_b\boldsymbol w}{\boldsymbol w^TS_w\boldsymbol w}</script><p>​    根据广义瑞利商的性质，矩阵 $S^{-1}_{w} S_b$ 的最大特征值为 $J(\boldsymbol w)$ 的最大值，矩阵 $S^{-1}_{w} S_b$ 的最大特征值对应的特征向量即为 $\boldsymbol w$。</p>
<h3 id="2-14-4-LDA算法流程总结"><a href="#2-14-4-LDA算法流程总结" class="headerlink" title="2.14.4 LDA算法流程总结"></a>2.14.4 LDA算法流程总结</h3><p>LDA算法降维流程如下：</p>
<p>​    输入：数据集 $D = \{ (x_1,y_1),(x_2,y_2), … ,(x_m,y_m) \}$，其中样本 $x_i $ 是n维向量，$y_i  \epsilon \{C_1, C_2, …, C_k\}$，降维后的目标维度 $d$ 。</p>
<p>​    输出：降维后的数据集 $\overline{D} $ 。</p>
<p>步骤：</p>
<ol>
<li>计算类内散度矩阵 $S_w$。</li>
<li>计算类间散度矩阵 $S_b​$ 。</li>
<li>计算矩阵 $S^{-1}_wS_b​$ 。</li>
<li>计算矩阵 $S^{-1}_wS_b$ 的最大的 d 个特征值。</li>
<li>计算 d 个特征值对应的 d 个特征向量，记投影矩阵为 W 。</li>
<li>转化样本集的每个样本，得到新样本 $P_i = W^Tx_i​$ 。</li>
<li>输出新样本集 $\overline{D} = \{ (p_1,y_1),(p_2,y_2),…,(p_m,y_m) \}​$</li>
</ol>
<h3 id="2-14-5-LDA和PCA区别"><a href="#2-14-5-LDA和PCA区别" class="headerlink" title="2.14.5 LDA和PCA区别"></a>2.14.5 LDA和PCA区别</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">异同点</th>
<th style="text-align:left">LDA</th>
<th style="text-align:left">PCA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">相同点</td>
<td style="text-align:left">1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布；</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:center">不同点</td>
<td style="text-align:left">有监督的降维方法；</td>
<td style="text-align:left">无监督的降维方法；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">降维最多降到k-1维；</td>
<td style="text-align:left">降维多少没有限制；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">可以用于降维，还可以用于分类；</td>
<td style="text-align:left">只用于降维；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">选择分类性能最好的投影方向；</td>
<td style="text-align:left">选择样本点投影具有最大方差的方向；</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:left">更明确，更能反映样本间差异；</td>
<td style="text-align:left">目的较为模糊；</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-14-6-LDA优缺点"><a href="#2-14-6-LDA优缺点" class="headerlink" title="2.14.6 LDA优缺点"></a>2.14.6 LDA优缺点</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">优缺点</th>
<th style="text-align:left">简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">优点</td>
<td style="text-align:left">1. 可以使用类别的先验知识；<br />2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr>
<td style="text-align:center">缺点</td>
<td style="text-align:left">1. LDA不适合对非高斯分布样本进行降维；<br />2. LDA降维最多降到分类数k-1维；<br />3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br />4. LDA可能过度拟合数据。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2-15-主成分分析（PCA）"><a href="#2-15-主成分分析（PCA）" class="headerlink" title="2.15  主成分分析（PCA）"></a>2.15  主成分分析（PCA）</h2><h3 id="2-15-1-主成分分析（PCA）思想总结"><a href="#2-15-1-主成分分析（PCA）思想总结" class="headerlink" title="2.15.1 主成分分析（PCA）思想总结"></a>2.15.1 主成分分析（PCA）思想总结</h3><ol>
<li>PCA就是将高维的数据通过线性变换投影到低维空间上去。</li>
<li>投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。</li>
<li>去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。</li>
<li>去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。</li>
<li>对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。</li>
<li>完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。</li>
<li>之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。</li>
</ol>
<h3 id="2-15-2-图解PCA核心思想"><a href="#2-15-2-图解PCA核心思想" class="headerlink" title="2.15.2 图解PCA核心思想"></a>2.15.2 图解PCA核心思想</h3><p>​    PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n &lt; m），这n维形成主元，是重构出来最能代表原始数据的正交特征。</p>
<p>​    假设数据集是m个n维，$(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$。如果$n=2$，需要降维到$n’=1$，现在想找到某一维度方向代表这两个维度的数据。下图有$u_1, u_2$两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？</p>
<p><img src="/img/ch2/2.34/1.png" alt=""></p>
<p>从图可看出，$u_1$比$u_2$好，为什么呢？有以下两个主要评价指标：</p>
<ol>
<li>样本点到这个直线的距离足够近。</li>
<li>样本点在这个直线上的投影能尽可能的分开。</li>
</ol>
<p>如果我们需要降维的目标维数是其他任意维，则：</p>
<ol>
<li>样本点到这个超平面的距离足够近。</li>
<li>样本点在这个超平面上的投影能尽可能的分开。</li>
</ol>
<h3 id="2-15-3-PCA算法推理"><a href="#2-15-3-PCA算法推理" class="headerlink" title="2.15.3 PCA算法推理"></a>2.15.3 PCA算法推理</h3><p>下面以基于最小投影距离为评价指标推理：</p>
<p>​    假设数据集是m个n维，$(x^{(1)}, x^{(2)},…,x^{(m)})$，且数据进行了中心化。经过投影变换得到新坐标为 ${w_1,w_2,…,w_n}$，其中 $w$ 是标准正交基，即 $| w |_2 = 1$，$w^T_iw_j = 0$。</p>
<p>​    经过降维后，新坐标为 $\{ w_1,w_2,…,w_n \}$，其中 $n’$ 是降维后的目标维数。样本点 $x^{(i)}$ 在新坐标系下的投影为 $z^{(i)} = \left(z^{(i)}_1, z^{(i)}_2, …, z^{(i)}_{n’}   \right)$，其中 $z^{(i)}_j = w^T_j x^{(i)}$ 是 $x^{(i)} ​$ 在低维坐标系里第 j 维的坐标。</p>
<p>​    如果用 $z^{(i)} $ 去恢复 $x^{(i)} $ ，则得到的恢复数据为 $\widehat{x}^{(i)} = \sum^{n’}_{j=1} x^{(i)}_j w_j = Wz^{(i)}$，其中 $W$为标准正交基组成的矩阵。</p>
<p>​    考虑到整个样本集，样本点到这个超平面的距离足够近，目标变为最小化 $\sum^m_{i=1} | \hat{x}^{(i)} - x^{(i)} |^2_2$ 。对此式进行推理，可得：</p>
<script type="math/tex; mode=display">
\sum^m_{i=1} \| \hat{x}^{(i)} - x^{(i)} \|^2_2 = 
    \sum^m_{i=1} \| Wz^{(i)} - x^{(i)} \|^2_2 \\
    = \sum^m_{i=1} \left( Wz^{(i)} \right)^T \left( Wz^{(i)} \right)
    - 2\sum^m_{i=1} \left( Wz^{(i)} \right)^T x^{(i)}
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
    - 2\sum^m_{i=1} \left( z^{(i)} \right)^T x^{(i)}
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = - \sum^m_{i=1} \left( z^{(i)} \right)^T \left( z^{(i)} \right)
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = -tr \left( W^T \left( \sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T \right)W \right)
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)} \\
    = -tr \left( W^TXX^TW \right)
    + \sum^m_{i=1} \left( x^{(i)} \right)^T x^{(i)}</script><p>​    在推导过程中，分别用到了 $\overline{x}^{(i)} = Wz^{(i)}$ ，矩阵转置公式 $(AB)^T = B^TA^T$，$W^TW = I$，$z^{(i)} = W^Tx^{(i)}$ 以及矩阵的迹，最后两步是将代数和转为矩阵形式。<br>​    由于 $W$ 的每一个向量 $w_j$ 是标准正交基，$\sum^m_{i=1} x^{(i)} \left(  x^{(i)} \right)^T$ 是数据集的协方差矩阵，$\sum^m_{i=1} \left(  x^{(i)} \right)^T x^{(i)} $ 是一个常量。最小化 $\sum^m_{i=1} | \hat{x}^{(i)} - x^{(i)} |^2_2$ 又可等价于</p>
<script type="math/tex; mode=display">
\underbrace{\arg \min}_W - tr \left( W^TXX^TW \right) s.t.W^TW = I</script><p>利用拉格朗日函数可得到</p>
<script type="math/tex; mode=display">
J(W) = -tr(W^TXX^TW) + \lambda(W^TW - I)</script><p>​    对 $W$ 求导，可得 $-XX^TW + \lambda W = 0 $ ，也即 $ XX^TW = \lambda W $ 。 $ XX^T $ 是 $ n’ $ 个特征向量组成的矩阵，$\lambda$ 为$ XX^T $ 的特征值。$W$ 即为我们想要的矩阵。<br>​    对于原始数据，只需要 $z^{(i)} = W^TX^{(i)}$ ，就可把原始数据集降维到最小投影距离的 $n’$ 维数据集。</p>
<p>​    基于最大投影方差的推导，这里就不再赘述，有兴趣的同仁可自行查阅资料。</p>
<h3 id="2-15-4-PCA算法流程总结"><a href="#2-15-4-PCA算法流程总结" class="headerlink" title="2.15.4 PCA算法流程总结"></a>2.15.4 PCA算法流程总结</h3><p>输入：$n​$ 维样本集 $D = \left( x^{(1)},x^{(2)},…,x^{(m)} \right)​$ ，目标降维的维数 $n’​$ 。</p>
<p>输出：降维后的新样本集 $D’  = \left( z^{(1)},z^{(2)},…,z^{(m)} \right)$ 。</p>
<p>主要步骤如下：</p>
<ol>
<li>对所有的样本进行中心化，$ x^{(i)} = x^{(i)} - \frac{1}{m} \sum^m_{j=1} x^{(j)} $ 。</li>
<li>计算样本的协方差矩阵 $XX^T​$ 。</li>
<li>对协方差矩阵 $XX^T$ 进行特征值分解。</li>
<li>取出最大的 $n’ $ 个特征值对应的特征向量 $\{ w_1,w_2,…,w_{n’} \}$ 。</li>
<li>标准化特征向量，得到特征向量矩阵 $W$ 。</li>
<li>转化样本集中的每个样本 $z^{(i)} = W^T x^{(i)}$ 。</li>
<li>得到输出矩阵 $D’ = \left( z^{(1)},z^{(2)},…,z^{(n)} \right)​$ 。<br><em>注</em>：在降维时，有时不明确目标维数，而是指定降维到的主成分比重阈值 $k(k \epsilon(0,1])​$ 。假设 $n​$ 个特征值为 $\lambda_1 \geqslant \lambda_2 \geqslant … \geqslant \lambda_n​$ ，则 $n’​$ 可从 $\sum^{n’}_{i=1} \lambda_i \geqslant k \times \sum^n_{i=1} \lambda_i ​$ 得到。</li>
</ol>
<h3 id="2-15-5-PCA算法主要优缺点"><a href="#2-15-5-PCA算法主要优缺点" class="headerlink" title="2.15.5 PCA算法主要优缺点"></a>2.15.5 PCA算法主要优缺点</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">优缺点</th>
<th style="text-align:left">简要说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">优点</td>
<td style="text-align:left">1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。3. 计算方法简单，主要运算是特征值分解，易于实现。</td>
</tr>
<tr>
<td style="text-align:center">缺点</td>
<td style="text-align:left">1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-15-6-降维的必要性及目的"><a href="#2-15-6-降维的必要性及目的" class="headerlink" title="2.15.6 降维的必要性及目的"></a>2.15.6 降维的必要性及目的</h3><p><strong>降维的必要性</strong>：</p>
<ol>
<li>多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。</li>
<li>高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。</li>
<li>过多的变量，对查找规律造成冗余麻烦。</li>
<li>仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。</li>
</ol>
<p><strong>降维的目的</strong>：</p>
<ol>
<li>减少预测变量的个数。</li>
<li>确保这些变量是相互独立的。</li>
<li>提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。</li>
<li>数据在低维下更容易处理、更容易使用。</li>
<li>去除数据噪声。</li>
<li>降低算法运算开销。</li>
</ol>
<h3 id="2-15-7-KPCA与PCA的区别"><a href="#2-15-7-KPCA与PCA的区别" class="headerlink" title="2.15.7 KPCA与PCA的区别"></a>2.15.7 KPCA与PCA的区别</h3><p>​    应用PCA算法前提是假设存在一个线性超平面，进而投影。那如果数据不是线性的呢？该怎么办？这时候就需要KPCA，数据集从 $n$ 维映射到线性可分的高维 $N &gt;n$，然后再从 $N$ 维降维到一个低维度 $n’(n’&lt;n&lt;N)$ 。</p>
<p>​    KPCA用到了核函数思想，使用了核函数的主成分分析一般称为核主成分分析(Kernelized PCA, 简称KPCA）。</p>
<p>假设高维空间数据由 $n​$ 维空间的数据通过映射 $\phi​$ 产生。</p>
<p>​    $n$ 维空间的特征分解为：</p>
<script type="math/tex; mode=display">
\sum^m_{i=1} x^{(i)} \left( x^{(i)} \right)^T W = \lambda W</script><p>​    其映射为</p>
<script type="math/tex; mode=display">
\sum^m_{i=1} \phi \left( x^{(i)} \right) \phi \left( x^{(i)} \right)^T W = \lambda W</script><p>​    通过在高维空间进行协方差矩阵的特征值分解，然后用和PCA一样的方法进行降维。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。</p>
<h2 id="2-16-模型评估"><a href="#2-16-模型评估" class="headerlink" title="2.16 模型评估"></a>2.16 模型评估</h2><h3 id="2-16-1-模型评估常用方法？"><a href="#2-16-1-模型评估常用方法？" class="headerlink" title="2.16.1 模型评估常用方法？"></a>2.16.1 模型评估常用方法？</h3><p>​    一般情况来说，单一评分标准无法完全评估一个机器学习模型。只用good和bad偏离真实场景去评估某个模型，都是一种欠妥的评估方式。下面介绍常用的分类模型和回归模型评估方法。</p>
<p><strong>分类模型常用评估方法：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">指标</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Accuracy</td>
<td style="text-align:center">准确率</td>
</tr>
<tr>
<td style="text-align:center">Precision</td>
<td style="text-align:center">精准度/查准率</td>
</tr>
<tr>
<td style="text-align:center">Recall</td>
<td style="text-align:center">召回率/查全率</td>
</tr>
<tr>
<td style="text-align:center">P-R曲线</td>
<td style="text-align:center">查准率为纵轴，查全率为横轴，作图</td>
</tr>
<tr>
<td style="text-align:center">F1</td>
<td style="text-align:center">F1值</td>
</tr>
<tr>
<td style="text-align:center">Confusion Matrix</td>
<td style="text-align:center">混淆矩阵</td>
</tr>
<tr>
<td style="text-align:center">ROC</td>
<td style="text-align:center">ROC曲线</td>
</tr>
<tr>
<td style="text-align:center">AUC</td>
<td style="text-align:center">ROC曲线下的面积</td>
</tr>
</tbody>
</table>
</div>
<p><strong>回归模型常用评估方法：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">指标</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Mean Square Error (MSE, RMSE)</td>
<td style="text-align:center">平均方差</td>
</tr>
<tr>
<td style="text-align:center">Absolute Error (MAE, RAE)</td>
<td style="text-align:center">绝对误差</td>
</tr>
<tr>
<td style="text-align:center">R-Squared</td>
<td style="text-align:center">R平方值</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-16-2-误差、偏差和方差有什么区别和联系"><a href="#2-16-2-误差、偏差和方差有什么区别和联系" class="headerlink" title="2.16.2 误差、偏差和方差有什么区别和联系"></a>2.16.2 误差、偏差和方差有什么区别和联系</h3><p>在机器学习中，Bias(偏差)，Error(误差)，和Variance(方差)存在以下区别和联系：</p>
<p><strong>对于Error </strong>：</p>
<ul>
<li><p>误差（error）：一般地，我们把学习器的实际预测输出与样本的真是输出之间的差异称为“误差”。</p>
</li>
<li><p>Error = Bias + Variance + Noise，Error反映的是整个模型的准确度。</p>
</li>
</ul>
<p><strong>对于Noise:</strong></p>
<p>噪声：描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
<p><strong>对于Bias：</strong></p>
<ul>
<li>Bias衡量模型拟合训练数据的能力（训练数据不一定是整个 training dataset，而是只用于训练它的那一部分数据，例如：mini-batch），Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度。</li>
<li>Bias 越小，拟合能力越高（可能产生overfitting）；反之，拟合能力越低（可能产生underfitting）。</li>
<li>偏差越大，越偏离真实数据，如下图第二行所示。</li>
</ul>
<p><strong>对于Variance：</strong></p>
<ul>
<li><p>方差公式：$S_{N}^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}$</p>
</li>
<li><p>Variance描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，模型的稳定程度越差。</p>
</li>
<li>Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。</li>
<li>Variance越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。</li>
<li>如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。</li>
</ul>
<blockquote>
<p><img src="/img/ch2/2.16.20.1.png" alt=""></p>
</blockquote>
<h3 id="2-16-3-经验误差与泛化误差"><a href="#2-16-3-经验误差与泛化误差" class="headerlink" title="2.16.3 经验误差与泛化误差"></a>2.16.3 经验误差与泛化误差</h3><p>经验误差（empirical error）：也叫训练误差（training error），模型在训练集上的误差。 </p>
<p>泛化误差（generalization error）：模型在新样本集（测试集）上的误差称为“泛化误差”。</p>
<h3 id="2-16-4-图解欠拟合、过拟合"><a href="#2-16-4-图解欠拟合、过拟合" class="headerlink" title="2.16.4 图解欠拟合、过拟合"></a>2.16.4 图解欠拟合、过拟合</h3><p>根据不同的坐标方式，欠拟合与过拟合图解不同。</p>
<ol>
<li><strong>横轴为训练样本数量，纵轴为误差</strong></li>
</ol>
<p><img src="/img/ch2/2.16.4.1.jpg" alt=""></p>
<p>如上图所示，我们可以直观看出欠拟合和过拟合的区别：</p>
<p>​    模型欠拟合：在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大；</p>
<p>​    模型过拟合：在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。</p>
<p>​    模型正常：在训练集以及测试集上，同时具有相对较低的偏差以及方差。</p>
<ol>
<li><strong>横轴为模型复杂程度，纵轴为误差</strong></li>
</ol>
<p><img src="/img/ch2/2.16.4.2.png" alt=""></p>
<p>​                    红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​    模型欠拟合：模型在点A处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​    模型过拟合：模型在点C处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。 </p>
<p>​    模型正常：模型复杂程度控制在点B处为最优。</p>
<ol>
<li><strong>横轴为正则项系数，纵轴为误差</strong></li>
</ol>
<p><img src="/img/ch2/2.16.4.3.png" alt=""></p>
<p>​                                             红线为测试集上的Error,蓝线为训练集上的Error</p>
<p>​    模型欠拟合：模型在点C处，在训练集以及测试集上同时具有较高的误差，此时模型的偏差较大。</p>
<p>​    模型过拟合：模型在点A处，在训练集上具有较低的误差，在测试集上具有较高的误差，此时模型的方差较大。 它通常发生在模型过于复杂的情况下，如参数过多等，会使得模型的预测性能变弱，并且增加数据的波动性。虽然模型在训练时的效果可以表现的很完美，基本上记住了数据的全部特点，但这种模型在未知数据的表现能力会大减折扣，因为简单的模型泛化能力通常都是很弱的。</p>
<p>​    模型正常：模型复杂程度控制在点B处为最优。</p>
<h3 id="2-16-5-如何解决过拟合与欠拟合"><a href="#2-16-5-如何解决过拟合与欠拟合" class="headerlink" title="2.16.5 如何解决过拟合与欠拟合"></a>2.16.5 如何解决过拟合与欠拟合</h3><p><strong>如何解决欠拟合：</strong></p>
<ol>
<li>添加其他特征项。组合、泛化、相关性、上下文特征、平台特征等特征是特征添加的重要手段，有时候特征项不够会导致模型欠拟合。</li>
<li>添加多项式特征。例如将线性模型添加二次项或三次项使模型泛化能力更强。例如，FM（Factorization Machine）模型、FFM（Field-aware Factorization Machine）模型，其实就是线性模型，增加了二阶多项式，保证了模型一定的拟合程度。</li>
<li>可以增加模型的复杂程度。</li>
<li>减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。</li>
</ol>
<p><strong>如何解决过拟合：</strong></p>
<ol>
<li>重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据。 </li>
<li>增加训练样本数量。 </li>
<li>降低模型复杂程度。 </li>
<li>增大正则项系数。 </li>
<li>采用dropout方法，dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作。 </li>
<li>early stopping。 </li>
<li>减少迭代次数。 </li>
<li>增大学习率。 </li>
<li>添加噪声数据。 </li>
<li>树结构中，可以对树进行剪枝。 </li>
<li>减少特征项。</li>
</ol>
<p>欠拟合和过拟合这些方法，需要根据实际问题，实际模型，进行选择。</p>
<h3 id="2-16-6-交叉验证的主要作用"><a href="#2-16-6-交叉验证的主要作用" class="headerlink" title="2.16.6 交叉验证的主要作用"></a>2.16.6 交叉验证的主要作用</h3><p>​    为了得到更为稳健可靠的模型，对模型的泛化误差进行评估，得到模型泛化误差的近似值。当有多个模型可以选择时，我们通常选择“泛化误差”最小的模型。 </p>
<p>​    交叉验证的方法有许多种，但是最常用的是：留一交叉验证、k折交叉验证。</p>
<h3 id="2-16-7-理解k折交叉验证"><a href="#2-16-7-理解k折交叉验证" class="headerlink" title="2.16.7 理解k折交叉验证"></a>2.16.7 理解k折交叉验证</h3><ol>
<li>将含有N个样本的数据集，分成K份，每份含有N/K个样本。选择其中1份作为测试集，另外K-1份作为训练集，测试集就有K种情况。 </li>
<li>在每种情况中，用训练集训练模型，用测试集测试模型，计算模型的泛化误差。 </li>
<li>交叉验证重复K次，每份验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测，得到模型最终的泛化误差。 </li>
<li>将K种情况下，模型的泛化误差取均值，得到模型最终的泛化误差。  </li>
<li>一般$2\leqslant K \leqslant10$。 k折交叉验证的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 </li>
<li>训练集中样本数量要足够多，一般至少大于总样本数的50%。 </li>
<li>训练集和测试集必须从完整的数据集中均匀取样。均匀取样的目的是希望减少训练集、测试集与原数据集之间的偏差。当样本数量足够多时，通过随机取样，便可以实现均匀取样的效果。 </li>
</ol>
<h3 id="2-16-8-混淆矩阵"><a href="#2-16-8-混淆矩阵" class="headerlink" title="2.16.8 混淆矩阵"></a>2.16.8 混淆矩阵</h3><p>第一种混淆矩阵:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">真实情况T or F</th>
<th style="text-align:left">预测为正例1，P</th>
<th style="text-align:left">预测为负例0，N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">本来label标记为1，预测结果真为T、假为F</td>
<td style="text-align:left">TP(预测为1，实际为1)</td>
<td style="text-align:left">FN(预测为0，实际为1)</td>
</tr>
<tr>
<td style="text-align:center">本来label标记为0，预测结果真为T、假为F</td>
<td style="text-align:left">FP(预测为1，实际为0)</td>
<td style="text-align:left">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
</div>
<p>第二种混淆矩阵:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">预测情况P or N</th>
<th style="text-align:left">实际label为1,预测对了为T</th>
<th style="text-align:left">实际label为0,预测对了为T</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">预测为正例1，P</td>
<td style="text-align:left">TP(预测为1，实际为1)</td>
<td style="text-align:left">FP(预测为1，实际为0)</td>
</tr>
<tr>
<td style="text-align:center">预测为负例0，N</td>
<td style="text-align:left">FN(预测为0，实际为1)</td>
<td style="text-align:left">TN(预测为0，实际也为0)</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-16-9-错误率及精度"><a href="#2-16-9-错误率及精度" class="headerlink" title="2.16.9 错误率及精度"></a>2.16.9 错误率及精度</h3><ol>
<li>错误率（Error Rate）：分类错误的样本数占样本总数的比例。</li>
<li>精度（accuracy）：分类正确的样本数占样本总数的比例。</li>
</ol>
<h3 id="2-16-10-查准率与查全率"><a href="#2-16-10-查准率与查全率" class="headerlink" title="2.16.10 查准率与查全率"></a>2.16.10 查准率与查全率</h3><p>将算法预测的结果分成四种情况： </p>
<ol>
<li>正确肯定（True Positive,TP）：预测为真，实际为真 </li>
<li>正确否定（True Negative,TN）：预测为假，实际为假 </li>
<li>错误肯定（False Positive,FP）：预测为真，实际为假 </li>
<li>错误否定（False Negative,FN）：预测为假，实际为真</li>
</ol>
<p>则： </p>
<p>查准率（Precision）=TP/（TP+FP）</p>
<p><strong>理解</strong>：预测出为阳性的样本中，正确的有多少。区别准确率（正确预测出的样本，包括正确预测为阳性、阴性，占总样本比例）。<br>例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 </p>
<p>查全率（Recall）=TP/（TP+FN）</p>
<p><strong>理解</strong>：正确预测为阳性的数量占总样本中阳性数量的比例。<br>例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 </p>
<h3 id="2-16-11-ROC与AUC"><a href="#2-16-11-ROC与AUC" class="headerlink" title="2.16.11 ROC与AUC"></a>2.16.11 ROC与AUC</h3><p>​    ROC全称是“受试者工作特征”（Receiver Operating Characteristic）。</p>
<p>​    ROC曲线的面积就是AUC（Area Under Curve）。</p>
<p>​    AUC用于衡量“二分类问题”机器学习算法性能（泛化能力）。</p>
<p>​    ROC曲线，通过将连续变量设定出多个不同的临界值，从而计算出一系列真正率和假正率，再以假正率为横坐标、真正率为纵坐标绘制成曲线，曲线下面积越大，推断准确性越高。在ROC曲线上，最靠近坐标图左上方的点为假正率和真正率均较高的临界值。 </p>
<p>​    对于分类器，或者说分类算法，评价指标主要有Precision，Recall，F-score。下图是一个ROC曲线的示例。</p>
<p><img src="/img/ch2/2.40.10/1.png" alt=""></p>
<p>ROC曲线的横坐标为False Positive Rate（FPR），纵坐标为True Positive Rate（TPR）。其中</p>
<script type="math/tex; mode=display">
TPR = \frac{TP}{TP+FN} ,FPR = \frac{FP}{FP+TN}</script><p>​    下面着重介绍ROC曲线图中的四个点和一条线。<br>​    第一个点(0,1)，即FPR=0, TPR=1，这意味着FN（False Negative）=0，并且FP（False Positive）=0。意味着这是一个完美的分类器，它将所有的样本都正确分类。<br>​    第二个点(1,0)，即FPR=1，TPR=0，意味着这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。<br>​    第三个点(0,0)，即FPR=TPR=0，即FP（False Positive）=TP（True Positive）=0，可以发现该分类器预测所有的样本都为负样本（Negative）。<br>​    第四个点(1,1)，即FPR=TPR=1，分类器实际上预测所有的样本都为正样本。<br>​    经过以上分析，ROC曲线越接近左上角，该分类器的性能越好。</p>
<p>​    ROC曲线所覆盖的面积称为AUC（Area Under Curve），可以更直观的判断学习器的性能，AUC越大则性能越好。  </p>
<h3 id="2-16-12-如何画ROC曲线"><a href="#2-16-12-如何画ROC曲线" class="headerlink" title="2.16.12 如何画ROC曲线"></a>2.16.12 如何画ROC曲线</h3><p>​    下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。</p>
<p>步骤：<br>    1、假设已经得出一系列样本被划分为正类的概率，按照大小排序。<br>    2、从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。<br>    3、每次选取一个不同的threshold，得到一组FPR和TPR，即ROC曲线上的一点。以此共得到20组FPR和TPR的值。<br>    4、根据3、中的每个坐标点，画图。</p>
<p><img src="/img/ch2/2.40.11/1.jpg" alt=""></p>
<h3 id="2-16-13-如何计算TPR，FPR"><a href="#2-16-13-如何计算TPR，FPR" class="headerlink" title="2.16.13 如何计算TPR，FPR"></a>2.16.13 如何计算TPR，FPR</h3><p>1、分析数据<br>y_true = [0, 0, 1, 1]；scores = [0.1, 0.4, 0.35, 0.8]；<br>2、列表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>样本</th>
<th>预测属于P的概率(score)</th>
<th>真实类别</th>
</tr>
</thead>
<tbody>
<tr>
<td>y[0]</td>
<td>0.1</td>
<td>N</td>
</tr>
<tr>
<td>y[1]</td>
<td>0.4</td>
<td>N</td>
</tr>
<tr>
<td>y[2]</td>
<td>0.35</td>
<td>P</td>
</tr>
<tr>
<td>y[3]</td>
<td>0.8</td>
<td>P</td>
</tr>
</tbody>
</table>
</div>
<p>3、将截断点依次取为score值，计算TPR和FPR。<br>当截断点为0.1时：<br>说明只要score&gt;=0.1，它的预测类别就是正例。 因为4个样本的score都大于等于0.1，所以，所有样本的预测类别都为P。<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [1, 1, 1, 1]；<br>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=2</td>
<td>TN=0</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 1；</p>
<p>当截断点为0.35时：<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 1, 1];<br>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=2</td>
<td>FN=0</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 1； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.4时：<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 1, 0, 1]；<br>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=1</td>
<td>TN=1</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0.5；</p>
<p>当截断点为0.8时：<br>scores = [0.1, 0.4, 0.35, 0.8]；y_true = [0, 0, 1, 1]；y_pred = [0, 0, 0, 1]；</p>
<p>正例与反例信息如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>正例</th>
<th>反例</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正例</strong></td>
<td>TP=1</td>
<td>FN=1</td>
</tr>
<tr>
<td><strong>反例</strong></td>
<td>FP=0</td>
<td>TN=2</td>
</tr>
</tbody>
</table>
</div>
<p>由此可得：<br>TPR = TP/(TP+FN) = 0.5； FPR = FP/(TN+FP) = 0；</p>
<p>4、根据TPR、FPR值，以FPR为横轴，TPR为纵轴画图。</p>
<h3 id="2-16-14-如何计算AUC"><a href="#2-16-14-如何计算AUC" class="headerlink" title="2.16.14 如何计算AUC"></a>2.16.14 如何计算AUC</h3><ul>
<li>将坐标点按照横坐标FPR排序 。</li>
<li>计算第$i$个坐标点和第$i+1$个坐标点的间距$dx$ 。 </li>
<li>获取第$i$或者$i+1$个坐标点的纵坐标y。</li>
<li>计算面积微元$ds=ydx$。</li>
<li>对面积微元进行累加，得到AUC。</li>
</ul>
<h3 id="2-16-15-为什么使用Roc和Auc评价分类器"><a href="#2-16-15-为什么使用Roc和Auc评价分类器" class="headerlink" title="2.16.15 为什么使用Roc和Auc评价分类器"></a>2.16.15 为什么使用Roc和Auc评价分类器</h3><p>​    模型有很多评估方法，为什么还要使用ROC和AUC呢？<br>​    因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。</p>
<h3 id="2-16-16-直观理解AUC"><a href="#2-16-16-直观理解AUC" class="headerlink" title="2.16.16 直观理解AUC"></a>2.16.16 直观理解AUC</h3><p>​    下图展现了三种AUC的值： </p>
<p><img src="/img/ch2/2.40.15/1.png" alt=""></p>
<p>​    AUC是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。其他评价指标有精确度、准确率、召回率，而AUC比这三者更为常用。<br>​    一般在分类模型中，预测结果都是以概率的形式表现，如果要计算准确率，通常都会手动设置一个阈值来将对应的概率转化成类别，这个阈值也就很大程度上影响了模型准确率的计算。<br>​    举例：<br>​    现在假设有一个训练好的二分类器对10个正负样本（正例5个，负例5个）预测，得分按高到低排序得到的最好预测结果为[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]，即5个正例均排在5个负例前面，正例排在负例前面的概率为100%。然后绘制其ROC曲线，由于是10个样本，除去原点我们需要描10个点，如下：</p>
<p><img src="/img/ch2/2.16.17-1.png" alt=""></p>
<p>​    描点方式按照样本预测结果的得分高低从左至右开始遍历。从原点开始，每遇到1便向y轴正方向移动y轴最小步长1个单位，这里是1/5=0.2；每遇到0则向x轴正方向移动x轴最小步长1个单位，这里也是0.2。不难看出，上图的AUC等于1，印证了正例排在负例前面的概率的确为100%。</p>
<p>​    假设预测结果序列为[1, 1, 1, 1, 0, 1, 0, 0, 0, 0]。</p>
<p><img src="/img/ch2/2.16.17-2.png" alt=""></p>
<p>​    计算上图的AUC为0.96与计算正例与排在负例前面的概率0.8 × 1 + 0.2 × 0.8 = 0.96相等，而左上角阴影部分的面积则是负例排在正例前面的概率0.2 × 0.2 = 0.04。</p>
<p>​    假设预测结果序列为[1, 1, 1, 0, 1, 0, 1, 0, 0, 0]。</p>
<p><img src="/img/ch2/2.16.17-3.png" alt=""></p>
<p>​    计算上图的AUC为0.88与计算正例与排在负例前面的概率0.6 × 1 + 0.2 × 0.8 + 0.2 × 0.6 = 0.88相等，左上角阴影部分的面积是负例排在正例前面的概率0.2 × 0.2 × 3 = 0.12。</p>
<h3 id="2-16-17-代价敏感错误率与代价曲线"><a href="#2-16-17-代价敏感错误率与代价曲线" class="headerlink" title="2.16.17 代价敏感错误率与代价曲线"></a>2.16.17 代价敏感错误率与代价曲线</h3><p>不同的错误会产生不同代价。以二分法为例，设置代价矩阵如下：</p>
<p><img src="/img/ch2/2-1.png" alt=""></p>
<p>当判断正确的时候，值为0，不正确的时候，分别为$Cost_{01}​$和$Cost_{10}​$ 。</p>
<p>$Cost_{10}$:表示实际为反例但预测成正例的代价。</p>
<p>$Cost_{01}$:表示实际为正例但是预测为反例的代价。</p>
<p><strong>代价敏感错误率</strong>=样本中由模型得到的错误值与代价乘积之和 / 总样本。<br>其数学表达式为：</p>
<script type="math/tex; mode=display">
E(f;D;cost)=\frac{1}{m}\left( \sum_{x_{i} \in D^{+}}({f(x_i)\neq y_i})\times Cost_{01}+ \sum_{x_{i} \in D^{-}}({f(x_i)\neq y_i})\times Cost_{10}\right)</script><p>$D^{+}、D^{-}​$分别代表样例集的正例子集和反例子集，x是预测值，y是真实值。</p>
<p><strong>代价曲线</strong>：<br>    在均等代价时，ROC曲线不能直接反应出模型的期望总体代价，而代价曲线可以。<br>代价曲线横轴为[0,1]的正例函数代价：</p>
<script type="math/tex; mode=display">
P(+)Cost=\frac{p*Cost_{01}}{p*Cost_{01}+(1-p)*Cost_{10}}</script><p>其中p是样本为正例的概率。</p>
<p>代价曲线纵轴维[0,1]的归一化代价：</p>
<script type="math/tex; mode=display">
Cost_{norm}=\frac{FNR*p*Cost_{01}+FNR*(1-p)*Cost_{10}}{p*Cost_{01}+(1-p)*Cost_{10}}</script><p>其中FPR为假阳率，FNR=1-TPR为假阴率。</p>
<p>注：ROC每个点，对应代价平面上一条线。</p>
<p>例如，ROC上(TPR,FPR),计算出FNR=1-TPR，在代价平面上绘制一条从(0,FPR)到(1,FNR)的线段，面积则为该条件下期望的总体代价。所有线段下界面积，所有条件下学习器的期望总体代价。</p>
<p><img src="/img/ch2/2.16.18.1.png" alt=""></p>
<h3 id="2-16-18-模型有哪些比较检验方法"><a href="#2-16-18-模型有哪些比较检验方法" class="headerlink" title="2.16.18 模型有哪些比较检验方法"></a>2.16.18 模型有哪些比较检验方法</h3><p>正确性分析：模型稳定性分析，稳健性分析，收敛性分析，变化趋势分析，极值分析等。<br>有效性分析：误差分析，参数敏感性分析，模型对比检验等。<br>有用性分析：关键数据求解，极值点，拐点，变化趋势分析，用数据验证动态模拟等。<br>高效性分析：时空复杂度分析与现有进行比较等。</p>
<h3 id="2-16-19-为什么使用标准差"><a href="#2-16-19-为什么使用标准差" class="headerlink" title="2.16.19 为什么使用标准差"></a>2.16.19 为什么使用标准差</h3><p>方差公式为：$S^2_{N}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}​$</p>
<p>标准差公式为：$S_{N}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}​$</p>
<p>样本标准差公式为：$S_{N}=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(x_{i}-\bar{x})^{2}}​$</p>
<p>与方差相比，使用标准差来表示数据点的离散程度有3个好处：<br>1、表示离散程度的数字与样本数据点的数量级一致，更适合对数据样本形成感性认知。</p>
<p>2、表示离散程度的数字单位与样本数据的单位一致，更方便做后续的分析运算。</p>
<p>3、在样本数据大致符合正态分布的情况下，标准差具有方便估算的特性：68%的数据点落在平均值前后1个标准差的范围内、95%的数据点落在平均值前后2个标准差的范围内，而99%的数据点将会落在平均值前后3个标准差的范围内。</p>
<h3 id="2-16-20-类别不平衡产生原因"><a href="#2-16-20-类别不平衡产生原因" class="headerlink" title="2.16.20 类别不平衡产生原因"></a>2.16.20 类别不平衡产生原因</h3><p>​    类别不平衡（class-imbalance）是指分类任务中不同类别的训练样例数目差别很大的情况。 </p>
<p>产生原因：</p>
<p>​    分类学习算法通常都会假设不同类别的训练样例数目基本相同。如果不同类别的训练样例数目差别很大，则会影响学习结果，测试结果变差。例如二分类问题中有998个反例，正例有2个，那学习方法只需返回一个永远将新样本预测为反例的分类器，就能达到99.8%的精度；然而这样的分类器没有价值。</p>
<h3 id="2-16-21-常见的类别不平衡问题解决方法"><a href="#2-16-21-常见的类别不平衡问题解决方法" class="headerlink" title="2.16.21 常见的类别不平衡问题解决方法"></a>2.16.21 常见的类别不平衡问题解决方法</h3><p>  防止类别不平衡对学习造成的影响，在构建分类模型之前，需要对分类不平衡性问题进行处理。主要解决方法有：</p>
<p>1、扩大数据集</p>
<p>​    增加包含小类样本数据的数据，更多的数据能得到更多的分布信息。</p>
<p>2、对大类数据欠采样</p>
<p>​    减少大类数据样本个数，使与小样本个数接近。<br>​    缺点：欠采样操作时若随机丢弃大类样本，可能会丢失重要信息。<br>​    代表算法：EasyEnsemble。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，但对于全局则不会丢失重要信息。</p>
<p>3、对小类数据过采样</p>
<p>​    过采样：对小类的数据样本进行采样来增加小类的数据样本个数。 </p>
<p>​    代表算法：SMOTE和ADASYN。 </p>
<p>​    SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。</p>
<p>​    新的少数类样本产生的策略：对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。<br>​    ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。 通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。</p>
<p>4、使用新评价指标</p>
<p>​    如果当前评价指标不适用，则应寻找其他具有说服力的评价指标。比如准确度这个评价指标在类别不均衡的分类任务中并不适用，甚至进行误导。因此在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。</p>
<p>5、选择新算法</p>
<p>​    不同的算法适用于不同的任务与数据，应该使用不同的算法进行比较。</p>
<p>6、数据代价加权</p>
<p>​    例如当分类任务是识别小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值，从而使得分类器将重点集中在小类样本身上。</p>
<p>7、转化问题思考角度</p>
<p>​    例如在分类问题时，把小类的样本作为异常点，将问题转化为异常点检测或变化趋势检测问题。 异常点检测即是对那些罕见事件进行识别。变化趋势检测区别于异常点检测在于其通过检测不寻常的变化趋势来识别。    </p>
<p>8、将问题细化分析</p>
<p>​    对问题进行分析与挖掘，将问题划分成多个更小的问题，看这些小问题是否更容易解决。 </p>
<h2 id="2-17-决策树"><a href="#2-17-决策树" class="headerlink" title="2.17 决策树"></a>2.17 决策树</h2><h3 id="2-17-1-决策树的基本原理"><a href="#2-17-1-决策树的基本原理" class="headerlink" title="2.17.1 决策树的基本原理"></a>2.17.1 决策树的基本原理</h3><p>​    决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。</p>
<h3 id="2-17-2-决策树的三要素？"><a href="#2-17-2-决策树的三要素？" class="headerlink" title="2.17.2 决策树的三要素？"></a>2.17.2 决策树的三要素？</h3><p>​    一棵决策树的生成过程主要分为下3个部分：  </p>
<p>​    1、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。 </p>
<p>​    2、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。 </p>
<p>​    3、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。</p>
<h3 id="2-17-3-决策树学习基本算法"><a href="#2-17-3-决策树学习基本算法" class="headerlink" title="2.17.3 决策树学习基本算法"></a>2.17.3 决策树学习基本算法</h3><p><img src="/img/ch2/2-5.png" alt=""></p>
<h3 id="2-17-4-决策树算法优缺点"><a href="#2-17-4-决策树算法优缺点" class="headerlink" title="2.17.4 决策树算法优缺点"></a>2.17.4 决策树算法优缺点</h3><p><strong>决策树算法的优点</strong>：  </p>
<p>1、决策树算法易理解，机理解释起来简单。 </p>
<p>2、决策树算法可以用于小数据集。</p>
<p>3、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。</p>
<p>4、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。</p>
<p>5、能够处理多输出的问题。 </p>
<p>6、对缺失值不敏感。</p>
<p>7、可以处理不相关特征数据。</p>
<p>8、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。</p>
<p><strong>决策树算法的缺点</strong>： </p>
<p>1、对连续性的字段比较难预测。</p>
<p>2、容易出现过拟合。</p>
<p>3、当类别太多时，错误可能就会增加的比较快。</p>
<p>4、在处理特征关联性比较强的数据时表现得不是太好。</p>
<p>5、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。</p>
<h3 id="2-17-5-熵的概念以及理解"><a href="#2-17-5-熵的概念以及理解" class="headerlink" title="2.17.5 熵的概念以及理解"></a>2.17.5 熵的概念以及理解</h3><p>​    熵：度量随机变量的不确定性。<br>​    定义：假设随机变量X的可能取值有$x_{1},x_{2},…,x_{n}$，对于每一个可能的取值$x_{i}$，其概率为$P(X=x_{i})=p_{i},i=1,2…,n$。随机变量的熵为：</p>
<script type="math/tex; mode=display">
H(X)=-\sum_{i=1}^{n}p_{i}log_{2}p_{i}</script><p>​       对于样本集合，假设样本有k个类别，每个类别的概率为$\frac{|C_{k}|}{|D|}$，其中 ${|C_{k}|}{|D|}$为类别为k的样本个数，$|D|​$为样本总数。样本集合D的熵为：</p>
<script type="math/tex; mode=display">
H(D)=-\sum_{k=1}^{k}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}</script><h3 id="2-17-6-信息增益的理解"><a href="#2-17-6-信息增益的理解" class="headerlink" title="2.17.6 信息增益的理解"></a>2.17.6 信息增益的理解</h3><p>​    定义：以某特征划分数据集前后的熵的差值。<br>​    熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。  ​    假设划分前样本集合D的熵为H(D)。使用某个特征A划分数据集D，计算划分后的数据子集的熵为H(D|A)。<br>​    则信息增益为：</p>
<script type="math/tex; mode=display">
g(D,A)=H(D)-H(D|A)</script><p>​    <em>注：</em>在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。<br>​    思想：计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。<br>​    另外这里提一下信息增益比相关知识：<br>​    $信息增益比=惩罚参数\times信息增益$<br>​    信息增益比本质：在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<br>​    惩罚参数：数据集D以特征A作为随机变量的熵的倒数。</p>
<h3 id="2-17-7-剪枝处理的作用及策略"><a href="#2-17-7-剪枝处理的作用及策略" class="headerlink" title="2.17.7 剪枝处理的作用及策略"></a>2.17.7 剪枝处理的作用及策略</h3><p>​    剪枝处理是决策树学习算法用来解决过拟合问题的一种办法。</p>
<p>​    在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点， 而导致过拟合。 因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。 </p>
<p>​    剪枝的基本策略有预剪枝（pre-pruning）和后剪枝（post-pruning）。</p>
<p>​    预剪枝：在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。 </p>
<p>​    后剪枝：生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。</p>
<h2 id="2-18-支持向量机"><a href="#2-18-支持向量机" class="headerlink" title="2.18 支持向量机"></a>2.18 支持向量机</h2><h3 id="2-18-1-什么是支持向量机"><a href="#2-18-1-什么是支持向量机" class="headerlink" title="2.18.1 什么是支持向量机"></a>2.18.1 什么是支持向量机</h3><p>​    支持向量：在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。</p>
<p>​    支持向量机（Support Vector Machine，SVM）：其含义是通过支持向量运算的分类器。</p>
<p>​    在一个二维环境中，其中点R，S，G点和其它靠近中间黑线的点可以看作为支持向量，它们可以决定分类器，即黑线的具体参数。</p>
<p><img src="/img/ch2/2-6.png" alt=""></p>
<p>​    支持向量机是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是边界最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括：</p>
<p>​    当训练样本线性可分时，通过硬边界（hard margin）最大化，学习一个线性可分支持向量机；</p>
<p>​    当训练样本近似线性可分时，通过软边界（soft margin）最大化，学习一个线性支持向量机；</p>
<p>​    当训练样本线性不可分时，通过核技巧和软边界最大化，学习一个非线性支持向量机；</p>
<h3 id="2-18-2-支持向量机能解决哪些问题"><a href="#2-18-2-支持向量机能解决哪些问题" class="headerlink" title="2.18.2 支持向量机能解决哪些问题"></a>2.18.2 支持向量机能解决哪些问题</h3><p><strong>线性分类</strong></p>
<p>​    在训练数据中，每个数据都有n个的属性和一个二分类类别标志，我们可以认为这些数据在一个n维空间里。我们的目标是找到一个n-1维的超平面，这个超平面可以将数据分成两部分，每部分数据都属于同一个类别。</p>
<p>​    这样的超平面有很多，假如我们要找到一个最佳的超平面。此时，增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的，成为最大边距超平面。这个分类器即为最大边距分类器。</p>
<p><strong>非线性分类</strong></p>
<p>​    SVM的一个优势是支持非线性分类。它结合使用拉格朗日乘子法（Lagrange Multiplier）和KKT（Karush Kuhn Tucker）条件，以及核函数可以生成非线性分类器。</p>
<h3 id="2-18-3-核函数特点及其作用"><a href="#2-18-3-核函数特点及其作用" class="headerlink" title="2.18.3 核函数特点及其作用"></a>2.18.3 核函数特点及其作用</h3><p>​    引入核函数目的：把原坐标系里线性不可分的数据用核函数Kernel投影到另一个空间，尽量使得数据在新的空间里线性可分。<br>​    核函数方法的广泛应用，与其特点是分不开的：  </p>
<p>1）核函数的引入避免了“维数灾难”，大大减小了计算量。而输入空间的维数n对核函数矩阵无影响。因此，核函数方法可以有效处理高维输入。</p>
<p>2）无需知道非线性变换函数Φ的形式和参数。</p>
<p>3）核函数的形式和参数的变化会隐式地改变从输入空间到特征空间的映射，进而对特征空间的性质产生影响，最终改变各种核函数方法的性能。</p>
<p>4）核函数方法可以和不同的算法相结合，形成多种不同的基于核函数技术的方法，且这两部分的设计可以单独进行，并可以为不同的应用选择不同的核函数和算法。</p>
<h3 id="2-18-4-SVM为什么引入对偶问题"><a href="#2-18-4-SVM为什么引入对偶问题" class="headerlink" title="2.18.4 SVM为什么引入对偶问题"></a>2.18.4 SVM为什么引入对偶问题</h3><p>1，对偶问题将原始问题中的约束转为了对偶问题中的等式约束，对偶问题往往更加容易求解。</p>
<p>2，可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）。</p>
<p>3，在优化理论中，目标函数 f(x) 会有多种形式：如果目标函数和约束条件都为变量 x 的线性函数，称该问题为线性规划；如果目标函数为二次函数，约束条件为线性函数，称该最优化问题为二次规划；如果目标函数或者约束条件均为非线性函数，称该最优化问题为非线性规划。每个线性规划问题都有一个与之对应的对偶问题，对偶问题有非常良好的性质，以下列举几个：</p>
<p>​    a, 对偶问题的对偶是原问题；</p>
<p>​    b, 无论原始问题是否是凸的，对偶问题都是凸优化问题；</p>
<p>​    c, 对偶问题可以给出原始问题一个下界；</p>
<p>​    d, 当满足一定条件时，原始问题与对偶问题的解是完全等价的。</p>
<h3 id="2-18-5-如何理解SVM中的对偶问题"><a href="#2-18-5-如何理解SVM中的对偶问题" class="headerlink" title="2.18.5 如何理解SVM中的对偶问题"></a>2.18.5 如何理解SVM中的对偶问题</h3><p>在硬边界支持向量机中，问题的求解可以转化为凸二次规划问题。</p>
<p>​    假设优化目标为</p>
<script type="math/tex; mode=display">
\begin{align}
&\min_{\boldsymbol w, b}\frac{1}{2}||\boldsymbol w||^2\\
&s.t. y_i(\boldsymbol w^T\boldsymbol x_i+b)\geqslant 1, i=1,2,\cdots,m.\\
\end{align}  \tag{1}</script><p><strong>step 1</strong>. 转化问题：</p>
<script type="math/tex; mode=display">
\min_{\boldsymbol w, b} \max_{\alpha_i \geqslant 0}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}  \tag{2}</script><p>上式等价于原问题，因为若满足(1)中不等式约束，则(2)式求max时,$\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))$必须取0，与(1)等价；若不满足(1)中不等式约束，(2)中求max会得到无穷大。 交换min和max获得其对偶问题:</p>
<script type="math/tex; mode=display">
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}</script><p>交换之后的对偶问题和原问题并不相等，上式的解小于等于原问题的解。</p>
<p><strong>step 2</strong>.现在的问题是如何找到问题(1) 的最优值的一个最好的下界? </p>
<script type="math/tex; mode=display">
\frac{1}{2}||\boldsymbol w||^2 < v\\
1 - y_i(\boldsymbol w^T\boldsymbol x_i+b) \leqslant 0\tag{3}</script><p>若方程组(3)无解， 则v是问题(1)的一个下界。若(3)有解， 则 </p>
<script type="math/tex; mode=display">
\forall \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} < v</script><p>由逆否命题得：若 </p>
<script type="math/tex; mode=display">
\exists \boldsymbol \alpha >  0 , \ \min_{\boldsymbol w, b}  \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\} \geqslant v</script><p>则(3)无解。</p>
<p>那么v是问题</p>
<p>(1)的一个下界。<br> 要求得一个好的下界，取最大值即可 </p>
<script type="math/tex; mode=display">
\max_{\alpha_i \geqslant 0}  \min_{\boldsymbol w, b} \left\{\frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))\right\}</script><p><strong>step 3</strong>. 令</p>
<script type="math/tex; mode=display">
L(\boldsymbol w, b,\boldsymbol a) =   \frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\boldsymbol w^T\boldsymbol x_i+b))</script><p>$p^<em>$为原问题的最小值，对应的$w,b$分别为$w^</em>,b^*$,则对于任意的$a&gt;0$:</p>
<script type="math/tex; mode=display">
p^* = \frac{1}{2}||\boldsymbol w^*||^2 \geqslant  L(\boldsymbol w^*, b,\boldsymbol a) \geqslant \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)</script><p>则 $\min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)$是问题（1）的一个下界。</p>
<p>此时，取最大值即可求得好的下界，即</p>
<script type="math/tex; mode=display">
\max_{\alpha_i \geqslant 0} \min_{\boldsymbol w, b} L(\boldsymbol w, b,\boldsymbol a)</script><h3 id="2-18-7-常见的核函数有哪些"><a href="#2-18-7-常见的核函数有哪些" class="headerlink" title="2.18.7 常见的核函数有哪些"></a>2.18.7 常见的核函数有哪些</h3><div class="table-container">
<table>
<thead>
<tr>
<th>核函数</th>
<th>表达式</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Kernel线性核</td>
<td>$k(x,y)=x^{t}y+c$</td>
<td></td>
</tr>
<tr>
<td>Polynomial Kernel多项式核</td>
<td>$k(x,y)=(ax^{t}y+c)^{d}$</td>
<td>$d\geqslant1$为多项式的次数</td>
</tr>
<tr>
<td>Exponential Kernel指数核</td>
<td>$k(x,y)=exp(-\frac{\left \</td>
<td>x-y \right \</td>
<td>}{2\sigma ^{2}})$</td>
<td>$\sigma&gt;0$</td>
</tr>
<tr>
<td>Gaussian Kernel高斯核</td>
<td>$k(x,y)=exp(-\frac{\left \</td>
<td>x-y \right \</td>
<td>^{2}}{2\sigma ^{2}})$</td>
<td>$\sigma$为高斯核的带宽，$\sigma&gt;0$,</td>
</tr>
<tr>
<td>Laplacian Kernel拉普拉斯核</td>
<td>$k(x,y)=exp(-\frac{\left \</td>
<td>x-y \right \</td>
<td>}{\sigma})$</td>
<td>$\sigma&gt;0$</td>
</tr>
<tr>
<td>ANOVA Kernel</td>
<td>$k(x,y)=exp(-\sigma(x^{k}-y^{k})^{2})^{d}$</td>
<td></td>
</tr>
<tr>
<td>Sigmoid Kernel</td>
<td>$k(x,y)=tanh(ax^{t}y+c)$</td>
<td>$tanh$为双曲正切函数，$a&gt;0,c&lt;0$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-18-9-SVM主要特点"><a href="#2-18-9-SVM主要特点" class="headerlink" title="2.18.9 SVM主要特点"></a>2.18.9 SVM主要特点</h3><p>特点：</p>
<p>(1)  SVM方法的理论基础是非线性映射，SVM利用内积核函数代替向高维空间的非线性映射。<br>(2)  SVM的目标是对特征空间划分得到最优超平面，SVM方法核心是最大化分类边界。<br>(3)  支持向量是SVM的训练结果，在SVM分类决策中起决定作用的是支持向量。<br>(4)  SVM是一种有坚实理论基础的新颖的适用小样本学习方法。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。<br>(5)  SVM的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。<br>(6)  少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且具有较好的“鲁棒性”。这种鲁棒性主要体现在：<br>​        ①增、删非支持向量样本对模型没有影响;<br>​        ②支持向量样本集具有一定的鲁棒性;<br>​        ③有些成功的应用中，SVM方法对核的选取不敏感<br>(7)  SVM学习问题可以表示为凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于贪心学习的策略来搜索假设空间，这种方法一般只能获得局部最优解。<br>(8)  SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。<br>(9)  SVM在小样本训练集上能够得到比其它算法好很多的结果。SVM优化目标是结构化风险最小，而不是经验风险最小，避免了过拟合问题，通过margin的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，有优秀的泛化能力。<br>(10)  它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。  </p>
<h3 id="2-18-10-SVM主要缺点"><a href="#2-18-10-SVM主要缺点" class="headerlink" title="2.18.10 SVM主要缺点"></a>2.18.10 SVM主要缺点</h3><p>(1) SVM算法对大规模训练样本难以实施<br>​        SVM的空间消耗主要是存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。<br>​        如果数据量很大，SVM的训练时间就会比较长，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。</p>
<p>(2) 用SVM解决多分类问题存在困难</p>
<p>​        经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗糙集理论结合，形成一种优势互补的多类问题的组合分类器。</p>
<p>(3) 对缺失数据敏感，对参数和核函数的选择敏感</p>
<p>​        支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造SVM算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。</p>
<h3 id="2-18-11-逻辑回归与SVM的异同"><a href="#2-18-11-逻辑回归与SVM的异同" class="headerlink" title="2.18.11 逻辑回归与SVM的异同"></a>2.18.11 逻辑回归与SVM的异同</h3><p>相同点：</p>
<ul>
<li>LR和SVM都是<strong>分类</strong>算法。</li>
<li>LR和SVM都是<strong>监督学习</strong>算法。</li>
<li>LR和SVM都是<strong>判别模型</strong>。</li>
<li>如果不考虑核函数，LR和SVM都是<strong>线性分类</strong>算法，也就是说他们的分类决策面都是线性的。<br> 说明：LR也是可以用核函数的.但LR通常不采用核函数的方法。（<strong>计算量太大</strong>）</li>
</ul>
<p>不同点：</p>
<p><strong>1、LR采用log损失，SVM采用合页(hinge)损失。</strong><br>逻辑回归的损失函数：</p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}\sum^m_{i=1}\left[y^{i}logh_{\theta}(x^{i})+ (1-y^{i})log(1-h_{\theta}(x^{i}))\right]</script><p>支持向量机的目标函数:</p>
<script type="math/tex; mode=display">
L(w,n,a)=\frac{1}{2}||w||^2-\sum^n_{i=1}\alpha_i \left( y_i(w^Tx_i+b)-1\right)</script><p>​    逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过<strong>极大似然估计</strong>的方法估计出参数的值。<br>​    支持向量机基于几何<strong>边界最大化</strong>原理，认为存在最大几何边界的分类面为最优分类面。</p>
<p>2、<strong>LR对异常值敏感，SVM对异常值不敏感</strong>。</p>
<p>​    支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局。LR模型找到的那个超平面，是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。<br>​    支持向量机改变非支持向量样本并不会引起决策面的变化。<br>​    逻辑回归中改变任何样本都会引起决策面的变化。  </p>
<p>3、<strong>计算复杂度不同。对于海量数据，SVM的效率较低，LR效率比较高</strong></p>
<p>​    当样本较少，特征维数较低时，SVM和LR的运行时间均比较短，SVM较短一些。准确率的话，LR明显比SVM要高。当样本稍微增加些时，SVM运行时间开始增长，但是准确率赶超了LR。SVM时间虽长，但在可接受范围内。当数据量增长到20000时，特征维数增长到200时，SVM的运行时间剧烈增加，远远超过了LR的运行时间。但是准确率却和LR相差无几。(这其中主要原因是大量非支持向量参与计算，造成SVM的二次规划问题)</p>
<p>4、<strong>对非线性问题的处理方式不同</strong></p>
<p>​    LR主要靠特征构造，必须组合交叉特征，特征离散化。SVM也可以这样，还可以通过核函数kernel（因为只有支持向量参与核计算，计算复杂度不高）。由于可以利用核函数，SVM则可以通过对偶求解高效处理。LR则在特征空间维度很高时，表现较差。</p>
<p>5、<strong>SVM的损失函数就自带正则</strong>。<br>​    损失函数中的1/2||w||^2项，这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！**</p>
<p>6、SVM自带<strong>结构风险最小化</strong>，LR则是<strong>经验风险最小化</strong>。</p>
<p>7、SVM会用核函数而LR一般不用核函数。</p>
<h2 id="2-19-贝叶斯分类器"><a href="#2-19-贝叶斯分类器" class="headerlink" title="2.19 贝叶斯分类器"></a>2.19 贝叶斯分类器</h2><h3 id="2-19-1-图解极大似然估计"><a href="#2-19-1-图解极大似然估计" class="headerlink" title="2.19.1 图解极大似然估计"></a>2.19.1 图解极大似然估计</h3><p>极大似然估计的原理，用一张图片来说明，如下图所示：</p>
<p><img src="/img/ch2/2.19.1.1.png" alt=""></p>
<p>​    例：有两个外形完全相同的箱子，1号箱有99只白球，1只黑球；2号箱有1只白球，99只黑球。在一次实验中，取出的是黑球，请问是从哪个箱子中取出的？</p>
<p>​    一般的根据经验想法，会猜测这只黑球最像是从2号箱取出，此时描述的“最像”就有“最大似然”的意思，这种想法常称为“最大似然原理”。</p>
<h3 id="2-19-2-极大似然估计原理"><a href="#2-19-2-极大似然估计原理" class="headerlink" title="2.19.2 极大似然估计原理"></a>2.19.2 极大似然估计原理</h3><p>​    总结起来，最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。</p>
<p>​    极大似然估计是建立在极大似然原理的基础上的一个统计方法。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。</p>
<p>​    由于样本集中的样本都是独立同分布，可以只考虑一类样本集$D$，来估计参数向量$\vec\theta$。记已知的样本集为：</p>
<script type="math/tex; mode=display">
D=\vec x_{1},\vec x_{2},...,\vec x_{n}</script><p>似然函数（likelihood function）：联合概率密度函数$p(D|\vec\theta )$称为相对于$\vec x_{1},\vec x_{2},…,\vec x_{n}$的$\vec\theta$的似然函数。</p>
<script type="math/tex; mode=display">
l(\vec\theta )=p(D|\vec\theta ) =p(\vec x_{1},\vec x_{2},...,\vec x_{n}|\vec\theta )=\prod_{i=1}^{n}p(\vec x_{i}|\vec \theta )</script><p>如果$\hat{\vec\theta}$是参数空间中能使似然函数$l(\vec\theta)$最大的$\vec\theta$值，则$\hat{\vec\theta}$应该是“最可能”的参数值，那么$\hat{\vec\theta}​$就是$\theta$的极大似然估计量。它是样本集的函数，记作：</p>
<script type="math/tex; mode=display">
\hat{\vec\theta}=d(D)= \mathop {\arg \max}_{\vec\theta} l(\vec\theta )</script><p>$\hat{\vec\theta}(\vec x_{1},\vec x_{2},…,\vec x_{n})$称为极大似然函数估计值。</p>
<h3 id="2-19-3-贝叶斯分类器基本原理"><a href="#2-19-3-贝叶斯分类器基本原理" class="headerlink" title="2.19.3 贝叶斯分类器基本原理"></a>2.19.3 贝叶斯分类器基本原理</h3><p>​    贝叶斯决策论通过<strong>相关概率已知</strong>的情况下利用<strong>误判损失</strong>来选择最优的类别分类。<br>假设有$N$种可能的分类标记，记为$Y=\{c_1,c_2,…,c_N\}$，那对于样本$\boldsymbol{x}$，它属于哪一类呢？</p>
<p>计算步骤如下：</p>
<p>step 1. 算出样本$\boldsymbol{x}$属于第i个类的概率，即$P(c_i|x)​$；</p>
<p>step 2. 通过比较所有的$P(c_i|\boldsymbol{x})$，得到样本$\boldsymbol{x}$所属的最佳类别。</p>
<p>step 3. 将类别$c_i$和样本$\boldsymbol{x}$代入到贝叶斯公式中，得到：</p>
<script type="math/tex; mode=display">
P(c_i|\boldsymbol{x})=\frac{P(\boldsymbol{x}|c_i)P(c_i)}{P(\boldsymbol{x})}.</script><p>​    一般来说，$P(c_i)$为先验概率，$P(\boldsymbol{x}|c_i)$为条件概率，$P(\boldsymbol{x})$是用于归一化的证据因子。对于$P(c_i)$可以通过训练样本中类别为$c_i$的样本所占的比例进行估计；此外，由于只需要找出最大的$P(\boldsymbol{x}|c_i)$，因此我们并不需要计算$P(\boldsymbol{x})$。<br>​    为了求解条件概率，基于不同假设提出了不同的方法，以下将介绍朴素贝叶斯分类器和半朴素贝叶斯分类器。</p>
<h3 id="2-19-4-朴素贝叶斯分类器"><a href="#2-19-4-朴素贝叶斯分类器" class="headerlink" title="2.19.4 朴素贝叶斯分类器"></a>2.19.4 朴素贝叶斯分类器</h3><p>​    假设样本$\boldsymbol{x}$包含$d$个属性，即$\boldsymbol{x}=\{ x_1,x_2,…,x_d\}$。于是有：</p>
<script type="math/tex; mode=display">
P(\boldsymbol{x}|c_i)=P(x_1,x_2,\cdots,x_d|c_i)</script><p>这个联合概率难以从有限的训练样本中直接估计得到。于是，朴素贝叶斯（Naive Bayesian，简称NB）采用了“属性条件独立性假设”：对已知类别，假设所有属性相互独立。于是有：</p>
<script type="math/tex; mode=display">
P(x_1,x_2,\cdots,x_d|c_i)=\prod_{j=1}^d P(x_j|c_i)</script><p>这样的话，我们就可以很容易地推出相应的判定准则了：</p>
<script type="math/tex; mode=display">
h_{nb}(\boldsymbol{x})=\mathop{\arg \max}_{c_i\in Y} P(c_i)\prod_{j=1}^dP(x_j|c_i)</script><p><strong>条件概率$P(x_j|c_i)​$的求解</strong></p>
<p>如果$x_j$是标签属性，那么我们可以通过计数的方法估计$P(x_j|c_i)$</p>
<script type="math/tex; mode=display">
P(x_j|c_i)=\frac{P(x_j,c_i)}{P(c_i)}\approx\frac{\#(x_j,c_i)}{\#(c_i)}</script><p>其中，$#(x_j,c_i)$表示在训练样本中$x_j$与$c_{i}$共同出现的次数。</p>
<p>如果$x_j​$是数值属性，通常我们假设类别中$c_{i}​$的所有样本第$j​$个属性的值服从正态分布。我们首先估计这个分布的均值$μ​$和方差$σ​$，然后计算$x_j​$在这个分布中的概率密度$P(x_j|c_i)​$。</p>
<h3 id="2-19-5-举例理解朴素贝叶斯分类器"><a href="#2-19-5-举例理解朴素贝叶斯分类器" class="headerlink" title="2.19.5 举例理解朴素贝叶斯分类器"></a>2.19.5 举例理解朴素贝叶斯分类器</h3><p>使用经典的西瓜训练集如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">色泽</th>
<th style="text-align:center">根蒂</th>
<th style="text-align:center">敲声</th>
<th style="text-align:center">纹理</th>
<th style="text-align:center">脐部</th>
<th style="text-align:center">触感</th>
<th style="text-align:center">密度</th>
<th style="text-align:center">含糖率</th>
<th style="text-align:center">好瓜</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.774</td>
<td style="text-align:center">0.376</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.634</td>
<td style="text-align:center">0.264</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.608</td>
<td style="text-align:center">0.318</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.556</td>
<td style="text-align:center">0.215</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.403</td>
<td style="text-align:center">0.237</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.481</td>
<td style="text-align:center">0.149</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.437</td>
<td style="text-align:center">0.211</td>
<td style="text-align:center">是</td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.666</td>
<td style="text-align:center">0.091</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.243</td>
<td style="text-align:center">0.267</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">硬挺</td>
<td style="text-align:center">清脆</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.245</td>
<td style="text-align:center">0.057</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.343</td>
<td style="text-align:center">0.099</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.639</td>
<td style="text-align:center">0.161</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.657</td>
<td style="text-align:center">0.198</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">15</td>
<td style="text-align:center">乌黑</td>
<td style="text-align:center">稍蜷</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">软粘</td>
<td style="text-align:center">0.360</td>
<td style="text-align:center">0.370</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">浅白</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">模糊</td>
<td style="text-align:center">平坦</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.593</td>
<td style="text-align:center">0.042</td>
<td style="text-align:center">否</td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">沉闷</td>
<td style="text-align:center">稍糊</td>
<td style="text-align:center">稍凹</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.719</td>
<td style="text-align:center">0.103</td>
<td style="text-align:center">否</td>
</tr>
</tbody>
</table>
</div>
<p>对下面的测试例“测1”进行 分类：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">色泽</th>
<th style="text-align:center">根蒂</th>
<th style="text-align:center">敲声</th>
<th style="text-align:center">纹理</th>
<th style="text-align:center">脐部</th>
<th style="text-align:center">触感</th>
<th style="text-align:center">密度</th>
<th style="text-align:center">含糖率</th>
<th style="text-align:center">好瓜</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">测1</td>
<td style="text-align:center">青绿</td>
<td style="text-align:center">蜷缩</td>
<td style="text-align:center">浊响</td>
<td style="text-align:center">清晰</td>
<td style="text-align:center">凹陷</td>
<td style="text-align:center">硬滑</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">？</td>
</tr>
</tbody>
</table>
</div>
<p>首先，估计类先验概率$P(c_j)$，有</p>
<script type="math/tex; mode=display">
\begin{align} 
&P(好瓜=是)=\frac{8}{17}=0.471 \newline 
&P(好瓜=否)=\frac{9}{17}=0.529 
\end{align}</script><p>然后，为每个属性估计条件概率（这里，对于连续属性，假定它们服从正态分布）</p>
<script type="math/tex; mode=display">
P_{青绿|是}=P（色泽=青绿|好瓜=是）=\frac{3}{8}=0.375</script><script type="math/tex; mode=display">
P_{青绿|否}=P（色泽=青绿|好瓜=否）=\frac{3}{9}\approx0.333</script><script type="math/tex; mode=display">
P_{蜷缩|是}=P（根蒂=蜷缩|好瓜=是）=\frac{5}{8}=0.625</script><script type="math/tex; mode=display">
P_{蜷缩|否}=P（根蒂=蜷缩|好瓜=否）=\frac{3}{9}=0.333</script><script type="math/tex; mode=display">
P_{浊响|是}=P（敲声=浊响|好瓜=是）=\frac{6}{8}=0.750</script><script type="math/tex; mode=display">
P_{浊响|否}=P（敲声=浊响|好瓜=否）=\frac{4}{9}\approx 0.444</script><script type="math/tex; mode=display">
P_{清晰|是}=P（纹理=清晰|好瓜=是）=\frac{7}{8}= 0.875</script><script type="math/tex; mode=display">
P_{清晰|否}=P（纹理=清晰|好瓜=否）=\frac{2}{9}\approx 0.222</script><script type="math/tex; mode=display">
P_{凹陷|是}=P（脐部=凹陷|好瓜=是）=\frac{6}{8}= 0.750</script><script type="math/tex; mode=display">
P_{凹陷|否}=P（脐部=凹陷|好瓜=否）=\frac{2}{9} \approx 0.222</script><script type="math/tex; mode=display">
P_{硬滑|是}=P（触感=硬滑|好瓜=是）=\frac{6}{8}= 0.750</script><script type="math/tex; mode=display">
P_{硬滑|否}=P（触感=硬滑|好瓜=否）=\frac{6}{9} \approx 0.667</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{密度：0.697|是}&=\rho（密度=0.697|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.129}exp\left( -\frac{(0.697-0.574)^2}{2\times0.129^2}\right) \approx 1.959
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{密度：0.697|否}&=\rho（密度=0.697|好瓜=否）\\&=\frac{1}{\sqrt{2 \pi}\times0.195}exp\left( -\frac{(0.697-0.496)^2}{2\times0.195^2}\right) \approx 1.203
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{含糖：0.460|是}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.101}exp\left( -\frac{(0.460-0.279)^2}{2\times0.101^2}\right) \approx 0.788
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\rho_{含糖：0.460|否}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.108}exp\left( -\frac{(0.460-0.154)^2}{2\times0.108^2}\right) \approx 0.066
\end{aligned}</script><p>于是有</p>
<script type="math/tex; mode=display">
\begin{align} 
P(&好瓜=是)\times P_{青绿|是} \times P_{蜷缩|是} \times P_{浊响|是} \times P_{清晰|是} \times P_{凹陷|是}\newline 
&\times P_{硬滑|是} \times p_{密度：0.697|是} \times p_{含糖：0.460|是} \approx 0.063 \newline\newline 
P(&好瓜=否)\times P_{青绿|否} \times P_{蜷缩|否} \times P_{浊响|否} \times P_{清晰|否} \times P_{凹陷|否}\newline 
&\times P_{硬滑|否} \times p_{密度：0.697|否} \times p_{含糖：0.460|否} \approx 6.80\times 10^{-5} 
\end{align}</script><p>由于$0.063&gt;6.80\times 10^{-5}$，因此，朴素贝叶斯分类器将测试样本“测1”判别为“好瓜”。</p>
<h3 id="2-19-6-半朴素贝叶斯分类器"><a href="#2-19-6-半朴素贝叶斯分类器" class="headerlink" title="2.19.6 半朴素贝叶斯分类器"></a>2.19.6 半朴素贝叶斯分类器</h3><p>​    朴素贝叶斯采用了“属性条件独立性假设”，半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的相互依赖信息。<strong>独依赖估计</strong>（One-Dependence Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略。顾名思义，独依赖是假设每个属性在类别之外最多依赖一个其他属性，即：</p>
<script type="math/tex; mode=display">
P(\boldsymbol{x}|c_i)=\prod_{j=1}^d P(x_j|c_i,{\rm pa}_j)</script><p>其中$pa_j$为属性$x_i$所依赖的属性，成为$x_i$的父属性。假设父属性$pa_j$已知，那么可以使用下面的公式估计$P(x_j|c_i,{\rm pa}_j)$</p>
<script type="math/tex; mode=display">
P(x_j|c_i,{\rm pa}_j)=\frac{P(x_j,c_i,{\rm pa}_j)}{P(c_i,{\rm pa}_j)}</script><h2 id="2-20-EM算法"><a href="#2-20-EM算法" class="headerlink" title="2.20 EM算法"></a>2.20 EM算法</h2><h3 id="2-20-1-EM算法基本思想"><a href="#2-20-1-EM算法基本思想" class="headerlink" title="2.20.1 EM算法基本思想"></a>2.20.1 EM算法基本思想</h3><p>​    最大期望算法（Expectation-Maximization algorithm, EM），是一类通过迭代进行极大似然估计的优化算法，通常作为牛顿迭代法的替代，用于对包含隐变量或缺失数据的概率模型进行参数估计。</p>
<p>​    最大期望算法基本思想是经过两个步骤交替进行计算：</p>
<p>​    第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值<strong>；</strong></p>
<p>​    第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。</p>
<p>​    M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</p>
<h3 id="2-20-2-EM算法推导"><a href="#2-20-2-EM算法推导" class="headerlink" title="2.20.2 EM算法推导"></a>2.20.2 EM算法推导</h3><p>​    对于$m$个样本观察数据$x=(x^{1},x^{2},…,x^{m})$，现在想找出样本的模型参数$\theta$，其极大化模型分布的对数似然函数为：</p>
<script type="math/tex; mode=display">
\theta = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta)</script><p>如果得到的观察数据有未观察到的隐含数据$z=(z^{(1)},z^{(2)},…z^{(m)})$，极大化模型分布的对数似然函数则为：</p>
<script type="math/tex; mode=display">
\theta =\mathop{\arg\max}_\theta\sum\limits_{i=1}^m logP(x^{(i)};\theta) = \mathop{\arg\max}_\theta\sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)  \tag{a}</script><p>由于上式不能直接求出$\theta$，采用缩放技巧：</p>
<script type="math/tex; mode=display">
\begin{align} \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}P(x^{(i)}, z^{(i)};\theta)   & = \sum\limits_{i=1}^m log\sum\limits_{z^{(i)}}Q_i(z^{(i)})\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \\ & \geqslant  \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align}   \tag{1}</script><p>上式用到了Jensen不等式：</p>
<script type="math/tex; mode=display">
log\sum\limits_j\lambda_jy_j \geqslant \sum\limits_j\lambda_jlogy_j\;\;,  \lambda_j \geqslant 0, \sum\limits_j\lambda_j =1</script><p>并且引入了一个未知的新分布$Q_i(z^{(i)})$。</p>
<p>此时，如果需要满足Jensen不等式中的等号，所以有：</p>
<script type="math/tex; mode=display">
\frac{P(x^{(i)}, z^{(i)};\theta)}{Q_i(z^{(i)})} =c, c为常数</script><p>由于$Q_i(z^{(i)})$是一个分布，所以满足</p>
<script type="math/tex; mode=display">
\sum\limits_{z}Q_i(z^{(i)}) =1</script><p>综上，可得：</p>
<script type="math/tex; mode=display">
Q_i(z^{(i)})  = \frac{P(x^{(i)}， z^{(i)};\theta)}{\sum\limits_{z}P(x^{(i)}, z^{(i)};\theta)} =  \frac{P(x^{(i)}, z^{(i)};\theta)}{P(x^{(i)};\theta)} = P( z^{(i)}|x^{(i)};\theta)</script><p>如果$Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)};\theta)$ ，则第(1)式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要最大化下式：</p>
<script type="math/tex; mode=display">
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log\frac{P(x^{(i)}， z^{(i)};\theta)}{Q_i(z^{(i)})}</script><p>简化得：</p>
<script type="math/tex; mode=display">
\mathop{\arg\max}_\theta \sum\limits_{i=1}^m \sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}</script><p>以上即为EM算法的M步，$\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}, z^{(i)};\theta)}​$可理解为$logP(x^{(i)}, z^{(i)};\theta) $基于条件概率分布$Q_i(z^{(i)}) $的期望。以上即为EM算法中E步和M步的具体数学含义。</p>
<h3 id="2-20-3-图解EM算法"><a href="#2-20-3-图解EM算法" class="headerlink" title="2.20.3 图解EM算法"></a>2.20.3 图解EM算法</h3><p>​    考虑上一节中的（a）式，表达式中存在隐变量，直接找到参数估计比较困难，通过EM算法迭代求解下界的最大值到收敛为止。</p>
<p><img src="/img/ch2/2.20.1.jpg" alt=""></p>
<p>​    图片中的紫色部分是我们的目标模型$p(x|\theta)$，该模型复杂，难以求解析解，为了消除隐变量$z^{(i)}$的影响，我们可以选择一个不包含$z^{(i)}$的模型$r(x|\theta)$，使其满足条件$r(x|\theta) \leqslant p(x|\theta) $。</p>
<p>求解步骤如下：</p>
<p>（1）选取$\theta_1$，使得$r(x|\theta_1) = p(x|\theta_1)$，然后对此时的$r$求取最大值，得到极值点$\theta_2$，实现参数的更新。</p>
<p>（2）重复以上过程到收敛为止，在更新过程中始终满足$r \leqslant p $.</p>
<h3 id="2-20-4-EM算法流程"><a href="#2-20-4-EM算法流程" class="headerlink" title="2.20.4 EM算法流程"></a>2.20.4 EM算法流程</h3><p>输入：观察数据$x=(x^{(1)},x^{(2)},…x^{(m)})$，联合分布$p(x,z ;\theta)$，条件分布$p(z|x; \theta)$，最大迭代次数$J$</p>
<p>1）随机初始化模型参数$\theta$的初值$\theta^0$。</p>
<p>2）$for  j   from  1   to   j$：</p>
<p>​    a） E步。计算联合分布的条件概率期望：</p>
<script type="math/tex; mode=display">
Q_i(z^{(i)}) = P( z^{(i)}|x^{(i)}, \theta^{j})</script><script type="math/tex; mode=display">
L(\theta, \theta^{j}) = \sum\limits_{i=1}^m\sum\limits_{z^{(i)}}P( z^{(i)}|x^{(i)}, \theta^{j})log{P(x^{(i)}, z^{(i)};\theta)}</script><p>​    b） M步。极大化$L(\theta, \theta^{j})$，得到$\theta^{j+1}$:</p>
<script type="math/tex; mode=display">
\theta^{j+1} = \mathop{\arg\max}_\theta L(\theta, \theta^{j})</script><p>​    c） 如果$\theta^{j+1}$收敛，则算法结束。否则继续回到步骤a）进行E步迭代。</p>
<p>输出：模型参数$\theta​$。</p>
<h2 id="2-21-降维和聚类"><a href="#2-21-降维和聚类" class="headerlink" title="2.21 降维和聚类"></a>2.21 降维和聚类</h2><h3 id="2-21-1-图解为什么会产生维数灾难"><a href="#2-21-1-图解为什么会产生维数灾难" class="headerlink" title="2.21.1 图解为什么会产生维数灾难"></a>2.21.1 图解为什么会产生维数灾难</h3><p>​    假如数据集包含10张照片，照片中包含三角形和圆两种形状。现在来设计一个分类器进行训练，让这个分类器对其他的照片进行正确分类（假设三角形和圆的总数是无限大），简单的，我们用一个特征进行分类：</p>
<p><img src="/img/ch2/2.21.1.1.png" alt=""></p>
<p>​                                            图2.21.1.a</p>
<p>​    从上图可看到，如果仅仅只有一个特征进行分类，三角形和圆几乎是均匀分布在这条线段上，很难将10张照片线性分类。那么，增加一个特征后的情况会怎么样：</p>
<p><img src="/img/ch2/2.21.1.2.png" alt=""></p>
<p>​                                            图2.21.1.b</p>
<p>增加一个特征后，我们发现仍然无法找到一条直线将猫和狗分开。所以，考虑需要再增加一个特征：</p>
<p><img src="/img/ch2/2.21.1.3.png" alt=""></p>
<p>​                                            图2.21.1.c</p>
<p><img src="/img/ch2/2.21.1.4.png" alt=""></p>
<p>​                                            图2.21.1.d</p>
<p>​    此时，可以找到一个平面将三角形和圆分开。</p>
<p>​    现在计算一下不同特征数是样本的密度：</p>
<p>​    （1）一个特征时，假设特征空间时长度为5的线段，则样本密度为$10 \div 5 = 2$。</p>
<p>​    （2）两个特征时，特征空间大小为$ 5\times5 = 25$，样本密度为$10 \div 25 = 0.4$。</p>
<p>​    （3）三个特征时，特征空间大小是$ 5\times5\times5 = 125$，样本密度为$10 \div 125 = 0.08$。</p>
<p>​    以此类推，如果继续增加特征数量，样本密度会越来越稀疏，此时，更容易找到一个超平面将训练样本分开。当特征数量增长至无限大时，样本密度就变得非常稀疏。</p>
<p>​    下面看一下将高维空间的分类结果映射到低维空间时，会出现什么情况？</p>
<p><img src="/img/ch2/2.21.1.5.png" alt=""></p>
<p>​                                        图2.21.1.e</p>
<p>​    上图是将三维特征空间映射到二维特征空间后的结果。尽管在高维特征空间时训练样本线性可分，但是映射到低维空间后，结果正好相反。事实上，增加特征数量使得高维空间线性可分，相当于在低维空间内训练一个复杂的非线性分类器。不过，这个非线性分类器太过“聪明”，仅仅学到了一些特例。如果将其用来辨别那些未曾出现在训练样本中的测试样本时，通常结果不太理想，会造成过拟合问题。</p>
<p><img src="/img/ch2/2.21.1.6a.png" alt=""></p>
<p>​                                        图2.21.1.f</p>
<p>​    上图所示的只采用2个特征的线性分类器分错了一些训练样本，准确率似乎没有图2.21.1.e的高，但是，采用2个特征的线性分类器的泛化能力比采用3个特征的线性分类器要强。因为，采用2个特征的线性分类器学习到的不只是特例，而是一个整体趋势，对于那些未曾出现过的样本也可以比较好地辨别开来。换句话说，通过减少特征数量，可以避免出现过拟合问题，从而避免“维数灾难”。</p>
<p><img src="/img/ch2/2.21.1.6.png" alt=""></p>
<p>​    上图从另一个角度诠释了“维数灾难”。假设只有一个特征时，特征的值域是0到1，每一个三角形和圆的特征值都是唯一的。如果我们希望训练样本覆盖特征值值域的20%，那么就需要三角形和圆总数的20%。我们增加一个特征后，为了继续覆盖特征值值域的20%就需要三角形和圆总数的45%($0.452^2\approx0.2$)。继续增加一个特征后，需要三角形和圆总数的58%($0.583^3\approx0.2$)。随着特征数量的增加，为了覆盖特征值值域的20%，就需要更多的训练样本。如果没有足够的训练样本，就可能会出现过拟合问题。</p>
<p>​    通过上述例子，我们可以看到特征数量越多，训练样本就会越稀疏，分类器的参数估计就会越不准确，更加容易出现过拟合问题。“维数灾难”的另一个影响是训练样本的稀疏性并不是均匀分布的。处于中心位置的训练样本比四周的训练样本更加稀疏。</p>
<p><img src="/img/ch2/2.21.1.7.png" alt=""></p>
<p>​    假设有一个二维特征空间，如上图所示的矩形，在矩形内部有一个内切的圆形。由于越接近圆心的样本越稀疏，因此，相比于圆形内的样本，那些位于矩形四角的样本更加难以分类。当维数变大时，特征超空间的容量不变，但单位圆的容量会趋于0，在高维空间中，大多数训练数据驻留在特征超空间的角落。散落在角落的数据要比处于中心的数据难于分类。</p>
<h3 id="2-21-2-怎样避免维数灾难"><a href="#2-21-2-怎样避免维数灾难" class="headerlink" title="2.21.2 怎样避免维数灾难"></a>2.21.2 怎样避免维数灾难</h3><p><strong>有待完善！！！</strong></p>
<p>解决维度灾难问题：</p>
<p>主成分分析法PCA，线性判别法LDA</p>
<p>奇异值分解简化数据、拉普拉斯特征映射</p>
<p>Lassio缩减系数法、小波分析法、</p>
<h3 id="2-21-3-聚类和降维有什么区别与联系"><a href="#2-21-3-聚类和降维有什么区别与联系" class="headerlink" title="2.21.3 聚类和降维有什么区别与联系"></a>2.21.3 聚类和降维有什么区别与联系</h3><p>​    聚类用于找寻数据内在的分布结构，既可以作为一个单独的过程，比如异常检测等等。也可作为分类等其他学习任务的前驱过程。聚类是标准的无监督学习。</p>
<p>​    1）在一些推荐系统中需确定新用户的类型，但定义“用户类型”却可能不太容易，此时往往可先对原有的用户数据进行聚类，根据聚类结果将每个簇定义为一个类,然后再基于这些类训练分类模型,用于判别新用户的类型。</p>
<p><img src="/img/ch2/2.21.3.1.png" alt=""></p>
<p>​    2）而降维则是为了缓解维数灾难的一个重要方法，就是通过某种数学变换将原始高维属性空间转变为一个低维“子空间”。其基于的假设就是，虽然人们平时观测到的数据样本虽然是高维的，但是实际上真正与学习任务相关的是个低维度的分布。从而通过最主要的几个特征维度就可以实现对数据的描述，对于后续的分类很有帮助。比如对于Kaggle（数据分析竞赛平台之一）上的泰坦尼克号生还问题。通过给定一个乘客的许多特征如年龄、姓名、性别、票价等，来判断其是否能在海难中生还。这就需要首先进行特征筛选，从而能够找出主要的特征，让学习到的模型有更好的泛化性。</p>
<p>​    聚类和降维都可以作为分类等问题的预处理步骤。</p>
<p><img src="/img/ch2/2-19.jpg" alt=""></p>
<p>​    但是他们虽然都能实现对数据的约减。但是二者适用的对象不同，聚类针对的是数据点，而降维则是对于数据的特征。另外它们有着很多种实现方法。聚类中常用的有K-means、层次聚类、基于密度的聚类等；降维中常用的则PCA、Isomap、LLE等。</p>
<h3 id="2-21-4-有哪些聚类算法优劣衡量标准"><a href="#2-21-4-有哪些聚类算法优劣衡量标准" class="headerlink" title="2.21.4 有哪些聚类算法优劣衡量标准"></a>2.21.4 有哪些聚类算法优劣衡量标准</h3><p>不同聚类算法有不同的优劣和不同的适用条件。可从以下方面进行衡量判断：<br>    1、算法的处理能力：处理大的数据集的能力，即算法复杂度；处理数据噪声的能力；处理任意形状，包括有间隙的嵌套的数据的能力；<br>    2、算法是否需要预设条件：是否需要预先知道聚类个数，是否需要用户给出领域知识； </p>
<p>​    3、算法的数据输入属性：算法处理的结果与数据输入的顺序是否相关，也就是说算法是否独立于数据输入顺序；算法处理有很多属性数据的能力，也就是对数据维数是否敏感，对数据的类型有无要求。</p>
<h3 id="2-21-5-聚类和分类有什么区别"><a href="#2-21-5-聚类和分类有什么区别" class="headerlink" title="2.21.5 聚类和分类有什么区别"></a>2.21.5 聚类和分类有什么区别</h3><p><strong>聚类（Clustering） </strong><br>    聚类，简单地说就是把相似的东西分到一组，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起。一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此聚类通常并不需要使用训练数据进行学习，在机器学习中属于无监督学习。 </p>
<p><strong>分类（Classification） </strong></p>
<p>​     分类，对于一个分类器，通常需要你告诉它“这个东西被分为某某类”。一般情况下，一个分类器会从它得到的训练集中进行学习，从而具备对未知数据进行分类的能力，在机器学习中属于监督学习。</p>
<h3 id="2-21-6-不同聚类算法特点性能比较"><a href="#2-21-6-不同聚类算法特点性能比较" class="headerlink" title="2.21.6 不同聚类算法特点性能比较"></a>2.21.6 不同聚类算法特点性能比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">算法名称</th>
<th style="text-align:center">可伸缩性</th>
<th style="text-align:center">适合的数据类型</th>
<th style="text-align:center">高维性</th>
<th style="text-align:center">异常数据抗干扰性</th>
<th style="text-align:center">聚类形状</th>
<th style="text-align:center">算法效率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">WAVECLUSTER</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">ROCK</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">混合型</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">BIRCH</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">球形</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">CURE</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">很高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较高</td>
</tr>
<tr>
<td style="text-align:center">K-PROTOTYPES</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">混合型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">DENCLUE</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较高</td>
</tr>
<tr>
<td style="text-align:center">OPTIGRID</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">CLIQUE</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">较低</td>
</tr>
<tr>
<td style="text-align:center">DBSCAN</td>
<td style="text-align:center">一般</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">任意形状</td>
<td style="text-align:center">一般</td>
</tr>
<tr>
<td style="text-align:center">CLARANS</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">数值型</td>
<td style="text-align:center">较低</td>
<td style="text-align:center">较高</td>
<td style="text-align:center">球形</td>
<td style="text-align:center">较低</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-21-7-四种常用聚类方法之比较"><a href="#2-21-7-四种常用聚类方法之比较" class="headerlink" title="2.21.7 四种常用聚类方法之比较"></a>2.21.7 四种常用聚类方法之比较</h3><p>​    聚类就是按照某个特定标准把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。<br>​    主要的聚类算法可以划分为如下几类：划分方法、层次方法、基于密度的方法、基于网格的方法以及基于模型的方法。下面主要对k-means聚类算法、凝聚型层次聚类算法、神经网络聚类算法之SOM,以及模糊聚类的FCM算法通过通用测试数据集进行聚类效果的比较和分析。</p>
<h3 id="2-21-8-k-means聚类算法"><a href="#2-21-8-k-means聚类算法" class="headerlink" title="2.21.8 k-means聚类算法"></a>2.21.8 k-means聚类算法</h3><p>k-means是划分方法中较经典的聚类算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。<br>k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。k-means算法的处理过程如下：首先，随机地 选择k个对象，每个对象初始地代表了一个簇的平均值或中心;对剩余的每个对象，根据其与各簇中心的距离，将它赋给最近的簇;然后重新计算每个簇的平均值。 这个过程不断重复，直到准则函数收敛。通常，采用平方误差准则，其定义如下：</p>
<script type="math/tex; mode=display">
E=\sum_{i=1}^{k}\sum_{p\in C_i}\left\|p-m_i\right\|^2</script><p>　这里E是数据中所有对象的平方误差的总和，p是空间中的点，$m_i$是簇$C_i$的平均值[9]。该目标函数使生成的簇尽可能紧凑独立，使用的距离度量是欧几里得距离，当然也可以用其他距离度量。</p>
<p><strong>算法流程</strong>：<br>​    输入：包含n个对象的数据和簇的数目k；<br>​    输出：n个对象到k个簇，使平方误差准则最小。<br>​    步骤：<br>　　(1) 任意选择k个对象作为初始的簇中心；<br>　　(2) 根据簇中对象的平均值，将每个对象(重新)赋予最类似的簇；<br>　　(3) 更新簇的平均值，即计算每个簇中对象的平均值；<br>　　(4) 重复步骤(2)、(3)直到簇中心不再变化；</p>
<h3 id="2-21-9-层次聚类算法"><a href="#2-21-9-层次聚类算法" class="headerlink" title="2.21.9 层次聚类算法"></a>2.21.9 层次聚类算法</h3><p>​    根据层次分解的顺序是自底向上的还是自上向下的，层次聚类算法分为凝聚的层次聚类算法和分裂的层次聚类算法。<br>　凝聚型层次聚类的策略是先将每个对象作为一个簇，然后合并这些原子簇为越来越大的簇，直到所有对象都在一个簇中，或者某个终结条件被满足。绝大多数层次聚类属于凝聚型层次聚类，它们只是在簇间相似度的定义上有所不同。</p>
<p><strong>算法流程</strong>：</p>
<p>注：以采用最小距离的凝聚层次聚类算法为例：</p>
<p>　(1) 将每个对象看作一类，计算两两之间的最小距离；<br>　(2) 将距离最小的两个类合并成一个新类；<br>　(3) 重新计算新类与所有类之间的距离；<br>　(4) 重复(2)、(3)，直到所有类最后合并成一类。</p>
<h3 id="2-21-10-SOM聚类算法"><a href="#2-21-10-SOM聚类算法" class="headerlink" title="2.21.10 SOM聚类算法"></a>2.21.10 SOM聚类算法</h3><p>​    SOM神经网络[11]是由芬兰神经网络专家Kohonen教授提出的，该算法假设在输入对象中存在一些拓扑结构或顺序，可以实现从输入空间(n维)到输出平面(2维)的降维映射，其映射具有拓扑特征保持性质,与实际的大脑处理有很强的理论联系。</p>
<p>​    SOM网络包含输入层和输出层。输入层对应一个高维的输入向量，输出层由一系列组织在2维网格上的有序节点构成，输入节点与输出节点通过权重向量连接。 学习过程中，找到与之距离最短的输出层单元，即获胜单元，对其更新。同时，将邻近区域的权值更新，使输出节点保持输入向量的拓扑特征。</p>
<p><strong>算法流程</strong>：</p>
<p>​    (1) 网络初始化，对输出层每个节点权重赋初值；<br>​    (2) 从输入样本中随机选取输入向量并且归一化，找到与输入向量距离最小的权重向量；<br>​    (3) 定义获胜单元，在获胜单元的邻近区域调整权重使其向输入向量靠拢；<br>​    (4) 提供新样本、进行训练；<br>​    (5) 收缩邻域半径、减小学习率、重复，直到小于允许值，输出聚类结果。</p>
<h3 id="2-21-11-FCM聚类算法"><a href="#2-21-11-FCM聚类算法" class="headerlink" title="2.21.11 FCM聚类算法"></a>2.21.11 FCM聚类算法</h3><p>​    1965年美国加州大学柏克莱分校的扎德教授第一次提出了‘集合’的概念。经过十多年的发展，模糊集合理论渐渐被应用到各个实际应用方面。为克服非此即彼的分类缺点，出现了以模糊集合论为数学基础的聚类分析。用模糊数学的方法进行聚类分析，就是模糊聚类分析[12]。<br>​    FCM算法是一种以隶属度来确定每个数据点属于某个聚类程度的算法。该聚类算法是传统硬聚类算法的一种改进。<br>​    设数据集$X={x_1,x_2,…,x_n}$,它的模糊$c$划分可用模糊矩阵$U=[u_{ij}]$表示，矩阵$U$的元素$u_{ij}$表示第$j(j=1,2,…,n)$个数据点属于第$i(i=1,2,…,c)$类的隶属度，$u_{ij}$满足如下条件：  </p>
<script type="math/tex; mode=display">
\begin{equation}
\left\{
\begin{array}{lr}
\sum_{i=1}^c u_{ij}=1 \quad\forall~j
\\u_{ij}\in[0,1] \quad\forall ~i,j
\\\sum_{j=1}^c u_{ij}>0 \quad\forall ~i
\end{array}
\right.
\end{equation}</script><p>目前被广泛使用的聚类准则是取类内加权误差平方和的极小值。即：</p>
<script type="math/tex; mode=display">
(min)J_m(U,V)=\sum^n_{j=1}\sum^c_{i=1}u^m_{ij}d^2_{ij}(x_j,v_i)</script><p>其中$V$为聚类中心，$m$为加权指数，$d_{ij}(x_j,v_i)=||v_i-x_j||$。</p>
<p><strong>算法流程</strong>：</p>
<p>　(1) 标准化数据矩阵；<br>　(2) 建立模糊相似矩阵，初始化隶属矩阵；<br>　(3) 算法开始迭代，直到目标函数收敛到极小值；<br>　(4) 根据迭代结果，由最后的隶属矩阵确定数据所属的类，显示最后的聚类结果。</p>
<h3 id="2-21-12-四种聚类算法试验"><a href="#2-21-12-四种聚类算法试验" class="headerlink" title="2.21.12 四种聚类算法试验"></a>2.21.12 四种聚类算法试验</h3><p>​    选取专门用于测试分类、聚类算法的国际通用的UCI数据库中的IRIS数据集，IRIS数据集包含150个样本数据，分别取自三种不同 的莺尾属植物setosa、versicolor和virginica的花朵样本,每个数据含有4个属性，即萼片长度、萼片宽度、花瓣长度、花瓣宽度，单位为cm。 在数据集上执行不同的聚类算法，可以得到不同精度的聚类结果。基于前面描述的各算法原理及流程，可初步得如下聚类结果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>聚类方法</th>
<th>聚错样本数</th>
<th>运行时间/s</th>
<th>平均准确率/（%）</th>
</tr>
</thead>
<tbody>
<tr>
<td>K-means</td>
<td>17</td>
<td>0.146001</td>
<td>89</td>
</tr>
<tr>
<td>层次聚类</td>
<td>51</td>
<td>0.128744</td>
<td>66</td>
</tr>
<tr>
<td>SOM</td>
<td>22</td>
<td>5.267283</td>
<td>86</td>
</tr>
<tr>
<td>FCM</td>
<td>12</td>
<td>0.470417</td>
<td>92</td>
</tr>
</tbody>
</table>
</div>
<p><strong>注</strong>：</p>
<p>(1) 聚错样本数：总的聚错的样本数，即各类中聚错的样本数的和；<br>(2) 运行时间：即聚类整个过程所耗费的时间，单位为s；<br>(3) 平均准确度：设原数据集有k个类,用$c_i$表示第i类，$n_i$为$c_i$中样本的个数，$m_i$为聚类正确的个数,则$m_i/n_i$为 第i类中的精度，则平均精度为：$avg=\frac{1}{k}\sum_{i=1}^{k}\frac{m_{i}}{n_{i}}$。  </p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]   Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.<br>[2]   周志华. 机器学习[M].清华大学出版社, 2016.<br>[3]   Michael A. Nielsen. “Neural Networks and Deep Learning”, Determination Press, 2015.<br>[4]   Suryansh S. Gradient Descent: All You Need to Know, 2018.<br>[5]   刘建平. 梯度下降小结,EM算法的推导, 2018<br>[6]   杨小兵．聚类分析中若干关键技术的研究[D]． 杭州：浙江大学, 2005.<br>[7]   XU Rui, Donald Wunsch 1 1． survey of clustering algorithm[J]．IEEE．Transactions on Neural Networks, 2005, 16(3)：645-67 8.<br>[8]   YI Hong, SAM K． Learning assignment order of instances for the constrained k-means clustering algorithm[J]．IEEE Transactions on Systems, Man, and Cybernetics, Part B：Cybernetics,2009,39 (2)：568-574.<br>[9]   贺玲, 吴玲达, 蔡益朝．数据挖掘中的聚类算法综述[J]．计算机应用研究, 2007, 24(1):10-13．<br>[10]  孙吉贵, 刘杰, 赵连宇．聚类算法研究[J]．软件学报, 2008, 19(1)：48-61．<br>[11]  孔英会, 苑津莎, 张铁峰等．基于数据流管理技术的配变负荷分类方法研究．中国国际供电会议, CICED2006．<br>[12]  马晓艳, 唐雁．层次聚类算法研究[J]．计算机科学, 2008, 34(7)：34-36．<br>[13]  FISHER R A． Iris Plants Database <a href="https://www.ics.uci.edu/vmlearn/MLRepository.html" target="_blank" rel="noopener">https://www.ics.uci.edu/vmlearn/MLRepository.html</a>, Authorized license．<br>[14]  Quinlan J R. Induction of decision trees[J]. Machine learning, 1986, 1(1): 81-106.<br>[15]  Breiman L. Random forests[J]. Machine learning, 2001, 45(1): 5-32.  </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/02/26/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/26/%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">算法分析</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-26 22:57:52" itemprop="dateCreated datePublished" datetime="2020-02-26T22:57:52+08:00">2020-02-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-30 13:40:09" itemprop="dateModified" datetime="2020-03-30T13:40:09+08:00">2020-03-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h1><p>什么是滑动窗口？其实就是一个队列，比如题目无重复字符的最长子串中的abcabcbb，进入这个队列（窗口）为abc满足题目，当再进入a，队列就变成了abca，这时候不满足要求。所以，我们就要移动这个队列。我们只要把队列的左边元素移出就行，直到满足题目要求。<br>例题1：leetcode3.无重复字符的最长子串(滑窗模板)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> s:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> len(s)==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    left, right, counter= <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    maxLen = <span class="number">0</span></span><br><span class="line">    windows = defaultdict(int)</span><br><span class="line">    <span class="keyword">while</span> right&lt;len(s):</span><br><span class="line">        <span class="keyword">if</span> windows[s[right]]&gt;<span class="number">0</span>:</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line">        windows[s[right]] += <span class="number">1</span></span><br><span class="line">        right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> counter&gt;<span class="number">0</span>:        <span class="comment"># 窗口条件决定左边元素是否移出</span></span><br><span class="line">            <span class="keyword">if</span> windows[s[left]]&gt;<span class="number">1</span>:</span><br><span class="line">                counter -= <span class="number">1</span></span><br><span class="line">            windows[s[left]] -= <span class="number">1</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        maxLen = max(maxLen, right-left)</span><br><span class="line">    <span class="keyword">return</span> maxLen</span><br></pre></td></tr></table></figure><br>例题：<br><a href="https://leetcode-cn.com/problems/minimum-window-substring/" target="_blank" rel="noopener">76.最小覆盖子串</a><br><a href="https://leetcode-cn.com/problems/minimum-size-subarray-sum/" target="_blank" rel="noopener">209.长度最小的子数组</a><br><a href="https://leetcode-cn.com/problems/permutation-in-string/" target="_blank" rel="noopener">567.字符串的排列</a></p>
<h1 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h1><h2 id="前序"><a href="#前序" class="headerlink" title="前序"></a>前序</h2><p>递归<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preOrder</span><span class="params">(node)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    print(node.val)</span><br><span class="line">    preOrder(node.left)</span><br><span class="line">    preOrder(node.right)</span><br></pre></td></tr></table></figure><br>非递归<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preOrder</span><span class="params">(node)</span>:</span></span><br><span class="line">    stack = [node]</span><br><span class="line">    <span class="keyword">while</span> stack:</span><br><span class="line">        print(node.val)</span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            stack.append(node.right)</span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            stack.append(node.left)</span><br><span class="line">        node = stack.pop()</span><br></pre></td></tr></table></figure></p>
<h2 id="中序遍历"><a href="#中序遍历" class="headerlink" title="中序遍历"></a>中序遍历</h2><p>递归<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inorder</span><span class="params">(node)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    inorder(node.left)</span><br><span class="line">    print(node.val)</span><br><span class="line">    inorder(node.right)</span><br></pre></td></tr></table></figure><br>非递归<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inorder</span><span class="params">(node)</span>:</span></span><br><span class="line">    stack = []</span><br><span class="line">    <span class="keyword">while</span> node <span class="keyword">or</span> len(stack)&gt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">while</span> node:</span><br><span class="line">            stack.append(node)</span><br><span class="line">            node = node.left</span><br><span class="line">        node = stack.pop()</span><br><span class="line">        print(node.val)</span><br><span class="line">        node = node.right</span><br></pre></td></tr></table></figure></p>
<h2 id="后序遍历"><a href="#后序遍历" class="headerlink" title="后序遍历"></a>后序遍历</h2><p>递归<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postOrder</span><span class="params">(node)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    postOrder(node.left)</span><br><span class="line">    postOrder(node.right)</span><br><span class="line">    print(node.val)</span><br></pre></td></tr></table></figure><br>非递归：使用两个栈结构，第一个栈进栈顺序：左节点-&gt;右节点-&gt;根节点，第一个栈弹出顺序：根节点-&gt;右节点-&gt;左节点，第二个栈存储为第一个栈的每个弹出依次入栈，最后第二个栈依次出栈。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postOrder</span><span class="params">(node)</span>:</span></span><br><span class="line">    stack = [node]</span><br><span class="line">    stack2 = []</span><br><span class="line">    <span class="keyword">while</span> len(stack)&gt;<span class="number">0</span>:</span><br><span class="line">        node = stack.pop()</span><br><span class="line">        stack2.append(node)</span><br><span class="line">        <span class="keyword">if</span> node.left:</span><br><span class="line">            stack.append(node.left)</span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            stack.append(node.right)</span><br><span class="line">    <span class="keyword">while</span> len(stack2)&gt;<span class="number">0</span>:</span><br><span class="line">        print(stack2.pop().val)</span><br></pre></td></tr></table></figure></p>
<h2 id="层次遍历"><a href="#层次遍历" class="headerlink" title="层次遍历"></a>层次遍历</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">queue = []</span><br><span class="line">queue.append(node)</span><br><span class="line"><span class="keyword">while</span> len(queue)&gt;<span class="number">0</span>:</span><br><span class="line">    node = queue.pop(<span class="number">0</span>)</span><br><span class="line">    print(node.val)</span><br><span class="line">    <span class="keyword">if</span> node.left:</span><br><span class="line">        queue.append(node.left)</span><br><span class="line">    <span class="keyword">if</span> node.right:</span><br><span class="line">        queue.append(node.right)</span><br></pre></td></tr></table></figure>
<h1 id="买卖股票"><a href="#买卖股票" class="headerlink" title="买卖股票"></a>买卖股票</h1><p>1）给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。<br>如果你最多只允许完成一笔交易（即买入和卖出一支股票），求获得的最大利润。<br>思路1：dp[i][j]表示第i天用户持股为j所获最大利润。<br>j只有两个值：0表示不持股，1表示持股。<br>则dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i])<br>dp[i][1] = max(dp[i-1][1], -prices[i])，因为题目只允许一次交易，因此不能加上dp[i-1][0]<br>初始值：第0天不持股，dp[0][0]=0；第0天持股，dp[0][1]=-prices[0]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(prices)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(prices)&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    n = len(prices)</span><br><span class="line">    dp = [[<span class="number">0</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>)]</span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i])</span><br><span class="line">        dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], -prices[i])</span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><br>考虑状态压缩，dp[i]仅仅依赖于dp[i-1]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxProfit2</span><span class="params">(prices)</span>:</span></span><br><span class="line">    n = len(prices)</span><br><span class="line">    <span class="keyword">if</span> n&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    dp = [<span class="number">0</span>]*<span class="number">2</span></span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    dp[<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        dp[<span class="number">0</span>] = max(dp[<span class="number">0</span>], dp[<span class="number">1</span>]+prices[i])</span><br><span class="line">        dp[<span class="number">1</span>] = max(dp[<span class="number">1</span>], -prices[i])</span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><br>2）可以尽可能地完成更多的交易（多次买卖一支股票），但不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）<br>思路：dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i])<br>dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i])<br>dp[0][0] = 0, dp[0][1] = -prices[0]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxProfitII</span><span class="params">(prices)</span>:</span></span><br><span class="line">    n = len(prices)</span><br><span class="line">    <span class="keyword">if</span> n&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    dp = [[<span class="number">0</span>]*<span class="number">2</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i])</span><br><span class="line">        dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-1</span>][<span class="number">0</span>]-prices[i])</span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><br>考虑状态压缩 dp[i]也仅仅依赖dp[i-1]</p>
<p>3）最多可以完成两笔交易。<br>注：必须在再次购买前出售掉之前的股票<br>思路：重新定义状态方程<br>dp[i][0]表示未交易<br>dp[i][1]表示第一次买入一支股票<br>dp[i][2]表示第一次卖出一支股票<br>dp[i][3]表示第二次买入一支股票<br>dp[i][4]表示第二次卖出一支股票<br>状态转移方程见代码<br>初始化：第0天初始化为前两个状态，而状态3（第二次持股）只能赋值为一个不可能的数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxProfitIII</span><span class="params">(prices)</span>:</span></span><br><span class="line">    n = len(prices)</span><br><span class="line">    <span class="keyword">if</span> n&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    dp = [[<span class="number">0</span>]*<span class="number">5</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        dp[i][<span class="number">3</span>] = float(<span class="string">"-inf"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-1</span>][<span class="number">0</span>]-prices[i])</span><br><span class="line">        dp[i][<span class="number">2</span>] = max(dp[i<span class="number">-1</span>][<span class="number">2</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i])</span><br><span class="line">        dp[i][<span class="number">3</span>] = max(dp[i<span class="number">-1</span>][<span class="number">3</span>], dp[i<span class="number">-1</span>][<span class="number">2</span>]-prices[i])</span><br><span class="line">        dp[i][<span class="number">4</span>] = max(dp[i<span class="number">-1</span>][<span class="number">4</span>], dp[i<span class="number">-1</span>][<span class="number">3</span>]+prices[i])</span><br><span class="line">    <span class="comment"># 最大值只发生在不持股的时候</span></span><br><span class="line">    <span class="keyword">return</span> max(<span class="number">0</span>, dp[<span class="number">-1</span>][<span class="number">2</span>], dp[<span class="number">-1</span>][<span class="number">4</span>])</span><br></pre></td></tr></table></figure><br>状态压缩，dp[i]仅仅依赖于dp[i-1]</p>
<p>4）最多可以完成 k 笔交易<br>注：必须在再次购买前出售掉之前的股票</p>
<p>5）在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）:<br>a) 必须在再次购买前出售掉之前的股票;<br>b) 卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)<br>思路：增加一个状态，j取三个值：<br>0表示不持股，1表示持股，2表示冷冻期<br>状态转移方程：<br>dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i])<br>dp[i][1] = max(dp[i-1][1], dp[i-1][2]-prices[i])<br>dp[i][2] = dp[i-1][0]<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxProfitV</span><span class="params">(prices)</span>:</span></span><br><span class="line">    n = len(prices)</span><br><span class="line">    <span class="keyword">if</span> n&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    dp = [[<span class="number">0</span>]*<span class="number">3</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i])</span><br><span class="line">        dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-1</span>][<span class="number">2</span>]-prices[i])</span><br><span class="line">        dp[i][<span class="number">2</span>] = dp[i<span class="number">-1</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> max(dp[<span class="number">-1</span>][<span class="number">0</span>], dp[<span class="number">-1</span>][<span class="number">2</span>])</span><br></pre></td></tr></table></figure><br>考虑状态压缩，dp[i]仅仅只依赖于dp[i-1]<br>6）你可以无限次地完成交易，但是你每次交易都需要付手续费fee。<br>如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。<br>思路：规定手续费在买入股票时扣除<br>状态转移方程：<br>dp[i][0] = max(dp[i-1][0], dp[i-1][1]+prices[i])<br>dp[i][1] = max(dp[i-1][1], dp[i-1][0]-prices[i]-fee)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxProfitVI</span><span class="params">(prices, fee)</span>:</span></span><br><span class="line">    n = len(prices)</span><br><span class="line">    <span class="keyword">if</span> n&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    dp = [[<span class="number">0</span>]*<span class="number">2</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">1</span>] = -prices[<span class="number">0</span>]-fee</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">        dp[i][<span class="number">0</span>] = max(dp[i<span class="number">-1</span>][<span class="number">0</span>], dp[i<span class="number">-1</span>][<span class="number">1</span>]+prices[i])</span><br><span class="line">        dp[i][<span class="number">1</span>] = max(dp[i<span class="number">-1</span>][<span class="number">1</span>], dp[i<span class="number">-1</span>][<span class="number">0</span>]-prices[i]-fee)</span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">-1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/02/25/%E5%9F%BA%E7%A1%80%E6%8A%80%E8%83%BD%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/25/%E5%9F%BA%E7%A1%80%E6%8A%80%E8%83%BD%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">基础技能知识</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-25 21:29:48" itemprop="dateCreated datePublished" datetime="2020-02-25T21:29:48+08:00">2020-02-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-31 09:29:15" itemprop="dateModified" datetime="2020-03-31T09:29:15+08:00">2020-03-31</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><p><a href="https://juejin.im/post/5b6bc1d16fb9a04f9c43edc3" target="_blank" rel="noopener">Python常问问题</a><br><a href="https://www.jianshu.com/p/8e42c49d3363" target="_blank" rel="noopener">Python常见的50题</a><br>常用的Python标准库有哪些？<br>常用的标准库：os操作系统，time时间，random随机，pymysql连接数据库，threading线程，multiprocessing进程，queue队列<br>第三方库：django和flask，requests，virtualenv，hashlib，md5<br>常用的科学计算库：Numpy,Scipy,Pandas等。</p>
<h1 id="Pytorch、Tensorflow"><a href="#Pytorch、Tensorflow" class="headerlink" title="Pytorch、Tensorflow"></a>Pytorch、Tensorflow</h1><p><a href="https://blog.csdn.net/weixin_31866177/article/details/87974664" target="_blank" rel="noopener">tensorflow</a></p>
<h1 id="Linux系统"><a href="#Linux系统" class="headerlink" title="Linux系统"></a>Linux系统</h1><p><a href="https://blog.csdn.net/qq_40910541/article/details/80686362?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">linux常用指令</a></p>
<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><p><a href="https://blog.csdn.net/qq_26768741/article/details/66975516" target="_blank" rel="noopener">git常见问题</a></p>
<h1 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h1><p><a href="https://www.jianshu.com/p/0fc161b9bcc7" target="_blank" rel="noopener">Java常见问题</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/02/25/%E9%9D%A2%E7%BB%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/25/%E9%9D%A2%E7%BB%8F/" class="post-title-link" itemprop="url">面经</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-25 08:52:11" itemprop="dateCreated datePublished" datetime="2020-02-25T08:52:11+08:00">2020-02-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-18 11:15:46" itemprop="dateModified" datetime="2020-03-18T11:15:46+08:00">2020-03-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="2020年快手Y-tech面经"><a href="#2020年快手Y-tech面经" class="headerlink" title="2020年快手Y-tech面经"></a>2020年快手Y-tech面经</h1><h2 id="一面"><a href="#一面" class="headerlink" title="一面:"></a>一面:</h2><p>1、PCA原理，应用<br>PCA原理是在原有n维特征的基础上重新构造出k维全新的正交特征，PCA的工作就是从原始空间顺序中到一组互相正交的坐标轴，第一个坐标轴选择就是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使方差最大的，第三个轴是与第1、2个轴正交的平面中方差最大的。依次类推可以得到n个这样的坐标轴。通过这种方式获得的新坐标轴，我们发现大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是我们可以忽略余下的坐标轴，只保留前面k个含有大部分方差的坐标轴。<br>PCA的应用范围有：1）数据压缩。数据压缩或者数据降维首先能够减少内存的使用，其次，数据降维能够加快机器学习的速度。2）数据可视化。在很多情况下，可能我们需要查看样本特征，但是高维度的特征根本无法观察，这个时候我们可以将样本的特征维数降到2个特征或3个特征，这样就可以采用可视化观察数据。3）去除数据噪声。<br>如何得到这些包含最大差异性的主成分方向呢？通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大（即方差最大）的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间中，实现数据特征的降维。得到协方差矩阵的特征值向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵。<br>2、SVD原理，应用<br><a href="https://blog.csdn.net/weixin_31866177/article/details/88079612?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">SVD推导过程</a><br>3、SVM<br>SVM是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔大使它有别于普通的感知机，通过核技巧隐式的在输入空间直接求解映射空间中特征向量的内积，使其成为一个非线性分类器。SVM的学习策略是间隔最大化，可形式化为一个求解凸二次规划问题。<br>（重点）<a href="https://blog.csdn.net/cppjava_/article/details/68060439" target="_blank" rel="noopener">SVM算法推导过程</a><br>4、BN 优缺点 IN<br>BN的基本思想相当直观：因为深层神经网络在做非线性变换前的激活输入值随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区域的上下限两端靠近，所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。而BN就是通过一定的规范化手段，把层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回到标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就是导致损失函数较大的变化。<br>BatchNorm的优点：1）极大提升了训练速度，使得收敛过程大大加快；2）还能增加分类效果，一种解释是这类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；3）调参过程也简单了，对于初始化要求没那么高，可以使用大的学习率。4）可以代替LRN<br>缺点：需要计算均值与方差，不适合动态网络或者RNN。计算均值方差依赖每批次，因此数据最好足够打乱。<br>IN的计算就是把每张图片的HW维度单独拿出来归一化处理，不受通道和batchsize 的影响<br>5、cross entropy和mse区别，求导<br>交叉熵是用来评估当前训练得到的概率分布于真实分布的差异情况，减少交叉熵损失就是在提高模型的预测准确率。交叉熵通常用作分类问题的代价函数。<br>均方误差是指参数估计值与真实值之差的平方的期望值，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精度。通常用来做回归问题的代价函数。<br>6、sigmoid, softmax用途，区别<br>Sigmoid函数或Softmax函数可以将分类器的原始输出值映射为概率。Sigmoid函数会分别处理各个原始输出值，因此其结果相互独立，概率总和不一定为1。相反，Softmax函数的输出值相互关联，其概率的总和始终为1。<br>Sigmoid = 多标签分类问题 = 多个正确答案 = 非独占输出<br>Softmax = 多类别分类问题 = 只有一个正确答案 = 互斥输出<br>如果模型输出为非互斥类别，且可以同时选择多个类别，则采用Sigmoid函数计算该网络的原始输出值。<br>如果模型输出为互斥类别，且只能选择一个类别，则采用Softmax函数计算该网络的原始输出值。<br>7、单链表快排思想<br>8、概率题，a服从均匀分布(0,1)，b服从均匀分布(0,1),求max(a,b)的期望，拓展为n个独立变量，求max期望</p>
<h2 id="二面"><a href="#二面" class="headerlink" title="二面:"></a>二面:</h2><p>1、cross entropy和mse能否互相替换？<br>不能。参见问题（为什么分类问题用 cross entropy，而回归问题用 MSE?）<br>2、多标签分类是否了解, 多标签分类激活函数?<br><a href="https://blog.csdn.net/rocling/article/details/89165463" target="_blank" rel="noopener">解答</a><br>3、单链表快排实现</p>
<h1 id="2019-海康威视"><a href="#2019-海康威视" class="headerlink" title="2019 海康威视"></a>2019 海康威视</h1><h2 id="一面-1"><a href="#一面-1" class="headerlink" title="一面"></a>一面</h2><p>1、faster-rcnn的整个从输入到输出的框架流程？<br>参见深度学习基础知识.faster rcnn<br>2、说一下rpn的原理<br>参见深度学习基础知识.faster rcnn<br>3、针对小目标的解决措施<br>（待解决）<br>4、如何解决样本不均衡的问题<br><a href="https://zhuanlan.zhihu.com/p/60612064" target="_blank" rel="noopener">参考答案</a><br>5、focal loss原理<br><a href="https://blog.csdn.net/hejin_some/article/details/100600663" target="_blank" rel="noopener">参考解答</a></p>
<h1 id="2019-旷视"><a href="#2019-旷视" class="headerlink" title="2019 旷视"></a>2019 旷视</h1><h2 id="一面-2"><a href="#一面-2" class="headerlink" title="一面"></a>一面</h2><p>1、讲一下SENet的模块，为什么有性能提升，有什么好处<br>2、讲一下Focal loss 的2个参数有什么作用<br>3、讲一下FPN为什么能提升小目标的准确率<br>4、对One-stage的模型有没有了解<br>5、说一说One-stage和Two-stage两者有什么特点<br>6、说一说SSD具体是怎么操作的<br>7、算法题：leetcode.754。一个人从原点出发，可以往左走可以往右走，每次走的步数递增1，问能不能到达一个位置x？如果能，给出走的步数最少的方案？</p>
<h1 id="蚂蚁金服"><a href="#蚂蚁金服" class="headerlink" title="蚂蚁金服"></a>蚂蚁金服</h1><h2 id="二面-1"><a href="#二面-1" class="headerlink" title="二面"></a>二面</h2><p>1、分类和定位的不一致具体是什么<br>分类是给定一张图片或一段视频判断里面是否包含某种类别的目标<br>定位是标注出图片中这个目标的位置<br>检测是标注出这个目标的位置并知道目标的类别是什么<br>分割是解决每一个像素属于哪个目标物体或场景。<br>2、Faster-RCNN比起RCNN有什么改进的地方<br>3、Faster-RCNN如何进行一次计算<br>4、RoI Pooling如何操作<br>根据输入的图片，将ROI映射到feature map对应的位置，映射的规则就是把各个坐标除以输入图片与feature map的大小比值，得到feature map上的box坐标。然后将映射的区域划分为相同大小的sections，对每个sections进行max pooling的操作。<br>5、如果有很长，很小，或者很宽的目标，应该如何处理<br>采用FPN网络的多尺度预测方式，将高层特征和低层特征进行融合，增强特征的表达能力。<br>6、FPN具体是怎么操作的<br>将高层的特征进行两倍上采样和前一层的特征进行逐元素求和，然后将融合的特征输入3x3卷积（消除上采样的混叠效应）得到最后的特征。<br>7、FPN的特征融合具体是怎么做的<br>element-wise逐元素求和<br>8、FPN的特征融合为什么是相加操作呢<br>add操作可以看做是加入一种先验知识，通过add操作，网络会得到新的特征，这个心特征会反映原始特征的一些特性，但是原始特征的一些信息也会在这个过程中损失。但是concate就是讲原始的特征直接拼接，让网络学习如何融合特征，这个过程中信息不会损失，但是concate会带来的计算量较大，在明确原始特征的关系可以使用add操作，可以节省计算资源。<br>9、Soft-NMS是如何操作的<br>10、Softmax是什么，公式是什么样的<br><a href="https://blog.csdn.net/weixin_37142859/article/details/95534590" target="_blank" rel="noopener">参考答案</a><br>11、Softmax的梯度是什么<br>12、BN原理<br>13、Pytorch的卷积是如何实现的<br><a href="https://blog.csdn.net/qq_37541097/article/details/102926037" target="_blank" rel="noopener">解答</a><br>14、还知道哪些传统计算机视觉的算法<br>(待解决)<br>15、SIFT原理，SIFT如何保持尺度不变性的<br>(待解决)</p>
<h2 id="三面"><a href="#三面" class="headerlink" title="三面"></a>三面</h2><p>1、说一个比较熟悉的网络<br>（项目中需准备）<br>2、对移动端的网络有没有什么了解<br>（待解决）<br>3、Focal loss怎么操作的<br>4、IoUNet怎么操作的<br>（待解决）<br>5、Soft-NMS怎么操作的<br>6、FPN的相加操作有没有尝试其他操作<br>7、对SSD和YOLO有没有什么了解？对SSD做过什么实验<br>8、对attention有什么了解<br>9、对跟踪有没有什么了解</p>
<h2 id="四面"><a href="#四面" class="headerlink" title="四面"></a>四面</h2><p>1、说一下focal loss的原理<br>2、介绍一下目标检测有哪些方向，最近的一些新的进展<br>3、说一下Soft-NMS的原理<br>4、说一下视觉的attention<br>5、平常是如何学习的<br>6、python多线程有了解吗<br>（待解决）<br>7、引用和指针的区别<br>（待解决）整理python相关问题<br>8、有什么想问的</p>
<h2 id="五面"><a href="#五面" class="headerlink" title="五面"></a>五面</h2><p>1、说比赛<br>2、比赛的数据集数量是怎么分布的<br>3、小目标占比如何<br>4、为什么说SENet泛化性好<br>5、SENet为什么效果好<br>6、FPN是怎么提升小目标检出率的<br>7、项目是一个什么问题呢<br>8、大目标如果有2个候选框和gt重合应该怎么处理<br>9、BN是解决什么问题的<br>10、BN为什么有效<br>11、什么时候开始接触这个方向的<br>12、学过哪些课程<br>13、学过机器学习吗</p>
<h1 id="商汤"><a href="#商汤" class="headerlink" title="商汤"></a>商汤</h1><h2 id="一面-3"><a href="#一面-3" class="headerlink" title="一面"></a>一面</h2><p>1、InceptionV1为什么能提升性能<br>2、RPN哪里也可以提升小目标检出率<br>3、为什么说resnet101不适用于目标检测<br>4、小目标在FPN的什么位置检测<br>5、sigmoid和softmax的区别<br>6、detnet原理<br>7、一道算法题：输入一个文件，等概率输出某一行，只能顺序遍历<br>8、手写nms<br>(参考答案)[<a href="https://zhuanlan.zhihu.com/p/64423753" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/64423753</a>]</p>
<h2 id="二面-2"><a href="#二面-2" class="headerlink" title="二面"></a>二面</h2><p>1、项目：最大的创新点是什么<br>2、SENet原理<br>3、Focal Loss 原理<br>4、Faster RCNN原理<br>5、用的roi pooling 还是roi align<br>6、IoUNet的原理<br>7、有了解过单阶段的检测器<br>8、说一下SSD的原理<br>9、还知道哪些轻量级的检测器<br>10、知道大网络用来学习小网络的方法吗<br>11、对分割有没有了解<br>12、用的什么机器(GPU)<br>13、比赛排名多少<br>14、用的什么模型</p>
<h2 id="三面-1"><a href="#三面-1" class="headerlink" title="三面"></a>三面</h2><p>1、什么时候开始接触CV课程<br>2、有没有上过CV的相关课程<br>3、SENet原理<br>4、SENet接在ResNet还有Inception的什么位置呢<br>5、IOUNet能有多少提升<br>6、NMS和soft-NMS原理<br>7、比赛遇到了什么不work的地方<br>8、假如一个图片中有一个很大的目标还有一个很小的目标，你会怎么处理呢<br>9、多尺度训练如何设置<br>10、为什么设置长边为固定的1600呢<br>11、介绍一下SSD和Faster-RCNN，说一下异同点<br>12、Faster-RCNN的回归目标是如何定义的<br>13、Faster-RCNN用的什么loss<br>14、smooth L1 loss为什么更有效<br>15、SGD、Adam之类优化的原理<br>16、BN、IN、LN、GN原理，BN为什么有效<br>17、Python有哪些常用的库报一遍<br>18、说一下使用Pytorch对cifar10数据集分类的整个代码流程，构建模型的过程是怎么样的<br>19、会C++吗<br>20、github的常用操作：上传、合并、分支之类的<br>（待整理）<br>21、linux的常用操作：查看文件大小、删除文件、查看文件行数、假如文件中有很多文件，每个文件中又有很多文件，如何删除全部文件<br>（待整理）<br>22、SiamRPN原理<br>23、轻量级模型<br>（待解决）</p>
<h1 id="精锐视觉上海研究院"><a href="#精锐视觉上海研究院" class="headerlink" title="精锐视觉上海研究院"></a>精锐视觉上海研究院</h1><h2 id="二面-3"><a href="#二面-3" class="headerlink" title="二面"></a>二面</h2><p>算法题：1、给定一个乱序的数组，给定一个整数target，要求找出数组中所有和为该target的两个数。2、给定一张100元纸币，兑换为20，5，1元的纸币，要求将100元纸币正好兑换为上述面额的纸币，且每种面额不少于一张，写代码实现总共有多少种兑换方法。<br>2、生成式模型和判别式模型<br>（待解决）<br>3、解释最大似然估计和最大后验概率以及区别<br>4、神经网络中激活层的作用<br>5、faster rcnn中为什么要用3x3的巻积核代替5x5和7x7的巻积核。<br>6、防止过拟合的方式有哪些，请写出不少于五种。<br>7、请用深度学习的方法设计一套手写签名的识别方案。（不会）<br>8、请自主设计一套人脸识别方案，详述数据集、backbone、训练策略、模型测试等。<br>9、opencv的cascade级联分类器的工作原理以及具体的实现方式，训练级联分类器的注意事项。<br>（待解决）<br>10、HOG，LBP，Haar特征<br>（待解决）<br>11、简单介绍一下deep_sort算法</p>
<h2 id="三面-2"><a href="#三面-2" class="headerlink" title="三面"></a>三面</h2><p>1、问了一个深度学习的项目，项目简介，自己负责的部分，技术原理细节，创新点，难点。后续改进方案。<br>2、详细介绍一下deepsort算法，以及和传统跟踪方法的区别。<br>3、faster rcnn原理<br>4、详细解释deep_sort算法，与online track的方法有什么区别，目标跟踪丢失怎么办</p>
<h1 id="聚时科技"><a href="#聚时科技" class="headerlink" title="聚时科技"></a>聚时科技</h1><h2 id="一面-4"><a href="#一面-4" class="headerlink" title="一面"></a>一面</h2><p>1、图像处理，中值滤波和平滑滤波的区别<br>2、说几个机器学习常用的算法，SVM的目标函数是什么，是如何实现分类的，KNN的原理以及分类流程<br>3、常见的目标分类的基础网络有哪些，mobilenet和shufflenet是如何实现卷积加速的。</p>
<h2 id="二面-4"><a href="#二面-4" class="headerlink" title="二面"></a>二面</h2><p>1、给了一个7x7的灰度图像和一个2x2的卷积核，分别写出腐蚀和膨胀运算后的图像。<br>2、手写代码实现二维卷积过程，不得调用TF、Keras等深度学习库。<br>3、手写代码实现一个8邻接连通域算法，不得调用opencv等图像处理库。<br>4、写出三个常用的机器学习算法，并介绍其原理和使用场景。<br>5、请写出如些解决样本不平衡问题。</p>
<h1 id="腾讯"><a href="#腾讯" class="headerlink" title="腾讯"></a>腾讯</h1><p>1、YOLO V2 V3你对哪个熟悉，讲一下细节实现<br>2、多尺度问题<br>3、anchor基础知识<br>4、人脸识别现在常用算法<br>5、语义分割到实例分割怎么做<br>6、GAN是否了解，如何通俗的讲其原理<br>7、PCA原理LDA原理<br>8、SVM+HOG<br>9、XGBoost<br>10、CNN、RCNN、FRCNN，有可能问你其中一个细节的关键<br>11、TensorFlow这些框架你谈一下看法以及对其他框架的了解<br>12、现在机器学习、深度学习这么火，你有什么看法<br>13、机器学习、深度学习你对他们的理解是什么<br>14、Relu比Sigmoid使用多的原因<br>15、Loss不升反降的原因，如何解决<br>16、SSD细节<br>17、Linux 权限的意义<br>18、块操作的操作的步骤以及快捷方式</p>
<h1 id="搜狗"><a href="#搜狗" class="headerlink" title="搜狗"></a>搜狗</h1><h2 id="一面-5"><a href="#一面-5" class="headerlink" title="一面"></a>一面</h2><p>1、RCNN系列的算法流程和区别；<br>2、Fast RCNN中 bbox 回归的损失函数什么；<br>3、解释 ROI Pooling 和 ROI Align；<br>4、Mask RCNN中 mask branch 如何接入 Faster RCNN中；<br>5、解释 FPN （没细看，面完补的）；<br>6、优化算法举例和他们的区别（SGD、SGDM、RMSprop、Adam）；<br>7、训练不收敛的原因有哪些；<br>8、简述 Inception v1-v4；<br>9、简述 CNN 的演变；<br>10、BN 的作用和缺陷，以及针对batch_size小的情况的改进（GN）</p>
<h1 id="旷世CV岗"><a href="#旷世CV岗" class="headerlink" title="旷世CV岗"></a>旷世CV岗</h1><p>1.自我介绍，项目介绍<br>2.FCN结构介绍，上采样的具体操作<br>3.空洞卷积原理，deeplab v1 v2的改进<br>4.focal loss介绍, lovasz loss数学原理<br>5.一道题，计算卷积操作的浮点计算量，比较简单<br>6.介绍下RPN的原理<br>7.mobile net<br>8.unet的缺点</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/02/21/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/21/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">排序算法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-21 19:41:40" itemprop="dateCreated datePublished" datetime="2020-02-21T19:41:40+08:00">2020-02-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-24 23:38:43" itemprop="dateModified" datetime="2020-02-24T23:38:43+08:00">2020-02-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/images/排序算法.png" alt="排序算法比较"></p>
<h1 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h1><p>算法过程：首先，找到数组中最小的那个元素，其次，将它和数组的第一个元素交换位置（如果第一个元素就是最小元素就和自己交换）。然后在剩下的元素中找到最小的元素，将它与数组的第二个元素交换位置。如此往复，知道整个数组排序。<br><img src="https://images2017.cnblogs.com/blog/849589/201710/849589-20171015224719590-1433219824.gif" alt="选择排序过程"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectionSort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>):</span><br><span class="line">        minIndex = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, len(nums)):</span><br><span class="line">            <span class="keyword">if</span> nums[j]&lt;nums[minIndex]:</span><br><span class="line">                minIndex = j</span><br><span class="line">        nums[i], nums[minIndex] = nums[minIndex], nums[i]</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><br>性质：时间复杂度：$O(n^2)$，空间复杂度：$O(1)$，非稳定排序，原地排序</p>
<h1 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h1><p>算法过程：1）从数组第2个元素开始抽取元素。2）把它与左边第一个元素比较，如果左边第一个元素比它大，则继续与左边第二个元素比较，知道遇到不必它大的元素，然后插入到这个元素的右边。3）继续选取第3,4,…,n个元素，重复步骤2，选择适当的位置插入。<br><img src="http://wuchong.me/img/Insertion-sort-example-300px.gif" alt="插入排序过程"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertionSort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>):</span><br><span class="line">        curNum, preIndex = nums[i+<span class="number">1</span>], i</span><br><span class="line">        <span class="keyword">while</span> preIndex&gt;=<span class="number">0</span> <span class="keyword">and</span> curNum&lt;nums[preIndex]:</span><br><span class="line">            nums[preIndex+<span class="number">1</span>] = nums[preIndex]</span><br><span class="line">            preIndex -= <span class="number">1</span></span><br><span class="line">        nums[preIndex+<span class="number">1</span>] = curNum</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><br>性质：时间复杂度：$O(n^2)$，空间复杂度：$O(1)$，稳定排序，原地排序。</p>
<h1 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h1><p>算法过程：1）把第一个元素和第二个元素比较，如果第一个比第二个大，则交换他们的位置。接着比较第二个与第三个元素，如果第二个比第三个大，则交换它们的位置…。这样一趟比较交换之后，排在最右的就会是最大的数。2）除去最右的数，我们对剩余的元素做同样的工作，如此重复下去，直到排序完成。<br><img src="https://images2017.cnblogs.com/blog/849589/201710/849589-20171015223238449-2146169197.gif" alt="冒泡排序过程"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubblesort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, i):</span><br><span class="line">            <span class="keyword">if</span> nums[j]&gt;nums[j+<span class="number">1</span>]:</span><br><span class="line">                nums[j], nums[j+<span class="number">1</span>] = nums[j+<span class="number">1</span>], nums[j]</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><br>性质：时间复杂度：$O(n^2)$，空间复杂度：$O(1)$，稳定排序，原地排序。<br><strong>优化冒泡排序算法:</strong>假如开始的第一队到结尾的最后一对，相邻的元素之间都没有发生交换的操作，这意味着右边的元素总是大于等于左边的元素，此时数组已经是有序的，无须再对剩余的元素重复比较下去。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubblesort2</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        swapped = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, i):</span><br><span class="line">            <span class="keyword">if</span> nums[j]&gt;nums[j+<span class="number">1</span>]:</span><br><span class="line">                nums[j], nums[j+<span class="number">1</span>] = nums[j+<span class="number">1</span>], nums[j]</span><br><span class="line">                swapped = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> swapped:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure></p>
<h1 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h1><p>希尔排序可以说是插入排序的一种变种，无论是插入排序还是冒泡排序，如果数组的最大值刚好是在第一位，要将它挪到正确的位置就需要n-1次移动。也就是说原数组的一个元素如果距离它正确的位置很远，则需要与相邻元素交换很多次才能到达正确的位置，这是相对比较花时间了。<br>希尔排序就是为了加快速度简单地改进插入排序，交换不相邻的元素对数组的局部进行排序。基本思想是：先将整个待排序列分割成为若干子序列分别进行插入排序，待整个序列中的记录基本有序时再对全体记录进行一次直接插入排序。希尔排序的核心在于间隔序列的设定，既可以提前设定好间隔序列，也可以动态地定义间隔序列。<br><img src="https://images2018.cnblogs.com/blog/849589/201803/849589-20180331170017421-364506073.gif" alt="希尔排序过程"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shellSort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    gap = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> gap&lt;len(nums)//<span class="number">3</span>:</span><br><span class="line">        gap = gap*<span class="number">3</span>+<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> gap&gt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gap, len(nums)):</span><br><span class="line">            curNum, preIndex = nums[i], i-gap</span><br><span class="line">            <span class="keyword">while</span> preIndex&gt;=<span class="number">0</span> <span class="keyword">and</span> curNum&lt;nums[preIndex]:</span><br><span class="line">                nums[preIndex+gap] = nums[preIndex]</span><br><span class="line">                preIndex -= gap</span><br><span class="line">            nums[preIndex+gap] = curNum</span><br><span class="line">        gap //= <span class="number">3</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><br>性质：时间复杂度：$O(nlogn)$，空间复杂度：$O(1)$，非稳定排序，原地排序。</p>
<h1 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h1><p>归并排序使用递归分治的思想，其基本思想是：把待排序列看成两个有序的子序列，然后合并两个子序列，接着把子序列再看成两个有序的子子序列，…，以此类推。<br><img src="http://wuchong.me/img/Insertion-sort-example-300px.gif" alt="归并排序过程"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeSort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(nums)&lt;=<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    <span class="comment"># 归并过程</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left, right)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        i = j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i&lt;len(left) <span class="keyword">and</span> j&lt;len(right):</span><br><span class="line">            <span class="keyword">if</span> left[i]&lt;=right[j]:</span><br><span class="line">                result.append(left[i])</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                result.append(right[j])</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        result = result + left[i:] + right[j:]</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="comment"># 递归过程</span></span><br><span class="line">    mid = len(nums)//<span class="number">2</span></span><br><span class="line">    left = mergeSort(nums[:mid])</span><br><span class="line">    right = mergeSort(nums[mid:])</span><br><span class="line">    <span class="keyword">return</span> merge(left, right)</span><br></pre></td></tr></table></figure><br>非递归做法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mergeSort2</span><span class="params">(nums)</span>:</span></span><br><span class="line">    i = <span class="number">1</span>  <span class="comment"># i是步长</span></span><br><span class="line">    <span class="keyword">while</span> i&lt;len(nums):</span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> left&lt;len(nums):</span><br><span class="line">            mid = left+i</span><br><span class="line">            right = min(left+<span class="number">2</span>*i, len(nums))</span><br><span class="line">            <span class="keyword">if</span> mid&lt;right:</span><br><span class="line">                nums[left:right]=merge(nums[left:mid], nums[mid:right])</span><br><span class="line">            left += <span class="number">2</span>*i</span><br><span class="line">        i *= <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure></p>
<h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><p>快速排序通常明显比同为$O(nlogn)$的其他算法更快，因此常被采用，而且快排采用了分治的思想，所以在面试中经常看到快排的影子。<br>步骤：<br>1）从数列中跳出一个元素作为基准数。<br>2）分区过程，将比基准数大的放在右边，小于或等于它的数都放在左边。<br>3）再对左右分区递归执行第二步，直至各区间只有一个数。<br><a href="http://wuchong.me/img/Quicksort-example.gif" target="_blank" rel="noopener">快排过程</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分策略，适用于链表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(nums, left, right)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> left == right:</span><br><span class="line">        <span class="keyword">return</span> left</span><br><span class="line">    pivot = left</span><br><span class="line">    slow, fast = left, left+<span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> fast&lt;=right:</span><br><span class="line">        <span class="keyword">if</span> nums[fast]&lt;nums[pivot]:</span><br><span class="line">            slow += <span class="number">1</span></span><br><span class="line">            nums[slow], nums[fast] = nums[fast], nums[slow]</span><br><span class="line">        fast += <span class="number">1</span></span><br><span class="line">    nums[pivot], nums[slow] = nums[slow], nums[pivot]</span><br><span class="line">    <span class="keyword">return</span> slow</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quickSort</span><span class="params">(nums, left, right)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> left&lt;=right:</span><br><span class="line">        midIndex = partition(nums, left, right)</span><br><span class="line">        quickSort(nums, left, midIndex<span class="number">-1</span>)</span><br><span class="line">        quickSort(nums, midIndex+<span class="number">1</span>, right)</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure><br>非递归方式，利用栈的思想将需要继续排序的首尾下标存入栈中，不断弹栈进行分区操作。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition2</span><span class="params">(nums, left, right)</span>:</span></span><br><span class="line">    mid = nums[left]</span><br><span class="line">    <span class="keyword">while</span> left&lt;right:</span><br><span class="line">        <span class="keyword">while</span> left&lt;right <span class="keyword">and</span> nums[right]&gt;mid:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        nums[left] = nums[right]</span><br><span class="line">        <span class="keyword">while</span> left&lt;right <span class="keyword">and</span> nums[left]&lt;=mid:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        nums[right] = nums[left]</span><br><span class="line">    nums[left] = mid</span><br><span class="line">    <span class="keyword">return</span> left</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quickSort2</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(nums)&lt;=<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums</span><br><span class="line">    stack = []</span><br><span class="line">    left, right = <span class="number">0</span>, len(nums)<span class="number">-1</span></span><br><span class="line">    stack.append(left)</span><br><span class="line">    stack.append(right)</span><br><span class="line">    <span class="keyword">while</span> stack:</span><br><span class="line">        r = stack.pop()</span><br><span class="line">        l = stack.pop()</span><br><span class="line">        midIndex = partition2(nums, l, r)</span><br><span class="line">        <span class="keyword">if</span> l&lt;midIndex:</span><br><span class="line">            stack.append(l)</span><br><span class="line">            stack.append(midIndex<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> r&gt;midIndex:</span><br><span class="line">            stack.append(midIndex+<span class="number">1</span>)</span><br><span class="line">            stack.append(r)</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure></p>
<h1 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h1><p>堆排序是借助堆来实现的选择排序，思想同选择排序，以下以大根堆为例。注意：如果想升序排序就使用大根堆，反之使用小根堆。原因是堆顶元素需要交换到序列尾部。<br>首先，实现堆排序需要解决两个问题：<br>1）如何由一个无序序列建成一个堆？<br>2）如何在输出堆顶元素之后，调整剩余元素成为一个新的堆？<br>第一个问题，可以直接使用线性数组来表示一个堆，由初始的无序序列建成一个堆就需要自底向上从第一个非叶元素开始按个调整成一个堆。<br>第二个问题，怎么调整成堆？首先是将堆顶元素和最后一个元素交换，然后比较当前堆顶元素的左右孩子节点，因为除了当前的堆顶元素，左右孩子均满足条件，这时需要选择当前堆顶元素与左右孩子节点的较大者（大根堆）交换，直到叶子节点。我们称这个自堆顶至叶子节点的调整为筛选。<br>从一个无序序列建堆的过程就是一个反复筛选的过程。若将此序列看成是一个完全二叉树，则最后一个非终端节点是n/2（取底）个元素，由此筛选即可。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 大根堆</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapSort</span><span class="params">(nums)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">adjustHeap</span><span class="params">(nums, i, size)</span>:</span></span><br><span class="line">        lchild = <span class="number">2</span>*i+<span class="number">1</span></span><br><span class="line">        rchild = <span class="number">2</span>*i+<span class="number">2</span></span><br><span class="line">        largest = i</span><br><span class="line">        <span class="keyword">if</span> lchild&lt;size <span class="keyword">and</span> nums[lchild]&gt;nums[largest]:</span><br><span class="line">            largest = lchild</span><br><span class="line">        <span class="keyword">if</span> rchild&lt;size <span class="keyword">and</span> nums[rchild]&gt;nums[largest]:</span><br><span class="line">            largest = rchild</span><br><span class="line">        <span class="keyword">if</span> largest != i:</span><br><span class="line">            nums[largest], nums[i] = nums[i], nums[largest]</span><br><span class="line">            adjustHeap(nums, largest, size)</span><br><span class="line">    <span class="comment"># 建立堆</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">buildHeap</span><span class="params">(nums, size)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)//<span class="number">2</span>)[::<span class="number">-1</span>]:  <span class="comment"># 从倒数第一个非叶子节点开始建立大根堆</span></span><br><span class="line">            adjustHeap(nums, i, size)        <span class="comment"># 对所有非叶子节点进行堆的调整</span></span><br><span class="line">    <span class="comment"># 堆排序</span></span><br><span class="line">    size = len(nums)</span><br><span class="line">    buildHeap(nums, size)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums))[::<span class="number">-1</span>]:</span><br><span class="line">        nums[<span class="number">0</span>], nums[i] = nums[i], nums[<span class="number">0</span>]</span><br><span class="line">        adjustHeap(nums, <span class="number">0</span>, i)</span><br><span class="line">    <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure></p>
<h1 id="计数排序"><a href="#计数排序" class="headerlink" title="计数排序"></a>计数排序</h1><p>（待补充）</p>
<h1 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h1><p>（待补充）</p>
<h1 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h1><p>（待补充）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">深度学习基础知识</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-18 21:13:44" itemprop="dateCreated datePublished" datetime="2020-02-18T21:13:44+08:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-03 21:45:04" itemprop="dateModified" datetime="2020-04-03T21:45:04+08:00">2020-04-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h2 id="SGD-Momentum-Adagard-Adam原理"><a href="#SGD-Momentum-Adagard-Adam原理" class="headerlink" title="SGD,Momentum,Adagard,Adam原理"></a>SGD,Momentum,Adagard,Adam原理</h2><h3 id="1-SGD、BGD和Mini-BGD"><a href="#1-SGD、BGD和Mini-BGD" class="headerlink" title="1. SGD、BGD和Mini-BGD:"></a>1. SGD、BGD和Mini-BGD:</h3><p>SGD(stochastic gradient descent):随机梯度下降，算法在每读入一个数据都会立刻计算loss function的梯度来更新参数，假设loss function为L(w),下同。</p>
<script type="math/tex; mode=display">w-=\eta \bigtriangledown_{w_{i}}L(w_{i})</script><p>优点：收敛的速度快，可以实现在线更新<br>缺点：很容易陷入到局部最优，困在马鞍点<br>BGD(batch gradient decent):批量梯度下降，算法在读取整个数据集后累加来计算损失函数的梯度。</p>
<script type="math/tex; mode=display">w-=\eta \bigtriangledown_{w}L(w)</script><p>优点：如果loss function为convex(凸函数)，则基本可以找到全局最优解<br>缺点：数据处理量大，导致梯度下降慢；不能实时增加实例，在线更新；训练占内存<br>Mini-BGD(mini-batch gradient descent):选择小批量数据进行梯度下降，这是一个折中的方法，采用训练集的子集(mini-batch)来计算loss function的梯度：</p>
<script type="math/tex; mode=display">w-=\eta \bigtriangledown_{w_{i:i+n}}L(w_{i:i+n})</script><p>这个优化方法用的比较多，计算效率高且收敛稳定，是现在深度学习的主流方法。当使用小批量样本来估计梯度时，由于估计的梯度往往会偏离真实的梯度，这可以视作在学习的过程中加入了噪声扰动，这种扰动会带来一些正则化的效果。<br>batch size越大，则小批量样本估计总体梯度约可靠，则每次参数更新沿总体梯度的负方向的概率越大。但是，训练误差收敛速度快，并不意味着模型的泛化性能强，此时的噪声太小，不足以将参数推出尖锐极小值的吸引区域。解决方案是：提高学习率，从而放大梯度噪声的贡献。<br>上面的方法都存在一个问题，就是update更新的方向完全依赖计算出来的梯度，很容易陷入局部最优的马鞍点。能不能改变其走向，又保证原本的梯度方向，就像向量变换一样，我们模拟物理中物体流动的动量概念（惯性），引入Momentum的概念。</p>
<h3 id="2-Momentum"><a href="#2-Momentum" class="headerlink" title="2. Momentum"></a>2. Momentum</h3><p>在更新方向的时候保留之前的方向，增加稳定性而且还有摆脱局部最优的能力。</p>
<script type="math/tex; mode=display">\Delta w=\alpha \Delta w- \eta \bigtriangledown L(w)</script><script type="math/tex; mode=display">w=w+\Delta w</script><p>若当前梯度的方向与历史梯度方向一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。一种形象的解释是：我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，<script type="math/tex">\eta</script>可视为空气阻力，若球的方向发生变化，则动量会衰减。</p>
<h3 id="3-Adagrad"><a href="#3-Adagrad" class="headerlink" title="3. Adagrad"></a>3. Adagrad</h3><p>Adagrad(adaptive gradient)自适应梯度算法，是一种改进的随机梯度下降算法。以前的算法中，每一个参数都使用相同的学习率<script type="math/tex">\alpha</script>,而Adagrad算法能够在训练中自动对learning_rate进行调整，出现频率较低参数采用较大的<script type="math/tex">\alpha</script>更新，出现频率较高的参数采用较小的<script type="math/tex">\alpha</script>更新，根据描述这个优化方法很适合处理稀疏数据。</p>
<script type="math/tex; mode=display">G=\sum ^{t}_{\tau=1}g_{\tau} g_{\tau}^{T} \quad s.t. \ g_{\tau}=\bigtriangledown L(w_{i})</script><script type="math/tex; mode=display">G_{j,j}=\sum _{\tau=1}^{t} g_{\tau,j\cdot}^{2}</script><p>这个对角线矩阵的元素代表的是参数的出现频率，每个参数的更新：</p>
<script type="math/tex; mode=display">w_{j}=w_{j}-\frac{\eta}{\sqrt{G_{j,j}}}g_{j}</script><h3 id="4-RMSprop"><a href="#4-RMSprop" class="headerlink" title="4. RMSprop"></a>4. RMSprop</h3><p>RMSprop(root mean square propagation)也是一种自适应学习率方法，不同之处在于，Adagrad会累加之前所有的梯度平方，RMSprop仅仅是计算对应的平均值，可以缓解Adagrad算法学习率下降较快的问题。</p>
<script type="math/tex; mode=display">v(w,t)=\gamma v(w,t-1)+(1-\gamma)(\bigtriangledown L(w_{i}))^{2}</script><p>其中 $\gamma$ 是遗忘因子</p>
<p>参数更新</p>
<script type="math/tex; mode=display">w=w-\frac{\eta}{\sqrt{v(w,t)}}\bigtriangledown L(w_{i})</script><h3 id="5-Adam"><a href="#5-Adam" class="headerlink" title="5. Adam"></a>5. Adam</h3><p>Adam(adaptive moment estimation)是对RMSprop优化器的更新，利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。优点：每一次迭代学习率都有一个明确的范围，使得参数变化很平稳。</p>
<script type="math/tex; mode=display">m_{w}^{t+1}=\beta_{1}m_{w}^{t}+(1-\beta_{1}) \bigtriangledown L^{t}</script><script type="math/tex; mode=display">v_{w}^{t+1}=\beta_{2}m_{w}^{t}+(1-\beta_{2}) (\bigtriangledown L^{t})^{2}</script><p>其中，m为一阶矩估计，v为二阶矩估计，然后进行估计校正，实现无偏估计</p>
<script type="math/tex; mode=display">\hat{m}_{w}=\frac{m_{w}^{t+1}}{1-\beta_{1}^{t+1}}</script><script type="math/tex; mode=display">\hat{v}_{w}=\frac{v_{w}^{t+1}}{1-\beta_{2}^{t+1}}</script><script type="math/tex; mode=display">w^{t+1} \leftarrow=w^{t}-\eta \frac{\hat{m}_{w}}{\sqrt{\hat{v}_{w}}+\epsilon}</script><p>Adam是实际中最常用的算法</p>
<h2 id="Batch-Size"><a href="#Batch-Size" class="headerlink" title="Batch Size"></a>Batch Size</h2><h3 id="在合理的范围内，增大batch-size的好处"><a href="#在合理的范围内，增大batch-size的好处" class="headerlink" title="在合理的范围内，增大batch size的好处"></a>在合理的范围内，增大batch size的好处</h3><p>1）内存利用率提高了，大矩阵乘法的并行化效率提高<br>2）跑完一次epoch所需的迭代次数减少，对于相同数据量的处理速度进一步加快。<br>3）在一定范围内，一般来说batch size越大，其确定的下降方向越准，引起训练振荡越小。</p>
<h3 id="盲目增大batch-size的坏处"><a href="#盲目增大batch-size的坏处" class="headerlink" title="盲目增大batch size的坏处"></a>盲目增大batch size的坏处</h3><p>1）内存利用率提高，但是内存容量可能撑不住<br>2）跑完一次epoch所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加，从而对参数的修正也就显得更加缓慢。<br>3）batch size增大到一定程度，其确定的下降方向已经基本不再改变。</p>
<h3 id="调节batch-size对训练效果的影响"><a href="#调节batch-size对训练效果的影响" class="headerlink" title="调节batch size对训练效果的影响"></a>调节batch size对训练效果的影响</h3><p>1）batch size太小，模型表现效果很差（error增大）<br>2）随着batch size增大，处理相同数据量的速度越快<br>3）随着batch size增大，达到相同精度所需的epoch数量越多。<br>4）由于上述两种因素的矛盾，batch size增大到某个时候，达到时间上的最优。<br>5）由于最终收敛精度会陷入不同的局部极值，因此batch size增大到某些时候，达到最终收敛精度上的最优。</p>
<h2 id="L1、L2范数"><a href="#L1、L2范数" class="headerlink" title="L1、L2范数"></a>L1、L2范数</h2><p>在机器学习中几乎可以看到在损失函数后面都会添加一个额外项，常用的额外项一般有两种，称为L1正则化和L2正则化，或者L1范数和L2范数。L1范数和L2范数可以看做是损失函数的惩罚项，所谓“惩罚”是指对损失函数中的某些参数做些限制。对于现在的回归模型，使用L1范数的模型叫做Lasso回归，使用L2范数的模型叫做Ridge回归（岭回归）。</p>
<h3 id="L1范数"><a href="#L1范数" class="headerlink" title="L1范数"></a>L1范数</h3><p>L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”(Lasso regularization)。稀疏的意思是可以让权重矩阵的一部分值等于0。为什么L1范数会使权值稀疏？有一种回答“它是L0范数的最优凸近似”，还存在一种更优雅的回答：任何的规则算子，如果他在$w_{i}=0$处不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子可以实现稀疏。</p>
<script type="math/tex; mode=display">||x||_{1}=\sum_{i}|x_{i}|</script><p>L1范数可以实现稀疏，而实现稀疏的作用为：<br>1) 可解释性：可以看到到底是哪些特征和预测的信息有关<br>2) 特征选择：输入的x的大部分特征与输出y是没有关系的，如果让参数矩阵w中出现许多0，则可以直接干掉与y无关的元素，也就是选择出于y真正相关的特征。如果不这么做，那么x中本来与y无关的特征也加入到模型中，虽然会更好的减小训练误差，但是在预测新样本时会考虑到无关的信息，干扰了预测。</p>
<h3 id="L2范数"><a href="#L2范数" class="headerlink" title="L2范数"></a>L2范数</h3><p>L2范数是指向量中各元素的平方和然后再求平方根，也叫做“岭回归(Ridge Regression)”，或叫做“权值衰减(weight decay)”。</p>
<script type="math/tex; mode=display">||x||_{2}=\sqrt{\sum_{i}x_{i}^2}</script><p>L2范数与L1范数不同，它不会让参数等于0，而是让每个参数都接近于0。L2范数的优点是：<br>1) 防止过拟合。一般的用法是在损失函数后面加上w的L2范数，即$||x||_{2}$，这是一种规则。<br>2) 优化求解变得稳定快速。简单地说它可以让$w$在接近全局最优点$w^*$的时候，还保持较大的梯度。这样可以跳出局部最优，也使得收敛速度变快。<br>总之，L1会趋向产生少量的特征，而其他的特征都是0，L2会产生更多地特征但都会接近于0。L1在特征选择时候非常有用，而L2就是一种规则化而已</p>
<h3 id="L1不可导的时候该怎么办"><a href="#L1不可导的时候该怎么办" class="headerlink" title="L1不可导的时候该怎么办"></a>L1不可导的时候该怎么办</h3><p>当损失函数不可导，梯度下降不再有效，可以使用坐标轴下降法。梯度下降是沿着当前点的负梯度方向进行参数更新，而坐标轴下降法是沿着坐标轴的方向。假设有m个特征个数，坐标轴下降法进行参数更新的时候，先固定m-1个值，然后再求另外一个的局部最优解，从而避免损失函数不可导问题。坐标轴下降法每轮迭代都需要O(mn)的计算，和梯度下降算法相同。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>神经网络的每个神经元接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入神经元节点会将输入属性值直接传递给下一层（隐藏层或输出层）。上层函数的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数。如果不用激活函数，在这种情况下每一层节点的输入都是上一层输出的线性函数，很容易验证，无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron），那么网络的逼近能力就相当有限。我们引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大，几乎可以逼近任意函数。</p>
<h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p>Signoid是常用的非线性的激活函数，它的数学形式如下：</p>
<script type="math/tex; mode=display">f(z)=\frac{1}{1+e^{-z}}</script><p>其导数为：</p>
<script type="math/tex; mode=display">\frac{df}{dz}=f(z)(1-f(z))</script><p>Sigmoid的几何如下：<br><img src="/images/Sigmoid.png" alt="Sigmoid函数"><br>特点：它能够把输入的连续实值变换为0和1之间的输出，对于非常大的负数则输出为0，非常大的正数则输出为1.<br>缺点：<br>1) 在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。Sigmoid函数的倒数如下所示：<br><img src="/images/Sigmoid导数.png" alt="Sigmoid的导数"><br>如果我们初始化神经网络的权值为[0,1]之间的随机值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减少为原来的0.25倍，如果神经网络隐藏层特别多时，那么梯度在多层传递之后就变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为$(1,+\infty)$区间的值，则会出现梯度爆炸情况。<br>2) Sigmoid的输出不是0均值，这会导致后一层的神经元将上一层的神经元输出的非0均值的信号作为输入。产生的结果是：如果$x&gt;0, f=w^Tx+b$，那么对$w$求局部梯度则都为正，这样反向传播的过程中$w$要么都向正方向更新，要么都往负方向更新，使得收敛缓慢。当然，如果按batch训练，那么那个batch可能会得到不同的信号，这个问题可以缓解一下。非0均值问题虽然会产生一些不好的影响，不过跟梯度消失问题相比还是要好很多。<br>3) 其解析式中含有幂运算，计算求解时相对比较耗时。对于规模较大的深度网络，这回较大地增加训练时间。</p>
<h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>tanh函数的解析式：</p>
<script type="math/tex; mode=display">tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p>其导数为：</p>
<script type="math/tex; mode=display">tanh'(x)=1-tanh^2(x)</script><p>tanh函数及其导数的几何图像如下：<br><img src="/images/tanh函数.png" alt="tanh函数及其导数"><br>tanh函数解决了Sigmoid函数不是zero-centered输出的问题，然而，梯度消失的问题和幂运算的问题仍然存在。</p>
<h3 id="Relu函数"><a href="#Relu函数" class="headerlink" title="Relu函数"></a>Relu函数</h3><p>Relu函数的解析式：</p>
<script type="math/tex; mode=display">Relu=max(0, x)</script><p>Relu函数及其导数的图像如下图所示：<br><img src="/images/Relu函数.png" alt="Relu函数及其导数"><br>ReLU其实是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。<br>优点：1）解决了梯度消失问题(gradient vanishing)问题（在正区间）。 2）计算速度非常快，只需要判断输入是否大于0。 3）收敛速度远快于sigmoid和tanh函数。<br>ReLU也有几个需要注意的问题：<br>1) ReLU的输出不是zero-centered。<br>2) Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不会被更新。有两个主要原因可能会导致这种情况：参数的初始化或learning_rate太大导致训练过程中参数更新太大进入这种状态。<br>解决的方法是采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。Xavier初始化方法是一种很有效的神经网络初始化方法，为了使得网络中信息更好的流动，每一层输出的方法应该尽量相等。</p>
<h3 id="Leaky-ReLU函数"><a href="#Leaky-ReLU函数" class="headerlink" title="Leaky ReLU函数"></a>Leaky ReLU函数</h3><p>函数表达式：</p>
<script type="math/tex; mode=display">f(x)=max(\alpha x, x)</script><p>Leaky Relu函数及其导数的图像如下图所示：<br><img src="/images/LeakyRelu函数.png" alt="Leaky ReLU函数及其导数"><br>图中左半边直线斜率非常接近于0，所以看起来像是平的。为了解决Dead ReLU Problem，通过将ReLU的前半段设为$\alpha x$而不是0，通常$\alpha =0.01$。理论上讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但在实际操作中，并没有完全证明Leaky ReLU总是好于ReLU。</p>
<h3 id="ELU函数"><a href="#ELU函数" class="headerlink" title="ELU函数"></a>ELU函数</h3><p>函数表达式为：</p>
<script type="math/tex; mode=display">
f(x)=
\begin{cases}
x &\text(if\ x>0)\\
\alpha(e^x-1) &\text{otherwise}
\end{cases}</script><p>函数及其导数的图像如下：<br><img src="/images/ELU函数.png" alt="ELU函数及导数图像"><br>显然，ELU有ReLU的基本所有优点，以及不会有Dead ReLU问题，输出的均值接近于0，是zero-centered。它的一个小问题在于计算量稍大。</p>
<h2 id="神经网络权重初始化方式"><a href="#神经网络权重初始化方式" class="headerlink" title="神经网络权重初始化方式"></a>神经网络权重初始化方式</h2><p>在深度学习找那个，神经网络的权重初始化方法(weight initialization)对模型的收敛速度和性能有着至关重要的影响。神经网络其实就是对权重参数w的不停迭代更新，以期达到较好的性能。在深度神经网络中，随着层数的增多，在梯度下降的过程中，极易出现梯度消失或者梯度爆炸。因此，对权重w的初始化显得至关重要，一个好的权重初始化虽然不能完全解决梯度消失和梯度爆炸问题，但是对于处理这两个问题是由很大的帮助的，并且十分有利于模型性能和收敛速度。</p>
<h3 id="初始化为0"><a href="#初始化为0" class="headerlink" title="初始化为0"></a>初始化为0</h3><p>在线性回归和logistics回归中可以使用，因为隐藏层只有一层。在超过一层的神经网络中就不能够使用了。因为如果所有的权重参数都为0，那么所有的神经元输出都是一样的，在反向传播时向后传递的梯度也是一致，将无法发挥多层的效果，实际上相当于一层隐藏层。</p>
<h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>卷积层的方差为：</p>
<script type="math/tex; mode=display">Var(w_ix_i)=E[w_i]^2Var(x_i)+E[x_i]^2Var(w_i)+Var(w_i)Var(x_i)</script><p>使用高斯随机初始化时要把W随机初始化到一个相对较小的值，因为如果X很大的话，W又相对较大，会导致输出值特别大，这样如果激活函数是sigmoid，就会导致sigmoid的输出值为1或0，导致更多的问题。但是随机初始化也有缺点，在均值为0，方差为1的高斯分布中，当神经网络层数增加时，会发现越往高层的激活函数（tanh函数）的输出值几乎都接近于0，使得神经元不被激活。</p>
<h3 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h3><p>每层的权重初始化为：</p>
<script type="math/tex; mode=display">W\sim U[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}]</script><p>服从均匀分布，$n_j$为输入层的参数，$n_{j+1}$为输出层参数。<br>Xavier是为了解决随机初始化问题而提出的一种初始化方式，其思想是尽可能让输入和输出服从相同的分布，这样能够避免高层的激活函数的输出值趋向于0。虽然Xavier初始化能很好地用于tanh函数，但是对于目前最常用的ReLU激活函数，还是无能为力，因此引出He initialization。</p>
<h3 id="MSRA-He-initialization"><a href="#MSRA-He-initialization" class="headerlink" title="MSRA/He initialization"></a>MSRA/He initialization</h3><p>Xavier初始化对于Relu激活函数表现非常不好，因此何恺明针对ReLU重新推导，每层的初始化公式为：</p>
<script type="math/tex; mode=display">W\sim U[0, \sqrt{\frac{2}{n}}]</script><p>是一个均值为0，方差为$\frac{2}{n}$的高斯分布。<br>缺点是：MSRA方法只考虑一个方向，无法使得正向反向传播时方差变化都很小。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="损失函数、代价函数与目标函数"><a href="#损失函数、代价函数与目标函数" class="headerlink" title="损失函数、代价函数与目标函数"></a>损失函数、代价函数与目标函数</h3><p>损失函数(Loss Function)：是定义在单个样本上的，是指一个样本的误差。<br>代价函数(Cost Function)：是定义在整个训练集上，是所有样本误差的平均值，也就是所有损失函数值的平均。<br>目标函数(Object Function)：是指最终需要优化的函数，一般来说是代价函数+正则化项。</p>
<h3 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h3><p>（1）0-1损失函数(0-1 loss function)</p>
<script type="math/tex; mode=display">
L(y, f(x))=
\begin{cases}
1, &\text{y}\ne\text{f(x)}\\
0, &\text{y}=\text{f(x)}
\end{cases}</script><p>即，当预测错误时，损失函数为1，当预测正确时，损失函数值为0.该损失函数不考虑预测值与真实值之间的误差程度。<br>（2）平方损失函数（quadratic loss function）</p>
<script type="math/tex; mode=display">L(y,f(x))=(y-f(x))^2</script><p>是指预测值与实际值差的平方。<br>（3）绝对值损失函数（absolute loss function）</p>
<script type="math/tex; mode=display">L(y,f(x))=|y-f(x)|</script><p>该损失函数只是取了绝对值而不是求平方值，差距不会被平方放大。<br>（4）对数损失函数(logarithmic loss function)</p>
<script type="math/tex; mode=display">L(y, p(y|x))=-logp(y|x)</script><p>该损失函数用到了极大似然估计思想。P(Y|X)通俗的解释是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该越小，因此再加个符号取反。<br>（5）Hinge loss<br>Hinge loss一般分类算法中的损失函数，尤其是SVM，其定义为：</p>
<script type="math/tex; mode=display">L(w,b)=max(0, 1-yf(x))</script><p>其中$y=+1$或$y=-1$，$f(x)=wx+b$，当为SVM的线性核时。</p>
<h3 id="常用的代价函数"><a href="#常用的代价函数" class="headerlink" title="常用的代价函数"></a>常用的代价函数</h3><p>（1）均方误差(Mean Squared Error)</p>
<script type="math/tex; mode=display">MSE=\frac{1}{N}\sum_{i=1}^N(y^{(i)}-f(x^{(i)}))^2</script><p>均方误差是指参数估计值与真实值之差的平方的期望值，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精度。（i表示第i个样本，N表示样本总数）。<br>通常用来做回归问题的代价函数。<br>（2）均方根误差</p>
<script type="math/tex; mode=display">RMSE=\sqrt{\frac{1}{N}\sum_{i=1}^N(y^{(i)}-f(x^{(i)}))^2}</script><p>均方根误差是均方误差的算术平方根，能够直观观测预测值与真实值的离散程度。通常用来作为回归算法的性能指标。<br>（3）平均绝对误差（Mean Absolute Error）</p>
<script type="math/tex; mode=display">MAE=\frac{1}{N}\sum_{i=1}^N|y^{(i)}-f(x^{(i)})|</script><p>平均绝对误差是绝对误差的平均值，平均绝对误差能更好地反映预测值误差的实际情况。通常用来作为回归算法的性能指标。<br>（4）交叉熵代价函数(Cross Entry)</p>
<script type="math/tex; mode=display">H(p,q)=-\frac{1}{N}\sum_{i=1}^Np(x^{(i)})log(x^{(-i)})</script><p>交叉熵是用来评估当前训练得到的概率分布于真实分布的差异情况，减少交叉熵损失就是在提高模型的预测准确率。其中p(x)是指真实分布的概率，q(x)是模型通过数据计算出来的概率估计。<br>对于二分类模型的交叉熵代价函数：</p>
<script type="math/tex; mode=display">L(w,b)=-\frac{1}{N}\sum_{i=1}^N(y^{(i)}logf(x^{(i)})+(1-y^{(i)})log(1-f(x^{(i)})))</script><p>其中f(x)可以是sigmoid函数或深度学习中的其他激活函数，而$y{(i)}\in 0,1$。<br>交叉熵通常用作分类问题的代价函数。<br><a href="https://zhuanlan.zhihu.com/p/37217242" target="_blank" rel="noopener">常用损失函数</a></p>
<h3 id="为什么分类问题用-cross-entropy，而回归问题用-MSE"><a href="#为什么分类问题用-cross-entropy，而回归问题用-MSE" class="headerlink" title="为什么分类问题用 cross entropy，而回归问题用 MSE?"></a>为什么分类问题用 cross entropy，而回归问题用 MSE?</h3><p><a href="https://blog.csdn.net/weixin_41888969/article/details/89450163" target="_blank" rel="noopener">优秀解答</a><br>对于多分类的标签，从本质上看，通过One-hot操作，就是把具体的标签（Label）空间，变换到一个概率测度空间（设为 p）。而对于多分类问题，在Softmax函数的“加工”下，神经网络的实际输出值就是一个概率向量，设其概率分布为q。现在我们想衡量p和q之间的差异（即损失），一种简单粗暴的方式，自然是可以比较p和q的差值，如MSE（不过效果不好而已）。一种更好的方式是衡量这二者的概率分布的差异，就是交叉熵，因为它的设计初衷，就是要衡量两个概率分布之间的差异。总之分类标签可以看做是概率分布（由one-hot变换而来），神经网络输出（经过softmax加工）也是一个概率分布，现在想衡量二者的差异（即损失），自然用交叉熵最好了。<br>当MSE和交叉熵同时应用到多分类场景下时，（标签的值为1时表示属于此分类，标签值为0时表示不属于此分类），MSE对于每一个输出的结果都非常看重，而交叉熵只对正确分类的结果看重。交叉熵的损失函数只和分类正确的预测结果有关系，而MSE的损失函数还和错误的分类有关系，该分类函数除了让正确的分类尽量变大，还会让错误的分类变得平均，但实际在分类问题中这个调整是没有必要的。但是对于回归问题来说，这样的考虑就显得很重要了。所以，回归问题熵使用交叉熵并不合适。</p>
<h2 id="Batch-Normalization原理"><a href="#Batch-Normalization原理" class="headerlink" title="Batch Normalization原理"></a>Batch Normalization原理</h2><p><a href="https://blog.csdn.net/e01528/article/details/89313518" target="_blank" rel="noopener">BN归纳</a><br>Batch Normalization就是在训练过程中使得每一层神经网络的输入保持相同分布的。BN的基本思想相当直观：随着网络深度加深或者在训练过程中，神经元的输入值分布逐渐发生偏移或变动，使得整体分布逐渐往非线性函数的取值区域的上下限两端靠近，所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因。而BN就是通过一定的规范化手段，把该层任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回到标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就是导致损失函数较大的变化，使得梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。<br>如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同吗？这意味着什么？我们知道，如果是多层的线性函数变换其实这个深层网络就没有意义，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力又下降了。所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的$x$又进行了scale加上shift操作($y=scale*x+shift$)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把输入值从标准正太分布左移或右移一点并拉伸或缩短一点，每个实例的挪动情况不一样，这样等价于非线性函数的值从正中心的线性区域往非线性区域动了动。核心思想是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠近非线性区两头使得网络的收敛速度太慢。<br>BatchNorm在网络中的作用：BN层添加在激活函数前，对输入激活函数的输入进行归一化，这样解决了输入数据发生偏移和增大的影响。<br>BatchNorm的优点：1）极大提升了训练速度，使得收敛过程大大加快；2）还能增加分类效果，一种解释是这类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；3）调参过程也简单了，对于初始化要求没那么高，可以使用大的学习率。<br>BatchNorm的缺点：1）高度依赖于batch size的大小，它要求batch size都比较大，因此不适合batch size较小的场景。2）不适合RNN网络，因为不同样本的长度不同，RNN的深度是不固定的。同一个batch中多个样本会产生不同深度的RNN，因此很难对同一层的样本进行归一化。<br>BN的计算流程<br>首先，对(B,W,H)通道计算样本的均值和方差，将样本数据进行标准化处理，然后引入$\gamma$和$\beta$两个参数进行平移和缩放处理，让网络可以学习恢复出原始网络所要学习的特征分布。</p>
<h2 id="LRN-局部归一化响应"><a href="#LRN-局部归一化响应" class="headerlink" title="LRN 局部归一化响应"></a>LRN 局部归一化响应</h2><p>为什么要有LRN 局部归一化响应？<br>在神经生物学有一概念叫 “侧抑制”，指的是被激活的神经元会抑制相邻的神经元。局部响应归一化借鉴了“侧抑制”的思想来实现局部抑制，对局部神经元的活动创建竞争机制，使得相应比较大的值相对大，小的更小。<br>优点：可提高模型泛化能力，当使用Relu时这种“侧抑制”的方法很管用。</p>
<h2 id="深度学习几种归一化（BN、LN、IN、GN）"><a href="#深度学习几种归一化（BN、LN、IN、GN）" class="headerlink" title="深度学习几种归一化（BN、LN、IN、GN）"></a>深度学习几种归一化（BN、LN、IN、GN）</h2><p> BN、LN、IN和GN这四个归一化的计算流程几乎是一样的，可以分为四步：1）计算出均值；2）计算出方差；3）归一化处理到均值为0，方差为1；4）变化重构，恢复出这一层网络所要学到的分布。<br> 我们先用一个示意图来形象的表现BN、LN、IN和GN的区别，在输入图片的维度为（NCHW）中，HW是被合成一个维度，这个是方便画出示意图，C和N各占一个维度。<br> <img src="/images/归一化区别.png" alt="BN,LN,IN,GN的区别"><br> Batch Normalization：<br>1)BN的计算就是把每个通道的NHW单独拿出来归一化处理<br>2)针对每个channel我们都有一组γ,β，所以可学习的参数为2*C<br>3)当batch size越小，BN的表现效果也越不好，因为计算过程中所得到的均值和方差不能代表全局。<br>Layer Normalizaiton：<br>1)LN的计算就是把每个CHW单独拿出来归一化处理，不受batchsize 的影响<br>2)常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理<br>Instance Normalization:<br>1)IN的计算就是把每个HW单独拿出来归一化处理，不受通道和batchsize 的影响<br>2)常用在风格化迁移，IN的效果优于BN，因为在这类生成式方法中，每张图片自己的风格比较独立，不应该与batch中其他图片产生太大联系。但如果特征图可以用到通道之间的相关性，那么就不建议使用它做归一化处理<br>Group Normalization:<br>1)GN的计算就是把先把通道C分成G组，然后把每个gHW单独拿出来归一化处理，最后把G组归一化之后的数据合并成CHW<br>2)GN介于LN和IN之间，当然可以说LN和IN就是GN的特列，比如G的大小为1或者为C</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout指的是让这些神经元失效或者状态抑制，可以防止参数过分依赖训练数据，增加参数对数据集的泛化能力。<br>在训练阶段，以概率p主动临时性地忽略掉部分隐藏节点。这一操作的好处在于，在较大程度上减小了网络的大小（解决了耗时的问题），而且多个这样的抑制不同隐藏神经元的网络组合可以解决过拟合问题。<br>在预测阶段，将参与学习的节点和那些被隐藏的节点以一定的概率p加权求和，综合计算得到网络的输出。对于这样的类似融合不同模型的学习过程，有学者认为，Drop out可视为一种集成学习。</p>
<h2 id="反向传播原理"><a href="#反向传播原理" class="headerlink" title="反向传播原理"></a>反向传播原理</h2><p>反向传播过程：将训练集的数据输入到神经网络的输入层，经过隐藏层最终到达输出层输出结果，这是神经网络的前向传播过程；由于神经网络的输出结果和实际结果存在误差，计算估计值和实际值之间的误差，并将误差从输出层向隐藏层反向传播，直至传播至输入层；在反向传播过程中，根据误差调整各种参数的值，不断迭代上述的过程，直至收敛。<br><a href="https://blog.csdn.net/u014313009/article/details/51039334?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">推导过程</a></p>
<h3 id="什么是梯度消失和梯度爆炸"><a href="#什么是梯度消失和梯度爆炸" class="headerlink" title="什么是梯度消失和梯度爆炸"></a>什么是梯度消失和梯度爆炸</h3><p>在反向传播过程中需要对激活函数进行求导，如果导数大于1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加这就是梯度爆炸。同样如果导数小于1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。</p>
<h3 id="梯度消失的原因"><a href="#梯度消失的原因" class="headerlink" title="梯度消失的原因"></a>梯度消失的原因</h3><p>1）在反向传播过程中梯度呈指数衰减，随着网络层数的加深，梯度减小为0；2）在反向传播过程中，神经元的输入值位于激活函数的非线性区域的上下限两端，使得梯度消失。<br>而梯度爆炸一般出现在深层网络和权值初始化太大的情况下。</p>
<h3 id="梯度消失解决方案"><a href="#梯度消失解决方案" class="headerlink" title="梯度消失解决方案"></a>梯度消失解决方案</h3><p>1）预训练加微调<br>2）添加正则项<br>3）使用relu、leakrelu、elu等激活函数<br>4）使用batch normalization<br>5）使用残差结构<br>6）使用LSTM</p>
<h2 id="pytorch与tensorflow的区别"><a href="#pytorch与tensorflow的区别" class="headerlink" title="pytorch与tensorflow的区别"></a>pytorch与tensorflow的区别</h2><p>创建和运行计算图可能是两个框架最不同的地方。在PyTorch中，图结构是动态的，这意味着图在运行时构建。而在TensorFlow中，图结构是静态的，这意味着图先被“编译”然后再运行。<br>动态图的还有一个好处就是比较容易调试, 在 PyTorch 中, 代码报错的地方, 往往就是代码错写的地方, 而静态图需要先根据代码生成 Graph 对象, 然后在 session.run() 时报错, 但是这种报错几乎很难直接找到代码中对应的出错段落。<br><a href="https://blog.csdn.net/zzlyw/article/details/78768991" target="_blank" rel="noopener">pytorch基础知识</a></p>
<h1 id="卷积神经网络模型"><a href="#卷积神经网络模型" class="headerlink" title="卷积神经网络模型"></a>卷积神经网络模型</h1><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>输入图像的维数通常很高，例如，1000x1000大小的彩色图像对应于三百万维特征。因此，继续沿用多层感知机中全连接层会导致庞大的参数量。大参数量需要繁重的计算，而更重要的是大参数量会有更高的过拟合风险。卷积是局部连接、共享参数的全连接层。这两个特征使参数量大大降低。卷积层中的权值通常被称为滤波器(filter)或卷积核(convolution kernel)。<br><strong>局部连接</strong>：所谓局部连接，就是卷积层的节点仅仅和前一层的部分节点相连接，只用来学习局部特征，而全连接层中，每个输出通过权值(weight)和所有输入相连。在计算机视觉中，图像中的某一块区域中，像素之间的相关性与像素之间的距离同样相关，距离较近的像素之间相关性强，距离较远则相关性比较弱，所以局部相关性理论同样适用于计算机视觉的图像处理领域。在卷积层中，每个输出神经元在通道方向保持全连接，在空间方向上只和上一层一部分输入神经元相连。局部感知采用部分神经元接收图像信息，再通过综合全部的图像信息达到增强图像信息的目的。这种局部连接的方式大大减少了参数数量，加快了学习速率，同时也在一定程度上减少过拟合的可能。<br><strong>共享参数</strong>：如果一组权值可以在图像中某个区域提取出有效的表示，那么它们也能在图像中的另外区域提取出有效的表示。也就是说，如果一个模式(pattern)出现在图像中的某个区域，那么它们也可以出现在图像中的其他任何区域。因此，卷积层不同空间位置的神经元共享权值，用于发现图像中不同空间位置的模式。权值共享其实就是整张图片在使用同一个卷积核内的参数提取特征，但是不同的卷积核提取的是不同特征，因此不同卷积核间的神经元权值是不共享的。卷积层在空间方向共享参数，而循环神经网络在时间方向共享参数。<br><strong>描述卷积的四个量</strong>：一个卷积层的配置由如下四个量确定。1）卷积核个数。使用一个卷积核进行卷积可以得到一个二维的特征图(feature map)。使用多个卷积核进行卷积，可以得到不同特征的feature map。2）感受野（receptive field）F，即卷积核的大小。3）零填充(zero-padding)P，随着卷积的进行，图像的大小将缩小，图像边缘的信息将逐渐丢失，因此在卷积前，我们在图像上下左右填补一些0，使得我们可以控制输出特征图的大小。4）步长(stride)S，卷积核在输入图像上每移动S个位置计算一个输出神经元。<br>假设输入图片大小为$I\times I$，卷积核大小为$K\times K$，步长为S，填充的像素为P，则卷积层输出的特征图大小为：</p>
<script type="math/tex; mode=display">O=(I-K+2P)/S+1</script><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化层根据特征图上的局部统计信息进行下采样，在保留有用信息的同时减少特征图的大小。和卷积层不同，池化层不包含需要学习的参数，最大池化层(max-pooling)在一个局部区域选最大值输出，而平均池化(average pooling)计算一个区域的均值作为输出。<br><img src="/images/pooling.png" alt="池化层"><br><strong>池化层的作用</strong>：1）增加特征平移不变性，池化可以提高网络对微小位移的容忍能力。2）减小特征图大小，池化层对空间局部区域进行下采样，使下一层需要的参数量和计算量减少，并降低过拟合风险。3）最大池化可以带来非线性，这是目前最大池化更常使用的原因。<br>平均池化和最大池化的区别：最大池化保留了纹理特征，平均池化保留整体的数据特征。最大池化提取边缘等“最重要”的特征，而平均池化提取的特征更加smoothly。</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1×1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽</p>
<h3 id="召回率、精确率、准确率"><a href="#召回率、精确率、准确率" class="headerlink" title="召回率、精确率、准确率"></a>召回率、精确率、准确率</h3><p>精确率是针对预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能，一种是把正类预测为正类（TP），另一种就是把负类预测为正类（FP），也就是：</p>
<script type="math/tex; mode=display">P=\frac{TP}{TP+FP}</script><p>而召回率是针对原来的样本而言，它表示的是样本中的正例有多少被预测正确，也有两种可能，一种是把原来的正类预测成正类（TP），另一种就是把原来的正类预测为负类（FN）。</p>
<script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script><p>准确率就是所有样本中预测出正例的概率。</p>
<script type="math/tex; mode=display">A=\frac{TP+TN}{TP+FN+FP+TN}</script><h3 id="Accuracy的局限性"><a href="#Accuracy的局限性" class="headerlink" title="Accuracy的局限性"></a>Accuracy的局限性</h3><p>当正负样本极度不均衡时存在问题！比如，正样本有99%时，分类器只要将所有样本划分为正样本就可以达到99%的准确率。但显然这个分类器是存在问题的。当正负样本不均衡时，常用的评价指标为ROC曲线和PR曲线。</p>
<h3 id="ROC曲线和PR曲线"><a href="#ROC曲线和PR曲线" class="headerlink" title="ROC曲线和PR曲线"></a>ROC曲线和PR曲线</h3><p>定义真正例率（True Positive Rate）为：$TPR=\frac{TP}{TP+FN}$，其刻画真正的正类中，有多少样本预测为正类的概率。定义假正例率（False Positive Rate）为：$FPR=\frac{FP}{TN+FP}$，其刻画了真正的负类中，有多少样本被预测为正类的概率。以真正例率为纵轴、假正例率为横轴作图，就得到ROC曲线。<br><img src="/images/ROC.png" alt="ROC曲线"><br>具体方法是在不同的分类阈值 (threshold) 设定下分别以TPR和FPR为纵、横轴作图。曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。<br>优点:1）兼顾正例和负例的权衡。因为TPR聚焦于正例，FPR聚焦于负例，使其成为一个比较均衡的评估方法。2）ROC曲线选用的两个指标， TPR和FPR，都不依赖于具体的类别分布。<br>缺点:1）ROC曲线的优点是不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。<br>2）在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。ROC曲线的横轴采用FPR，根据公式 ，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。（当然也可以只分析ROC曲线左边一小段）<br>PR曲线以查准率（精确率P）为纵轴，查全率（召回率R）为横轴作图，就得到查准率-查全率曲线，简称P-R曲线。<br>PR曲线与ROC曲线的相同点是都采用了TPR (Recall)，都可以用AUC来衡量分类器的效果。不同点是ROC曲线使用了FPR，而PR曲线使用了Precision，因此PR曲线的两个指标都聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。<br><img src="/images/PR.png" alt="PR曲线"><br>使用场景：<br>ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。<br>如果有多份数据且存在不同的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。<br>如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。<br>类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。<br>最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1 score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。</p>
<h3 id="AUC曲线"><a href="#AUC曲线" class="headerlink" title="AUC曲线"></a>AUC曲线</h3><p>AUC（Area Under the Curve）可解读为：从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。<br>我们最直观的有两种计算AUC的方法：1）绘制ROC曲线，ROC曲线下面的面积就是AUC的值。2）假设总共有（m+n）个样本，其中正样本m个，负样本n个，总共有mn个样本对，计数，正样本预测为正样本的概率值大于负样本预测为正样本的概率值记为1，累加计数，然后除以（mn）就是AUC的值。<br>AUC指标有什么特点？放缩结果对AUC是否有影响？<br>AUC（Area under Curve）指的是ROC曲线下的面积，介于0和1之间。AUC作为数值可以直观地评价分类器的好坏，值越大越好。它的统计意义是从所有正样本随机抽取一个正样本，从所有负样本随机抽取一个负样本，当前score使得正样本排在负样本前面的概率。放缩结果对AUC没有影响。</p>
<h3 id="训练集、验证集和测试集"><a href="#训练集、验证集和测试集" class="headerlink" title="训练集、验证集和测试集"></a>训练集、验证集和测试集</h3><h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><p>给定一张图像，图像分类任务旨在判断该图像所属类别。</p>
<h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>LeCun等将BP算法应用到多层神经网络中，提出了LeNet5模型，并将其用于手写数字识别，卷积神经网络才算正式提出。<br><img src="/images/LeNet5.png" alt="LeNet5网络模型"><br><img src="/images/LeNet5参数.png" alt="LeNet5网络模型参数"><br>网络输入32*32的手写字体图片，这些手写字体包含0~9数字，也就是相当于10个类别的图片；输出：分类结果在0~9之间。LeNet的网络结构十分简单且单一，卷积层C1、C3和C5除了输出维数外采用的是相同的参数，池化层S2和S4采用的也是相同的参数。</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>2012年Krizhevsky使用卷积神经网络在ILSRC 2012图像分类大赛上夺冠，提出了AlexNet模型。AlexNet网络的提出对于卷积神经网络具有里程碑式的意义，相较于LeNet5的改进有以下几点：<br>1）数据增强：水平翻转、随机裁剪（平移变换）、颜色光照变换<br>2）Dropout:Dropout方法和数据增强一样，都是防止过拟合。dropout能按照一定的概率将神经元从网络中丢弃，dropout能在一定程度上防止过拟合，并且加快网络的训练速度。<br>3）ReLU激活函数：ReLU具有一些优良的特性，在为网络引入非线性的同时，也能引入稀疏性。稀疏性可以选择性激活和分布式激活神经元，能学习到相对稀疏的特征，起到自动化解离的效果。此外，ReLU的导数曲线在输入大于0时，函数的导数为1，这种特性能保证在其输入大于0时梯度不衰减，从而避免或抑制网络训练时梯度消失现象，网络模型的收敛速度会相对稳定。4）Local Response Normalization:局部响应归一化，简称LRN，实际就是利用临近的数据做归一化。5）Overlapping Pooling：即Pooling的步长比Pooling Kernel对应边要小。6）多GPU并行：极大加快网络训练。<br><img src="/images/AlexNet.png" alt="AlexNet网络模型"><br><img src="/images/AlexNet.png" alt="AlexNet网络模型参数"></p>
<h3 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h3><p>VGGNet是由牛津大学计算机视觉组合Google DeepMind项目的研究员共同研发的卷积神经网络模型，包含VGG16和VGG19两种模型。<br><img src="/images/VGG16.png" alt="VGG16网络模型"><br>从网络模型中可以看出，VGG16相比于AlexNet类的模型具有较深的深度，通过反复堆叠$3\times 3$的卷积层和$2\times 2$的池化层，VGG16构建了较深层次的网络结构。与AlexNet主要有以下不同：<br>1）Vgg16有16层网络，AlexNet只有8层；<br>2）在训练和测试时使用了多尺度做数据增强。</p>
<h3 id="GoogleNet-22层"><a href="#GoogleNet-22层" class="headerlink" title="GoogleNet(22层)"></a>GoogleNet(22层)</h3><p>GoogleNet进一步增加了网络模型的深度，但是单纯的在VGG16的基础上增加网络的宽度会带来以下的缺陷：1）过多的参数容易引起过拟合；2）层数的过深，容易引起梯度消息现象。<br>GoogleNet的提出受到论文Network in Network(NIN)的启发，NIN有两个贡献：1）提出多层感知卷积层，使用卷积层后加上多层感知机，增强网络提取特征的能力。普通的卷积层和多层感知卷积层结构如图所示，Mlpconv相当于在一般卷积层后加一个1*1的卷积层。2）提出了全局平均池化代替全连接层，全连接层具有大量的参数，使用全局平均池化代替全连接层，能很大程度减少参数空间，便于加深网络，还能防止过拟合。<br><img src="/images/多层感知卷积层.png" alt="普通卷积层和多层感知卷积层结构图"><br>GoogleNet根据Mlpconv的思想提出了Inception结构，该结构有两个版本，下图是Inception的naive版，该结构巧妙的将$1\times 1$、$3\times 3$和$5\times 5$三种卷积核和最大池化层结合起来作为一层结构。<br><img src="/images/Inception1.png" alt="Inception结构的native版"><br>而Inception的native版中$5\times 5$的卷积核会带来很大的计算量，因此采用了与NIN类似的结构，在原始的卷积层之后加上$1\times 1$卷积层，最终版本的Inception如下图所示：<br><img src="/images/Inception.png" alt="降维后的Inception模块"></p>
<h3 id="Inception-v3-v4"><a href="#Inception-v3-v4" class="headerlink" title="Inception v3/v4"></a>Inception v3/v4</h3><p>在GoogleNet的基础上进一步降低参数，其和GoogleNet有相似的Inception模块，但将$7\times 7$和$5\times 5$卷积分解为若干等效$3\times 3$卷积，并在网络中后部分把$3\times 3$卷积分解为$1\times 3$和$3\times 1$卷积，这使得在相似的网络参数下网络可以部署到42层。此外，Inception v3使用了批量归一化层。Inception v3是GoogleNet计算量的2.5倍，而错误率较后者下降了3%。Inception v4在Inception模块基础上结合residual模块，进一步降低了0.4%的错误率。<br><img src="/images/Inceptionv3.png" alt="Inceptionv3模块"></p>
<h3 id="Inception-v1-v2-v3-v4"><a href="#Inception-v1-v2-v3-v4" class="headerlink" title="Inception v1/v2/v3/v4"></a>Inception v1/v2/v3/v4</h3><p><a href="https://blog.csdn.net/langb2014/article/details/52787095?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">Inception v1/v2/v3/v4对比</a></p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>卷积神经网络模型的发展历程一次次证明加深网络的深度和宽度能得到更好的效果，但是后来的研究发现，网络层次较深的网络模型的效果反而不如较浅层的网络，称为“退化”现象。退化现象产生的原因在于当模型的结构变得复杂随机梯度下降的优化变得更加困难，导致网络模型的效果反而不如浅层网络。针对这个问题，MSRA何凯明团队提出Residual Network，该网络具有Residual结构如下所示：<br><img src="/images/Residual.png" alt="Residual结构"><br>ResNet的基本思想是引入了能够跳过一层或多层的“shortcut connection”，即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换为F(x)+x，作者认为这两种表达的效果相同，但是优化的难度却并不相同。这个Residual block通过将shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的叠加，这个简单地加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度，提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。对于shortcut的方式，作者提出了三个策略：1）使用恒等映射，如果residual block的输入和输出维度不一致，对增加的维度用0来填充；2）在block输入输出维度一致时使用恒等映射，不一致时使用线性投影以保证维度一致；3）对于所有的block使用线性投影。论文最后用三个$1\times 1,3\times 3, 1\times 1$的卷积层代替前面说的两个$3\times 3$卷积层，第一个$1\times 1$用来降低维度，第三个$1\times 1$用来增加维度，这样可以保证中间的$3\times 3$卷积层拥有比较小的输入输出维度。<br><img src="/images/bottleneck.png" alt="更深的residual block"></p>
<h4 id="ResNet提出的背景和核心理论是什么？"><a href="#ResNet提出的背景和核心理论是什么？" class="headerlink" title="ResNet提出的背景和核心理论是什么？"></a>ResNet提出的背景和核心理论是什么？</h4><p>提出背景：解决或缓解深层的神经网络训练中梯度消失的问题<br>ResNet的一个重要假设：假设有一个L层的深度神经网络, 如果我们在上面加入一层, 直观来讲得到的L+1层深度神经网络的效果应该至少不会比L层的差。因为可以简单的设最后一层为前一层的拷贝(相当于恒等映射), 并且其它层参数设置不变。<br>但是最终实验发现: 层数更深的神经网络反而会具有更大的训练误差, 因此, 作者认为深层网络效果差的原因很大程度上也许不在于过拟合, 而在于梯度消失问题。<br>解决办法：<br>根据梯度消失问题的根本原因所在(链式法则连乘), ResNet通过调整网络结构来解决上述问题。ResNet的结构设计思想为: 既然离输入近的神经网络层较难训练, 那么我们可以将它短接到更靠近输出的层来解决梯度消失的问题。在梯度更新期间，梯度的相关性会随着层数的变深呈指数性衰减，导致梯度趋近于白噪声，而skip-connections 可以减缓衰减速度，使相关性和之前比起来变大。</p>
<h3 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h3><p><a href="https://blog.csdn.net/xiaohu2022/article/details/85560788" target="_blank" rel="noopener">DenseNet网络介绍</a><br>DenseNet脱离了加深网络层数(ResNet)和旁路网络结构(Inception)来提升网络性能的定式思维，从特征的家督考虑，通过特征重用和旁路（Bypass）设置，既大幅度减少了网络的参数量，又在一定程度上缓解了gradient vanishing问题的产生。DenseNet作为另一种较深层数的卷积神经网络，具有如下优点：<br>1）相比于ResNet拥有更少的参数数量；2）旁路加强了特征的重用；3）网络更容易训练，并具有一定的正则效果；4）缓解了gradient vanishing和model degradation的问题。<br><img src="/images/DenseNet.jpg" alt="DenseNet网络结构图"><br>如图所示，第i层的输入不仅与i-1层的输出相关，还与所有之前层的输出相关。记作：<br>$X_l=H_l([X_0,X_1,…,X_{l-1}])$<br>其中[]代表concatenation（拼接），既将$X_0$到$X_{l-1}$层的所有输出feature map按Channel组合在一起，这里所用到的非线性变换H为BN+ReLU+Conv(3x3)的组合。<br>由于在DenseNet中需要对不同层的feature map进行cat操作，所以需要不同层的feature map保持相同的feature size，这就限制了网络中的down sampling的实现。为了使用down sampling在实验中transition layer由BN+Conv(1x1)+average-pooling(2x2)组成。<br><img src="/images/Denseblock.jpg" alt="Denseblock网络图"></p>
<h3 id="SENet"><a href="#SENet" class="headerlink" title="SENet"></a>SENet</h3><p>Squeeze-and-Excitation Networks（SENet）是由自动驾驶公司Momenta在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提升准确率。<br><img src="/images/SEblock.jpg" alt="SE Bolck结构"><br>上图是SENet的Block单元，图中的$F_{tr}$是传统的卷积结构，X和U是$F_{tr}$的输入（C’xH’xW’）和输出（CxHxW），这些都是以往结构中已存在的。SENet增加的部分是U后的结构：对U先做一个Global Average Pooling（图中的$F_{sq(.)}$，作者称为Squeeze过程），输出的1x1xC数据再经过两级全连接（图中的$F_{ex(.)}$，作者称为Excitation过程），最后用sigmoid限制到[0，1]的范围，把这个值作为scale乘到U的C个通道上， 作为下一级的输入数据。这种结构的原理是想通过控制scale的大小，把重要的特征增强，不重要的特征减弱，从而让提取的特征指向性更强。<br>先是Squeeze部分。GAP有很多算法，作者用了最简单的求平均的方法，将空间上所有点的信息都平均成了一个值。这么做是因为最终的scale是对整个通道作用的，这就得基于通道的整体信息来计算scale。另外作者要利用的是通道间的相关性，而不是空间分布中的相关性，用GAP屏蔽掉空间上的分布信息能让scale的计算更加准确。</p>
<script type="math/tex; mode=display">z_c=F_{sq}(u_c)=\frac{1}{W\times H}\sum_{i=1}^W\sum_{j=1}^Hu_c(i,j)</script><p>Excitation部分是用2个全连接来实现 ，第一个全连接把C个通道压缩成了C/r个通道来降低计算量（后面跟了RELU），第二个全连接再恢复回C个通道（后面跟了Sigmoid），r是指压缩的比例。作者尝试了r在各种取值下的性能 ，最后得出结论r=16时整体性能和计算量最平衡。<br>为什么要加全连接层呢？这是为了利用通道间的相关性来训练出真正的scale。一次mini-batch个样本的squeeze输出并不代表通道真实要调整的scale值，真实的scale要基于全部数据集来训练得出，而不是基于单个batch，所以后面要加个全连接层来进行训练。<br>可以拿SE Block和下面3种错误的结构比较来进一步理解：<br>下图最上方的结构，squeeze的输出直接scale到输入上，没有了全连接层，某个通道的调整值完全基于单个通道GAP的结果，事实上只有GAP的分支是完全没有反向计算、没有训练的过程的，就无法基于全部数据集来训练得出通道增强、减弱的规律。<br>下图中间是经典的卷积结构，有人会说卷积训练出的权值就含有了scale的成分在里面，也利用了通道间的相关性，为啥还要多个SE Block？那是因为这种卷积有空间的成分在里面，为了排除空间上的干扰就得先用GAP压缩成一个点后再作卷积，压缩后因为没有了Height、Width的成分，这种卷积就是全连接了。<br>下图最下面的结构，SE模块和传统的卷积间采用并联而不是串联的方式，这时SE利用的是$F_{tr}$输入X的相关性来计算scale，X和U的相关性是不同的，把根据X的相关性计算出的scale应用到U上明显不合适。<br><img src="/images/SEnet2.jpg" alt="三种错误的SE结构"><br>下图是两个SENet实际应用的例子，左侧是SE-Inception的结构，即Inception模块和SENet组和在一起；右侧是SE-ResNet，ResNet和SENet的组合，这种结构scale放到了直连相加之前。<br><img src="/images/SENet3.jpg" alt="SE-Inception和SE-ResNet结构"></p>
<h3 id="历年来所有的网络"><a href="#历年来所有的网络" class="headerlink" title="历年来所有的网络"></a>历年来所有的网络</h3><p>LeNet-5:第一个卷积，用来识别手写数组，使用的卷积大小为5×5,s=1，就是普通的卷积核池化层结合起来，最后加上全连接层。<br>AlexNet:在第一个卷积中使用了11×11卷积，第一次使用Relu，使用了NormLayer，但不是我们经常说的BN。使用了dropout，并在两个GPU上进行了训练。<br>VGGNet:只使用了小卷积3×3(s=1)以及常规的池化层，不过深度比上一个深了一些，最后几层也都是全连接层接一个softmax。为什么使用3×3卷积，是因为三个3×3卷积的有效感受野和7×7的感受野一致，而且更深、更加非线性，卷积层的参数也更加地少，所以速度更快也可以适当加深层数。<br>GoogleNet:没有使用FC层，参数量相比之前的大大减少，提出了Inception module结构，也就是NIN结构(network within a network)。但是原始的Inception module计算量非常大，所以在每一个分支加了1×1 conv “bottleneck”结构。googlenet网络结构中为了避免梯度消失，在中间的两个位置加了两个softmax损失，所以会有三个loss，整个网络的loss是通过三个loss乘上权重相加后得到。<br>inception结构的特点：<br>1）增加了网络的宽度，同时也提高了对于不同尺度的适应程度。<br>2）使用 1×1 卷积核对输入的特征图进行降维处理，这样就会极大地减少参数量，从而减少计算量。<br>3）在V3中使用了多个小卷积核代替大卷积核的方法，除了规整的的正方形，我们还有分解版本的 3×3 = 3×1 + 1×3，这个效果在深度较深的情况下比规整的卷积核更好。<br>4）发明了Bottleneck 的核心思想还是利用多个小卷积核替代一个大卷积核，利用 1×1 卷积核替代大的卷积核的一部分工作。也就是先1×1降低通道然后普通3×3然后再1×1回去。<br>ResNet：其基本思想是引入了能够跳过一层或多层的“shortcut connection”，即增加一个identity mapping（恒等映射），将原始所需要学的函数H(x)转换为F(x)+x，作者认为这两种表达的效果相同，但是优化的难度却并不相同。这个Residual block通过将shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的叠加，这个简单地加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度，提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。<br>DenseNet:ResNet是更一般的模型，DenseNet是更特化的模型。DenseNet用于图像处理可能比ResNet表现更好，本质是DenseNet更能和图像的信息分布特点匹配，是使用了多尺度的Kernel。densenet因为需要重复利用比较靠前的feature map，导致显存占用过大，但正是这种通道特征的concat造成densenet能更密集的连接。<br>SeNet：全称为Squeeze-and-Excitation Networks。核心思想就是去学习每个特征通道的重要程度，然后根据这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。这个给每一个特征层通道去乘以通过sigmoid得到的重要系数，其实和用bn层去观察哪个系数重要一样。</p>
<h2 id="图像检测"><a href="#图像检测" class="headerlink" title="图像检测"></a>图像检测</h2><p>分类任务关系整体，给出的是整张图片的内容描述，而检测则关注特定的物体目标，要求同时获得这一目标的类别信息和位置信息。相比于分类，检测给出的是对图片前景和背景的理解，我们需要从背景中分离出感兴趣的目标，并确定这一目标的描述（类别和位置）。因此，检测模型的输出是一个列表，列表的每一项使用一个数据组给出检测目标的类别和位置（常用矩形检测框的坐标表示）。<br>目前主流的目标检测算法主要是基于深度模型，大致可以分成两大类别：（1）One-Stage目标检测算法，这类检测算法不需要Region Proposal阶段，可以通过一个Stage直接产生物体的类别概率和位置坐标值，比较典型的算法有YOLO、SSD和CornerNet；（2）Two-Stage目标检测算法，这类检测算法将检测问题划分为两个阶段，第一个阶段首先产生候选区域（Region Proposals），包含目标大概的位置信息，然后第二个阶段对候选区域进行分类和位置精修，这类算法的典型代表有R-CNN、Fast R-CNN、Faster R-CNN等。目标检测模型的主要性能是检测准确度和速度，其中准确度主要考虑物体的定位及分类准确度。一般情况下，Two-Stage算法在准确度上有优势，而One-Stage算法在速度上有优势。不过随着研究的发展，两类算法都在两个方面做改进，均能在准确度及速度上取得较好结果。</p>
<h3 id="IOU的定义"><a href="#IOU的定义" class="headerlink" title="IOU的定义"></a>IOU的定义</h3><p><a href="https://blog.csdn.net/sinat_34474705/article/details/80045294?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">手写IOU</a><br>物体检测需要定位出物体的bounding box，对于bounding box的定位精度，存在一个定位精度评价公式：IOU。<br>IOU定义了两个bounding box的重叠度，如下图所示：<br><img src="/images/IOU.png" alt="IOU图像"><br>矩形框A、B的一个重合度计算公式为：</p>
<script type="math/tex; mode=display">IOU=\frac{A\cap B}{A\cup B}</script><p>就是矩形框A、B的重叠面积$S_I$占A、B并集的面积的比例：</p>
<script type="math/tex; mode=display">IOU=\frac{S_I}{S_A+S_B-S_I}</script><h3 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h3><p>在目标检测时一般会采取窗口滑动的方式，在图像上生成很多的候选框，把这些候选框进行特征提取送入到分类器，一般会得到一个得分，然后把这些得分全部排序。选取得分最高的那个框，接下来计算其他框与当前框的重合程度(IOU)，如果重合程度大于一定的阈值就删除，这样不停的迭代下去就会得到所有想要找到的目标物体的区域。</p>
<h3 id="Soft-NMS"><a href="#Soft-NMS" class="headerlink" title="Soft-NMS"></a>Soft-NMS</h3><p><a href="https://blog.csdn.net/lcczzu/article/details/86518615" target="_blank" rel="noopener">非极大值抑制算法改进</a></p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>算法摘要：首先输入一张图片，我们先定位出2000个物体候选框，然后采用CNN提取每个候选框中图片的特征向量，特征向量的维度是4096维，接着采用SVM算法对各个候选框中的物体进行分类识别。<br><img src="/images/RCNN.png" alt="R-CNN算法流程"><br>我们采用selective search算法搜索出候选框，由于搜索到的候选框是矩形的，而且大小各不相同，而CNN对输入的图片大小是固定的，因此需要对每个输入的候选框都要缩放到固定的大小。然而人工标注的图片中就只标注了一个正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此我们需要用IOU为2000个bounding box打标签，以便下一步CNN训练使用。在CNN阶段，如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域大于0.5，我们就把这个候选框标注成物体类别，否则我们就把它当做背景类别。最后，我们需要对上面预训练的CNN模型进行fine-tuning训练。假设需要检测的物体类别有N类，那么我们就需要将上面与训练的CNN模型的最后一层给替换点，替换成N+1个输出的神经元（还有一个背景），然后这一层直接采用参数随机初始化的方法，其他的网络层数不变。<br>R-CNN缺点：<br>1）耗时的selective search，对一帧图像，需要花费2s。<br>2）耗时的串行式CNN前向传播，对于每一个ROI，都需要经过一个AlexNet提取特征，为所有的ROI提供特征需要花费47s。<br>3）三个模块分别训练，并且在训练的时候，对于存储空间的消耗很大。<br><img src="/images/rcnn2.png" alt="RCNN算法流程框图"></p>
<h3 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h3><p><img src="/images/sppnet.png" alt="SPPNet网络流程框图"><br>算法特点：1）通过Spatial Pyramid Pooling解决了深度网络固定输入层尺寸的这个限制，使得网络可以享受不限制输入尺寸带来的好处。2）解决了RCNN速度慢的问题，不需要对每个Proposal(2000个左右)进行wrap或crop输入CNN提取feature map，只需要对整图提一次feature map，然后将proposal区域映射到卷积特征层得到全连接层的输入特征。<br><img src="/images/rcnn_vs_spp.png" alt="RCNN与SPPNet对比"><br>一、ROI在特征图上的对应的特征区域的维度不满足全连接层的输入要求怎么办？<br>事实上，CNN的卷积层不需要固定尺寸的图像，而是全连接层是需要固定大小的输入。根据Pooling规则，每个Pooling bin对应一个输出，所以最终的Pooling后的输出特征由bin的个数来决定。SPP网络就是分级固定bin的个数，调整bin的尺寸来实现多级Pooling固定输出。如图所示，SPP网络中最后一个卷积层的feature map维数为16x24，按照图中所示分为3级：<br><img src="/images/SPP.png" alt="SPP网络结构"><br>其中，第一级bin个数为1，最终对应的window大小为16x24；第二级bin个数为4个，最终对应的window大小为4x8；第三级bin个数为16个，最终对应的window大小为1x1.5(小数需要舍入处理)。通过融合各级bin的输出，最终每一个feature map经过SPP处理后，得到1+4+16维的feature map，经过融合后输入分类器。这样就可以在任意输入size和scale下获得固定的输出；不同的scale下网络可以提取不同尺度的特征，有利于分类。<br>二、原始图像的ROI如何映射到特征图？<br>下面将从感受野、感受野上坐标映射及原始图像的ROI如何映射三方面阐述。<br>1）感受野<br>在卷积神经网络中，感受野的定义是卷积神经网络每一层输出的特征图(feature map)的像素点在原始图像上映射的区域大小。</p>
<script type="math/tex; mode=display">output\ field\ size = (input\ field\ size - kernel size + 2*padding) / stride + 1</script><p>其中output field size是卷积层的输出，input field size是卷积层的输入，反过来卷积层的输入为:</p>
<script type="math/tex; mode=display">input\ field\ size = (output\ field\ size - 1) * stride - 2*padding + kernel size</script><p>2）感受野上的坐标映射<br>对于Convolution/Pooling Layer:</p>
<script type="math/tex; mode=display">p_i=s_i \cdot p_{i+1}+[(k_i-1)/2-padding]</script><p>对于Neuronlayer（ReLU/Sigmoid/…）:</p>
<script type="math/tex; mode=display">p_i=p_{i+1}</script><p>其中$p_i$为第$i$层感受野上的坐标，$s_i$为Stride的大小，$k_i$为感受野的大小。<br>何凯明在SPP-NET中使用的是简化版本，将公式中的Padding都设为$\left \lfloor k_i/2 \right \rfloor$，公式可进一步简化为：<script type="math/tex">p_i=s_i \cdot p_{i+1}</script><br>3）原始图像的ROI如何映射<br>SPP-NET是把原始ROI的左上角和右下角 映射到Feature Map上的两个对应点。 有了Feature Map上的两队角点就确定了对应的Feature Map 区域（下图中橙色）。<br><img src="/images/ROImap.png" alt="ROI映射过程"><br>左上角取$x’=\left \lfloor x/S \right \rfloor+1, y’=\left \lfloor y/S \right \rfloor+1$；右下角的点取$x’=\left \lceil x/S \right \rceil-1, y’=\left \lceil y/S \right \rceil-1$。其中S为坐标映射的简化计算版本，即$S=\prod_{0}^{i}s_i$。<br>ROI pooling具体操作：<br>1）根据输入image，将ROI映射到feature map对应位置。<br>2）将映射后的区域划分为相同大小的sections（sections数量和输出的维度相同）；<br>3）对每个sections进行max pooling操作。<br>这样就可以从不同大小的方框得到固定大小的相应的feature maps。<br><a href="https://zhuanlan.zhihu.com/p/73654026" target="_blank" rel="noopener">ROI原理</a></p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>Fast R-CNN针对R-CNN在训练时multi-state pipeline和训练的过程很耗时间和空间的问题进行了改进，主要改进为：<br>1）在最后一个卷积层加了一个ROI pooling layer。ROI Pooling layer首先可以将image中的ROI定位到feature map，然后用一个单层的SPP layer将这个feature map池化到固定大小的feature map后再传入全连接层。<br>2）损失函数使用多任务损失函数（multi-task loss），将边框回归直接加入到CNN网络进行训练。<br><img src="/images/frcnn.png" alt="Fast R-CNN流程框图"><br>首先还是采用selective search提取2000个候选框，然后对全图进行特征提取，接着使用一个ROI pooling layer在全体特征上获取每一个ROI对应的特征，再通过全连接层进行分类和检测框修正。即最后得到的ROI feature vector被分开，一个经过全连接层后用作softmax回归，用来分类，另一个经过全连接后用作bbox回归。需要注意的是，输入到后面ROI Pooling layer的feature map是在卷积层上的feature map上提取的，故整个特征提取过程只计算一次卷积。虽然在最开始也提取了大量的ROI，但他们还是作为整体输入到卷积网络的，最开始提取的ROI区域只是为了最后的bounding box回归时使用。<br><strong>联合训练</strong>：联合训练（Joint Training）指如何将分类和边框回归联合到一起在CNN阶段训练，主要难点是损失函数的设计。Fast-RCNN中，有两个输出层：第一个是针对每个ROI区域的分类概率预测，$p=(p_0, p_1, \cdots, p_K)$；第二个则是针对每个ROI区域坐标的偏移优化，$t^k = (t^k_x, t^k_y, t^k_w, t^k_h)$，$0 \le k \le K$是多类检测的类别序号。每个训练ROI都对应着真实类别$u$和边框回归目标$v=(v_x,v_y,v_w,v_h)$，对于类别$u$预测边框为$t^u=(t_x^u,t_y^u,t_w^u,t_h^u)$，使用多任务损失$L$来定义ROI上分类和边框回归的损失：</p>
<script type="math/tex; mode=display">L(p,u,t^u,v)=L_{cls}(p,u)+\lambda [u \ge 1]L_{loc}(t^u,v)</script><p>其中$L_{cls}(p,u)=-\log p_u$表示真实类别的log损失，当$u \ge 1$时，$[u \ge 1]$的值为1，否则为0。下面将重点介绍多任务损失中的边框回归部分（对应坐标偏移优化部分）。<br><strong>边框回归</strong>：假设对于类别$u$，在图片中标注的真实坐标和对应的预测值理论上两者越接近越好，相应的损失函数为：</p>
<script type="math/tex; mode=display">L_{loc}(t^u, v) = \sum_{i \in {x, y, w, h}} \text{smooth}_{L_1}(t_i^u- v_i)</script><script type="math/tex; mode=display">\text{smooth}_{L_1}(x) = \left \{ \begin{aligned} & 0.5x^2 & |x| \le 1 \\ &|x|-0.5 & \text{otherwise}\end{aligned} \right.</script><p>Fast-RCNN在上面用到的鲁棒$L_1$函数对外点比RCNN和SPP-NET中用的$L_2$函数更为鲁棒，该函数在$(-1, 1)$之间为二次函数，其他区域为线性函数。<br>存在问题：使用Selective Search提取Region Proposals，没有实现真正意义上的端到端，操作耗时。<br>采用$\text{smooth}_{L_1}$的原因：边框的预测是一个回归问题，通常可以选择平方损失函数（L2损失）$f(x)=x^2$，但这个损失对于比较大的误差的惩罚很高。可以采用稍微缓和一点绝对损失函数（L1损失）$f(x)=|x|$，这个函数在0点处导数不存在，因此可能会影响收敛。因此采用分段函数，在0点附近使用平方函数使得它更加平滑。</p>
<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p><img src="/images/faster rcnn.png" alt="Faster R-CNN流程图"><br>算法特点：1）提出了Region Proposal Network(RPN)，将Proposal阶段和CNN分类融合到一起，实现了一个完全的End-To-End的CNN目标检测模型。RPN可以快速提取高质量的Proposal，不仅加快了目标检测速度，还提高了目标检测性能。2）将Fast R-CNN和RPN放在同一个网络结构中训练，共享网络参数。</p>
<h4 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h4><p>Region Proposal Network(RPN)的核心思想是使用卷积神经网络直接产生Region Proposal，使用的方法本质上就是滑动窗口。RPN的设计比较巧妙，RPN只需要在最后的卷积层上滑动以便，借助Anchor机制和边框回归就可以得到多尺度多长宽比的Region Proposal。下图是RPN的网络结构图。<br><img src="/images/RPN.png" alt="RPN网络结构图"><br>给定输入图像（假设分辨率为$600*1000$），经过卷积操作得到最后一层卷积特征图（大小约为$40\times 60$）。在这个特征图上使用$3\times 3$的卷积核（滑动窗口）与特征图进行卷积，最后一层卷积层共有256个feature map，那么这个$3\times 3$的区域卷积后可以获得一个256维的特征向量，后边接cls layer和reg layer分别用于分类和边框回归（跟Fast R-CNN类似，只不过这里的类别只有目标和背景两个类别）。$3\times 3$滑窗对应的每个特征区域同时预测输入图像的3种尺度（128，256，512），3中长宽比（1：1，1：2，2：1）的Region Proposal，这种映射的机制称为Anchor。所以对于这个$40\times 60$的feature map，总共有约20000（$40\times 60\times 9$）个Anchor，也就是预测20000个Region Proposal。<br>Anchor是在原图上的区域而不是在特征图上<br><img src="/images/Anchor.png" alt="Anchor示例"><br>这样设计的好处是什么？虽然现在也是在用的滑动窗口策略，但是滑动串口操作是在卷积特征图上进行的，维度较原始图像降低了$16\times 16$倍；多尺度使用了9中Anchor，对应了三种尺度和三种长宽比，加上后边接了边框回归，所以几遍是这9种Anchor外的窗口也能得到一个跟目标比较接近的Region Proposal。<br><a href="https://www.okcode.net/article/65311" target="_blank" rel="noopener">anchor讲解</a></p>
<h4 id="RPN的损失函数"><a href="#RPN的损失函数" class="headerlink" title="RPN的损失函数"></a>RPN的损失函数</h4><p>损失函数的定义为：</p>
<script type="math/tex; mode=display">L({p_i}{t_i}) = \frac{1}{N_{cls}} \sum_i L_{cls}(p_i, p_i^*) +\lambda \frac{1}{N_{reg}} \sum_i p_i^* L_{reg}(t_i, t_i^*)</script><p>其中$i$表示一次Mini-Batch中Anchor的索引，$p_i$是Anchor $i$是否是一个物体，$L_{reg}$即为上面提到的$\text{smooth}_{L_1}(x)$函数，$N_{cls}$和$N_{reg}$是两个归一化项，分别表示Mini-Batch的大小和Anchor位置的数目。</p>
<h4 id="网络的训练"><a href="#网络的训练" class="headerlink" title="网络的训练"></a>网络的训练</h4><p>作者采用了4-step Alternating Training:<br>1) 用ImageNet模型初始化，独立训练一个RPN网络；<br>2) 仍然用ImageNet模型初始化，但是使用上一步RPN网络产生的Proposal作为输入，训练一个Fast-RCNN网络，至此，两个网络每一层的参数完全不共享；<br>3) 使用第二步的Fast-RCNN网络参数初始化一个新的RPN网络，但是把RPN、Fast-RCNN共享的那些卷积层的Learning Rate设置为0，也就是不更新，仅仅更新RPN特有的那些网络层，重新训练，此时，两个网络已经共享了所有公共的卷积层；<br>4) 仍然固定共享的那些网络层，把Fast-RCNN特有的网络层也加入进来，形成一个Unified Network，继续训练，Fine Tune Fast-RCNN特有的网络层，此时，该网络已经实现我们设想的目标，即网络内部预测Proposal并实现检测的功能。<br>在训练分类器和RoI边框修正时，步骤如下：<br>1）首先通过RPN生成约20000个anchor（40x60x9）<br>2）对20000个anchor进行第一次边框修正，得到修订边框后的proposal；<br>3）对超过图像边界的proposal的边进行clip，使得该proposal不超过图像范围；<br>4）忽略掉长或宽太小的proposal；<br>5）将所有的proposal按照前景分数从高到低排序，选取前12000个proposal；<br>6）使用阈值为0.7的NMS算法排除掉重叠的proposal；<br>7）针对上一步剩下的proposal，选取前2000个proposal进行分类和第二次边框修正。<br><img src="/images/fasterrcnn.png" alt="faster rcnn训练过程"></p>
<h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h3><p>ROI Align是在Mask R-CNN这篇论文里提出的一种区域特征聚集方式，很好地解决了ROI Pooling操作中两次量化造成的区域不匹配的问题。在检测阶段将ROI Pooling替换为ROI Align可以提升检测模型的准确性。<br>在常见的两级检测框架中，ROI Pooling的作用是根据候选框的位置坐标在特征图中将相应的区域池化为固定尺寸的特征图，以便后续的分类和bounding box回归操作。由于预选框的位置通常是由模型回归得到的，一般来讲都是浮点数，而池化后的特征图要求尺寸固定，故ROI Pooling这一操作存在两次量化的过程。1）将候选框的边界量化为整数点坐标值。2）将量化后的边界区域平均分割成$k\times k$个单元（bin），对每一个单元的边界进行量化。经过上述的两次量化，此时的候选框已经和最开始回归出来的位置存在一定的偏差，这个偏差会影响检测或分割的精确度。在论文里作者将它归结为“不匹配问题(misalignment)”。<br>下面我们用直观的例子具体分析一下上述区域不匹配问题。如图所示，这是一个Faster-RCNN检测框架。输入一张$800\times 800$的图片，图片上有一个$665\times 665$的包围框(框着一只狗)。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是ROI Pooling 直接将它量化成20。接下来需要把框内的特征池化$7\times 7$的大小，因此将上述包围框平均分割成$7\times 7$个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。<br><img src="/images/ROIAlign.png" alt="ROIAlign示例"><br>为了解决ROI Pooling的上述缺点，作者提出了ROI Align这一改进的方法。ROI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值，从而将整个特征聚集过程转化为一个连续的操作。ROI Align流程如下：<br>1）遍历每一个候选区域，保持浮点数边界不做量化；2）将候选区域分割成$k\times k$个单元，每个单元的边界也不做量化；3）在每个单元中计算出固定的四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化的操作。<br>需要注意的是，这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。事实上，ROI Align 在遍历取样点的数量上没有ROIPooling那么多，但却可以获得更好的性能，这主要归功于解决了misalignment的问题。<br><img src="/images/ROIAlign2.png" alt="ROIAlign示例2"><br><img src="/images/maskrcnn.jpg" alt="Mask R-CNN结构图"><br>注意的是，在Mask R-CNN中的ROI Align之后有一个“head”部分，主要作用是将ROI Align的输出维度扩大，这样在预测Mask时会更加精确。<br>在Mask Branch的训练环节，作者没有采用FCN式的SoftmaxLoss，反而是输出了K个Mask预测图（为每一个类都输出一张），并采用average binary cross-entropy loss训练，当然在训练Mask branch的时候，输出K个特征图中，也就是对应ground truth类别的哪一个特征图对Mask loss有贡献。也就是说， Mask RCNN定义为多任务损失为$L=L_{class}+L_{boxes}+L_{mask}$。<br>$L_{class}$与$L_{boxes}$与Faster RCNN没区别。$L_{mask}$中，假设一共有K个类别，则mask分割分支的输出维度是$K\times m\times m$，对于$m\times m$中的每个点，都会输出K个二值Mask(每个类别使用sigmoid输出)。需要注意的是，计算loss的时候，并不是每个类别的sigmoid输出都计算二值交叉熵损失，而是该像素属于哪个类，哪个类的sigmoid输出才要计算损失函数，并且在测试的时候，我们通过分类分支预测类别来选择相应的mask预测。这样，mask预测和分类分支预测就彻底解耦了。</p>
<h3 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h3><p><a href="https://www.aiuai.cn/aifarm887.html" target="_blank" rel="noopener">特征金字塔网络FPN</a><br>在物体检测里面，在有限的计算量的情况下，网络的深度（对应到感受野）与 stride 通常是一对矛盾的东西，常用的网络结构对应的 stride 一般会比较大（如 32），而图像中的小物体甚至会小于 stride 的大小，造成的结果就是小物体的检测性能急剧下降。传统解决这个问题的思路包括：<br>（1）多尺度训练和测试，有称图像金字塔，如图（a）所示。目前几乎所有在 ImageNet 和 COCO 检测任务上取得好成绩的方法都使用了图像金字塔方法。然而这样的方法由于很高的时间及计算量消耗，难以在实际中应用。<br>（2）特征分层，即每层分别预测对应的scale分辨率的检测结果，如图（c）所示。SSD检测框架采用了相似的思想，这样的方法的问题在于直接强行让不同层学习同样的语义信息，而对于卷积神经网络而言，不同的深度对应着不同层次的语义特征，浅层网络的分辨率高，学的更多是细节特征，深层网络分辨率低，学的更多是语义特征。<br><img src="/images/FPN.jpg" alt="FPN网络结构"><br>目前多尺度的物体检测主要面临的挑战是：<br>1）如何学习具有强语义信息的多尺度特征表示？<br>2）如何设计通用的特征表示来解决物体检测中的多个子问题？如object proposal,box localization,instance segmentation.<br>3）如何高效计算多尺度的特征表示？<br>本文针对这些问题，提出了特征金字塔FPN，如图（d）所示，网络直接在原来的按网络上做修改，每个分辨率的feature map引入后，将分辨率缩放两倍的feature map做element-wise相加的操作。通过这样的连接，每一层预测所用的feature map都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的feature map分别做对应分辨率的物体检测。这样保证了每一层都有合适的分辨率以及强语义特征。同事，由于此方法只是在原网络的基础上家里额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。<br>自下而上的路径<br>CNN的前馈计算就是自下而上的路径，特征图经过卷积核计算，通常是越变越小，也有一些特征层的输出和原来的大小一样，称为“相同网络阶段”。对于本文的特征金字塔，作者为每个阶段定义一个金字塔级别，然后选择每个阶段的最后一层的输出作为特征图的参考集，这是因为每个阶段的最深层应该具有最强的特征。具体来说，对于ResNets，作者使用了每个阶段的最后一个残差结构的特征激活输出。将这些残差模块输出表示为{C2, C3, C4, C5}，对应于conv2，conv3，conv4和conv5的输出，并且注意它们相对于输入图像具有{4, 8, 16, 32}像素的步长。考虑到内存占用，没有将conv1包含在金字塔中。<br>自上而下的路径和横向连接<br>自上而下的路径（the top-down pathway ）是如何去结合低层高分辨率的特征呢？方法就是，把更抽象，语义更强的高层特征图进行上采样，然后把该特征横向连接（lateral connections）至前一层的特征，因此高层特征得到加强。值得注意的是，横向连接的两层特征在空间尺寸上要相同，这样做主要是为了利用底层的定位细节信息。<br>如图（d）所示，把高层特征做2倍上采样（最近邻上采样），然后将其和对应的前一层特征结合（前一层要经过$1\times 1$的卷积核才能用，目的是改变channels,使之和后一层的channels相同），结合方式就是element-wise相加的操作。重复迭代该过程，直至生成$1\times 1$最精细的特征图。迭代开始阶段，作者在C5层后面加了一个$1\times 1$的卷积核来产生最粗略的特征图，最后，作者用$3\times 3$的卷积核去处理已经融合的特征图（为了消除上采样的混叠效应），以生成最后需要的特征图。{C2, C3, C4, C5}层对应的融合特征层为{P2, P3, P4, P5}，对应的层空间尺寸是相通的。<br>FPN的优点：<br>1）低层的特征经过卷积和高层的信息上采样之后进行融合，在卷积神经网络中高层的特征具有较强的语义信息，低层的特征具有结构信息，将高层和低层的信息进行结合，是可以增强特征的表达能力。2）将候选框产生和提取特征的位置分散到特征金字塔的每一层，这样可以增加小目标特征映射的分辨率，对最后的预测是有好处的。</p>
<h3 id="Retinanet"><a href="#Retinanet" class="headerlink" title="Retinanet"></a>Retinanet</h3><p>基于深度学习的目标检测算法有两类经典的结构：Two Stage 和 One Stage。<br>Two Stage：例如Faster-RCNN算法。第一级专注于proposal的提取，第二级对提取出的proposal进行分类和精确坐标回归。两级结构准确度较高，但因为第二级需要单独对每个proposal进行分类/回归，速度上就变慢。<br>One Stage：例如SSD，YOLO算法。此类算法摒弃了提取proposal的过程，只用一级就完成了识别/回归，虽然速度较快但准确率远远比不上两级结构。<br>产生精度差异的主要原因：类别失衡（Class Imbalance）。One Stage方法在得到特征图后，会产生密集的目标候选区域，而这些大量的候选区域中只有很少一部分是真正的目标，这样就造成了机器学习中经典的训练样本正负不平衡的问题。它往往会造成最终算出的training loss为占绝大多数但包含信息量却很少的负样本所支配，少样正样本提供的关键信息却不能在一般所用的training loss中发挥正常作用，从而无法得出一个能对模型训练提供正确指导的loss（而Two Stage方法得到proposal后，其候选区域要远远小于One Stage产生的候选区域，因此不会产生严重的类别失衡问题）。常用的解决此问题的方法就是负样本挖掘，或其它更复杂的用于过滤负样本从而使正负样本数维持一定比率的样本取样方法。该论文中提出了Focal Loss来对最终的Loss进行校正。<br>Focal Loss的目的：消除类别不平衡 + 挖掘难分样本<br>Focal Loss非常简单，就是在原有的交叉熵损失函数上增加了一个因子，让损失函数更加关注hard examples，以下是用于二值分类的交叉熵损失函数。其中$y\in{\pm1}$为类别真实标签，$p\in[0,1]$是模型预测的$y=1$的概率。</p>
<script type="math/tex; mode=display">CE(p,y)=
\begin{cases}
-log(p) & if\ y=1 \\
-log(1-p) & otherwise
\end{cases}</script><p>可以进行如下定义：</p>
<script type="math/tex; mode=display">p_t=
\begin{cases}
p & if\ y=1 \\
1-p & otherwise
\end{cases}</script><p>因此交叉熵可以写成如下形式，即如下loss曲线图中蓝色曲线所示，可以认为当模型预测得到的$p_t\ge 0.6$的样本为容易分类的样本，而$p_t$值预测较小的样本为hard examples，最后整个网络的loss就是所有训练样本经过模型预测得到的值的累加，因为hard examples通常为少数样本，所以虽然其对应的loss值较高，但是最后全部累加后，大部分的loss值来自于容易分类的样本，这样在模型优化的过程中就会将更多的优化放到容易分类的样本中，而忽略hard examples。</p>
<script type="math/tex; mode=display">CE(p,y)=CE(p_t)=-log(p_t)</script><p>对于这种类别不均衡问题常用的方法是引入一个权重因子$\alpha$，对于类别1的使用权重$\alpha$，对于类别-1使用权重$1-\alpha$，公式如下所示。但采用这种加权方式可以平衡正负样本的重要性，但无法区分容易分类的样本与难分类的样本。</p>
<script type="math/tex; mode=display">CE(p_t)=-\alpha_tlog(p_t)</script><p>因此论文中提出在交叉熵前增加一个调节因子$(1-p_t)^{\gamma}$，其中$\gamma$为focusing parameter，且$\gamma \ge 0$，其公式变为如下，当$\gamma$取不同数值时loss曲线如图1所示。通过图中可以看到，当$\gamma$越来越大时，loss函数在容易分类的部分其loss几乎为零，而$p_t$较小的部分（hard examples部分）loss值仍然较大，这样就可以保证在类别不平衡较大时，累加样本loss，可以让hard examples贡献更多的loss，从而可以在训练时给与hard examples部分更多的优化。</p>
<script type="math/tex; mode=display">FL(p_t)=-(1-p_t)^{\gamma}log(p_t)</script><p>在实际使用时，论文中提出在上述公式的基础上，增加一个$\alpha$平衡因子，可以产生一个轻微的精度提升，公式如下所示。</p>
<script type="math/tex; mode=display">CE(p_t)=-\alpha_t(1-p_t)^{\gamma}log(p_t)</script><p><img src="/images/focalloss.jpg" alt="focal loss曲线图"><br>下图是RetinaNet的网络结构，整个网络相对Faster-RCNN简单了很多，主要由ResNet+FPN+2xFCN子网络构成。<br><img src="/images/RetinaNet.jpg" alt="RetinaNet网络结构图"><br>首先RetinaNet的Backbone是由ResNet+FPN构成，输入图像经过Backbone的特征提取后，可以得到$P_3~P_7$特征图金字塔，其中下标$l$表示特征金字塔的层数（$P_l$特征图的分辨率比输入图像小$2^l$），得到的特征金字塔的每层$C=256$通道。<br>在得到特征金字塔后，对每层特征金字塔分别使用两个子网络（分类网络+检测框位置回归）。这两个子网络由RPN网络修改得到。<br>与RPN网络类似，也使用anchors来产生proposals。特征金字塔的每层对应一个anchor面积，为了产生更加密集的coverage，增加了三个面积比例$\begin{Bmatrix}2^0,2^{\frac{1}{2}},2^{\frac{2}{3}} \end{Bmatrix}$（即使用当前anchor对应的面积分别乘以相应的比例，形成三个尺度），然后anchors的长宽比仍为$\begin{Bmatrix}1:2,1:1,2:1  \end{Bmatrix}$，因此特征金字塔的每一层对应A = 9种Anchors。原始RPN网络的分类网络只是区分前景与背景两类，此处将其改为目标类别的个数K。<br>特征金字塔每层都相应的产生目标类别与位置的预测，最后再将其融合起来，同时使用NMS来得到最后的检测结果。</p>
<h3 id="Faster-R-CNN-FPN-Focal-loss"><a href="#Faster-R-CNN-FPN-Focal-loss" class="headerlink" title="Faster R-CNN+FPN+Focal loss"></a>Faster R-CNN+FPN+Focal loss</h3><p><a href="https://blog.csdn.net/qq_33547191/article/details/88695405" target="_blank" rel="noopener">Faster R-CNN+FPN结合细节</a><br><a href="https://github.com/jwyang/fpn.pytorch" target="_blank" rel="noopener">代码</a><br><a href="https://blog.csdn.net/dcxhun3/article/details/59055974" target="_blank" rel="noopener">网络结构</a><br><a href="https://www.cnblogs.com/leebxo/p/11291140.html" target="_blank" rel="noopener">优化方案</a></p>
<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><p>算法特点：1）将物体检测作为回归问题求解。基于一个单独的End-To-End网络，完成从原始图像的输入到物体位置和类别的输出，输入图像经过一次Inference，便能得到图像中所有物体的位置和其所属类别及相应的置信概率。2）YOLO网络借鉴GoogleNet网络结构，不同的是，YOLO未使用Inception Module，而是使用$1\times 1$卷积层（此处$1\times 1$卷积层的存在是为了跨通道信息整合）+ $3\times 3$卷积层简单代替。3）Fast YOLO使用9个卷积层代替YOLO的24个，网络更轻快，速度从YOLO的45fps提升到155fps，但是同时损失了检测准确率。4）使用全图作为Context信息，背景错误（把背景错认为物体）比较少。5）泛化能力强。在自然图像上训练好的结果在艺术作品中依然具有很好的效果。<br><img src="/images/Yolo.png" alt="YOLO网络结构"><br>一、大致流程<br>1）对于一个输入图像，首先将图像划分成$7\times 7$的网络。<br>2）对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）。<br>3）根据上一步可以预测出$7\times 7\times 2$个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。<br><img src="/images/Yolo2.png" alt="Yolo实例"><br>二、训练<br>1）预训练分类网络：在ImageNet 1000-class Competition Dataset预训练一个分类网络，这个网络时前文网络结构中前20个卷积网络+Average-Pooling Layer+Fully Connected Layer(此时网络的输入是$224\times 224$)。<br>2）训练检测网络：在预训练网络中增加卷积和全连接层可以改善性能。YOLO添加4个卷积层和2个全连接层，随机初始化权重。检测要求细粒度的视觉信息，所以把网络输入也从$224\times 224$变成$448\times 448$。一幅图像分成$7\times 7$个网格，某个物体的中心落在这个网格中此网络就负责预测这个物体。每个网络预测两个Bounding Box。网格负责类别信息，Bounding Box负责坐标信息（4个坐标信息及一个置信度），所以最后一层输出为$7\times 7\times (2\times (4+1) + 20) = 7\times 7\times 30$的维度。Bounding Box的坐标使用图像的大小进行归一化0-1，Confidence使用$P_r(Object) * IOU_{pred}^{truth}$计算，其中第一项表示是否有物体落在网格中，第二项表示预测的框和实际的框之间的IOU值。<br>3）损失函数的确定：损失函数的定义如下，损失函数的设计目标就是让坐标，置信度和类别这三个方面达到很好的平和。简单地全部采用Sum-Squared Error Loss来做这件事会有以下不足：a)8维的Localization Error和20维的Classification Error同等重要显然是不合理的；b)如果一个网格中没有Object（一幅图中这种玩个很多），那么就会将这些网络中的Box的COnfidence降低到0，相对于较少的有Object的网络，这种做法是Overpowering的，这会导致网络不稳定甚至发散。解决方案如下：<br><img src="/images/Yolo3.png" alt="Yolo的损失函数"><br>首先更重视8维的坐标预测，给这些损失前面赋予更大的Loss Weight，记为$\lambda_{coord}$，在Pascal VOC训练中取5（上图蓝色框）。对于没有Object的Bbox的Confidence Loss，赋予小的Loss Weight，记为$\lambda_{noobj}$，在Pascal VOC训练中取0.5（上图橙色框）。有Object的Bbox的Confidence Loss（上图红色框）和类别的Loss（上图紫色框）的Loss Weight正常取1。对于不同大小的Bbox预测中，相比于大Bbox预测偏一点，小Bbox预测偏一点更不能忍受。而Sum-Square Error Loss中对同样的偏移Loss是一样的。为了缓和这个问题，将Bbox的Width和Height取平方根代替原本的Height和Width。如下图所示：Small Bbox的横轴值较小，发生偏移时，反应到y轴上的Loss(下图绿色)比Big Bbox(下图红色)要大。一个网格预测多个Bbox，在训练时我们希望每个Object（Ground True box）只有一个Bbox专门负责（一个Object一个Bbox）。具体做法是与Ground True Box(Object)的IOU最大的Bbox负责该Ground True Box(Object)的预测。这种做法称作Bbox Predictor的Specialization（专职化）。每个预测器会对特定（Size,Aspect Ratio or Classed of Object）的Ground True Box预测的越来越好。<br>三、测试<br>1）计算每个Bbox的Class-Specific Confidence Score:每个网格预测的Class信息($Pr(Class_i|Object)$)和Bbox预测的Confidence信息($Pr(Object)\times IOU_{pred}^{truth}$)相乘，就得到每个Bbox的Class-Specific Confidence Score。</p>
<script type="math/tex; mode=display">Pr(Class_i|Object)\times Pr(Object)\times IOU_{pred}^{truth}=Pr(Class_i)\times IOU_{pred}^{truth}</script><p>2）进行Non-Maximum Suppression(NMS)：得到每个Bbox的Class-Specific Confidence Score以后，设置阈值，滤掉得分低的Bboxes，对保留的Bboxes进行NMS处理就得到最终的检测结果。<br>四、存在问题<br>1）YOLO对相互靠的很近的物体（挨在一起且中点都落在同一个格子上的情况），还有很小的群体检测效果不好，这是因为一个网络中只预测了两个框，并且只属于一类。<br>2）测试图像中，当同一类物体出现的不常见的长宽比和其他情况时泛化能力偏弱。<br>3）由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强。</p>
<h3 id="YOLOV2、YOLOV3"><a href="#YOLOV2、YOLOV3" class="headerlink" title="YOLOV2、YOLOV3"></a>YOLOV2、YOLOV3</h3><h4 id="YOLOV2-YOLO9000-更准、更快、更强"><a href="#YOLOV2-YOLO9000-更准、更快、更强" class="headerlink" title="YOLOV2/YOLO9000 更准、更快、更强"></a>YOLOV2/YOLO9000 更准、更快、更强</h4><p>YOLO v1对于bounding box的定位不是很好，在精度上比同类网络还有一定的差距。作者希望改进的方向是改善 recall，提升定位的准确度，同时保持分类的准确度。YOLO V2在V1基础上做出改进：<br>1）受到Faster RCNN方法的启发，引入了anchor。使用了K-Means方法，对anchor数量进行了讨论。<br>2）修改了网络结构，去掉了全连接层，改成了全卷积结构。<br>3）训练时引入了世界树（WordTree）结构，将检测和分类问题做成了一个统一的框架，并且提出了一种层次性联合训练方法，将ImageNet分类数据集和COCO检测数据集同时对模型训练。<br><strong>更准</strong><br>Batch Normalization<br>使用 Batch Normalization 对网络进行优化，让网络提高了收敛性，同时还消除了对其他形式的正则化（regularization）的依赖。通过对 YOLO 的每一个卷积层增加 Batch Normalization，最终使得 mAP 提高了2%，同时还使模型正则化。使用 Batch Normalization 可以从模型中去掉 Dropout，而不会产生过拟合。<br>High resolution classifier<br>目前业界标准的检测方法，都要先把分类器（classiﬁer）放在ImageNet上进行预训练。从 Alexnet 开始，大多数的分类器都运行在小于 $256\times 256$ 的图片上。而现在 YOLO 从 $224\times 224$ 增加到了 $448\times 448$，这就意味着网络需要适应新的输入分辨率。<br>为了适应新的分辨率，YOLO v2 的分类网络以 $448\times 448$ 的分辨率先在 ImageNet上进行微调，微调 10 个 epochs，让网络有时间调整滤波器（filters），好让其能更好的运行在新分辨率上，还需要调优用于检测的 Resulting Network。最终通过使用高分辨率，mAP 提升了4%。<br>Convolution with anchor boxes<br>YOLOV1包含有全连接层，从而能直接预测 Bounding Boxes 的坐标值。 Faster R-CNN 的方法只用卷积层与 Region Proposal Network 来预测 Anchor Box 偏移值与置信度，而不是直接预测坐标值。作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。<br>收缩网络让其运行在 $416\times 416$ 而不是 $448\times 448$。由于图片中的物体都倾向于出现在图片的中心位置，特别是那种比较大的物体，所以有一个单独位于物体中心的位置用于预测这些物体。YOLO 的卷积层采用 32 这个值来下采样图片，所以通过选择 $416\times 416$ 用作输入尺寸最终能输出一个 $13\times 13$ 的特征图。 使用 Anchor Box 会让精确度稍微下降，但用了它能让 YOLO 能预测出大于一千个框，同时 recall 达到88%，mAP 达到 69.2%。<br>Dimension clusters<br>Anchor boxes的宽高维度往往是精选的先验框（hand-picked priors）也就是说人工选定的先验框。虽然在训练过程中网络也会学习调整框的宽高维度，最终得到准确的bounding boxes。但是，如果一开始就选择了更好的、更有代表性的先验框维度，那么网络就更容易学到准确的预测位置。为了优化，在训练集的 Bounding Boxes 上跑一下 k-means聚类，来找到一个比较好的值。<br>Fine-Grained Features<br>YOLOv1在对于大目标检测有很好的效果，但是对小目标检测上，效果欠佳。为了改善这一问题，作者参考了Faster R-CNN和SSD的想法，在不同层次的特征图上获取不同分辨率的特征。作者将上层的(前面26×26)高分辨率的特征图（feature map）直接连到13×13的feature map上。把26×26×512转换为13×13×2048，并拼接在一起使整体性能提升1%。<br>Multi-Scale Training<br>和GoogleNet训练时一样，为了提高模型的鲁棒性（robust），在训练的时候使用多尺度的输入进行训练。YOLOv2 每迭代几次都会改变网络参数。每 10 个 Batch，网络会随机地选择一个新的图片尺寸，由于使用了下采样参数是 32，所以不同的尺寸大小也选择为 32 的倍数 {320，352,…,608}，最小 320x320，最大 608x608，网络会自动改变尺寸，并继续训练的过程。<br><strong>更快</strong><br>大多数目标检测的框架是建立在VGG-16上的，VGG-16在ImageNet上能达到90%的top-5，但是单张图片需要30.69 billion 浮点运算，YOLO2是依赖于DarkNet-19的结构，该模型在ImageNet上能达到91%的top-5，并且单张图片只需要5.58 billion 浮点运算，大大的加快了运算速度。<br>YOLOv2去掉YOLOv1的全连接层，同时去掉YOLO v1的最后一个池化层，增加特征的分辨率，修改网络的输入，保证特征图有一个中心点，这样可提高效率。并且是以每个anchor box来预测物体种类的。<br>作者将分类和检测分开训练，在训练分类时，以Darknet-19为模型在ImageNet上用随机梯度下降法（Stochastic gradient descent）跑了160epochs，跑完了160 epochs后，把输入尺寸从224×224上调为448×448，这时候学习率调到0.001，再跑了10 epochs， DarkNet达到了top-1准确率76.5%，top-5准确率93.3%。<br>在训练检测时，作者把分类网络改成检测网络，去掉原先网络的最后一个卷积层，取而代之的是使用3个3×3x1024的卷积层，并且每个新增的卷积层后面接1×1的卷积层，数量是我们要检测的类的数量。<br><strong>更强</strong><br>论文提出了一种联合训练的机制：使用识别数据集训练模型识别相关部分，使用分类数据集训练模型分类相关部分。<br>众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。所以在YOLOv1中，边界框的预测其实并不依赖于物体的标签，YOLOv2实现了在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。<br>作者选择在COCO和ImageNet数据集上进行联合训练，遇到的第一问题是两者的类别并不是完全互斥的，比如”Norfolk terrier”明显属于”dog”，所以作者提出了一种层级分类方法（Hierarchical classification），根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的词树（WordTree）如下图所示：<br><img src="/images/WordTree.jpg" alt="词树结构"><br>WordTree中的根节点为”physical object”，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个路径，然后计算路径上各个节点的概率之积。<br>在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是 ，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。</p>
<h4 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h4><p>改进之处：<br>1）多尺度预测<br>2）更好的基础网络（类ResNet）和分类器darknet-53<br>多尺度预测<br>原来的YOLO v2有一个层叫：passthrough layer，假设最后提取的feature map的size是$13\times 13$，那么这个层的作用就是将前面一层的$26\times 26$的feature map和本层的$13\times 13$的feature map进行连接，有点像ResNet。这样的操作也是为了加强YOLO算法对小目标检测的精确度。这个思想在YOLO v3中得到了进一步加强，在YOLO v3中采用类似FPN的上采样（upsample）和融合做法（最后融合了3个scale，其他两个scale的大小分别是$26\times 26$和$52\times 52$），在多个scale的feature map上做检测，对于小目标的检测效果提升还是比较明显的。虽然在YOLO v3中每个网格预测3个边界框，看起来比YOLO v2中每个grid cell预测5个边界框要少，但因为YOLO v3采用了多个尺度的特征融合，所以边界框的数量要比之前多很多。<br>darknet-53<br><img src="/images/darknet53.jpg" alt="YOLOv3框图"></p>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p><a href="https://blog.csdn.net/xiaohu2022/article/details/79833786?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">SSD模型</a><br>算法特点<br>1）SSD结合了YOLO中的回归思想和Faster R-CNN中的Anchor机制，使用全图各个位置的尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster-RCNN一样比较精准。<br>2）SSD的核心是在特征图上采用卷积核来预测一系列Default Bounding Boxes的类别、坐标偏移。为了提高检测准确率，SSD在不同尺度的特征图上进行预测。<br><img src="/images/SSD.png" alt="SSD网络结构"><br>一、模型结构<br>1、多尺度特征图（Multi-scale Feature Map For Detection）<br>在图像Base Network基础上，将Fc6，Fc7变为Conv6，Conv7两个卷积层，添加了一些卷积层（Conv8,Conv9,Conv10,Conv11），这些层的大小逐渐减小，可以进行多尺度预测。<br>2、卷积预测器（Convolutional Predictors For Detection）<br>每个新添加的卷积层和之前的部分卷积层，使用一系列的卷积核进行预测。对于一个大小为$m\times n$，$p$通道的卷积层，使用$3\times 3$的$p$通道卷积核作为基础预测元素进行预测，在某个位置上预测出一个值，该值可以是某一类别的得分，也可是相对于Default Bounding Boxes的偏移量，并且在图像中每个位置都将产生一个值。<br>3、默认框和比例（Default Boxes And Aspect Ratio）<br>在特征图的每个位置预测K个Box，对于每个Box，预测C个类别得分，以及相对于Default Bounding Box的4个偏移值，这样需要$(C+4)<em> k$个预测器，在$m </em> n$的特征图上将产生$(C+4)\times k\times m\times n$个预测值。这里，Default Bounding Box类似于Faster R-CNN中Anchor是，下图所示。<br><img src="/images/SSD2.png" alt="SSD默认框"><br>二、模型训练<br>1、监督学习的训练关键是人工标注的label，对于包含Default Box（在Faster R-CNN中叫做Anchor）的网络模型（如:YOLO,Faster R-CNN,MultiBox）关键点就是如何把标注信息（Ground True Box,Ground True Category）映射到（Default Box）上。<br>2、给定输入图像以及每个物体的Ground Truth，首先找到每个Ground True Box对应的Default Box中IOU最大的作为正样本。然后，在剩下的Default Box中找到那些与任意一个Ground truth Box的IOU大于0.5的Default Box作为正样本。其他的作为负样本（每个Default Box要么是正样本Box要么是负样本Box）。如上图中，两个Default Box与猫匹配，一个与狗匹配。在训练过程中，采用Hard Negative Mining 的策略（根据Confidence Loss对所有的Box进行排序，使正负例的比例保持在1:3） 来平衡正负样本的比率。<br>3、损失函数<br>与Faster-RCNN中的RPN是一样的，不过RPN是预测Box里面有Object或者没有，没有分类，SSD直接用的Softmax分类。Location的损失，还是一样，都是用Predict box和Default Box/Anchor的差 与Ground Truth Box和Default Box/Anchor的差进行对比，求损失。<br><img src="/images/SSDloss.png" alt="SSD loss求解"><br>其中，$x_{i,j}^p=1$表示 第i个Default Box与类别p的第j个Ground Truth Box相匹配，否则若不匹配的话，则$x_{i,j}^p=0$<br>4、Default Box的生成<br>对每一张特征图，按照不同的大小（Scale） 和长宽比（Ratio）生成生成k个默认框（Default Boxes）。<br>（1）Scale：每一个Feature Map中Default Box的尺寸大小计算如下：</p>
<script type="math/tex; mode=display">s_k=s_{min}+\frac{s_{max}-s_{min}}{m-1}(k-1), \quad k\in [1, m]</script><p>其中，$S_{min}$取值0.2，$s_{max}$取值0.95，意味着最低层的尺度是0.2，最高层的尺度是0.95。<br>（2）Ratio：使用不同的Ratio值$a_=1,2,3,\frac{1}{2},\frac{1}{3}$计算Default Box的宽度和高度：$w_k^a=s_k\sqrt{a_r},h_k^a=s_k/\sqrt{a_r}$。另外对于Ratio=1的情况，还增加了一个Default Box，这个Box的Scale为$s’_k=\sqrt{s_ks_k+1}$。也就是总共有6种不同的Default Box。<br>（3）Default Box中心：每个Default Box的中心位置设置成$(\frac{i+0.5}{|f_k|},\frac{j+0.5}{|f_k|})$，其中$|f_k|$表示第k个特征图的大小$i,j\in[0,|f_k|]$。<br>5）Data Augmentation：为了模型更加鲁棒，需要使用不同尺寸的输入和形状，作者对数据进行了多种方式的随机采样。</p>
<h3 id="mobile-net"><a href="#mobile-net" class="headerlink" title="mobile net"></a>mobile net</h3><h2 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h2><h3 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h3><h3 id="UNet"><a href="#UNet" class="headerlink" title="UNet"></a>UNet</h3><h2 id="目标追踪"><a href="#目标追踪" class="headerlink" title="目标追踪"></a>目标追踪</h2><h3 id="多目标跟踪介绍"><a href="#多目标跟踪介绍" class="headerlink" title="多目标跟踪介绍"></a>多目标跟踪介绍</h3><p>多目标跟踪，即MOT（Multi-Object Tracking），顾名思义，就是在一段视频中同时跟踪多个目标。MOT主要应用场景是安防监控和自动驾驶等，这些场景中我们往往需要对众多目标同时进行追踪。<br>而由于是多目标，自然就会产生新目标进入与旧目标消失的问题，这就是与单目标跟踪算法区别最大的一点。而由于这一点区别，也就导致跟踪策略的不同。在单目标跟踪中，我们往往会使用给定的初始框，在后续视频帧中对初始框内的物体进行位置预测。而多目标跟踪算法，大部分都是不考虑初始框的，原因就是上面的目标消失与产生问题。取而代之，在多目标跟踪领域常用的跟踪策略是TBD（Tracking-by-Detecton），又或者也可叫DBT（Detection-Based-Tracking）。即在每一帧进行目标检测，再利用目标检测的结果来进行目标跟踪，这一步我们一般称之为数据关联（Data Assoiation）。<br>这里自然引出了多目标跟踪算法的一种分类：TBD（Tracking-by-Detecton）与DFT（Detection-Free Tracking），也即基于检测的多目标跟踪与基于初始框无需检测器的多目标跟踪。基于初始化帧的跟踪，在视频第一帧中选择你的目标，之后交给跟踪算法去实现目标的跟踪。这种方式基本上只能跟踪你第一帧选中的目标，如果后续帧中出现了新的物体目标，算法是跟踪不到的。这种方式的优点是速度相对较快。缺点很明显，不能跟踪新出现的目标。基于目标检测的跟踪，在视频每帧中先检测出来所有感兴趣的目标物体，然后将其与前一帧中检测出来的目标进行关联来实现跟踪的效果。这种方式的优点是可以在整个视频中跟踪随时出现的新目标。TBD则是目前学界业界研究的主流。<br><img src="/images/TBDvsDFT.jpg" alt="TBD与DFT对比"><br>不得不提的是另一种多目标跟踪算法的分类方式：在线跟踪（Online）与离线跟踪（Offline）。上文提到，大家往往会使用数据关联来进行多目标跟踪。而数据关联的效果，与你能使用的数据是有着直接的关系的。在Online跟踪中，我们只能使用当前帧及之前帧的信息来进行当前帧的跟踪。而在Offline跟踪中则没有了这个限制，我们对每一帧的预测，都可以使用整个视频的信息，这样更容易获得一个全局最优解。两种方式各有优劣，一般视应用场合而定，Offline算法的效果一般会优于Online算法。而介于这两者之间，还有一种称之为Near-Online的跟踪方式，即可以部分利用未来帧的信息。下图形象解释了Online与Offline跟踪的区别。<br><img src="/images/OnlineAndOffline.jpg" alt="Online与Offline区别"></p>
<h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><p>Trajectory（轨迹）：一条轨迹对应这一目标在一段时间内的位置序列。<br>Tracklet（轨迹段）：形成Trajectory过程中的轨迹片段。完整的Trajectory是由属于同一物理目标的Tracklets构成。<br>ID switch（ID 切换）：又称ID sw.。对于同一个目标，由于跟踪算法误判，导致其ID发生切换的次数。跟踪算法中理想的ID switch应该为0.<br>对于多目标跟踪，最主要的评价指标就是MOTA。这个指标综合了三点因素：FP、FN、IDsw.。FP即False Postive，为误检测的目标数量；FN即False Negetive，为未检出的真实目标数量；IDsw.即同一目标发生ID切换的次数。</p>
<script type="math/tex; mode=display">MOTA=1-\frac{(FN+FP+IDSW)}{GT}\ \in(-\infty,1]</script><script type="math/tex; mode=display">MOTP=\frac{\sum_{t,i}d_{t,i}}{\sum_tc_t}</script><p>其中$c_t$表示帧t中的匹配数，并且$d_{t,i}$是假设i与分配的真实对象之间的边界框重叠。此度量标准很少考虑有关跟踪的信息，而侧重于检测的质量。</p>
<h3 id="SORT"><a href="#SORT" class="headerlink" title="SORT"></a>SORT</h3><p>现在多目标跟踪算法的效果，与目标检测的结果息息相关。在实际工程中，为了提高多目标跟踪的效果，可以从检测器入手，跟踪效果也会水涨船高。<br>SORT采用的是在线跟踪的方式，不使用未来帧的信息。在保持100fps以上的帧率的同时，也获得了较高的MOTA（在当时16年的结果中）。<br>多目标跟踪中SORT算法思想理解流程：<br>在跟踪之前，对所有目标已经完成检测，实现了特征建模过程。<br>1）对第一帧使用目标检测模型进行目标检测，得到第一帧中所有目标的分类和位置（假设有M个目标），并标注一个独有id。对每个目标初始化卡尔曼滤波跟踪器，预测每个目标在下一帧的位置；<br>2）对第二帧使用目标检测模型进行目标检测，得到第二帧中所有目标的分类和位置（假设有N个目标），求第一帧M个目标和第二帧N个目标两两目标之间的IOU，建立代价矩阵，使用匈牙利匹配算法得到IOU最大的唯一匹配（数据关联部分），再去掉匹配值小于iou_threshold的匹配对；<br>3）用第二帧中匹配到的目标的位置去更新卡尔曼跟踪器，计算第二帧时的卡尔曼增益Kk，状态估计值xk，估计误差协方差Pk。并输出状态估计值xk用来计算下一帧中的预测位置。对于本帧中没有匹配到的目标重新初始化卡尔曼滤波跟踪器；<br>后面每一帧图像都按第一帧和第二帧的做法进行类似处理即可。其中，卡尔曼跟踪器联合了历史跟踪记录，调节历史box与本帧box的残差，更好的匹配跟踪id。<br>匈牙利算法是一种寻找二分图的最大匹配的算法，在多目标跟踪问题中可以简单理解为寻找前后两帧的若干目标的匹配最优解的一种算法。而卡尔曼滤波可以看作是一种运动模型，用来对目标的轨迹进行预测，并且使用确信度较高的跟踪结果进行预测结果的修正。<br>SORT在以往二阶段匹配算法的基础上进行了创新。以往二阶段匹配算法是先使用匈牙利算法对相邻帧之间的目标进行匹配生成很多tracklets，之后使用这些tracklets进行二次匹配，以解决遮挡等问题引起的轨迹中断。但这种二阶段匹配方式弊端也很明显，因为这种方式先天地要求必须以Offline的方法进行跟踪，而无法做到Online。SORT将这种二阶段匹配算法改进为了一阶段方法，并且可以在线跟踪。具体而言，SORT引入了线性速度模型与卡尔曼滤波来进行位置预测，在无合适匹配检测框的情况下，使用运动模型来预测物体的位置。在数据关联的阶段，SORT使用的依旧是匈牙利算法逐帧关联，不过作者还引入了IOU（Intersection-Over-Union）距离。不过SORT用的是带权重的匈牙利算法，其实就是KM算法，用IOU距离作为权重（也叫cost矩阵）。作者代码里是直接用sklearn的linear_assignment实现。并且当IOU小于一定数值时，不认为是同一个目标，理论基础是视频中两帧之间物体移动不会过多。作者在代码中选取的阈值是0.3。<br>预测模型（卡尔曼滤波器）<br>作者近似地认为目标的不同帧间地运动是和其他物体及相机运动无关的线性运动。每一个目标的状态可以表示为：</p>
<script type="math/tex; mode=display">x=[u,v,s,r,\dot u,\dot v,\dot s]^T</script><p>其中u和v分别代表目标的中心坐标，而s和r分别代表目标边界框的比例（面积）和长宽比，长宽比被认为是常数，需要保持不变。<br>当进行目标关联时，使用卡尔曼滤波器，用上一帧中目标的位置信息预测下一帧中这个目标的位置。若上一帧中没有检测到下一帧中的某个目标，则对于这个目标，重新初始化一个新的卡尔曼滤波器。关联完成后，使用新关联的下一帧中该目标的位置来更新卡尔曼滤波器。<br>数据关联（匈牙利匹配）<br>SORT算法中的代价矩阵为上一帧的M个目标与下一帧的N个目标两两目标之间的IOU。当然，小于指定IOU阈值的指派结果是无效的（源码中阈值设置为0.3）。<br>此外，作者发现使用IOU能够解决目标的短时被遮挡问题。这是因为目标被遮挡时，检测到了遮挡物，没有检测到原有目标，假设把遮挡物和原有目标进行了关联。那么在遮挡结束后，因为在相近大小的目标IOU往往较大，因此很快就可以恢复正确的关联。这是建立在遮挡物面积大于目标的基础上的。</p>
<h3 id="deep-SORT"><a href="#deep-SORT" class="headerlink" title="deep SORT"></a>deep SORT</h3><p>之前的SORT算法使用简单的卡尔曼滤波处理逐帧数据的关联性以及使用匈牙利算法进行关联度量，这种简单的算法在高帧速率下获得了良好的性能。但由于SORT忽略了被检测物体的表面特征，因此只有在物体状态估计不确定性较低是才会准确，在Deep SORT中，我们使用更加可靠的度量来代替关联度量，并使用CNN网络在大规模行人数据集进行训练，并提取特征，已增加网络对遗失和障碍的鲁棒性。<br>状态估计<br>使用一个8维空间去刻画轨迹在某时刻的状态：</p>
<script type="math/tex; mode=display">(u,v,\gamma,h,\dot x,\dot y,\dot\gamma,\dot h)</script><p>使用一个kalman滤波器预测更新轨迹，该卡尔曼滤波器采用匀速模型和线性观测模型。通过卡尔曼估计对u,v,r,h进行估计，u，v是物体中心点的位置，r是长宽比，h是高。运动估计对于运动状态变化不是很剧烈和频繁的物体能取得比较好的效果。其观测变量为：</p>
<script type="math/tex; mode=display">(u,v,\gamma,h)</script><p>轨迹处理<br>对于每一个追踪目标，都有一个阈值ak用于记录轨迹从上一次成功匹配到当前时刻的时间（即连续没有匹配的帧数），我们称之为轨迹。当该值大于提前设定的阈值Amax则认为该轨迹终止，直观上说就是长时间匹配不上的轨迹则认为该轨迹已经结束。<br>在匹配时，对于没有匹配成功的目标都认为可能产生新的轨迹。但由于这些检测结果可能是一些错误警告，所以对这种情形新生成的轨迹标注状态’tentative’，然后观查在接下来的连续若干帧（论文中是3帧）中是否连续匹配成功，是的话则认为是新轨迹产生，标注为’confirmed’，否则则认为是假性轨迹,状态标注为’deleted’。<br>分配问题<br>在SORT中，我们直接使用匈牙利算法去解决预测的Kalman状态和新来的状态之间的关联度，现在我们需要将目标运动和表面特征信息相结合，通过融合这两个相似的测量指标。<br>Motion Metric<br>使用马氏距离来评测预测的Kalman状态和新来的状态：</p>
<script type="math/tex; mode=display">d^{(1)}(i,j)=(d_j-y_i)^TS_i^{-1}(d_j-y_i)</script><p>表示第j个detection和第i条轨迹之间的运动匹配度，其中$S_i$是轨迹由kalman滤波器预测得到的在当前时刻观测空间的协方差矩阵， $y_i$是轨迹在当前时刻的预测观测量， $d_i$时第j个detection的状态$(u,v,r,h)$<br>考虑到运动的连续性，可以通过该马氏距离对detections进行筛选，文中使用卡方分布的0.95分位点作为阈值$ t^(1)=0.4877$,我们可以定义一个门限函数。</p>
<script type="math/tex; mode=display">b_{i,j}^{(1)}=\mathbf{1}[d^{(1)}(i,j)\leq t^{(1)}]</script><p>Appearance Metric<br>当目标运动不确定性较低时，马氏距离是一个很好的关联度量，但在实际中，如相机运动时会造成马氏距离大量不能匹配，也就会使这个度量失效，因此，我们整合第二个度量标准，对每一个BBox检测框$d_j$我们计算一个表面特征描述子 $r_j,|r_j|=1$ , 我们会创建一个gallery用来存放最新的$L_k=100$个轨迹的描述子，即$R_k=\begin{Bmatrix}r_k^{(i)}\end{Bmatrix}_{k=1}^{L_k}$，然后我们使用第i个轨迹和第j个轨迹的最小余弦距离作为第二个衡量尺度！</p>
<script type="math/tex; mode=display">d^(2)(i,j)=min{1-r_j^Tr_k^{(i)}|r_k^(i)\in R_i}</script><p>当然，我们也可以用一个门限函数来表示</p>
<script type="math/tex; mode=display">b_{i,j}^{(2)}=\mathbf{d^{(2)}(i,j)\le t^{(2)}}</script><p>接着，我们把这两个尺度相融合为：</p>
<script type="math/tex; mode=display">c_{i,j}=\lambda d^{(1)}(i,j)+(1-\lambda)d^{(2)}(i,j)</script><script type="math/tex; mode=display">b_{i,j}=\prod_{m=1}^2b_{i,j}^{(m)}</script><p>总之，距离度量对于短期的预测和匹配效果很好，而表观信息对于长时间丢失的轨迹而言，匹配度度量的比较有效。超参数的选择要看具体的数据集，比如文中说对于相机运动幅度较大的数据集，直接不考虑运动匹配程度。<br>级联匹配<br>如果一条轨迹被遮挡了一段较长的时间，那么在kalman滤波器的不断预测中就会导致概率弥散。那么假设现在有两条轨迹竞争同一个目标，那么那条遮挡时间长的往往得到马氏距离更小，使目标倾向于匹配给丢失时间更长的轨迹，但是直观上，该目标应该匹配给时间上最近的轨迹。<br>导致这种现象的原因正是由于kalman滤波器连续预测没法更新导致的概率弥散。假设本来协方差矩阵是一个正态分布，那么连续的预测不更新就会导致这个正态分布的方差越来越大，那么离均值欧氏距离远的点可能和之前分布中离得较近的点获得同样的马氏距离值。<br>所以本文中才引入了级联匹配的策略将遮挡时间按等级分层，遮挡时间越小的匹配等级更高，即更容易被匹配。<br>首先是得到追踪框集合T和检测框集合D，设置最大的Amax为轨迹最大允许丢失匹配的帧数。通过计算上面的评价指标（两种度量的加权和）得到成本矩阵，再通过级联条件，设定阈值分别对外观和位置因素进行计算，满足条件则返回1，否则返回0。然后初始化匹配矩阵为空，初始化未匹配矩阵等于D。通过匈牙利算法，对于每个属于追踪框集合的元素T，在检测框里面查找成本最低且满足阈值过滤条件的检测框作为匹配结果，同时更新匹配矩阵和非匹配矩阵。<br>在匹配的最后阶段还对unconfirmed和age=1的未匹配轨迹进行基于IOU的匹配。这可以缓解因为表观突变或者部分遮挡导致的较大变化。当然有好处就有坏处，这样做也有可能导致一些新产生的轨迹被连接到了一些旧的轨迹上。但这种情况较少。<br>深度表观描述子<br>预训练的网络时一个在大规模ReID数据集上训练得到的，这个ReID数据集包含1261个人的1100000幅图像，使得学到的特征很适合行人跟踪。<br>然后使用该预训练网络作为基础网络，构建wide ResNet，用来提取bounding box的表观特征。</p>
<h3 id="匈牙利算法"><a href="#匈牙利算法" class="headerlink" title="匈牙利算法"></a>匈牙利算法</h3><p>（待补充）</p>
<h3 id="卡尔曼滤波器"><a href="#卡尔曼滤波器" class="headerlink" title="卡尔曼滤波器"></a>卡尔曼滤波器</h3><p>（待补充）</p>
<h2 id="姿态估计"><a href="#姿态估计" class="headerlink" title="姿态估计"></a>姿态估计</h2><p><a href="https://zhuanlan.zhihu.com/p/102457223" target="_blank" rel="noopener">姿态估计综述</a></p>
<h3 id="OpenPose"><a href="#OpenPose" class="headerlink" title="OpenPose"></a>OpenPose</h3><h2 id="注意机制"><a href="#注意机制" class="headerlink" title="注意机制"></a>注意机制</h2><p>注意力是人类大脑固有的一种信号处理机制。人类大脑通过快速从视觉信号中选择出需要重点关注的区域，也就是通常所说的注意力焦点，然后重点处理这些区域的细节信息。通过注意力机制可以利用有限的大脑资源从大量的信息中筛选出有价值的信息。<br>与传统的注意力机制不同，self-attention的查询（query）和键（key）属于同一个域，计算的是同一条语句（或同一张图片）中不同位置之间的注意力分配，从而提取该语句（或图片）的特征。何凯明首先将self-attention用于视觉任务中，提出了non-local network，来捕获图片（或视频）中的长程依赖（long-range dependency）。Self-attention机制在视觉任务解决了卷积神经网络的局部感受野问题，使得每个位置都可以获得全局的感受野。不过，由于在视觉任务中，像素数目极多，利用所有位置来计算每个位置的attention会导致巨大的计算和显存开销；另一方面，由于self-attention简单将图像当成一个序列进行处理，没有考虑不同位置之间的相对位置关系，使得所得到的attention丧失了图像的结构信息。<br>除了self-attention，视觉任务中另一类注意力机制为scale attention。与self-attention不同，scale attention基于每个位置本身的响应。就分类任务而言，每个位置的响应越大，则其对于最终的分类结果影响越大，那么这个位置本身的重要性就越强。根据响应大小有选择地对特征图进行强化或抑制，就可以在空间（或其他维度）上达到分配attention的目的。例如SENet，就相当于channel-wise的attention。这一类注意力机制仅仅基于图像中每个位置本身，对显著区域进行增强，非显著区域进行抑制，比self-attention机制更接近与人类视觉系统的注意力机制。</p>
<h3 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h3><p>普通卷积将特征图的每个位置作为中心点，对该位置及其周围的位置进行加权求和，得到新的特征图上该位置对应的滤波结果，对于边缘，必要时可以用0进行填充。这一操作可以有效提取图片的局部信息。随着网络加深，卷积层不断堆叠，每个位置的感受域也越来越大，网络提取到的特征也逐渐由一些low-level的特征，如颜色、纹理，转变到一些high-level的结构信息。但是，简单通过加深网络来获取全局感受域，所带来的计算开销是很大的，并且，更深的网络会带来更大的优化难度。<br>Self-attention操作可以有效地捕获不同位置之间的long-range dependency，每个位置的特征都由所有位置的加权求和得到，这里的权重就是attention weight。由此，每个位置都可以获取全局的感受域，并且不会造成特征图的退化（分辨率降低），这对于一些密集的预测任务，如语义分割、目标检测等，具有很大的优势。<br>下图展示了self-attention的网络结构。给定输入X，将两个1x1卷积分别作用于X上，得到的两个特征利用f(⋅)得到相关性矩阵，图中展示的f(⋅)为矩阵乘法。最后将相关性矩阵作用在原特征经过1x1卷积变换后的特征上。</p>
<script type="math/tex; mode=display">y_i=\frac{1}{C(x)}\sum_{\forall j}f(x_i,x_j)g(x_j)</script><p>其中$f(⋅)$为相关性函数，$g(⋅)$为变换函数，$x_i$为输入第i个位置的特征，$y_i$为第i个位置的输出特征，$C(x)$为归一化因子，一般采用总位置的个数。<br><img src="/images/selfattention.png" alt="self-attention结构"><br>在NLP任务中，对于Attention机制的整个计算过程，可以总结为以下三个过程：<br>首先根据 Query 与 Key 计算两者之间的相似性或相关性， 即 socre 的计算。通过一个 softmax 来对值进行归一化处理获得注意力权重值， 即$a_{i,j}$的计算。最后通过注意力权重值对value进行加权求和， 即$c_{i,j}$ 的计算。</p>
<script type="math/tex; mode=display">\alpha_{i,j} = \frac{e^{score(Query,Key(j))}}{\sum_{k=1}^te^{score(Query,Key(k))}}</script><script type="math/tex; mode=display">c_i=\sum_{j=1}^n\alpha_{i,j}h_j</script><p>self attention本质上是为序列中每个元素都分配一个权重系数，这也可以理解为软寻址。如果序列中每一个元素都以(K,V)形式存储，那么attention则通过计算Q和K的相似度来完成寻址。Q和K计算出来的相似度反映了取出来的V值的重要程度，即权重，然后加权求和就得到了attention值。<br>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。<br>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p>
<h3 id="scale-attention"><a href="#scale-attention" class="headerlink" title="scale attention"></a>scale attention</h3><p>Scale attention是另一种注意力机制，与self-attention不同，scale attention是只基于key context的，对图像中的显著性区域进行增强，其他区域相应的进行抑制，从而使得输出的特征具有更强的区分性。这一类注意力机制的代表工作包括，residual attention network，squeeze-and-excite network，gather-and-excite network以及CBAM。</p>
<h4 id="Bottom-up-and-top-down形式的scale-attention"><a href="#Bottom-up-and-top-down形式的scale-attention" class="headerlink" title="Bottom-up and top-down形式的scale attention"></a>Bottom-up and top-down形式的scale attention</h4><p>在分类网络中，网络深层比浅层更关注于被分类的物体，也就是图片的主体内容，这是因为，深层网络具有更大的视野域，可以看到更广的范围；而浅层网络只能看到每个位置及其邻域。因此，如果将网络较深层的信息作为一种mask，作用在较浅层的特征上，就能更好的增强浅层特征中对于最终分类结果有帮助的特征，抑制不相关的特征。如图5所示，将attention作为mask作用在原来特征上，得到的输出就会更加集中在对分类有帮助的区域上。<br><img src="/images/attention1.png" alt="attention作用机制"><br>文章提出一种bottom-up top-down的前向传播方法来得到图片的attention map，并且将其作用在原来的特征上，使得输出的特征有更强的区分度。图6展示了这种attention的计算方式。由于更大的视野域可以看到更多的内容，从而获得更多的attention信息，因此，作者设计了一条支路，通过快速下采样和上采样来提前获得更大的视野域，将输出的特征进行归一化后作用在原有的特征上，将作用后的特征以残差的形式加到原来的特征上，就完成了一次对原有特征的注意力增强。文章还提出了一个堆叠的网络结构，即residual attention network，中间多次采用这种attention模块进行快速下采样和上采样。<br><img src="/images/attention2.png" alt="Bottom-up注意力机制"></p>
<h4 id="Squeeze-and-excite形式的注意力"><a href="#Squeeze-and-excite形式的注意力" class="headerlink" title="Squeeze and excite形式的注意力"></a>Squeeze and excite形式的注意力</h4><p>与residual attention不同，squeeze-and-excite通过global pooling来获得全局的视野域，并将其作为一种指导的信息，也就是attention信息，作用到原来的特征上。<br>SENet提出了channel-wise的scale attention。特征图的每个通道对应一种滤波器的滤波结果，即图片的某种特定模式的特征。对于最终的分类结果，这些模式的重要性是不同的，有些模式更重要，因此其全局的响应更大；有些模式不相关，其全局的响应较小。通过对不同通道的特征根据其全局响应值，进行响应的增强或抑制，就可以起到在channel上进行注意力分配的作用。其网络结构如图7所示，首先对输入特征进行global pooling，即为squeeze阶段，对得到的特征进行线性变换，即为excite阶段，最后将变换后的向量通过广播，乘到原来的特征图上，就完成了对不同通道的增强或抑制。<br><img src="/images/SEblock.jpg" alt="SE Bolck结构"><br>作为SENet的一个延续，convolutional block attention module （CBAM）将SENet中提出的channel attention扩展到了spatial attention上，通过一个串行的支路，将channel attention和spatial attention连接起来，对原特征进行增强。其网络结构如图9所示，首先进行channel attention，对通道进行增强和抑制，这一过程与SENet的操作完全相同，然后在每个位置上进行通道的squeeze和excite操作，得到与原特征图一样分辨率的1通道spatial attention，再作用到原特征图上，即为spatial attention操作。最终的输出即为spatial attention module的输出。相比SENet，CBAM带来的性能提升有限，在该模块中其主要作用的还是channel attention模块。<br><img src="/images/attention3.png" alt="CBAM网络结构"></p>
<h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><h1 id="序列神经网络"><a href="#序列神经网络" class="headerlink" title="序列神经网络"></a>序列神经网络</h1><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>RNN相对传统的ANN网络结构实现了信息的保留，具有一定的记忆功能。可以将过去的信息应用到当前的任务中。<br>下图展示了RNN的闭环结构和开环结构<br><img src="/images/RNN.png" alt="闭环和开环结构"><br>完成当前任务如果仅仅需要短期的信息而不需要长期的信息可以使用RNN。但是如果任务需要更多的上下文信息，仅仅依靠少量的过去信息无法完成准确的预测。也就是过去信息和当前任务存在较大的跳动，甚至需要未来的信息才能完成预测。这时经典的RNN就无法满足需要,而需要特殊的时间序列模型LSTM。LSTMs 就是用来解决长期依赖问题，这类模型可以记住长期信息。<br><img src="/images/RNN2.png" alt="RNN和LSTM结构"><br>典的RNN模型中的激活函数可能就是一个简单的tanh函数，但是LSTMs引入了四个门结构，具有增加或者移除信息状态的功能。门限可以有选择的让信息通过，它是由sigmoid神经网络层和pointwise乘法操作构成的。sigmoid层输入数值0-1 代表可以通过的比例，输入为0时代表不允许通过，输出为1时代表允许全部通过。<br>1）forget gate 遗忘门<br><img src="/images/latm1.png" alt="遗忘门结构"><br>遗忘门输入是$h_{t-1}$，输出是$f_t$，$f_t$作用于$C_{t-1}$。当$f_t$为1时，代表保留该值，当$f_t$为0时，代表完全舍去该值。<br>2）input gate 输入门<br><img src="images/lstm2.png" alt="输入门结构"><br>存储什么样的新信息包括两步，第一步输入门决定哪些值可以更新，第二步tanh层创造候选向量。$i_t$是sigmoid函数输出结果表示是否产生输入，其取值范围是0-1。$\tilde{C_t}$是新产生的候选向量。忘记门$f_t$乘$C_{t-1}$：忘掉决定忘掉的早期信息，其结果加上$i_t * \tilde{C_t}$，候选向量通过$i_t$缩放后表示多大程度上更新状态值。<br><img src="/images/lstm3.png" alt="遗忘门和输入门组合"><br>通过忘记门和输入门的组合可以表达出这样的信息：多大程度上忘记旧的信息以及多大程度上更新新的信息。<br>3）Output gate 输出门<br><img src="/images/lstm4.png" alt="输出门"><br>首先sigmoid函数决定输出的缩放比例$o_t$，然后cell 状态通过tanh函数，其结果与$o_t$相乘。</p>
<h2 id="TCN"><a href="#TCN" class="headerlink" title="TCN"></a>TCN</h2><p>时序问题的建模大家一般习惯性的采用循环神经网络（RNN）来建模，这是因为RNN天生的循环自回归的结构是对时间序列的很好的表示。传统的卷积神经网络一般认为不太适合时序问题的建模，这主要由于其卷积核大小的限制，不能很好的抓取长时的依赖信息。一种特殊的卷积神经网络——时序卷积网络（Temporal convolutional network， TCN）与多种RNN结构相对比，发现在多种任务上TCN都能达到甚至超过RNN模型。</p>
<h3 id="因果卷积-Causal-Convolution"><a href="#因果卷积-Causal-Convolution" class="headerlink" title="因果卷积(Causal Convolution)"></a>因果卷积(Causal Convolution)</h3><p><img src="/images/causalConv.jpg" alt="因果卷积"><br>因果卷积可以用上图直观表示。 即对于上一层t时刻的值，只依赖于下一层t时刻及其之前的值。和传统的卷积神经网络的不同之处在于，因果卷积不能看到未来的数据，它是单向的结构，不是双向的。也就是说只有有了前面的因才有后面的果，是一种严格的时间约束模型，因此被成为因果卷积。</p>
<h3 id="膨胀卷积-Dilated-Convolution"><a href="#膨胀卷积-Dilated-Convolution" class="headerlink" title="膨胀卷积(Dilated Convolution)"></a>膨胀卷积(Dilated Convolution)</h3><p> 单纯的因果卷积还是存在传统卷积神经网络的问题，即对时间的建模长度受限于卷积核大小的，如果要想抓去更长的依赖关系，就需要线性的堆叠很多的层。为了解决这个问题，研究人员提出了膨胀卷积。如下图所示。<br> <img src="/images/dilatedConv.jpg" alt="膨胀卷积"><br> 和传统卷积不同的是，膨胀卷积允许卷积时的输入存在间隔采样，采样率受图中的d控制。 最下面一层的d=1，表示输入时每个点都采样，中间层d=2，表示输入时每2个点采样一个作为输入。一般来讲，越高的层级使用的d的大小越大。所以，膨胀卷积使得有效窗口的大小随着层数呈指数型增长。这样卷积网络用比较少的层，就可以获得很大的感受野。</p>
<script type="math/tex; mode=display">F(s)=\sum_{i=0}^{k-1}f(i)\cdot x_{s-d\cdot i}</script><p> <a href="https://zhuanlan.zhihu.com/p/50369448" target="_blank" rel="noopener">空洞卷积</a></p>
<h3 id="残差连接-Residual-Connections"><a href="#残差连接-Residual-Connections" class="headerlink" title="残差连接(Residual Connections)"></a>残差连接(Residual Connections)</h3><p> <img src="/images/residualConnection.jpg" alt="残差连接"><br> 残差链接被证明是训练深层网络的有效方法，它使得网络可以以跨层的方式传递信息。本文构建了一个残差块来代替一层的卷积。如上图所示，一个残差块包含两层的卷积和非线性映射，在每层中还加入了WeightNorm和Dropout来正则化网络。</p>
<h3 id="TCN优点和缺点"><a href="#TCN优点和缺点" class="headerlink" title="TCN优点和缺点"></a>TCN优点和缺点</h3><p>优点<br>（1）并行性。当给定一个句子时，TCN可以将句子并行的处理，而不需要像RNN那样顺序的处理。<br>（2）灵活的感受野。TCN的感受野的大小受层数、卷积核大小、扩张系数等决定。可以根据不同的任务不同的特性灵活定制。<br>（3）稳定的梯度。RNN经常存在梯度消失和梯度爆炸的问题，这主要是由不同时间段上共用参数导致的，和传统卷积神经网络一样，TCN不太存在梯度消失和爆炸问题。<br>（4）内存更低。RNN在使用时需要将每步的信息都保存下来，这会占据大量的内存，TCN在一层里面卷积核是共享的，内存使用更低。<br>缺点<br>（1）TCN 在迁移学习方面可能没有那么强的适应能力。这是因为在不同的领域，模型预测所需要的历史信息量可能是不同的。因此，在将一个模型从一个对记忆信息需求量少的问题迁移到一个需要更长记忆的问题上时，TCN 可能会表现得很差，因为其感受野不够大。<br>（2）论文中描述的TCN还是一种单向的结构，在语音识别和语音合成等任务上，纯单向的结构还是相当有用的。但是在文本中大多使用双向的结构，当然将TCN也很容易扩展成双向的结构，不使用因果卷积，使用传统的卷积结构即可。<br>（3）TCN毕竟是卷积神经网络的变种，虽然使用扩展卷积可以扩大感受野，但是仍然受到限制，相比于Transformer那种可以任意长度的相关信息都可以抓取到的特性还是差了点。TCN在文本中的应用还有待检验。</p>
<h1 id="图网络"><a href="#图网络" class="headerlink" title="图网络"></a>图网络</h1><p><a href="http://bbs.cvmart.net/articles/281/cong-cnn-dao-gcn-de-lian-xi-yu-qu-bie-gcn-cong-ru-men-dao-jing-fang-tong-qi" target="_blank" rel="noopener">图卷积网络</a><br>卷积是通过计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征提取。图卷积主要分为空域图卷积和谱域图卷积。<br>空域图卷积，其思想是将每个节点与其相邻节点的特征进行加权，使用领域来确定每个节点的加权平均范围（卷积范围），使用label策略来为参与卷积的节点分配权重，其层间传播矩阵为：</p>
<script type="math/tex; mode=display">X^{l+1}=\delta(W^lX^lA)</script><p>其中A是邻接矩阵，包含邻居节点信息，A中节点相连为1否则为0。通过矩阵乘法可以使得每个节点和其邻居节点特征进行聚合。W是权重矩阵，主要是对边进行加权。这样的传播方法是没有对邻接矩阵进行归一化，这使得A在传播过程中不断扩张（每次乘A会导致特征值越来越大）。此外，由于A对角线元素为0，每个节点自身在进行卷积时不能将自己的特征计算在内。因此，在空域图卷积中引入了谱域图卷积的思想来解决这两个问题。谱域图卷积主要借助图的拉普拉斯矩阵和傅里叶变换来进行的，最后得到的层间传播公式为：</p>
<script type="math/tex; mode=display">X^{l+1}=\delta(\land^{(-\frac{1}{2})}(A+I)\land^{(-\frac{1}{2})}X^lW^l)</script><p>其中矩阵$\land$用于对扩展邻接矩阵(A+I)进行归一化。<br>首先和单纯的空域图卷积相比，对邻接矩阵做了自环操作(A+I)，通过加单位矩阵的方法，使得中心节点自身的特征能参与卷积。然后通过对邻接矩阵实现归一化，解决了A随层数不断增长的问题。其他的部分就和公式(1)完全一致了。因此我们可以说公式(2)是利用谱域图卷积的思想来弥补了空域图卷积的缺点，本质上是两种卷积思想的结合。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Kay</p>
  <div class="site-description" itemprop="description">千里之行，始于足下</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kay</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
