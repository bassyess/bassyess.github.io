---
title: 快手实习记录
date: 2020-06-01 20:19:11
tags:
keywords: Please input password!!!
password: gaoxinkai
abstract: xxx
---
# 实习内容
2020-04-20至2020-06-01，参与快手的“蝴蝶消失”特效的算法研究，主要是去除用户上传的照片中人像区域部分，在蝴蝶飞舞的特效场景下，还原图片中被人体遮挡的区域。算法实现是通过人像分割算法，先找出区域中人体的mask区域，然后利用图像修复算法，脑补遮挡区域的信息，实现“蝴蝶消失”特效。

# 参考论文
## Image Inpainting with Learnable Bidirectional Attention Maps
[github链接](https://github.com/Vious/LBAM_Pytorch)
模型测试结果：该模型在简单场景下，人像mask修复区域还不错，但在稍微复杂点的场景（图片颜色变化较大）容易出现伪影，不合理的纹理信息等等。

## EdgeConnect: Structure Guided Image Inpainting using Edge Prediction
[github链接](https://github.com/knazeri/edge-connect)
模型测试结果：该模型效果并未达到论文中的修复结果，在稍微复杂点的场景中也会出现伪影和不合理的纹理信息等。

## Free-Form Image Inpainting with Gated Convolution(Deepfill V2)
[github链接](https://github.com/JiahuiYu/generative_inpainting)
测试结果：模型修复效果较好，在添加感知损失后，修复效果达到预期效果。

# 工作记录
## 模型loss修改
### GAN各类Loss整理
[常见的loss函数](http://www.twistedwg.com/2018/10/05/GAN_loss_summary.html)
不同loss损失的分析：
a.hinge-loss生成的纹理信息比较ok，偶然会出现块状，比较频繁; b.log-loss以及ls-loss的生成纹理信息相对来说比较差，可以避免斑块问题； c.relgan-loss也会出现块状， 通过对比来看，说明了损失会对生成的结果有很大的影响。
在原有的损失函数中引入perceptual loss和style loss，通过VGG16计算全图区域的perceptual loss能有效平滑修复区域的斑块信息，减少伪影现象。而添加style loss计算修复图像的风格变化，并没有带来有效的提升。故模型最终选择hinge loss和perceptual loss。
## 问题整理
### 图像处理/255.0和127.5-1
在代码中看到图像的2种处理方式：
1) img/255.0
2) img/127.5 - 1
第一种是对图像进行归一化，范围为[0, 1]，第二种也是对图像进行归一化，范围为[-1, 1]，这两种只是归一化范围不同，为了直观的看出2种区别，分别对图像进行两种处理：
![](/images/compare_init.png)
### 图像质量评估指标SSIM/PSNR
计算图像degrade后的质量，最直接的思路即比较degrade后的图像与真实图像（distortion-free）之间的差剖面，即可视误差，通过 visibility of errors 评价图像质量。
PSNR（Peak Signal to Noise Ratio），峰值信噪比，即峰值信号的能量与噪声的平均能量之比，通常表示的时候取 log 变成分贝（dB），由于 MSE 为真实图像与含噪图像之差的能量均值，而两者的差即为噪声，因此 PSNR 即峰值信号能量与 MSE 之比。定义式如下：
$$PSNR=10\log_{10}\frac{Max Value^2}{MSE}$$
```Python
def cal_psnr(im1,im2):
    mse = (np.abs(im1-im2)**2).mean()
    psnr = 10 * np.log10(255 * 255 / mse)
    return psnr
```
 一般地，针对 uint8 数据，最大像素值为 255,；针对浮点型数据，最大像素值为 1。
上面是针对灰度图像的计算方法，如果是彩色图像，通常有三种方法来计算。
（1）分别计算 RGB 三个通道的 PSNR，然后取平均值。
（2）计算 RGB 三通道的 MSE ，然后再除以 3 。
（3）将图片转化为 YCbCr 格式，然后只计算 Y 分量也就是亮度分量的 PSNR。

SSIM：Structural Similarity，结构相似性，也是一种全参考的图像质量评价指标，它分别从亮度、对比度、结构三方面度量图像相似性。SSIM取值范围[0,1]，值越大，表示图像失真越小。SSIM在图像去噪、图像相似度评价上是优于PSNR的。
![](/images/ssim.png)
每次计算的时候都从图片上取一个NxN的窗口，然后不断滑动窗口进行计算，最后取平均值作为全局的 SSIM。

###	衡量GAN的两个指标IS和FID
我们使用IS（inception score）和FID（Fréchet Inception Distance）这两个指标来评价不同的GAN模型，来衡量生成的图片的质量和多样性。
IS（Inception score）
IS用来衡量GAN网络的两个指标：生成图片的质量和多样性。
熵（entropy）可以用来描述随机性：如果一个随机变量是高度可预测的，那么它就有较低的熵；相反，如果它是高度不可预测，那么它就用较高的熵。
在GAN中，我们希望条件概率 P(y∣x) 可以被高度预测（x 表示给定的图片，y 表示这个图片包含的主要物体），也就是希望它的熵值较低。
因此，我们使用inception network（可以理解这是一个固定的分类网络）来对生成的图像进行分类。（这里都是针对ImageNet数据集而言）然后预测 $P(y∣x)$， 这里的 y 就是标签。用这个概率来反应图片的质量。综上，我们知道概率 $P(y∣x)$ 代表了图片的质量，概率越大，质量则越高。
1）图片质量：针对每一张生成的图片，已知的分类器应该很确信的知道它属于哪一类。而这可以用条件概率$p(y∣x)$来表示，它越大越好。而$p(y∣x)$ 熵应该是越小越好。
2）图片的多样性：我们这时候考虑的是标签的分布情况，我们希望标签分布均与，而不希望模型生成的都是某一类图片。这时候我们考虑的不是条件概率了，而是边缘概率，也就是$p(y)$，展开来写应该是$p(y1),p(y2),...,p(yn)$, 这里的n就是原训练数据的类数。我们希望$p(y1)=p(y2)=...=p(yn)=1/n$。 从熵的角度来说，我们希望$p(y)$的熵越大越好。
为了综合两个指标，我们使用 KL-divergence 并用下面的公式计算得到IS的值：
$$IS(G)=e^{E_{x\in p}D_{KL(p(y|x)||p(y))}}$$
其中，$D_{KL}$就是KL-divergence的计算公式。
IS缺点：当只产生一种物体的图像时，我们仍会认为这是均匀分布，而导致评价不正确。当模型坍塌时，结果就可能产生同样的图片。
FID（Fréchet Inception Distance ）
在计算FID中我们也同样使用inception network网络。它就是一个特征提取的深度网络，最后一层是一个pooling层，然后可以输出一张图像的类别。在计算FID时，我们去掉这个最后一层pooling层，得到的是一个2048维的高层特征，以下简称n维特征。我们继续简化一下，那么这个n维特征是一个向量。则有：对于我们已经拥有的真实图像，这个向量是服从一个分布的，（我们可以假设它是服从一个高斯分布）；对于那些用GAN来生成的n维特征它也是一个分布；我们应该立马能够知道了，GAN的目标就是使得两个分布尽量相同。假如两个分布相同，那么生成图像的真实性和多样性就和训练数据相同了。于是，现在的问题就是，怎么计算两个分布之间的距离呢？我们需要注意到这两个分布是多变量的，也就是前面提到的n维特征。也就是说我们计算的是两个多维变量分布之间的距离，数学上可以用Wasserstein-2 distance或者Frechet distance来进行计算。
假如一个随机变量服从高斯分布，这个分布可以用一个均值和方差来确定。那么两个分布只要均值和方差相同，则两个分布相同。我们就利用这个均值和方差来计算这两个单变量高斯分布之间的距离。但我们这里是多维的分布，我们知道协方差矩阵可以用来衡量两个维度之间的相关性。所以，我们使用均值和协方差矩阵来计算两个分布之间的距离。均值的维度就是前面n维特征的维度，也就是n维；协方差矩阵则是n*n的矩阵。
 
较低的FID意味着两个分布之间更接近，也就意味着生成图片的质量较高、多样性较好。FID对模型坍塌更加敏感。相比较IS来说，FID对噪声有更好的鲁棒性。因为假如只有一种图片时，FID这个距离将会相当的高。因此，FID更适合描述GAN网络的多样性。
FID和IS都是基于特征提取，也就是依赖于某些特征的出现或者不出现。但是他们都无法描述这些特征的空间关系。
###	Perceptual loss感知损失
Perceptual loss将真实图片卷积得到的feature与生成图片卷积得到的feature作比较，使得高层信息（内容和全局结构）接近，也就是感知的意思。
![](/images/perceptual_loss.png)
注意看蓝线加黑线，它就是内容损失，而且它作用在较低层特征层上的。然后同样看红线和黑线，它是风格损失，他是作用在从低到高所有特征层上。
上图左边是图像转移网络，右边是损失网络。这里对风格迁移变换yc相当于输入图像X，ys就是输入的风格图片，y算是content Target和Style Target的基本结合。在风格重建时，高层特征，全局结构，纹理明显。内容重建时，底层特征，边缘，颜色，细节信息多，效果越好。

## 工作记录
### 基于pytorch模型训练
1）训练基于deepfillv2的pytorch版本代码，在GAN的loss函数中加入hinge-loss和perceptual loss，模型的修复效果对比与之前效果增加。
2）在原始版本的基础上进行模型压缩工作，采用mobilenet的dw卷积方式替代普通的卷积，输入图片大小为256x256，模型的计算量由之前的25G减少为4G。
```Python
self.dw_conv = nn.Conv2d(input_dim, input_dim, kernel_size, stride, padding=conv_padding, dilation=dilation, groups=input_dim, bias=False)
self.pw_conv = nn.Conv2d(input_dim, output_dim, 1, 1, 0, bias=True)
```
3) 在4G模型压缩版本的基础上修改输入图片大小为128x128，模型的计算量降低为1G。
4）在1G模型的基础上，将卷积的通道数由48减少到36，模型的计算量降低为600M。
5）在指定的数据集上计算压缩模型的ssim、psnr和rmse指标。
数据集分布如下：
验证集(valid): 1000张图片，其中100张mask区域占比为(0,0.3)，400张图片mask区域占比为(0.3,0.5)，400张图片mask区域占比(0.5,0.7)，100张图片mask占比为(0.7,0.9)。
验证集(test1): 1000张图片，其中100张mask区域占比为(0,0.3)，400张图片mask区域占比为(0.3,0.5)，400张图片mask区域占比(0.5,0.7)，100张图片mask占比为(0.7,0.9)。
验证集(test2): 800张图片，其中80张mask区域占比为(0,0.3)，320张图片mask区域占比为(0.3,0.5)，320张图片mask区域占比(0.5,0.7)，80张图片mask占比为(0.7,0.9)。
![](/images/ssim_psnr1.png)
实验结果说明：
1）指标ssim和psnr越大，rmse越小，模型修复结果越好。从指标和效果展示上看，4G版本的效果最好，600M版本的效果稍差。
2）从tensorflow和pytorch版本的结果对比，tensorflow的模型效果优于pytorch训练的版本，后续的模型可考虑基于tensorflow优化。

### 基于tensorflow的模型训练
由于快手的修复算法先利用人像分割算法，获取人体的区域边界可能存在分割不准确的问题，导致后期模型修复困难。因此，在训练的过程中，尝试在人像mask的边界区域随机加入攻击，即在人像mask边界添加非常小的矩形框，造成人像分割的不完整性进行训练，使得模型训练的鲁棒性更好。
在tensorflow代码中，使用tf.layers.separable_conv2d实现可分离卷积，分别训练4G、1G、600M和300M模型。其中，300M模型是在600M模型的基础上将通道数由36降低为24。
由于模型压缩后，模型的计算量大大降低，故修复图像存在严重的伪影和不合理线条，采用知识蒸馏的方法，将未压缩的模型作为teacher网络，通过teacher模型生成的粗糙图、精细图和注意图监督压缩模型作为的学生网络，有效提高了模型训练的ssim、psnr和rmse指标。
![](/images/ssim_psnr2.png)
测试结果说明：
1）4G压缩模型指标最高，但修复结果在图像颜色变化复杂区域仍存在亮斑。
2）修改知识蒸馏中loss的参数权重，使得蒸馏效果相对于原始版本有明显的提高，蒸馏后图像修复的区域也相对平滑。
3）600M模型的压缩效果在背景单一的图像中修复效果和teacher模型近似，但在图像颜色变化复杂的区域中，修复效果还是会呈现明显的亮斑，尝试在蒸馏中添加teacher模型的感知损失来平滑图像修复的结果。
在蒸馏函数中，添加teacher模型的感知损失，模型的修复效果有明显的提高。
![](/images/ssim_psnr3.png)
在验证集中，添加30%的边界攻击进行验证，模型的测试字表降低了0.1个百分点左右。
在GeForce GTX 1080Ti服务器上进行速度计算（计算800张图片推理的平均速度）。
![](/images/ssim_psnr4.png)