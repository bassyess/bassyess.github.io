<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bassyess.github.io","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="千里之行，始于足下">
<meta property="og:type" content="website">
<meta property="og:title" content="Home">
<meta property="og:url" content="https://bassyess.github.io/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="千里之行，始于足下">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Kay">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://bassyess.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Home</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" class="post-title-link" itemprop="url">第十七章_模型压缩、加速及移动端部署</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:40:29 / 修改时间：12:40:51" itemprop="dateCreated datePublished" datetime="2020-03-03T12:40:29+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第十七章-模型压缩及移动端部署"><a href="#第十七章-模型压缩及移动端部署" class="headerlink" title="第十七章 模型压缩及移动端部署"></a>第十七章 模型压缩及移动端部署</h1><p>​    深度神经网络在人工智能的应用中，包括语音识别、计算机视觉、自然语言处理等各方面，在取得巨大成功的同时，这些深度神经网络需要巨大的计算开销和内存开销，严重阻碍了资源受限下的使用。本章总结了模型压缩、加速一般原理和方法，以及在移动端如何部署。</p>
<h2 id="17-1-模型压缩理解"><a href="#17-1-模型压缩理解" class="headerlink" title="17.1 模型压缩理解"></a>17.1 模型压缩理解</h2><p>​    模型压缩是指利用数据集对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署再受限的硬件环境中。</p>
<h2 id="17-2-为什么需要模型压缩和加速？"><a href="#17-2-为什么需要模型压缩和加速？" class="headerlink" title="17.2 为什么需要模型压缩和加速？"></a>17.2 为什么需要模型压缩和加速？</h2><p>（1）随着AI技术的飞速发展，越来越多的公司希望在自己的移动端产品中注入AI能力。</p>
<p>（2）对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要。  </p>
<p>（3）模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。</p>
<p>（4）复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。</p>
<p>（5）智能设备的流行提供了内存、CPU、能耗和宽带等资源，使得深度学习模型部署在智能移动设备上变得可行。<br>（6）高效的深度学习方法可以有效的帮助嵌入式设备、分布式系统完成复杂工作，在移动端部署深度学习有很重要的意义。   </p>
<h2 id="17-3-模型压缩的必要性及可行性"><a href="#17-3-模型压缩的必要性及可行性" class="headerlink" title="17.3 模型压缩的必要性及可行性"></a>17.3 模型压缩的必要性及可行性</h2><div class="table-container">
<table>
<thead>
<tr>
<th>必要性</th>
<th>首先是资源受限，其次在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。</th>
</tr>
</thead>
<tbody>
<tr>
<td>可行性</td>
<td>模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。论文<Predicting parameters in deep learning>提出，很多的深度神经网络仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。</td>
</tr>
<tr>
<td>最终目的</td>
<td>最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测</td>
</tr>
</tbody>
</table>
</div>
<h2 id="17-4-目前有哪些深度学习模型压缩方法？"><a href="#17-4-目前有哪些深度学习模型压缩方法？" class="headerlink" title="17.4 目前有哪些深度学习模型压缩方法？"></a>17.4 目前有哪些深度学习模型压缩方法？</h2><p>​    目前深度学习模型压缩方法主要分为更精细化模型设计、模型裁剪、核的稀疏化、量化、低秩分解、迁移学习等方法，而这些方法又可分为前端压缩和后端压缩。</p>
<h3 id="17-4-1-前端压缩和后端压缩对比"><a href="#17-4-1-前端压缩和后端压缩对比" class="headerlink" title="17.4.1 前端压缩和后端压缩对比"></a>17.4.1 前端压缩和后端压缩对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">对比项目</th>
<th style="text-align:center">前端压缩</th>
<th style="text-align:center">后端压缩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">含义</td>
<td style="text-align:center">不会改变原始网络结构的压缩技术</td>
<td style="text-align:center">会大程度上改变原始网络结构的压缩技术</td>
</tr>
<tr>
<td style="text-align:center">主要方法</td>
<td style="text-align:center">知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝</td>
<td style="text-align:center">低秩近似、未加限制的剪枝、参数量化、二值网络</td>
</tr>
<tr>
<td style="text-align:center">实现难度</td>
<td style="text-align:center">较简单</td>
<td style="text-align:center">较难</td>
</tr>
<tr>
<td style="text-align:center">是否可逆</td>
<td style="text-align:center">可逆</td>
<td style="text-align:center">不可逆</td>
</tr>
<tr>
<td style="text-align:center">成熟应用</td>
<td style="text-align:center">剪枝</td>
<td style="text-align:center">低秩近似、参数量化</td>
</tr>
<tr>
<td style="text-align:center">待发展应用</td>
<td style="text-align:center">知识蒸馏</td>
<td style="text-align:center">二值网络</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-2-网络剪枝"><a href="#17-4-2-网络剪枝" class="headerlink" title="17.4.2 网络剪枝"></a>17.4.2 网络剪枝</h3><p>深度学习模型因其<strong>稀疏性</strong>，可以被裁剪为结构精简的网络模型，具体包括结构性剪枝与非结构性剪枝。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>非结构化剪枝</td>
<td>通常是连接级、细粒度的剪枝方法，精度相对较高，但依赖于特定算法库或硬件平台的支持</td>
<td>Deep Compression [5], Sparse-Winograd [6] 算法等；</td>
</tr>
<tr>
<td>结构化剪枝</td>
<td>是filter级或layer级、粗粒度的剪枝方法，精度相对较低，但剪枝策略更为有效，不需要特定算法库或硬件平台的支持，能够直接在成熟深度学习框架上运行。</td>
<td>如局部方式的、通过layer by layer方式的、最小化输出FM重建误差的Channel Pruning [7], ThiNet [8], Discrimination-aware Channel Pruning [9]；全局方式的、通过训练期间对BN层Gamma系数施加L1正则约束的Network Slimming [10]；全局方式的、按Taylor准则对Filter作重要性排序的Neuron Pruning [11]；全局方式的、可动态重新更新pruned filters参数的剪枝方法 [12];<br /><a href="https://blog.csdn.net/baidu_31437863/article/details/84474847" target="_blank" rel="noopener">https://blog.csdn.net/baidu_31437863/article/details/84474847</a></td>
</tr>
</tbody>
</table>
</div>
<p>如果按剪枝粒度分，从粗到细，可分为中间隐含层剪枝、通道剪枝、卷积核剪枝、核内剪枝、单个权重剪枝。下面按照剪枝粒度的分类从粗（左）到细（右）。</p>
<p><img src="/img/ch17/剪枝粒度分类.png" alt=""></p>
<p>（a）层间剪枝   （b）特征图剪枝    （c）k*k核剪枝   （d）核内剪枝</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>单个权重粒度</td>
<td>早期 Le Cun[16]提出的 OBD(optimal brain damage)将网络中的任意权重参数都看作单个参数,能够有效地提高预测准确率,却不能减小运行时间;同时,剪枝代价过高,只适用于小网络</td>
</tr>
<tr>
<td>核内权重粒度</td>
<td>网络中的任意权重被看作是单个参数并进行随机非结构化剪枝,该粒度的剪枝导致网络连接不规整,需要通过稀疏表达来减少内存占用,进而导致在前向传播预测时,需要大量的条件判断和额外空间来标明零或非零参数的位置,因此不适用于并行计算</td>
</tr>
<tr>
<td>卷积核粒度与通道粒度</td>
<td>卷积核粒度与通道粒度属于粗粒度剪枝,不依赖任何稀疏卷积计算库及专用硬件;同时,能够在获得高压缩率的同时大量减小测试阶段的计算时间.由</td>
</tr>
</tbody>
</table>
</div>
<p>从剪枝目标上分类，可分为减少参数/网络复杂度、减少过拟合/增加泛化能力/提高准确率、减小部署运行时间/提高网络效率及减小训练时间等。</p>
<h3 id="17-4-3-典型剪枝方法对比"><a href="#17-4-3-典型剪枝方法对比" class="headerlink" title="17.4.3 典型剪枝方法对比"></a>17.4.3 典型剪枝方法对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">剪枝方法</th>
<th style="text-align:center">修剪对象</th>
<th style="text-align:center">修剪方式</th>
<th style="text-align:center">效果</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Deep Compression</td>
<td style="text-align:center">权重</td>
<td style="text-align:center">随机修剪</td>
<td style="text-align:center">50倍压缩</td>
</tr>
<tr>
<td style="text-align:center">Structured Pruning</td>
<td style="text-align:center">权重</td>
<td style="text-align:center">组稀疏+排他性稀疏</td>
<td style="text-align:center">性能提升</td>
</tr>
<tr>
<td style="text-align:center">Network Slimming</td>
<td style="text-align:center">特征图通道</td>
<td style="text-align:center">根据尺度因子修剪</td>
<td style="text-align:center">节省计算资源</td>
</tr>
<tr>
<td style="text-align:center">mProp</td>
<td style="text-align:center">梯度</td>
<td style="text-align:center">修剪幅值小的梯度</td>
<td style="text-align:center">加速</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-4-网络蒸馏"><a href="#17-4-4-网络蒸馏" class="headerlink" title="17.4.4 网络蒸馏"></a>17.4.4 网络蒸馏</h3><p>​    网络精馏是指利用大量未标记的迁移数据(transfer data),让小模型去拟合大模型,从而让小模型学到与大模型相似的函数映射.网络精馏可以看成在同一个域上迁移学习[34]的一种特例,目的是获得一个比原模型更为精简的网络,整体的框架图如图 4所示. </p>
<p><img src="/img/ch17/网络蒸馏.png" alt=""></p>
<h3 id="17-4-5-前端压缩"><a href="#17-4-5-前端压缩" class="headerlink" title="17.4.5 前端压缩"></a>17.4.5 前端压缩</h3><p>（1）知识蒸馏</p>
<p>​    一个复杂模型可由多个简单模型或者强约束条件训练得到。复杂模型特点是性能好，但其参数量大，计算效率低。小模型特点是计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中,使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。<br>（2）紧凑的模型结构设计<br>​    紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个3x3的卷积替换一个5x5的卷积、使用深度可分离卷积等等方式降低计算参数量。  目前很多网络基于模块化设计思想，在深度和宽度两个维度上都很大，导致参数冗余。因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。<br>（3）滤波器层面的剪枝<br>​    滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。  具体操作是在训练时使用稀疏约束（加入权重的稀疏正则项，引导模型的大部分权重趋向于0）。完成训练后，剪去滤波器上的这些 0 。</p>
<p>​    优点是简单，缺点是剪得不干净，非结构化剪枝会增加内存访问成本。</p>
<h3 id="17-4-6-后端压缩"><a href="#17-4-6-后端压缩" class="headerlink" title="17.4.6 后端压缩"></a>17.4.6 后端压缩</h3><p>（1）低秩近似<br>​    在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">事项</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">优点</td>
<td style="text-align:left">可以降低存储和计算消耗；<br />一般可以压缩2-3倍；精度几乎没有损失；</td>
</tr>
<tr>
<td style="text-align:left">缺点</td>
<td style="text-align:left">模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能 ；   <br />超参数的数量随着网络层数的增加呈线性变化趋势，例如中间层的特征通道数等等。 <br />随着模型复杂度的提升，搜索空间急剧增大。</td>
</tr>
</tbody>
</table>
</div>
<p>（2）未加限制的剪枝    </p>
<p>​    完成训练后，不加限制地剪去那些冗余参数。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>保持模型性能不损失的情况下，减少参数量9-11倍； <br />剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力；</td>
</tr>
<tr>
<td>缺点</td>
<td>极度依赖专门的运行库和特殊的运行平台，不具有通用性；<br /> 压缩率过大时，破坏性能；</td>
</tr>
</tbody>
</table>
</div>
<p>（3）参数量化    </p>
<p>​    神经网络的参数类型一般是32位浮点型，使用较小的精度代替32位所表示的精度。或者是将多个权重映射到同一数值，权重共享。<strong>量化其实是一种权值共享的策略</strong>。量化后的权值张量是一个高度稀疏的有很多共享权值的矩阵，对非零参数，我们还可以进行定点压缩，以获得更高的压缩率。 </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>模型性能损失很小，大小减少8-16倍；</td>
</tr>
<tr>
<td>缺点</td>
<td>压缩率大时，性能显著下降； <br />依赖专门的运行库，通用性较差；</td>
</tr>
<tr>
<td>举例</td>
<td>二值化网络：XNORnet [13], ABCnet with Multiple Binary Bases [14], <br />Bin-net with High-Order Residual Quantization [15], Bi-Real Net [16]；<br/>三值化网络：Ternary weight networks [17], Trained Ternary Quantization [18]；</td>
</tr>
</tbody>
</table>
</div>
<p>W1-A8 或 W2-A8量化： Learning Symmetric Quantization [19]；<br>INT8量化：TensorFlow-lite [20], TensorRT [21]；<br>其他（非线性）：Intel INQ [22], log-net, CNNPack [23] 等；<br>原文：<a href="https://blog.csdn.net/baidu_31437863/article/details/84474847" target="_blank" rel="noopener">https://blog.csdn.net/baidu_31437863/article/details/84474847</a> |<br>| 总结 | 最为典型就是二值网络、XNOR网络等。其主要原理就是采用1bit对网络的输入、权重、响应进行编码。减少模型大小的同时，原始网络的卷积操作可以被bit-wise运算代替，极大提升了模型的速度。但是，如果原始网络结果不够复杂（模型描述能力），由于二值网络会较大程度降低模型的表达能力。因此现阶段有相关的论文开始研究n-bit编码方式成为n值网络或者多值网络或者变bit、组合bit量化来克服二值网络表达能力不足的缺点。 |</p>
<p>（4）二值网络</p>
<p>​    相对量化更为极致，对于32bit浮点型数用1bit二进制数-1或者1表示，可大大减小模型尺寸。  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>网络体积小，运算速度快，有时可避免部分网络的overfitting</td>
</tr>
<tr>
<td>缺点</td>
<td>二值神经网络损失的信息相对于浮点精度是非常大；<br />粗糙的二值化近似导致训练时模型收敛速度非常慢</td>
</tr>
</tbody>
</table>
</div>
<p>（5）三值网络</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>相对于二值神经网络，三值神经网络(Ternary Weight Networks)在同样的模型结构下可以达到成百上千倍的表达能力提升;并且，在计算时间复杂度上，三元网络和二元网络的计算复杂度是一样的。<br />例如，对于ResNet-18层网络中最常出现的卷积核(3x3大小)，二值神经网络模型最多可以表达2的3x3次方(=512)种结构，而三元神经网络则可以表达3的3x3次方(=19683)种卷积核结构。在表达能力上，三元神经网络相对要高19683/512 = 38倍。因此，三元神经网络模型能够在保证计算复杂度很低的情况下大幅的提高网络的表达能力，进而可以在精度上相对于二值神经网络有质的飞跃。另外，由于对中间信息的保存更多，三元神经网络可以极大的加快网络训练时的收敛速度，从而更快、更稳定的达到最优的结果。</td>
</tr>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-6-低秩分解"><a href="#17-4-6-低秩分解" class="headerlink" title="17.4.6 低秩分解"></a>17.4.6 低秩分解</h3><p>基于低秩分解的深度神经网络压缩与加速的核心思想是利用矩阵或张量分解技术估计并分解深度模型中的原始卷积核．卷积计算是整个卷积神经网络中计算复杂 度 最 高 的 计 算 操 作，通 过 分 解４Ｄ 卷积核张量，可以有效地减少模型内部的冗余性．此外对于２Ｄ的全 连 接 层 矩 阵 参 数，同样可以利用低秩分解技术进行处理．但由于卷积层与全连接层的分解方式不同，本文分别从卷积层和全连接层２个不同角度回顾与分析低秩分解技术在深度神经网络中的应用.</p>
<p>在２０１３年，Ｄｅｎｉｌ等人［５７］从理论上利用低秩分解的技术并分析了深度神经网络存在大量的冗余信<br>息，开创了基于低秩分解的深度网络模型压缩与加速的新思路．如图７所示，展示了主流的张量分解后卷积 计 算．</p>
<p><img src=".\img\ch17\低秩分解模型压缩加速.jpg" alt=""></p>
<p>(出自《深度神经网络压缩与加速综述》)</p>
<h3 id="17-4-7-总体压缩效果评价指标有哪些？"><a href="#17-4-7-总体压缩效果评价指标有哪些？" class="headerlink" title="17.4.7 总体压缩效果评价指标有哪些？"></a>17.4.7 总体压缩效果评价指标有哪些？</h3><p>​    网络压缩评价指标包括运行效率、参数压缩率、准确率.与基准模型比较衡量性能提升时,可以使用提升倍数(speedup)或提升比例(ratio)。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评价指标</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率</td>
<td>目前,大部分研究工作均会测量 Top-1 准确率,只有在 ImageNet 这类大型数据集上才会只用 Top-5 准确率.为方便比较</td>
</tr>
<tr>
<td>参数压缩率</td>
<td>统计网络中所有可训练的参数,根据机器浮点精度转换为字节(byte)量纲,通常保留两位有效数字以作近似估计.</td>
</tr>
<tr>
<td>运行效率</td>
<td>可以从网络所含浮点运算次数(FLOP)、网络所含乘法运算次数(MULTS)或随机实验测得的网络平均前向传播所需时间这 3 个角度来评价</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-8-几种轻量化网络结构对比"><a href="#17-4-8-几种轻量化网络结构对比" class="headerlink" title="17.4.8 几种轻量化网络结构对比"></a>17.4.8 几种轻量化网络结构对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络结构</th>
<th style="text-align:center">TOP1 准确率/%</th>
<th style="text-align:center">参数量/M</th>
<th style="text-align:center">CPU运行时间/ms</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MobileNet V1</td>
<td style="text-align:center">70.6</td>
<td style="text-align:center">4.2</td>
<td style="text-align:center">123</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNet(1.5)</td>
<td style="text-align:center">69.0</td>
<td style="text-align:center">2.9</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNet(x2)</td>
<td style="text-align:center">70.9</td>
<td style="text-align:center">4.4</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">MobileNet V2</td>
<td style="text-align:center">71.7</td>
<td style="text-align:center">3.4</td>
<td style="text-align:center">80</td>
</tr>
<tr>
<td style="text-align:center">MobileNet V2(1.4)</td>
<td style="text-align:center">74.7</td>
<td style="text-align:center">6.9</td>
<td style="text-align:center">149</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-9-网络压缩未来研究方向有哪些？"><a href="#17-4-9-网络压缩未来研究方向有哪些？" class="headerlink" title="17.4.9 网络压缩未来研究方向有哪些？"></a>17.4.9 网络压缩未来研究方向有哪些？</h3><p>网络剪枝、网络精馏和网络分解都能在一定程度上实现网络压缩的目的.回归到深度网络压缩的本质目的上,即提取网络中的有用信息,以下是一些值得研究和探寻的方向.<br>(1) 权重参数对结果的影响度量.深度网络的最终结果是由全部的权重参数共同作用形成的,目前,关于单个卷积核/卷积核权重的重要性的度量仍然是比较简单的方式,尽管文献[14]中给出了更为细节的分析,但是由于计算难度大,并不实用.因此,如何通过更有效的方式来近似度量单个参数对模型的影响,具有重要意义.<br>(2) 学生网络结构的构造.学生网络的结构构造目前仍然是由人工指定的,然而,不同的学生网络结构的训练难度不同,最终能够达到的效果也有差异.因此,如何根据教师网络结构设计合理的网络结构在精简模型的条件下获取较高的模型性能,是未来的一个研究重点.<br>(3) 参数重建的硬件架构支持.通过分解网络可以无损地获取压缩模型,在一些对性能要求高的场景中是非常重要的.然而,参数的重建步骤会拖累预测阶段的时间开销,如何通过硬件的支持加速这一重建过程,将是未来的一个研究方向.<br>(4) 任务或使用场景层面的压缩.大型网络通常是在量级较大的数据集上训练完成的,比如,在 ImageNet上训练的模型具备对 1 000 类物体的分类,但在一些具体场景的应用中,可能仅需要一个能识别其中几类的小型模型.因此,如何从一个全功能的网络压缩得到部分功能的子网络,能够适应很多实际应用场景的需求.<br>(5) 网络压缩效用的评价.目前,对各类深度网络压缩算法的评价是比较零碎的,侧重于和被压缩的大型网络在参数量和运行时间上的比较.未来的研究可以从提出更加泛化的压缩评价标准出发,一方面平衡运行速度和模型大小在不同应用场景下的影响;另一方面,可以从模型本身的结构性出发,对压缩后的模型进行评价. </p>
<p>（出自《深度网络模型压缩综述》）</p>
<h2 id="17-5-目前有哪些深度学习模型优化加速方法？"><a href="#17-5-目前有哪些深度学习模型优化加速方法？" class="headerlink" title="17.5 目前有哪些深度学习模型优化加速方法？"></a>17.5 目前有哪些深度学习模型优化加速方法？</h2><p><a href="https://blog.csdn.net/nature553863/article/details/81083955" target="_blank" rel="noopener">https://blog.csdn.net/nature553863/article/details/81083955</a></p>
<h3 id="17-5-1-模型优化加速方法"><a href="#17-5-1-模型优化加速方法" class="headerlink" title="17.5.1 模型优化加速方法"></a>17.5.1 模型优化加速方法</h3><p>模型优化加速能够提升网络的计算效率，具体包括：<br>（1）Op-level的快速算法：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；<br>（2）Layer-level的快速算法：Sparse-block net [1] 等；<br>（3）优化工具与库：TensorRT (Nvidia), Tensor Comprehension (Facebook) 和 Distiller (Intel) 等；   </p>
<p>原文：<a href="https://blog.csdn.net/nature553863/article/details/81083955" target="_blank" rel="noopener">https://blog.csdn.net/nature553863/article/details/81083955</a>   </p>
<h3 id="17-5-2-TensorRT加速原理"><a href="#17-5-2-TensorRT加速原理" class="headerlink" title="17.5.2 TensorRT加速原理"></a>17.5.2 TensorRT加速原理</h3><p><a href="https://blog.csdn.net/xh_hit/article/details/79769599" target="_blank" rel="noopener">https://blog.csdn.net/xh_hit/article/details/79769599</a></p>
<p>​    在计算资源并不丰富的嵌入式设备上，TensorRT之所以能加速神经网络的的推断主要得益于两点：</p>
<ul>
<li><p>首先是TensorRT支持int8和fp16的计算，通过在减少计算量和保持精度之间达到一个理想的trade-off，达到加速推断的目的。</p>
</li>
<li><p>更为重要的是TensorRT对于网络结构进行了重构和优化，主要体现在一下几个方面。</p>
<p>(1) TensorRT通过解析网络模型将网络中无用的输出层消除以减小计算。</p>
<p>(2) 对于网络结构的垂直整合，即将目前主流神经网络的Conv、BN、Relu三个层融合为了一个层，例如将图1所示的常见的Inception结构重构为图2所示的网络结构。</p>
<p>(3) 对于网络结构的水平组合，水平组合是指将输入为相同张量和执行相同操作的层融合一起，例如图2向图3的转化。</p>
</li>
</ul>
<p><img src="/img/ch17/tensorRT1.png" alt=""></p>
<p><img src="/img/ch17/tensorRT2.png" alt=""></p>
<p><img src="/img/ch17/tensorRT3.png" alt=""></p>
<p>​    以上3步即是TensorRT对于所部署的深度学习网络的优化和重构，根据其优化和重构策略，第一和第二步适用于所有的网络架构，但是第三步则对于含有Inception结构的神经网络加速效果最为明显。</p>
<p>​    Tips: 想更好地利用TensorRT加速网络推断，可在基础网络中多采用Inception模型结构，充分发挥TensorRT的优势。</p>
<h3 id="17-5-3-TensorRT如何优化重构模型？"><a href="#17-5-3-TensorRT如何优化重构模型？" class="headerlink" title="17.5.3 TensorRT如何优化重构模型？"></a>17.5.3 TensorRT如何优化重构模型？</h3><div class="table-container">
<table>
<thead>
<tr>
<th>条件</th>
<th>方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>若训练的网络模型包含TensorRT支持的操作</td>
<td>1、对于Caffe与TensorFlow训练的模型，若包含的操作都是TensorRT支持的，则可以直接由TensorRT优化重构</td>
</tr>
<tr>
<td></td>
<td>2、对于MXnet, PyTorch或其他框架训练的模型，若包含的操作都是TensorRT支持的，可以采用TensorRT API重建网络结构，并间接优化重构；</td>
</tr>
<tr>
<td>若训练的网络模型包含TensorRT不支持的操作</td>
<td>1、TensorFlow模型可通过tf.contrib.tensorrt转换，其中不支持的操作会保留为TensorFlow计算节点；</td>
</tr>
<tr>
<td></td>
<td>2、不支持的操作可通过Plugin API实现自定义并添加进TensorRT计算图；</td>
</tr>
<tr>
<td></td>
<td>3、将深度网络划分为两个部分，一部分包含的操作都是TensorRT支持的，可以转换为TensorRT计算图。另一部则采用其他框架实现，如MXnet或PyTorch；</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-5-4-TensorRT加速效果如何？"><a href="#17-5-4-TensorRT加速效果如何？" class="headerlink" title="17.5.4 TensorRT加速效果如何？"></a>17.5.4 TensorRT加速效果如何？</h3><p>以下是在TitanX (Pascal)平台上，TensorRT对大型分类网络的优化加速效果：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Network</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Framework/GPU:TitanXP</th>
<th style="text-align:center">Avg.Time(Batch=8,unit:ms)</th>
<th style="text-align:center">Top1 Val.Acc.(ImageNet-1k)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TensorFlow</td>
<td style="text-align:center">24.1</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">MXnet</td>
<td style="text-align:center">15.7</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">12.1</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">int8</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">0.7226</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TensorFlow</td>
<td style="text-align:center">36.7</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">MXnet</td>
<td style="text-align:center">25.8</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">19.3</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">int8</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">9</td>
<td style="text-align:center">0.7574</td>
</tr>
</tbody>
</table>
</div>
<h2 id="17-6-影响神经网络速度的4个因素（再稍微详细一点）"><a href="#17-6-影响神经网络速度的4个因素（再稍微详细一点）" class="headerlink" title="17.6 影响神经网络速度的4个因素（再稍微详细一点）"></a>17.6 影响神经网络速度的4个因素（再稍微详细一点）</h2><ol>
<li><p>FLOPs(FLOPs就是网络执行了多少multiply-adds操作)；  </p>
</li>
<li><p>MAC(内存访问成本)；   </p>
</li>
<li><p>并行度(如果网络并行度高，速度明显提升)；   </p>
</li>
<li><p>计算平台(GPU，ARM)   </p>
</li>
</ol>
<h2 id="17-7-压缩和加速方法如何选择？"><a href="#17-7-压缩和加速方法如何选择？" class="headerlink" title="17.7 压缩和加速方法如何选择？"></a>17.7 压缩和加速方法如何选择？</h2><p>​    １）对于在线计算内存存储有限的应用场景或设备，可以选择参数共享和参数剪枝方法，特别是二值量化权值和激活、结构化剪枝．其他方法虽然能够有效的压缩模型中的权值参数，但无法减小计算中隐藏的内存大小（如特征图）．<br>​    ２）如果在应用中用到的紧性模型需要利用预训练模型，那么参数剪枝、参数共享以及低秩分解将成为首要考虑的方法．相反地，若不需要借助预训练模型，则可以考虑紧性滤波设计及知识蒸馏方法．<br>​    ３）若需要一次性端对端训练得到压缩与加速后模型，可以利用基于紧性滤波设计的深度神经网络压缩与加速方法．<br>​    ４）一般情况下，参数剪枝，特别是非结构化剪枝，能大大压缩模型大小，且不容易丢失分类精度．对于需要稳定的模型分类的应用，非结构化剪枝成为首要选择．<br>​    ５）若采用的数据集较小时，可以考虑知识蒸馏方法．对于小样本的数据集，学生网络能够很好地迁移教师模型的知识，提高学生网络的判别性．<br>​    ６）主流的５个深度神经网络压缩与加速算法相互之间是正交的，可以结合不同技术进行进一步的压缩与加速．如：韩 松 等 人［３０］结合了参数剪枝和参数共享；温伟等人［６４］以及 Ａｌｖａｒｅｚ等人［８５］结合了参数剪枝和低秩分解．此外对于特定的应用场景，如目标检测，可以对卷积层和全连接层使用不同的压缩与加速技术分别处理．</p>
<p>参考《深度神经网络压缩与加速综述》</p>
<h2 id="17-8-改变网络结构设计为什么会实现模型压缩、加速？"><a href="#17-8-改变网络结构设计为什么会实现模型压缩、加速？" class="headerlink" title="17.8 改变网络结构设计为什么会实现模型压缩、加速？"></a>17.8 改变网络结构设计为什么会实现模型压缩、加速？</h2><h3 id="17-8-1-Group-convolution"><a href="#17-8-1-Group-convolution" class="headerlink" title="17.8.1 Group convolution"></a>17.8.1 Group convolution</h3><p>​    Group convolution最早出现在AlexNet中，是为了解决单卡显存不够，将网络部署到多卡上进行训练而提出。Group convolution可以减少单个卷积1/g的参数量。如何计算的呢？  </p>
<p>​    假设</p>
<ul>
<li>输入特征的的维度为$H<em>W</em>C_1$;</li>
<li>卷积核的维度为$H_1<em>W_1</em>C_1$，共$C_2$个；</li>
<li>输出特征的维度为$H_1<em>W_1</em>C_2$ 。  </li>
</ul>
<p>传统卷积计算方式如下：<br><img src="/img/ch17/1.png" alt="image"><br>传统卷积运算量为：  </p>
<script type="math/tex; mode=display">
A = H*W * h1 * w1 * c1 * c2</script><p>Group convolution是将输入特征的维度c1分成g份，每个group对应的channel数为c1/g，特征维度H * W * c1/g；，每个group对应的卷积核的维度也相应发生改变为h1 * w1 * c1/9，共c2/g个；每个group相互独立运算，最后将结果叠加在一起。<br>Group convolution计算方式如下：<br><img src="/img/ch17/2.png" alt="image"><br>Group convolution运算量为：  </p>
<script type="math/tex; mode=display">
B = H * W * h1 * w1 * c1/g * c2/g * g</script><p>Group卷积相对于传统卷积的运算量：  </p>
<script type="math/tex; mode=display">
\dfrac{B}{A} = \dfrac{ H * W * h1 * w1 * c1/g * c2/g * g}{H * W * h1 * w1 * c1 * c2} = \dfrac{1}{g}</script><p>由此可知：group卷积相对于传统卷积减少了1/g的参数量。</p>
<h3 id="17-8-2-Depthwise-separable-convolution"><a href="#17-8-2-Depthwise-separable-convolution" class="headerlink" title="17.8.2. Depthwise separable convolution"></a>17.8.2. Depthwise separable convolution</h3><p>Depthwise separable convolution是由depthwise conv和pointwise conv构成。<br>depthwise conv(DW)有效减少参数数量并提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。<br>pointwise conv(PW)实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。<br>假设输入特征的的维度为H * W * c1；卷积核的维度为h1 * w1 * c1，共c2个；输出特征的维度为 H1 * W1 * c2。<br>传统卷积计算方式如下：<br><img src="/img/ch17/3.png" alt="image"><br>传统卷积运算量为：  </p>
<script type="math/tex; mode=display">
A = H * W * h1 * w1 * c1 * c2</script><p>DW卷积的计算方式如下：<br><img src="/img/ch17/4.png" alt="image"><br>DW卷积运算量为： </p>
<script type="math/tex; mode=display">
B_DW = H * W * h1 * w1 * 1 * c1</script><p>PW卷积的计算方式如下：<br><img src="/img/ch17/5.png" alt="image"></p>
<script type="math/tex; mode=display">
B_PW = H_m * W_m * 1 * 1 * c_1 * c_2</script><p>Depthwise separable convolution运算量为：</p>
<script type="math/tex; mode=display">
B = B_DW + B_PW</script><p>Depthwise separable convolution相对于传统卷积的运算量：</p>
<script type="math/tex; mode=display">
\dfrac{B}{A} = \dfrac{ H * W * h_1 * w_1 * 1 * c_1 + H_m * W_m * 1 * 1 * c_1 * c_2}{H * W * h1 * w1 * c_1 * c_2}  

= \dfrac{1}{c_2} + \dfrac{1}{h_1 * w_1}</script><p>由此可知，随着卷积通道数的增加，Depthwise separable convolution的运算量相对于传统卷积更少。</p>
<h3 id="17-8-3-输入输出的channel相同时，MAC最小"><a href="#17-8-3-输入输出的channel相同时，MAC最小" class="headerlink" title="17.8.3 输入输出的channel相同时，MAC最小"></a>17.8.3 输入输出的channel相同时，MAC最小</h3><p><strong>卷积层的输入和输出特征通道数相等时MAC最小，此时模型速度最快。</strong><br>假设feature map的大小为h*w，输入通道$c_1$，输出通道$c_2$。<br>已知：</p>
<script type="math/tex; mode=display">
FLOPs = B = h * w * c1 * c2   
=> c1 * c2 = \dfrac{B}{h * w}</script><script type="math/tex; mode=display">
MAC = h * w * (c1 + c2) + c1 * c2</script><script type="math/tex; mode=display">
=> MAC \geq 2 * h * w \sqrt{\dfrac{B}{h * w}} + \dfrac{B}{h * w}</script><p>根据均值不等式得到(c1-c2)^2&gt;=0，等式成立的条件是c1=c2，也就是输入特征通道数和输出特征通道数相等时，在给定FLOPs前提下，MAC达到取值的下界。</p>
<h3 id="17-8-4-减少组卷积的数量"><a href="#17-8-4-减少组卷积的数量" class="headerlink" title="17.8.4 减少组卷积的数量"></a>17.8.4 减少组卷积的数量</h3><p><strong>过多的group操作会增大MAC，从而使模型速度变慢</strong><br>由以上公式可知，group卷积想比与传统的卷积可以降低计算量，提高模型的效率；如果在相同的FLOPs时，group卷积为了满足FLOPs会是使用更多channels，可以提高模型的精度。但是随着channel数量的增加，也会增加MAC。<br>FLOPs：</p>
<script type="math/tex; mode=display">
B = \dfrac{h * w * c1 * c2}{g}</script><p>MAC：</p>
<script type="math/tex; mode=display">
MAC = h * w * (c1 + c2) + \dfrac{c1 * c2}{g}</script><p>由MAC，FLOPs可知：</p>
<script type="math/tex; mode=display">
MAC = h * w * c1 + \dfrac{B*g}{c1} + \dfrac{B}{h * w}</script><p>当FLOPs固定(B不变)时，g越大，MAC越大。</p>
<h3 id="17-8-5-减少网络碎片化程度-分支数量"><a href="#17-8-5-减少网络碎片化程度-分支数量" class="headerlink" title="17.8.5 减少网络碎片化程度(分支数量)"></a>17.8.5 减少网络碎片化程度(分支数量)</h3><p><strong>模型中分支数量越少，模型速度越快</strong><br>此结论主要是由实验结果所得。<br>以下为网络分支数和各分支包含的卷积数目对神经网络速度的影响。<br><img src="/img/ch17/6.png" alt="image"><br>实验中使用的基本网络结构，分别将它们重复10次，然后进行实验。实验结果如下：<br><img src="/img/ch17/7.png" alt="image"><br>由实验结果可知，随着网络分支数量的增加，神经网络的速度在降低。网络碎片化程度对GPU的影响效果明显，对CPU不明显，但是网络速度同样在降低。</p>
<h3 id="17-8-7-减少元素级操作"><a href="#17-8-7-减少元素级操作" class="headerlink" title="17.8.7 减少元素级操作"></a>17.8.7 减少元素级操作</h3><p><strong>元素级操作所带来的时间消耗也不能忽视</strong><br>ReLU ，Tensor 相加，Bias相加的操作，分离卷积（depthwise convolution）都定义为元素级操作。<br>FLOPs大多数是对于卷积计算而言的，因为元素级操作的FLOPs相对要低很多。但是过的元素级操作也会带来时间成本。ShuffleNet作者对ShuffleNet v1和MobileNet v2的几种层操作的时间消耗做了分析，发现元素级操作对于网络速度的影响也很大。<br><img src="/img/ch17/8.png" alt="image"></p>
<h2 id="17-9-常用的轻量级网络有哪些？"><a href="#17-9-常用的轻量级网络有哪些？" class="headerlink" title="17.9 常用的轻量级网络有哪些？"></a>17.9 常用的轻量级网络有哪些？</h2><h3 id="17-9-1-SequeezeNet"><a href="#17-9-1-SequeezeNet" class="headerlink" title="17.9.1 SequeezeNet"></a>17.9.1 SequeezeNet</h3><p>SqueenzeNet出自F. N. Iandola, S.Han等人发表的论文《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5MB model size》，作者在保证精度不损失的同时，将原始AlexNet压缩至原来的510倍。  </p>
<h4 id="1-1-设计思想"><a href="#1-1-设计思想" class="headerlink" title="1.1 设计思想"></a>1.1 设计思想</h4><p>在网络结构设计方面主要采取以下三种方式：</p>
<ul>
<li>用1*1卷积核替换3*3卷积<ul>
<li>理论上一个1*1卷积核的参数是一个3*3卷积核的1/9，可以将模型尺寸压缩9倍。</li>
</ul>
</li>
<li>减小3*3卷积的输入通道数<ul>
<li>根据上述公式，减少输入通道数不仅可以减少卷积的运算量，而且输入通道数与输出通道数相同时还可以减少MAC。</li>
</ul>
</li>
<li>延迟降采样<ul>
<li>分辨率越大的输入能够提供更多特征的信息，有利于网络的训练判断，延迟降采样可以提高网络精度。<h4 id="1-2-网络架构"><a href="#1-2-网络架构" class="headerlink" title="1.2 网络架构"></a>1.2 网络架构</h4>SqueezeNet提出一种多分支结构——fire model，其中是由Squeeze层和expand层构成。Squeeze层是由s1个1*1卷积组成，主要是通过1*1的卷积降低expand层的输入维度；expand层利用e1个1*1和e3个3*3卷积构成多分支结构提取输入特征，以此提高网络的精度(其中e1=e3=4*s1)。<br><img src="/img/ch17/9.png" alt="image"><br>SqueezeNet整体网络结构如下图所示：<br><img src="/img/ch17/10.png" alt="image"></li>
</ul>
</li>
</ul>
<h4 id="1-3实验结果"><a href="#1-3实验结果" class="headerlink" title="1.3实验结果"></a>1.3实验结果</h4><p>不同压缩方法在ImageNet上的对比实验结果<br><img src="/img/ch17/11.png" alt="image"><br>由实验结果可知，SqueezeNet不仅保证了精度，而且将原始AlexNet从240M压缩至4.8M，压缩50倍，说明此轻量级网络设计是可行。</p>
<h3 id="17-9-2-MobileNet"><a href="#17-9-2-MobileNet" class="headerlink" title="17.9.2 MobileNet"></a>17.9.2 MobileNet</h3><p>MobileNet 是Google团队于CVPR-2017的论文《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》中针对手机等嵌入式设备提出的一种轻量级的深层神经网络，该网络结构在VGG的基础上使用DW+PW的组合，在保证不损失太大精度的同时，降低模型参数量。</p>
<h4 id="2-1-设计思想"><a href="#2-1-设计思想" class="headerlink" title="2.1 设计思想"></a>2.1 设计思想</h4><ul>
<li>采用深度可分离卷积代替传统卷积<ul>
<li>采用DW卷积在减少参数数量的同时提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。</li>
<li>采用PW卷积实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。</li>
</ul>
</li>
<li>使用stride=2的卷积替换pooling<ul>
<li>直接在卷积时利用stride=2完成了下采样，从而节省了需要再去用pooling再去进行一次下采样的时间，可以提升运算速度。同时，因为pooling之前需要一个stride=1的 conv，而与stride=2 conv的计算量想比要高近4倍(<strong>个人理解</strong>)。<h4 id="2-2-网络架构"><a href="#2-2-网络架构" class="headerlink" title="2.2 网络架构"></a>2.2 网络架构</h4></li>
</ul>
</li>
<li><p>DW conv和PW conv<br>MobileNet的网络架构主要是由DW conv和PW conv组成，相比于传统卷积可以降低$\dfrac{1}{N} + \dfrac{1}{Dk}​$倍的计算量。<br>标准卷积与DW conv和PW conv如图所示:<br><img src="/img/ch17/12.png" alt="image"><br>深度可分离卷积与传统卷积运算量对比：<br><img src="/img/ch17/13.png" alt="image"><br>网络结构：<br><img src="/img/ch17/14.png" alt="image"></p>
</li>
<li><p>MobileNets的架构<br><img src="/img/ch17/15.png" alt="image"></p>
</li>
</ul>
<h4 id="2-3-实验结果"><a href="#2-3-实验结果" class="headerlink" title="2.3 实验结果"></a>2.3 实验结果</h4><p><img src="/img/ch17/16.png" alt="image"><br>由上表可知，使用相同的结构，深度可分离卷积虽然准确率降低1%，但是参数量减少了6/7。</p>
<h3 id="17-9-3-MobileNet-v2"><a href="#17-9-3-MobileNet-v2" class="headerlink" title="17.9.3 MobileNet-v2"></a>17.9.3 MobileNet-v2</h3><p>MobileNet-V2是2018年1月公开在arXiv上论文《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》，是对MobileNet-V1的改进，同样是一个轻量化卷积神经网络。</p>
<h4 id="3-1-设计思想"><a href="#3-1-设计思想" class="headerlink" title="3.1 设计思想"></a>3.1 设计思想</h4><ul>
<li>采用Inverted residuals<ul>
<li>为了保证网络可以提取更多的特征，在residual block中第一个1*1 Conv和3*3 DW Conv之前进行通道扩充</li>
</ul>
</li>
<li>Linear bottlenecks<ul>
<li>为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu</li>
</ul>
</li>
<li>stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut</li>
</ul>
<h4 id="3-2-网络架构"><a href="#3-2-网络架构" class="headerlink" title="3.2 网络架构"></a>3.2 网络架构</h4><ul>
<li>Inverted residuals<br>ResNet中Residuals block先经过1*1的Conv layer，把feature map的通道数降下来，再经过3*3 Conv layer，最后经过一个1*1 的Conv layer，将feature map 通道数再“扩张”回去。即采用先压缩，后扩张的方式。而 inverted residuals采用先扩张，后压缩的方式。<br>MobileNet采用DW conv提取特征，由于DW conv本身提取的特征数就少，再经过传统residuals block进行“压缩”，此时提取的特征数会更少，因此inverted residuals对其进行“扩张”，保证网络可以提取更多的特征。<br><img src="/img/ch17/17.png" alt="image"></li>
<li>Linear bottlenecks<br>ReLu激活函数会破坏特征。ReLu对于负的输入，输出全为0，而本来DW conv特征通道已经被“压缩”，再经过ReLu的话，又会损失一部分特征。采用Linear，目的是防止Relu破坏特征。<br><img src="/img/ch17/18.png" alt="image"></li>
<li>shortcut<br>stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut<br><img src="/img/ch17/19.png" alt="image"></li>
<li>网络架构<br><img src="/img/ch17/20.png" alt="image"></li>
</ul>
<h3 id="17-9-4-Xception"><a href="#17-9-4-Xception" class="headerlink" title="17.9.4 Xception"></a>17.9.4 Xception</h3><p>Xception是Google提出的，arXiv 的V1 于2016年10月公开《Xception: Deep Learning with Depthwise Separable Convolutions 》，Xception是对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。</p>
<h4 id="4-1设计思想"><a href="#4-1设计思想" class="headerlink" title="4.1设计思想"></a>4.1设计思想</h4><ul>
<li>采用depthwise separable convolution来替换原来Inception v3中的卷积操作<br>  与原版的Depth-wise convolution有两个不同之处：<ul>
<li>第一个：原版Depth-wise convolution，先逐通道卷积，再1<em>1卷积; 而Xception是反过来，先1\</em>1卷积，再逐通道卷积；</li>
<li>第二个：原版Depth-wise convolution的两个卷积之间是不带激活函数的，而Xception在经过1*1卷积之后会带上一个Relu的非线性激活函数；</li>
</ul>
</li>
</ul>
<h4 id="4-2网络架构"><a href="#4-2网络架构" class="headerlink" title="4.2网络架构"></a>4.2网络架构</h4><p>feature map在空间和通道上具有一定的相关性，通过Inception模块和非线性激活函数实现通道之间的解耦。增多3*3的卷积的分支的数量，使它与1*1的卷积的输出通道数相等，此时每个3*3的卷积只作用与一个通道的特征图上，作者称之为“极致的Inception（Extream Inception）”模块，这就是Xception的基本模块。<br><img src="/img/ch17/21.png" alt="image"></p>
<h3 id="17-9-5-ShuffleNet-v1"><a href="#17-9-5-ShuffleNet-v1" class="headerlink" title="17.9.5 ShuffleNet-v1"></a>17.9.5 ShuffleNet-v1</h3><p>ShuffleNet 是Face++团队提出的，晚于MobileNet两个月在arXiv上公开《ShuffleNet： An Extremely Efficient Convolutional Neural Network for Mobile Devices 》用于移动端前向部署的网络架构。ShuffleNet基于MobileNet的group思想，将卷积操作限制到特定的输入通道。而与之不同的是，ShuffleNet将输入的group进行打散，从而保证每个卷积核的感受野能够分散到不同group的输入中，增加了模型的学习能力。</p>
<h4 id="5-1-设计思想"><a href="#5-1-设计思想" class="headerlink" title="5.1 设计思想"></a>5.1 设计思想</h4><ul>
<li>采用group conv减少大量参数<ul>
<li>roup conv与DW conv存在相同的“信息流通不畅”问题 </li>
</ul>
</li>
<li>采用channel shuffle解决上述问题<ul>
<li>MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle</li>
</ul>
</li>
<li>采用concat替换add操作<ul>
<li>avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失</li>
</ul>
</li>
</ul>
<h4 id="5-2-网络架构"><a href="#5-2-网络架构" class="headerlink" title="5.2 网络架构"></a>5.2 网络架构</h4><p>MobileNet中1*1卷积的操作占据了约95%的计算量，所以作者将1*1也更改为group卷积，使得相比MobileNet的计算量大大减少。<br><img src="/img/ch17/22.png" alt="image"><br>group卷积与DW存在同样使“通道信息交流不畅”的问题，MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle。<br>ShuffleNet的shuffle操作如图所示<br><img src="/img/ch17/24.png" alt="image"><br>avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失；实验表明：多多使用通道(提升通道的使用率)，有助于提高小模型的准确率。<br><img src="/img/ch17/23.png" alt="image"><br>网络结构：<br><img src="/img/ch17/25.png" alt="image"></p>
<h3 id="17-9-6-ShuffleNet-v2"><a href="#17-9-6-ShuffleNet-v2" class="headerlink" title="17.9.6 ShuffleNet-v2"></a>17.9.6 ShuffleNet-v2</h3><p>huffleNet-v2 是Face++团队提出的《ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design》，旨在设计一个轻量级但是保证精度、速度的深度网络。</p>
<h4 id="6-1-设计思想"><a href="#6-1-设计思想" class="headerlink" title="6.1 设计思想"></a>6.1 设计思想</h4><ul>
<li>文中提出影响神经网络速度的4个因素：<ul>
<li>a. FLOPs(FLOPs就是网络执行了多少multiply-adds操作)</li>
<li>b. MAC(内存访问成本)</li>
<li>c. 并行度(如果网络并行度高，速度明显提升)</li>
<li>d. 计算平台(GPU，ARM)</li>
</ul>
</li>
<li>ShuffleNet-v2 提出了4点网络结构设计策略：<ul>
<li>G1.输入输出的channel相同时，MAC最小</li>
<li>G2.过度的组卷积会增加MAC</li>
<li>G3.网络碎片化会降低并行度</li>
<li>G4.元素级运算不可忽视  </li>
</ul>
</li>
</ul>
<h4 id="6-2-网络结构"><a href="#6-2-网络结构" class="headerlink" title="6.2 网络结构"></a>6.2 网络结构</h4><p>depthwise convolution 和 瓶颈结构增加了 MAC，用了太多的 group，跨层连接中的 element-wise Add 操作也是可以优化的点。所以在 shuffleNet V2 中增加了几种新特性。<br>所谓的 channel split 其实就是将通道数一分为2，化成两分支来代替原先的分组卷积结构（G2），并且每个分支中的卷积层都是保持输入输出通道数相同（G1），其中一个分支不采取任何操作减少基本单元数（G3），最后使用了 concat 代替原来的 elementy-wise add，并且后面不加 ReLU 直接（G4），再加入channle shuffle 来增加通道之间的信息交流。 对于下采样层，在这一层中对通道数进行翻倍。 在网络结构的最后，即平均值池化层前加入一层 1x1 的卷积层来进一步的混合特征。<br><img src="/img/ch17/26.png" alt="image"><br>网络结构<br><img src="/img/ch17/27.png" alt="image"></p>
<h4 id="6-4-ShuffleNet-v2具有高精度的原因"><a href="#6-4-ShuffleNet-v2具有高精度的原因" class="headerlink" title="6.4  ShuffleNet-v2具有高精度的原因"></a>6.4  ShuffleNet-v2具有高精度的原因</h4><ul>
<li>由于高效，可以增加更多的channel，增加网络容量</li>
<li>采用split使得一部分特征直接与下面的block相连，特征复用(DenseNet)</li>
</ul>
<h2 id="17-10-现有移动端开源框架及其特点"><a href="#17-10-现有移动端开源框架及其特点" class="headerlink" title="17.10 现有移动端开源框架及其特点"></a>17.10 现有移动端开源框架及其特点</h2><h3 id="17-10-1-NCNN"><a href="#17-10-1-NCNN" class="headerlink" title="17.10.1 NCNN"></a>17.10.1 NCNN</h3><p>１、开源时间：2017年7月　　　</p>
<p>２、开源用户：腾讯优图　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/ncnn" target="_blank" rel="noopener">https://github.com/Tencent/ncnn</a> 　　</p>
<p>4、特点：</p>
<ul>
<li>1）NCNN考虑了手机端的硬件和系统差异以及调用方式，架构设计以手机端运行为主要原则。</li>
<li>2）无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架（以开源时间为参照对象）。</li>
<li>3）基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP。   </li>
</ul>
<p>5、功能：    </p>
<ul>
<li>1、NCNN支持卷积神经网络、多分支多输入的复杂网络结构，如vgg、googlenet、resnet、squeezenet 等。</li>
<li>2、NCNN无需依赖任何第三方库。    </li>
<li>3、NCNN全部使用C/C++实现，以及跨平台的cmake编译系统，可轻松移植到其他系统和设备上。    </li>
<li>4、汇编级优化，计算速度极快。使用ARM NEON指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。 </li>
<li>5、精细的数据结构设计，没有采用需消耗大量内存的通常框架——im2col + 矩阵乘法，使得内存占用极低。   </li>
<li>6、支持多核并行计算，优化CPU调度。   </li>
<li>7、整体库体积小于500K，可精简到小于300K。   </li>
<li>8、可扩展的模型设计，支持8bit 量化和半精度浮点存储。   </li>
<li>9、支持直接内存引用加载网络模型。   </li>
<li>10、可注册自定义层实现并扩展。   </li>
</ul>
<p>6、NCNN在Android端部署示例</p>
<ul>
<li>1）选择合适的Android Studio版本并安装。</li>
<li>2）根据需求选择NDK版本并安装。</li>
<li>3）在Android Studio上配置NDK的环境变量。</li>
<li>4）根据自己需要编译NCNN sdk</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir build-android cd build-android cmake -DCMAKE_TOOLCHAIN_FILE&#x3D;$ANDROID_NDK&#x2F;build&#x2F;cmake&#x2F;android.toolchain.cmake \ -DANDROID_ABI&#x3D;&quot;armeabi-v7a&quot; -DANDROID_ARM_NEON&#x3D;ON \ -DANDROID_PLATFORM&#x3D;android-14 .. make make install</span><br></pre></td></tr></table></figure>
<p>​    安装完成之后，install下有include和lib两个文件夹。</p>
<p>​    备注：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ANDROID_ABI 是架构名字，&quot;armeabi-v7a&quot; 支持绝大部分手机硬件 </span><br><span class="line">ANDROID_ARM_NEON 是否使用 NEON 指令集，设为 ON 支持绝大部分手机硬件 </span><br><span class="line">ANDROID_PLATFORM 指定最低系统版本，&quot;android-14&quot; 就是 android-4.0</span><br></pre></td></tr></table></figure>
<ul>
<li>5）进行NDK开发。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1）assets文件夹下放置你的bin和param文件。</span><br><span class="line">2）jni文件夹下放置你的cpp和mk文件。</span><br><span class="line">3）修改你的app gradle文件。</span><br><span class="line">4）配置Android.mk和Application.mk文件。</span><br><span class="line">5）进行java接口的编写。</span><br><span class="line">6）读取拷贝bin和param文件（有些则是pb文件，根据实际情况）。</span><br><span class="line">7）进行模型的初始化和执行预测等操作。</span><br><span class="line">8）build。</span><br><span class="line">9）cd到src&#x2F;main&#x2F;jni目录下，执行ndk-build，生成.so文件。</span><br><span class="line">10）接着就可写自己的操作处理需求。</span><br></pre></td></tr></table></figure>
<h3 id="17-10-2-QNNPACK"><a href="#17-10-2-QNNPACK" class="headerlink" title="17.10.2 QNNPACK"></a>17.10.2 QNNPACK</h3><p>全称：Quantized Neural Network PACKage（量化神经网络包）　　　</p>
<p>１、开源时间：2018年10月　　　</p>
<p>２、开源用户：Facebook　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/pytorch/QNNPACK" target="_blank" rel="noopener">https://github.com/pytorch/QNNPACK</a>　　　　</p>
<p>４、特点：　　　</p>
<p>​    １）低密度卷积优化函数库；　　　</p>
<p>　    ２）可在手机上实时运行Mask R-CNN 和 DensePose;</p>
<p>​    ３） 能在性能受限的移动设备中用 100ms 以内的时间实施图像分类；　　　</p>
<p>5、QNNPACK 如何提高效率？</p>
<p>1)<strong>QNNPACK 使用与安卓神经网络 API 兼容的线性量化方案</strong></p>
<p>QNNPACK 的输入矩阵来自低精度、移动专用的计算机视觉模型。其它库在计算A和B矩阵相乘时，重新打包 A 和 B 矩阵以更好地利用缓存层次结构，希望在大量计算中分摊打包开销，QNNPACK 删除所有计算非必需的内存转换，针对 A和B矩阵相乘适用于一级缓存的情况进行了优化。</p>
<p><img src="/img/ch17/QNNPACK1.jpeg" alt=""></p>
<p>​    1）优化了L1缓存计算，不需要输出中间结果，直接输出最终结果，节省内存带宽和缓存占用。</p>
<p>具体分析：</p>
<ul>
<li>常规实现：在量化矩阵-矩阵乘法中，8位整数的乘积通常会被累加至 32 位的中间结果中，随后重新量化以产生 8 位的输出。遇到大矩阵尺寸时，比如有时K太大，A和B的面板无法直接转入缓存，此时，需利用缓存层次结构，借助GEMM将A和B的面板沿着K维分割成固定大小的子面板，以便于每个子面板都能适应L1缓存，随后为每个子面板调用微内核。这一缓存优化需要 PDOT 为内核输出 32  位中间结果，最终将它们相加并重新量化为 8 位整数。</li>
<li>优化实现：由于  ONNPACK 对于面板 A 和 B 总是适应 L1 缓存的移动神经网络进行了优化，因此它在调用微内核时处理整个 A 和 B  的面板。而由于无需在微内核之外积累 32 位的中间结果，QNNPACK 会将 32 位的中间结果整合进微内核中并写出 8  位值，这节省了内存带宽和缓存占用。</li>
</ul>
<p><img src="/img/ch17/QNNPACK2.jpeg" alt=""></p>
<p>​    2）取消了矩阵 A 的重新打包。</p>
<ul>
<li><p>常规实现：</p>
<pre><code>  矩阵 B 包含静态权重，可以一次性转换成任何内存布局，但矩阵  A 包含卷积输入，每次推理运行都会改变。因此，重新打包矩阵 A 在每次运行时都会产生开销。尽管存在开销，传统的 GEMM实现还是出于以下两个原因对矩阵 A 进行重新打包：

  a 缓存关联性及微内核效率受限。如果不重新打包，微内核将不得不读取被潜在的大跨距隔开的几行A。如果这个跨距恰好是 2 的许多次幂的倍数，面板中不同行 A  的元素可能会落入同一缓存集中。如果冲突的行数超过了缓存关联性，它们就会相互驱逐，性能也会大幅下降。

  b 打包对微内核效率的影响与当前所有移动处理器支持的  SIMD  向量指令的使用密切相关。这些指令加载、存储或者计算小型的固定大小元素向量，而不是单个标量（scalar）。在矩阵相乘中，充分利用向量指令达到高性能很重要。在传统的  GEMM 实现中，微内核把 MR 元素重新打包到向量暂存器里的 MR 线路中。
</code></pre></li>
<li><p>优化实现：</p>
<pre><code>  a 当面板适配一级缓存时，不会存在缓存关联性及微内核效率受限的问题。

  b 在 QNNPACK 实现中，MR  元素在存储中不是连续的，微内核需要把它们加载到不同的向量暂存器中。越来越大的暂存器压力迫使 QNNPACK 使用较小的 MRxNR  拼贴，但实际上这种差异很小，而且可以通过消除打包开销来补偿。例如，在 32 位 ARM 架构上，QNNPACK 使用 4×8 微内核，其中  57% 的向量指令是乘-加；另一方面，gemmlowp 库使用效率稍高的 4×12 微内核，其中 60% 的向量指令是乘-加。微内核加载 A  的多个行，乘以 B 的满列，结果相加，然后完成再量化并记下量化和。A 和 B 的元素被量化为 8 位整数，但乘积结果相加到 32 位。大部分  ARM 和 ARM64 处理器没有直接完成这一运算的指令，所以它必须分解为多个支持运算。QNNPACK  提供微内核的两个版本，其不同之处在于用于乘以 8 位值并将它们累加到 32 位的指令序列。
</code></pre></li>
</ul>
<p>2)<strong>从矩阵相乘到卷积</strong></p>
<p><img src="/img/ch17/QNNPACK3.jpeg" alt=""></p>
<p>​    传统实现：</p>
<p>​    简单的 1×1  卷积可直接映射到矩阵相乘</p>
<p>​    但对于具备较大卷积核、padding 或子采样（步幅）的卷积而言则并非如此。但是，这些较复杂的卷积能够通过记忆变换  im2col 映射到矩阵相乘。对于每个输出像素，im2col 复制输入图像的图像块并将其计算为 2D 矩阵。由于每个输出像素都受 KHxKWxC  输入像素值的影响（KH 和 KW 分别指卷积核的高度和宽度，C 指输入图像中的通道数），因此该矩阵的大小是输入图像的 KHxKW  倍，im2col 给内存占用和性能都带来了一定的开销。和 Caffe 一样，大部分深度学习框架转而使用基于 im2col  的实现，利用现有的高度优化矩阵相乘库来执行卷积操作。</p>
<p>​    优化实现：</p>
<p>​    Facebook  研究者在 QNNPACK 中实现了一种更高效的算法。</p>
<ul>
<li>他们没有变换卷积输入使其适应矩阵相乘的实现，而是调整 PDOT 微内核的实现，在运行中执行  im2col 变换。这样就无需将输入张量的实际输入复制到 im2col 缓存，而是使用输入像素行的指针设置 indirection  buffer，输入像素与每个输出像素的计算有关。</li>
<li>研究者还修改了矩阵相乘微内核，以便从 indirection buffer  加载虚构矩阵（imaginary matrix）A 的行指针，indirection buffer 通常比 im2col buffer  小得多。</li>
<li>此外，如果两次推断运行的输入张量存储位置不变，则 indirection buffer  还可使用输入张量行的指针进行初始化，然后在多次推断运行中重新使用。研究者观察到具备 indirection buffer 的微内核不仅消除了  im2col 变换的开销，其性能也比矩阵相乘微内核略好（可能由于输入行在计算不同输出像素时被重用）。</li>
</ul>
<p>3)<strong>深度卷积</strong></p>
<p><img src="/img/ch17/QNNPACK4.jpeg" alt=""></p>
<p>分组卷积（grouped   convolution）将输入和输出通道分割成多组，然后对每个组进行分别处理。在有限条件下，当组数等于通道数时，该卷积就是深度卷积，常用于当前的神经网络架构中。深度卷积对每个通道分别执行空间滤波，展示了与正常卷积非常不同的计算模式。因此，通常要向深度卷积提供单独实现，QNNPACK  包括一个高度优化版本 3×3 深度卷积。</p>
<p>深度卷积的传统实现是每次都在卷积核元素上迭代，然后将一个卷积核行和一个输入行的结果累加到输出行。对于一个  3×3 的深度卷积，此类实现将把每个输出行更新 9 次。在 QNNPACK 中，研究者计算所有 3×3 卷积核行和 3×3  输入行的结果，一次性累加到输出行，然后再处理下个输出行。</p>
<p>QNNPACK  实现高性能的关键因素在于完美利用通用暂存器（GPR）来展开卷积核元素上的循环，同时避免在 hot loop 中重新加载地址寄存器。32-bit  ARM 架构将实现限制在 14 个 GPR。在 3×3 深度卷积中，需要读取 9 个输入行和 9 个卷积核行。这意味着如果想完全展开循环必须存储  18 个地址。然而，实践中推断时卷积核不会发生变化。因此 Facebook 研究者使用之前在 CxKHxKW 中的滤波器，将它们封装进  [C/8]xKWxKHx8，这样就可以仅使用具备地址增量（address increment）的一个 GPR 访问所有滤波器。（研究者使用数字 8  的原因在于，在一个命令中加载 8 个元素然后减去零，在 128-bit NEON 暂存器中生成 8 个 16-bit 值。）然后使用 9  个输入行指针，指针将滤波器重新装进 10 个 GPR，完全展开滤波器元素上的循环。64-bit ARM 架构相比 32-bit 架构，GPR  的数量翻了一倍。QNNPACK 利用额外的 ARM64 GPR，一次性存储 3×5 输入行的指针，并计算 3 个输出行。</p>
<p>7、性能优势：</p>
<p>​    测试结果显示出 QNNPACK 在端到端基准上的性能优势。在量化当前最优 MobileNetV2 架构上，基于QNNPACK 的 Caffe2 算子的速度大约是 TensorFlow Lite 速度的 2 倍，在多种手机上都是如此。除了 QNNPACK 之外，Facebook 还开源了 Caffe2 quantized MobileNet v2 模型，其 top-1 准确率比相应的 TensorFlow 模型高出 1.3%。    </p>
<p><strong>MobileNetV1</strong></p>
<p>MobileNetV1  架构在使用深度卷积（depthwise convolution）使模型更适合移动设备方面具备开创性。MobileNetV1 包括几乎整个  1×1 卷积和 3×3 卷积。Facebook 研究者将量化 MobileNetV1 模型从 TensorFlow Lite 转换而来，并在  TensorFlow Lite 和 QNNPACK 的 32-bit ARM 设备上对 MobileNetV1 进行基准测试。二者运行时均使用 4  线程，研究者观察到 QNNPACK 的运行速度几何平均值是 TensorFlow Lite 的 1.8 倍。</p>
<p><img src="/img/ch17/mv1.jpg" alt=""></p>
<p><strong>MobileNetV2</strong></p>
<p>作为移动视觉任务的当前最优架构之一，MobileNetV2  引入了瓶颈构造块和瓶颈之间的捷径连接。研究者在 MobileNetV2 分类模型的量化版上对比基于 QNNPACK 的 Caffe2 算子和  TensorFlow Lite 实现。使用的量化 Caffe2 MobileNetV2 模型已开源，量化 TensorFlow Lite  模型来自官方库：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md。下表展示了二者在常用测试集上的" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md。下表展示了二者在常用测试集上的</a>  top1 准确率：</p>
<p><img src="/img/ch17/mv2.jpg" alt=""></p>
<p>​    Facebook 研究者利用这些模型建立了 Facebook AI 性能评估平台（<a href="https://github.com/facebook/FAI-PEP）的基准，该基准基于" target="_blank" rel="noopener">https://github.com/facebook/FAI-PEP）的基准，该基准基于</a> 32-bit ARM 环境的大量手机设备。对于 TensorFlow Lite 线程设置，研究者尝试了一到四个线程，并报告了最快速的结果。结果显示 TensorFlow Lite 使用四线程的性能最优，因此后续研究中使用四线程来对比 TensorFlow Lite 和 QNNPACK。下表展示了结果，以及在典型智能手机和高端机上，基于 QNNPACK 的算子速度比 TensorFlow Lite 快得多。</p>
<p><img src="/img/ch17/mv3.jpg" alt=""></p>
<p>Facebook开源高性能内核库QNNPACK<br><a href="https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc</a><br><a href="http://www.sohu.com/a/272158070_610300" target="_blank" rel="noopener">http://www.sohu.com/a/272158070_610300</a></p>
<p>支持移动端深度学习的几种开源框架<br><a href="https://blog.csdn.net/zchang81/article/details/74280019" target="_blank" rel="noopener">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="17-10-3-Prestissimo"><a href="#17-10-3-Prestissimo" class="headerlink" title="17.10.3 Prestissimo"></a>17.10.3 Prestissimo</h3><p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：九言科技　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/in66-dev/In-Prestissimo" target="_blank" rel="noopener">https://github.com/in66-dev/In-Prestissimo</a>　　</p>
<p>４、功能特点：　</p>
<p><strong>基础功能</strong></p>
<ul>
<li>支持卷积神经网络，支持多输入和多分支结构</li>
<li>精炼简洁的API设计，使用方便</li>
<li>提供调试接口，支持打印各个层的数据以及耗时</li>
<li>不依赖任何第三方计算框架，整体库体积 500K 左右（32位 约400k，64位 约600k）</li>
<li>纯 C++ 实现，跨平台，支持 android 和 ios</li>
<li>模型为纯二进制文件，不暴露开发者设计的网络结构</li>
</ul>
<p><strong>极快的速度</strong></p>
<ul>
<li>大到框架设计，小到汇编书写上全方位的优化，iphone7 上跑 SqueezeNet 仅需 26ms（单线程）</li>
<li>支持浮点(float)和整型(int)两种运算模式，float模式精度与caffe相同，int模式运算速度快，大部分网络用int的精度便已经足够</li>
<li>以巧妙的内存布局提升cpu的cache命中率，在中低端机型上性能依然强劲</li>
<li>针对 float-arm32, float-arm64, int-arm32, int-arm64 四个分支均做了细致的优化，保证arm32位和arm64位版本都有非常好的性能</li>
</ul>
<p><strong>SqueezeNet-v1.1 测试结果</strong></p>
<p><strong>Note</strong>: 手机测试性能存在一定的抖动，连续多次运算取平均时间</p>
<p><strong>Note</strong>: 像华为mate8, mate9，Google nexus 6 虽然是64位的CPU，但测试用的是 32位的库，因此cpu架构依然写 arm-v7a</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">CPU架构</th>
<th style="text-align:center">机型</th>
<th style="text-align:center">CPU</th>
<th style="text-align:center">ncnn（4线程）</th>
<th style="text-align:center">mdl</th>
<th style="text-align:center">Prestissimo_float(单线程)</th>
<th style="text-align:center">Prestissimo_int(单线程)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">小米2</td>
<td style="text-align:center">高通APQ8064 1.5GHz</td>
<td style="text-align:center">185 ms</td>
<td style="text-align:center">370 ms</td>
<td style="text-align:center">184 ms</td>
<td style="text-align:center">115 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">小米2s</td>
<td style="text-align:center">四核 骁龙APQ8064 Pro 1.7GHz</td>
<td style="text-align:center">166 ms</td>
<td style="text-align:center">-</td>
<td style="text-align:center">136 ms</td>
<td style="text-align:center">96 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">红米Note 4x</td>
<td style="text-align:center">骁龙625 四核2.0GHz</td>
<td style="text-align:center">124 ms</td>
<td style="text-align:center">306 ms</td>
<td style="text-align:center">202 ms</td>
<td style="text-align:center">110 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">Google Nexus 6</td>
<td style="text-align:center">骁龙805 四核 2.7GHz</td>
<td style="text-align:center">84 ms</td>
<td style="text-align:center">245 ms</td>
<td style="text-align:center">103 ms</td>
<td style="text-align:center">63 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">Vivo x6d</td>
<td style="text-align:center">联发科 MT6752 1.7GHz</td>
<td style="text-align:center">245 ms</td>
<td style="text-align:center">502 ms</td>
<td style="text-align:center">370 ms</td>
<td style="text-align:center">186 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">华为 Mate 8</td>
<td style="text-align:center">海思麒麟950 4大4小 2.3GHz 1.8GHz</td>
<td style="text-align:center">75 ms</td>
<td style="text-align:center">180 ms</td>
<td style="text-align:center">95 ms</td>
<td style="text-align:center">57 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">华为 Mate 9</td>
<td style="text-align:center">海思麒麟960 4大4小 2.4GHz 1.8GHz</td>
<td style="text-align:center">61 ms</td>
<td style="text-align:center">170 ms</td>
<td style="text-align:center">94 ms</td>
<td style="text-align:center">48 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v8</td>
<td style="text-align:center">iphone7</td>
<td style="text-align:center">Apple A10 Fusion 2.34GHz</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">27 ms</td>
<td style="text-align:center">26 ms</td>
</tr>
</tbody>
</table>
</div>
<p><strong>未开放特性</strong></p>
<ul>
<li>多核并行加速（多核机器可以再提升30%-100% 的速度）</li>
<li>depthwise卷积运算（支持mobilenet）</li>
<li>模型压缩功能，压缩后的模型体积可缩小到20%以下</li>
<li>GPU 运算模式（Android 基于opengl es 3.1，ios 基于metal）</li>
</ul>
<p><strong>同类框架对比</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">框架</th>
<th style="text-align:center">caffe</th>
<th style="text-align:center">tensorflow</th>
<th style="text-align:center">mdl-android</th>
<th style="text-align:center">mdl-ios</th>
<th style="text-align:center">ncnn</th>
<th style="text-align:center">CoreML</th>
<th style="text-align:center">Prestissimo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">计算硬件</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">gpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">gpu</td>
<td style="text-align:center">cpu （gpu版本未开放）</td>
</tr>
<tr>
<td style="text-align:center">计算速度</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">很快</td>
<td style="text-align:center">很快</td>
<td style="text-align:center">极快</td>
<td style="text-align:center">极快</td>
</tr>
<tr>
<td style="text-align:center">库大小</td>
<td style="text-align:center">大</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
</tr>
<tr>
<td style="text-align:center">兼容性</td>
<td style="text-align:center">好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">限ios8以上</td>
<td style="text-align:center">很好</td>
<td style="text-align:center">仅支持 ios11</td>
<td style="text-align:center">很好</td>
</tr>
<tr>
<td style="text-align:center">模型支持度</td>
<td style="text-align:center">很好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">-</td>
<td style="text-align:center">差（仅限指定模型）</td>
<td style="text-align:center">较好</td>
<td style="text-align:center">-</td>
<td style="text-align:center">中等（当前版本不支持mobilenet）</td>
</tr>
</tbody>
</table>
</div>
<p><strong>使用方法-模型转换</strong></p>
<p>绝影支持的是私有的模型文件格式，需要把 caffe 训练出来的模型转换为 .prestissimo 格式，模型转换工具为 caffe2Prestissimo.out。caffe2Prestissimo.out 依赖 protobuf 3.30。将 XXX.prototxt 和 YYY.caffemodel 转化为 Prestissimo 模型 ZZZ.prestissimo：（得到）./caffe2Prestissimo.out XXX.prototxt YYY.caffemodel ZZZ.prestissimo</p>
<h3 id="17-10-4-MDL（mobile-deep-learning）"><a href="#17-10-4-MDL（mobile-deep-learning）" class="headerlink" title="17.10.4 MDL（mobile-deep-learning）"></a>17.10.4 MDL（mobile-deep-learning）</h3><p>１、开源时间：2017年9月（已暂停更新）　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/allonli/mobile-deep-learning" target="_blank" rel="noopener">https://github.com/allonli/mobile-deep-learning</a></p>
<p>４、功能特点：</p>
<ul>
<li>一键部署，脚本参数就可以切换ios或者android</li>
<li>支持iOS  gpu运行MobileNet、squeezenet模型</li>
<li>已经测试过可以稳定运行MobileNet、GoogLeNet v1、squeezenet、ResNet-50模型</li>
<li>体积极小，无任何第三方依赖。纯手工打造。</li>
<li>提供量化函数，对32位float转8位uint直接支持，模型体积量化后4M上下</li>
<li>与ARM相关算法团队线上线下多次沟通，针对ARM平台会持续优化</li>
<li>NEON使用涵盖了卷积、归一化、池化所有方面的操作</li>
<li>汇编优化，针对寄存器汇编操作具体优化</li>
<li>loop unrolling 循环展开，为提升性能减少不必要的CPU消耗，全部展开判断操作</li>
<li>将大量繁重的计算任务前置到overhead过程</li>
</ul>
<p>5、框架结构</p>
<p><img src="/img/ch17/MDL1.png" alt=""></p>
<p>MDL 框架主要包括：<strong>模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层（JNI Interfaces）。</strong></p>
<p>​    其中，模型转换模块主要负责将Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。</p>
<p>6、MDL 的性能及兼容性</p>
<ul>
<li>体积 armv7 300k+</li>
<li>速度 iOS GPU mobilenet 可以达到 40ms、squeezenet 可以达到 30ms</li>
</ul>
<p>​        MDL  从立项到开源，已经迭代了一年多。移动端比较关注的多个指标都表现良好，如体积、功耗、速度。百度内部产品线在应用前也进行过多次对比，和已开源的相关项目对比，MDL  能够在保证速度和能耗的同时支持多种深度学习模型，如 mobilenet、googlenet v1、squeezenet 等，且具有 iOS  GPU 版本，squeezenet 一次运行最快可以达到 3-40ms。</p>
<p><strong>同类框架对比</strong></p>
<p>​     框架Caffe2TensorFlowncnnMDL(CPU)MDL(GPU)硬件CPUCPUCPUCPUGPU速度慢慢快快极快体积大大小小小兼容Android&amp;iOSAndroid&amp;iOSAndroid&amp;iOSAndroid&amp;iOSiOS</p>
<p>​     与支持 CNN 的移动端框架对比，MDL 速度快、性能稳定、兼容性好、demo 完备。</p>
<p><strong>兼容性</strong></p>
<p>​     MDL 在 iOS 和 Android 平台均可以稳定运行，其中 iOS10 及以上平台有基于 GPU 运算的 API，性能表现非常出色，在 Android 平台则是纯 CPU 运行。高中低端机型运行状态和手机百度及其他 App 上的覆盖都有绝对优势。</p>
<p>​     MDL 同时也支持 Caffe 模型直接转换为 MDL 模型。</p>
<h3 id="17-10-5-Paddle-Mobile"><a href="#17-10-5-Paddle-Mobile" class="headerlink" title="17.10.5 Paddle-Mobile"></a>17.10.5 Paddle-Mobile</h3><p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/PaddlePaddle/paddle-mobile" target="_blank" rel="noopener">https://github.com/PaddlePaddle/paddle-mobile</a>　</p>
<p>４、功能特点：</p>
<p><strong>功能特点</strong></p>
<ul>
<li><p>高性能支持ARM CPU </p>
</li>
<li><p>支持Mali GPU</p>
</li>
<li><p>支持Andreno GPU</p>
</li>
<li><p>支持苹果设备的GPU Metal实现</p>
</li>
<li><p>支持ZU5、ZU9等FPGA开发板</p>
</li>
<li><p>支持树莓派等arm-linux开发板</p>
</li>
</ul>
<h3 id="17-10-6-MACE（-Mobile-AI-Compute-Engine）"><a href="#17-10-6-MACE（-Mobile-AI-Compute-Engine）" class="headerlink" title="17.10.6 MACE（ Mobile AI Compute Engine）"></a>17.10.6 MACE（ Mobile AI Compute Engine）</h3><p>１、开源时间：2018年4月(持续更新，v0.9.0 (2018-07-20))　　　</p>
<p>２、开源用户：小米　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/XiaoMi/mace" target="_blank" rel="noopener">https://github.com/XiaoMi/mace</a>    </p>
<p>４、简介：Mobile AI Compute Engine (MACE) 是一个专为移动端异构计算设备优化的深度学习前向预测框架。<br>MACE覆盖了常见的移动端计算设备（CPU，GPU和DSP），并且提供了完整的工具链和文档，用户借助MACE能够很方便地在移动端部署深度学习模型。MACE已经在小米内部广泛使用并且被充分验证具有业界领先的性能和稳定性。</p>
<p>5、MACE的基本框架：</p>
<p><img src="/img/ch17/mace-arch.png" alt=""></p>
<p><strong>MACE Model</strong></p>
<p>MACE定义了自有的模型格式（类似于Caffe2），通过MACE提供的工具可以将Caffe和TensorFlow的模型 转为MACE模型。</p>
<p><strong>MACE Interpreter</strong></p>
<p>MACE Interpreter主要负责解析运行神经网络图（DAG）并管理网络中的Tensors。</p>
<p><strong>Runtime</strong></p>
<p>CPU/GPU/DSP Runtime对应于各个计算设备的算子实现。</p>
<p>6、MACE使用的基本流程</p>
<p><img src="/img/ch17/mace-work-flow-zh.png" alt=""></p>
<p><strong>1. 配置模型部署文件(.yml)</strong></p>
<p>模型部署文件详细描述了需要部署的模型以及生成库的信息，MACE根据该文件最终生成对应的库文件。</p>
<p><strong>2.编译MACE库</strong></p>
<p>编译MACE的静态库或者动态库。</p>
<p><strong>3.转换模型</strong></p>
<p>将TensorFlow 或者 Caffe的模型转为MACE的模型。</p>
<p><strong>4.1. 部署</strong></p>
<p>根据不同使用目的集成Build阶段生成的库文件，然后调用MACE相应的接口执行模型。</p>
<p><strong>4.2. 命令行运行</strong></p>
<p>MACE提供了命令行工具，可以在命令行运行模型，可以用来测试模型运行时间，内存占用和正确性。</p>
<p><strong>4.3. Benchmark</strong></p>
<p>MACE提供了命令行benchmark工具，可以细粒度的查看模型中所涉及的所有算子的运行时间。</p>
<p>7、MACE在哪些角度进行了优化?</p>
<p><strong>MACE</strong> 专为移动端异构计算平台优化的神经网络计算框架。主要从以下的角度做了专门的优化：</p>
<ul>
<li>性能<ul>
<li>代码经过NEON指令，OpenCL以及Hexagon HVX专门优化，并且采用<br><a href="https://arxiv.org/abs/1509.09308" target="_blank" rel="noopener">Winograd算法</a>来进行卷积操作的加速。<br>此外，还对启动速度进行了专门的优化。</li>
</ul>
</li>
<li><p>功耗</p>
<ul>
<li>支持芯片的功耗管理，例如ARM的big.LITTLE调度，以及高通Adreno GPU功耗选项。</li>
</ul>
</li>
<li>系统响应<ul>
<li>支持自动拆解长时间的OpenCL计算任务，来保证UI渲染任务能够做到较好的抢占调度，<br>从而保证系统UI的相应和用户体验。</li>
</ul>
</li>
<li>内存占用<ul>
<li>通过运用内存依赖分析技术，以及内存复用，减少内存的占用。另外，保持尽量少的外部<br>依赖，保证代码尺寸精简。</li>
</ul>
</li>
<li><p>模型加密与保护</p>
<ul>
<li>模型保护是重要设计目标之一。支持将模型转换成C++代码，以及关键常量字符混淆，增加逆向的难度。</li>
</ul>
</li>
<li>硬件支持范围<ul>
<li>支持高通，联发科，以及松果等系列芯片的CPU，GPU与DSP(目前仅支持Hexagon)计算加速。</li>
<li>同时支持在具有POSIX接口的系统的CPU上运行。</li>
</ul>
</li>
</ul>
<p>8、性能对比：</p>
<p>MACE 支持 TensorFlow 和 Caffe 模型，提供转换工具，可以将训练好的模型转换成专有的模型数据文件，同时还可以选择将模型转换成C++代码，支持生成动态库或者静态库，提高模型保密性。</p>
<p><img src="/img/ch17/maca_com.jpg" alt=""></p>
<h3 id="17-10-7-FeatherCNN"><a href="#17-10-7-FeatherCNN" class="headerlink" title="17.10.7 FeatherCNN"></a>17.10.7 FeatherCNN</h3><p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：腾讯AI　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/FeatherCNN" target="_blank" rel="noopener">https://github.com/Tencent/FeatherCNN</a></p>
<p>４、功能特点：</p>
<p><strong>FeatherCNN 是由腾讯 AI 平台部研发的基于 ARM 架构的高效 CNN 推理库，该项目支持 Caffe 模型，且具有高性能、易部署、轻量级三大特性。</strong></p>
<p><strong>该项目具体特性如下：</strong></p>
<ul>
<li><p>高性能：无论是在移动设备（iOS / Android），嵌入式设备（Linux）还是基于 ARM 的服务器（Linux）上，FeatherCNN 均能发挥最先进的推理计算性能；</p>
</li>
<li><p>易部署：FeatherCNN 的所有内容都包含在一个代码库中，以消除第三方依赖关系。因此，它便于在移动平台上部署。FeatherCNN 自身的模型格式与 Caffe 模型完全兼容。</p>
</li>
<li><p>轻量级：编译后的 FeatherCNN 库的体积仅为数百 KB。</p>
</li>
</ul>
<h3 id="17-10-8-TensorFlow-Lite"><a href="#17-10-8-TensorFlow-Lite" class="headerlink" title="17.10.8 TensorFlow Lite"></a>17.10.8 TensorFlow Lite</h3><p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：谷歌　　　</p>
<p>３、GitHub地址：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite</a></p>
<p>４、简介：</p>
<p>Google 表示 Lite 版本 TensorFlow 是 TensorFlow Mobile 的一个延伸版本。此前，通过TensorFlow Mobile API，TensorFlow已经支持手机上的模型嵌入式部署。TensorFlow Lite应该被视为TensorFlow Mobile的升级版。</p>
<p>TensorFlow Lite可以与Android 8.1中发布的神经网络API完美配合，即便在没有硬件加速时也能调用CPU处理，确保模型在不同设备上的运行。 而Android端版本演进的控制权是掌握在谷歌手中的，从长期看，TensorFlow Lite会得到Android系统层面上的支持。</p>
<p>5、架构：</p>
<p><img src="/img/ch17/tflite_artc.JPEG" alt=""></p>
<p>其组件包括：</p>
<ul>
<li>TensorFlow 模型（TensorFlow Model）：保存在磁盘中的训练模型。</li>
<li>TensorFlow Lite 转化器（TensorFlow Lite Converter）：将模型转换成 TensorFlow Lite 文件格式的项目。</li>
<li>TensorFlow Lite 模型文件（TensorFlow Lite Model File）：基于 FlatBuffers，适配最大速度和最小规模的模型。</li>
</ul>
<p>6、移动端开发步骤：</p>
<p>Android Studio 3.0, SDK Version API26, NDK Version 14</p>
<p>步骤：</p>
<ol>
<li><p>将此项目导入到Android Studio：<br> <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo</a></p>
</li>
<li><p>下载移动端的模型（model）和标签数据（lables）：<br> <a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip" target="_blank" rel="noopener">https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip</a></p>
</li>
<li><p>下载完成解压mobilenet_v1_224_android_quant_2017_11_08.zip文件得到一个xxx.tflite和labes.txt文件，分别是模型和标签文件，并且把这两个文件复制到assets文件夹下。</p>
</li>
<li><p>构建app，run……</p>
</li>
</ol>
<p>17.7.9 TensorFlow Lite和TensorFlow Mobile的区别？</p>
<ul>
<li>TensorFlow Lite是TensorFlow Mobile的进化版。</li>
<li>在大多数情况下，TensorFlow Lite拥有跟小的二进制大小，更少的依赖以及更好的性能。</li>
<li>相比TensorFlow Mobile是对完整TensorFlow的裁减，TensorFlow Lite基本就是重新实现了。从内部实现来说，在TensorFlow内核最基本的OP，Context等数据结构，都是新的。从外在表现来说，模型文件从PB格式改成了FlatBuffers格式，TensorFlow的size有大幅度优化，降至300K，然后提供一个converter将普通TensorFlow模型转化成TensorFlow Lite需要的格式。因此，无论从哪方面看，TensorFlow Lite都是一个新的实现方案。</li>
</ul>
<h3 id="17-10-9-PocketFlow"><a href="#17-10-9-PocketFlow" class="headerlink" title="17.10.9 PocketFlow"></a>17.10.9 PocketFlow</h3><p>１、开源时间：2018年9月　　　</p>
<p>２、开源用户：腾讯　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/PocketFlow" target="_blank" rel="noopener">https://github.com/Tencent/PocketFlow</a></p>
<p>４、简介：</p>
<p>全球首个自动模型压缩框架</p>
<p>一款面向移动端AI开发者的自动模型压缩框架，集成了当前主流的模型压缩与训练算法，结合自研超参数优化组件实现了全程自动化托管式的模型压缩与加速。开发者无需了解具体算法细节，即可快速地将AI技术部署到移动端产品上，实现了自动托管式模型压缩与加速，实现用户数据的本地高效处理。</p>
<p>5、框架介绍</p>
<p>PocketFlow 框架主要由两部分组件构成，分别是模型压缩/加速算法组件和超参数优化组件，具体结构如下图所示。</p>
<p><img src="/img/ch17/framework_design.png" alt=""></p>
<p>​    开发者将未压缩的原始模型作为 PocketFlow 框架的输入，同时指定期望的性能指标，例如模型的压缩和/或加速倍数；在每一轮迭代过程中，超参数优化组件选取一组超参数取值组合，之后模型压缩/加速算法组件基于该超参数取值组合，对原始模型进行压缩，得到一个压缩后的候选模型；基于对候选模型进行性能评估的结果，超参数优化组件调整自身的模型参数，并选取一组新的超参数取值组合，以开始下一轮迭代过程；当迭代终止时，PocketFlow 选取最优的超参数取值组合以及对应的候选模型，作为最终输出，返回给开发者用作移动端的模型部署。</p>
<p>6、PocketFlow如何实现模型压缩与加速？</p>
<p>​    具体地，PocketFlow 通过下列各个算法组件的有效结合，实现了精度损失更小、自动化程度更高的深度学习模型的压缩与加速：</p>
<ul>
<li><p>a) 通道剪枝（channel pruning）组件：在CNN网络中，通过对特征图中的通道维度进行剪枝，可以同时降低模型大小和计算复杂度，并且压缩后的模型可以直接基于现有的深度学习框架进行部署。在CIFAR-10图像分类任务中，通过对  ResNet-56 模型进行通道剪枝，可以实现2.5倍加速下分类精度损失0.4%，3.3倍加速下精度损失0.7%。</p>
</li>
<li><p>b) 权重稀疏化（weight sparsification）组件：通过对网络权重引入稀疏性约束，可以大幅度降低网络权重中的非零元素个数；压缩后模型的网络权重可以以稀疏矩阵的形式进行存储和传输，从而实现模型压缩。对于  MobileNet 图像分类模型，在删去50%网络权重后，在 ImageNet 数据集上的 Top-1 分类精度损失仅为0.6%。</p>
</li>
<li><p>c) 权重量化（weight quantization）组件：通过对网络权重引入量化约束，可以降低用于表示每个网络权重所需的比特数；团队同时提供了对于均匀和非均匀两大类量化算法的支持，可以充分利用  ARM 和 FPGA 等设备的硬件优化，以提升移动端的计算效率，并为未来的神经网络芯片设计提供软件支持。以用于 ImageNet  图像分类任务的 ResNet-18 模型为例，在8比特定点量化下可以实现精度无损的4倍压缩。</p>
</li>
<li><p>d)网络蒸馏（network distillation）组件：对于上述各种模型压缩组件，通过将未压缩的原始模型的输出作为额外的监督信息，指导压缩后模型的训练，在压缩/加速倍数不变的前提下均可以获得0.5%-2.0%不等的精度提升。</p>
</li>
<li><p>e) 多GPU训练（multi-GPU training）组件：深度学习模型训练过程对计算资源要求较高，单个GPU难以在短时间内完成模型训练，因此团队提供了对于多机多卡分布式训练的全面支持，以加快使用者的开发流程。无论是基于  ImageNet 数据的Resnet-50图像分类模型还是基于 WMT14 数据的 Transformer  机器翻译模型，均可以在一个小时内训练完毕。[1] </p>
</li>
<li><p>f) 超参数优化（hyper-parameter optimization）组件：多数开发者对模型压缩算法往往不甚了解，但超参数取值对最终结果往往有着巨大的影响，因此团队引入了超参数优化组件，采用了包括强化学习等算法以及  AI Lab 自研的 AutoML  自动超参数优化框架来根据具体性能需求，确定最优超参数取值组合。例如，对于通道剪枝算法，超参数优化组件可以自动地根据原始模型中各层的冗余程度，对各层采用不同的剪枝比例，在保证满足模型整体压缩倍数的前提下，实现压缩后模型识别精度的最大化。</p>
<p><img src="/img/ch17/packflow1.jpg" alt=""></p>
</li>
</ul>
<p>7、PocketFlow 性能</p>
<p>​    通过引入超参数优化组件，不仅避免了高门槛、繁琐的人工调参工作，同时也使得  PocketFlow 在各个压缩算法上全面超过了人工调参的效果。以图像分类任务为例，在 CIFAR-10 和 ImageNet  等数据集上，PocketFlow 对 ResNet 和 MobileNet 等多种 CNN 网络结构进行有效的模型压缩与加速。</p>
<p>​    在  CIFAR-10 数据集上，PocketFlow 以 ResNet-56  作为基准模型进行通道剪枝，并加入了超参数优化和网络蒸馏等训练策略，实现了 2.5 倍加速下分类精度损失 0.4%，3.3 倍加速下精度损失  0.7%，且显著优于未压缩的 ResNet-44 模型； 在 ImageNet 数据集上，PocketFlow 可以对原本已经十分精简的  MobileNet 模型继续进行权重稀疏化，以更小的模型尺寸取得相似的分类精度；与 Inception-V1、ResNet-18  等模型相比，模型大小仅为后者的约 20~40%，但分类精度基本一致（甚至更高）。</p>
<p><img src="/img/ch17/packflow2.jpg" alt=""></p>
<p><img src="/img/ch17/packflow3.jpg" alt=""></p>
<p>相比于费时费力的人工调参，PocketFlow 框架中的 AutoML 自动超参数优化组件仅需 10<br>余次迭代就能达到与人工调参类似的性能，在经过 100 次迭代后搜索得到的超参数组合可以降低约 0.6%<br>的精度损失；通过使用超参数优化组件自动地确定网络中各层权重的量化比特数，PocketFlow 在对用于 ImageNet 图像分类任务的<br>ResNet-18 模型进行压缩时，取得了一致性的性能提升；当平均量化比特数为 4 比特时，超参数优化组件的引入可以将分类精度从 63.6%<br>提升至 68.1%（原始模型的分类精度为 70.3%）。</p>
<p><img src="/img/ch17/packflow4.jpg" alt=""></p>
<p><img src="/img/ch17/packflow5.jpg" alt=""></p>
<p><strong>参考文献</strong></p>
<p>[1]  Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao,  Qingyao Wu, Junzhou Huang, Jinhui Zhu,「Discrimination-aware Channel  Pruning for Deep Neural Networks”, In Proc. of the 32nd Annual  Conference on Neural Information Processing Systems, NIPS ‘18, Montreal,  Canada, December 2018.</p>
<p>[2] Jiaxiang  Wu, Weidong Huang, Junzhou Huang, Tong Zhang,「Error Compensated  Quantized SGD and its Applications to Large-scale Distributed  Optimization」, In Proc. of the 35th International Conference on Machine  Learning, ICML’18, Stockholm, Sweden, July 2018.</p>
<h3 id="17-10-10-其他几款支持移动端深度学习的开源框架"><a href="#17-10-10-其他几款支持移动端深度学习的开源框架" class="headerlink" title="17.10.10 其他几款支持移动端深度学习的开源框架"></a>17.10.10 其他几款支持移动端深度学习的开源框架</h3><p><a href="https://blog.csdn.net/zchang81/article/details/74280019" target="_blank" rel="noopener">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="17-10-11-MDL、NCNN和-TFLite比较"><a href="#17-10-11-MDL、NCNN和-TFLite比较" class="headerlink" title="17.10.11 MDL、NCNN和 TFLite比较"></a>17.10.11 MDL、NCNN和 TFLite比较</h3><p>百度-MDL框架、腾讯-NCNN框架和谷歌TFLite框架比较。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">MDL</th>
<th style="text-align:center">NCNN</th>
<th style="text-align:center">TFLite</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">代码质量</td>
<td style="text-align:center">中</td>
<td style="text-align:center">高</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">跨平台</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">支持caffe模型</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">×</td>
</tr>
<tr>
<td style="text-align:center">支持TensorFlow模型</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">CPU NEON指令优化</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">GPU加速</td>
<td style="text-align:center">√</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
</tr>
</tbody>
</table>
</div>
<p>相同点：</p>
<ul>
<li>只含推理（inference）功能，使用的模型文件需要通过离线的方式训练得到。</li>
<li>最终生成的库尺寸较小，均小于500kB。</li>
<li>为了提升执行速度，都使用了ARM NEON指令进行加速。</li>
<li>跨平台，iOS和Android系统都支持。</li>
</ul>
<p>不同点：</p>
<ul>
<li>MDL和NCNN均是只支持Caffe框架生成的模型文件，而TfLite则毫无意外的只支持自家大哥TensorFlow框架生成的模型文件。</li>
<li>MDL支持利用iOS系统的Matal框架进行GPU加速，能够显著提升在iPhone上的运行速度，达到准实时的效果。而NCNN和TFLite还没有这个功能。</li>
</ul>
<h2 id="17-11-移动端开源框架部署"><a href="#17-11-移动端开源框架部署" class="headerlink" title="17.11 移动端开源框架部署"></a>17.11 移动端开源框架部署</h2><h3 id="17-8-1-以NCNN为例"><a href="#17-8-1-以NCNN为例" class="headerlink" title="17.8.1 以NCNN为例"></a>17.8.1 以NCNN为例</h3><p>部署步骤   </p>
<h3 id="17-8-2-以QNNPACK为例"><a href="#17-8-2-以QNNPACK为例" class="headerlink" title="17.8.2 以QNNPACK为例"></a>17.8.2 以QNNPACK为例</h3><p>部署步骤     </p>
<h3 id="17-8-4-在Android手机上使用MACE实现图像分类"><a href="#17-8-4-在Android手机上使用MACE实现图像分类" class="headerlink" title="17.8.4 在Android手机上使用MACE实现图像分类"></a>17.8.4 在Android手机上使用MACE实现图像分类</h3><h3 id="17-8-3-在Android手机上使用PaddleMobile实现图像分类"><a href="#17-8-3-在Android手机上使用PaddleMobile实现图像分类" class="headerlink" title="17.8.3 在Android手机上使用PaddleMobile实现图像分类"></a>17.8.3 在Android手机上使用PaddleMobile实现图像分类</h3><p><strong>编译paddle-mobile库</strong></p>
<p>1）编译Android能够使用的CPP库：编译Android的paddle-mobile库，可选择使用Docker编译和Ubuntu交叉编译，这里介绍使用Ubuntu交叉编译paddle-mobile库。</p>
<p><em>注</em>：在Android项目，Java代码调用CPP代码，CPP的函数需要遵循一定的命名规范，比如Java_包名_类名_对应的Java的方法名。</p>
<p>​    目前官方提供了5个可以给Java调用的函数，该代码在：paddle-mobile/src/jni/paddle_mobile_jni.cpp，如果想要让这些函数能够在自己的包名下的类调用，就要修改CPP的函数名称修改如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">JNIEXPORT jboolean JNICALL <span class="title">Java_com_baidu_paddle_PML_load</span><span class="params">(JNIEnv *env, </span></span></span><br><span class="line"><span class="function"><span class="params">	jclass thiz,</span></span></span><br><span class="line"><span class="function"><span class="params">	jstring modelPath)</span> </span>&#123; </span><br><span class="line">		ANDROIDLOGI(<span class="string">"load invoked"</span>); </span><br><span class="line">		bool optimize = <span class="keyword">true</span>; </span><br><span class="line">		<span class="keyword">return</span> getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath), optimize); &#125;</span><br></pre></td></tr></table></figure>
<p>​    笔者项目的包名为<code>com.example.paddlemobile1</code>，在这个包下有一个<code>ImageRecognition.java</code>的程序来对应这个CPP程序，那么修改<code>load</code>函数如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">JNIEXPORT jboolean JNICALL <span class="title">Java_com_example_paddlemobile1_ImageRecognition_load</span><span class="params">(JNIEnv *env,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                          jclass thiz,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                          jstring modelPath)</span> </span>&#123;</span><br><span class="line">  ANDROIDLOGI(<span class="string">"load invoked"</span>);</span><br><span class="line">  bool optimize = <span class="keyword">true</span>;</span><br><span class="line">  <span class="keyword">return</span> getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath),</span><br><span class="line">                                         optimize);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>使用Ubuntu交叉编译paddle-mobile库</strong></p>
<p>1、下载和解压NDK。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;dl.google.com&#x2F;android&#x2F;repository&#x2F;android-ndk-r17b-linux-x86_64.zip</span><br><span class="line">unzip android-ndk-r17b-linux-x86_64.zip</span><br></pre></td></tr></table></figure>
<p>2、设置NDK环境变量，目录是NDK的解压目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export NDK_ROOT&#x3D;&quot;&#x2F;home&#x2F;test&#x2F;paddlepaddle&#x2F;android-ndk-r17b&quot;</span><br></pre></td></tr></table></figure>
<p>设置好之后，可以使用以下的命令查看配置情况。</p>
<pre><code>root@test:/home/test/paddlepaddle# echo $NDK_ROOT
/home/test/paddlepaddle/android-ndk-r17b
</code></pre><p>3、安装cmake，需要安装较高版本的，笔者的cmake版本是3.11.2。</p>
<p>下载cmake源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;cmake.org&#x2F;files&#x2F;v3.11&#x2F;cmake-3.11.2.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压cmake源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cmake-3.11.2.tar.gz</span><br></pre></td></tr></table></figure>
<p>进入到cmake源码根目录，并执行bootstrap。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd cmake-3.11.2</span><br><span class="line">.&#x2F;bootstrap</span><br></pre></td></tr></table></figure>
<p>最后执行以下两条命令开始安装cmake。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>安装完成之后，可以使用cmake —version是否安装成功.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@test:&#x2F;home&#x2F;test&#x2F;paddlepaddle# cmake --version</span><br><span class="line">cmake version 3.11.2</span><br><span class="line"></span><br><span class="line">CMake suite maintained and supported by Kitware (kitware.com&#x2F;cmake).</span><br></pre></td></tr></table></figure>
<p>4、克隆paddle-mobile源码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;PaddlePaddle&#x2F;paddle-mobile.git</span><br></pre></td></tr></table></figure>
<p>5、进入到paddle-mobile的tools目录下，执行编译。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd paddle-mobile&#x2F;tools&#x2F;</span><br><span class="line">sh build.sh android</span><br></pre></td></tr></table></figure>
<p>（可选）如果想编译针对某一个网络编译更小的库时，可以在命令后面加上相应的参数，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh build.sh android googlenet</span><br></pre></td></tr></table></figure>
<p>6、最后会在paddle-mobile/build/release/arm-v7a/build目录下生产paddle-mobile库。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@test:/home/test/paddlepaddle/paddle-mobile/build/release/arm-v7a/build# ls</span><br><span class="line">libpaddle-mobile.so</span><br></pre></td></tr></table></figure>
<p>libpaddle-mobile.so就是我们在开发Android项目的时候使用到的paddle-mobile库。</p>
<p><strong>创建Android项目</strong></p>
<p>1、首先使用Android Studio创建一个普通的Android项目，包名为<code>com.example.paddlemobile1</code></p>
<p>2、在main目录下创建l两个assets/paddle_models文件夹，这个文件夹存放PaddleFluid训练好的预测模型。PaddleMobile支持量化模型，使用模型量化可以把模型缩小至原来的四分之一，如果使用量化模型，那加载模型的接口也有修改一下，使用以下的接口加载模型：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadQualified</span><span class="params">(String modelDir)</span></span>;</span><br></pre></td></tr></table></figure>
<p>3、在<code>main</code>目录下创建一个<code>jniLibs</code>文件夹，这个文件夹是存放CPP编译库的，在本项目中就存放上一部分编译的<code>libpaddle-mobile.so</code></p>
<p>4、在Android项目的配置文件夹中加上权限声明，因为我们要使用到读取相册和使用相机，所以加上以下的权限声明：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.CAMERA"</span> /&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.WRITE_EXTERNAL_STORAGE"</span> /&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.READ_EXTERNAL_STORAGE"</span> /&gt;</span><br></pre></td></tr></table></figure>
<p>5、修改<code>activity_main.xml</code>界面，修改成如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span><br><span class="line">&lt;RelativeLayout xmlns:android=<span class="string">"http://schemas.android.com/apk/res/android"</span></span><br><span class="line">    xmlns:app=<span class="string">"http://schemas.android.com/apk/res-auto"</span></span><br><span class="line">    xmlns:tools=<span class="string">"http://schemas.android.com/tools"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"match_parent"</span></span><br><span class="line">    tools:context=<span class="string">".MainActivity"</span>&gt;</span><br><span class="line">&lt;LinearLayout</span><br><span class="line">    android:id=<span class="string">"@+id/btn_ll"</span></span><br><span class="line">    android:layout_alignParentBottom=<span class="string">"true"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">    android:orientation=<span class="string">"horizontal"</span>&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Button</span><br><span class="line">        android:id=<span class="string">"@+id/use_photo"</span></span><br><span class="line">        android:layout_weight=<span class="string">"1"</span></span><br><span class="line">        android:layout_width=<span class="string">"0dp"</span></span><br><span class="line">        android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">        android:text=<span class="string">"相册"</span> /&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Button</span><br><span class="line">        android:id=<span class="string">"@+id/start_camera"</span></span><br><span class="line">        android:layout_weight=<span class="string">"1"</span></span><br><span class="line">        android:layout_width=<span class="string">"0dp"</span></span><br><span class="line">        android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">        android:text=<span class="string">"拍照"</span> /&gt;</span><br><span class="line">&lt;/LinearLayout&gt;</span><br><span class="line"></span><br><span class="line">&lt;TextView</span><br><span class="line">    android:layout_above=<span class="string">"@id/btn_ll"</span></span><br><span class="line">    android:id=<span class="string">"@+id/result_text"</span></span><br><span class="line">    android:textSize=<span class="string">"16sp"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:hint=<span class="string">"预测结果会在这里显示"</span></span><br><span class="line">    android:layout_height=<span class="string">"100dp"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;ImageView</span><br><span class="line">    android:layout_alignParentTop=<span class="string">"true"</span></span><br><span class="line">    android:layout_above=<span class="string">"@id/result_text"</span></span><br><span class="line">    android:id=<span class="string">"@+id/show_image"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"match_parent"</span> /&gt;</span><br><span class="line">&lt;/RelativeLayout&gt;</span><br></pre></td></tr></table></figure>
<p>6、创建一个<code>ImageRecognition.java</code>的Java程序，这个程序的作用就是调用<code>paddle-mobile/src/jni/paddle_mobile_jni.cpp</code>的函数，对应的是里面的函数。目前支持一下几个接口。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ImageRecognition</span> </span>&#123;</span><br><span class="line">    <span class="comment">// set thread num</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">setThread</span><span class="params">(<span class="keyword">int</span> threadCount)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Load seperated parameters</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">load</span><span class="params">(String modelDir)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load qualified model</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadQualified</span><span class="params">(String modelDir)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Load combined parameters</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadCombined</span><span class="params">(String modelPath, String paramPath)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load qualified model</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadCombinedQualified</span><span class="params">(String modelPath, String paramPath)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// object detection</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">float</span>[] predictImage(<span class="keyword">float</span>[] buf, <span class="keyword">int</span>[]ddims);</span><br><span class="line"></span><br><span class="line"><span class="comment">// predict yuv image</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">float</span>[] predictYuv(<span class="keyword">byte</span>[] buf, <span class="keyword">int</span> imgWidth, <span class="keyword">int</span> imgHeight, <span class="keyword">int</span>[] ddims, <span class="keyword">float</span>[]meanValues);</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear model</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>7、然后编写一个<code>PhotoUtil.java</code>的工具类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.app.Activity;</span><br><span class="line"><span class="keyword">import</span> android.content.Context;</span><br><span class="line"><span class="keyword">import</span> android.content.Intent;</span><br><span class="line"><span class="keyword">import</span> android.database.Cursor;</span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap;</span><br><span class="line"><span class="keyword">import</span> android.graphics.BitmapFactory;</span><br><span class="line"><span class="keyword">import</span> android.net.Uri;</span><br><span class="line"><span class="keyword">import</span> android.os.Build;</span><br><span class="line"><span class="keyword">import</span> android.provider.MediaStore;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.content.FileProvider;</span><br><span class="line"><span class="keyword">import</span> android.util.Log;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhotoUtil</span> </span>&#123;</span><br><span class="line"><span class="comment">// start camera</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Uri <span class="title">start_camera</span><span class="params">(Activity activity, <span class="keyword">int</span> requestCode)</span> </span>&#123;</span><br><span class="line">    Uri imageUri;</span><br><span class="line">    <span class="comment">// save image in cache path</span></span><br><span class="line">    File outputImage = <span class="keyword">new</span> File(activity.getExternalCacheDir(), <span class="string">"out_image.jpg"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (outputImage.exists()) &#123;</span><br><span class="line">            outputImage.delete();</span><br><span class="line">        &#125;</span><br><span class="line">        outputImage.createNewFile();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (Build.VERSION.SDK_INT &gt;= <span class="number">24</span>) &#123;</span><br><span class="line">        <span class="comment">// compatible with Android 7.0 or over</span></span><br><span class="line">        imageUri = FileProvider.getUriForFile(activity,</span><br><span class="line">                <span class="string">"com.example.paddlemobile1"</span>, outputImage);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        imageUri = Uri.fromFile(outputImage);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// set system camera Action</span></span><br><span class="line">    Intent intent = <span class="keyword">new</span> Intent(MediaStore.ACTION_IMAGE_CAPTURE);</span><br><span class="line">    <span class="comment">// set save photo path</span></span><br><span class="line">    intent.putExtra(MediaStore.EXTRA_OUTPUT, imageUri);</span><br><span class="line">    <span class="comment">// set photo quality, min is 0, max is 1</span></span><br><span class="line">    intent.putExtra(MediaStore.EXTRA_VIDEO_QUALITY, <span class="number">0</span>);</span><br><span class="line">    activity.startActivityForResult(intent, requestCode);</span><br><span class="line">    <span class="keyword">return</span> imageUri;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// get picture in photo</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">use_photo</span><span class="params">(Activity activity, <span class="keyword">int</span> requestCode)</span></span>&#123;</span><br><span class="line">    Intent intent = <span class="keyword">new</span> Intent(Intent.ACTION_PICK);</span><br><span class="line">    intent.setType(<span class="string">"image/*"</span>);</span><br><span class="line">    activity.startActivityForResult(intent, requestCode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// get photo from Uri</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">get_path_from_URI</span><span class="params">(Context context, Uri uri)</span> </span>&#123;</span><br><span class="line">    String result;</span><br><span class="line">    Cursor cursor = context.getContentResolver().query(uri, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (cursor == <span class="keyword">null</span>) &#123;</span><br><span class="line">        result = uri.getPath();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cursor.moveToFirst();</span><br><span class="line">        <span class="keyword">int</span> idx = cursor.getColumnIndex(MediaStore.Images.ImageColumns.DATA);</span><br><span class="line">        result = cursor.getString(idx);</span><br><span class="line">        cursor.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Compress the image to the size of the training image，and change RGB</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">float</span>[] getScaledMatrix(Bitmap bitmap, <span class="keyword">int</span> desWidth,</span><br><span class="line">                               <span class="keyword">int</span> desHeight) &#123;</span><br><span class="line">    <span class="keyword">float</span>[] dataBuf = <span class="keyword">new</span> <span class="keyword">float</span>[<span class="number">3</span> * desWidth * desHeight];</span><br><span class="line">    <span class="keyword">int</span> rIndex;</span><br><span class="line">    <span class="keyword">int</span> gIndex;</span><br><span class="line">    <span class="keyword">int</span> bIndex;</span><br><span class="line">    <span class="keyword">int</span>[] pixels = <span class="keyword">new</span> <span class="keyword">int</span>[desWidth * desHeight];</span><br><span class="line">    Bitmap bm = Bitmap.createScaledBitmap(bitmap, desWidth, desHeight, <span class="keyword">false</span>);</span><br><span class="line">    bm.getPixels(pixels, <span class="number">0</span>, desWidth, <span class="number">0</span>, <span class="number">0</span>, desWidth, desHeight);</span><br><span class="line">    <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; pixels.length; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> clr = pixels[i];</span><br><span class="line">        j = i / desHeight;</span><br><span class="line">        k = i % desWidth;</span><br><span class="line">        rIndex = j * desWidth + k;</span><br><span class="line">        gIndex = rIndex + desHeight * desWidth;</span><br><span class="line">        bIndex = gIndex + desHeight * desWidth;</span><br><span class="line">        dataBuf[rIndex] = (<span class="keyword">float</span>) ((clr &amp; <span class="number">0x00ff0000</span>) &gt;&gt; <span class="number">16</span>) - <span class="number">148</span>;</span><br><span class="line">        dataBuf[gIndex] = (<span class="keyword">float</span>) ((clr &amp; <span class="number">0x0000ff00</span>) &gt;&gt; <span class="number">8</span>) - <span class="number">148</span>;</span><br><span class="line">        dataBuf[bIndex] = (<span class="keyword">float</span>) ((clr &amp; <span class="number">0x000000ff</span>)) - <span class="number">148</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (bm.isRecycled()) &#123;</span><br><span class="line">        bm.recycle();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dataBuf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// compress picture</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Bitmap <span class="title">getScaleBitmap</span><span class="params">(String filePath)</span> </span>&#123;</span><br><span class="line">    BitmapFactory.Options opt = <span class="keyword">new</span> BitmapFactory.Options();</span><br><span class="line">    opt.inJustDecodeBounds = <span class="keyword">true</span>;</span><br><span class="line">    BitmapFactory.decodeFile(filePath, opt);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> bmpWidth = opt.outWidth;</span><br><span class="line">    <span class="keyword">int</span> bmpHeight = opt.outHeight;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> maxSize = <span class="number">500</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// compress picture with inSampleSize</span></span><br><span class="line">    opt.inSampleSize = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (bmpWidth / opt.inSampleSize &lt; maxSize || bmpHeight / opt.inSampleSize &lt; maxSize) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        opt.inSampleSize *= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    opt.inJustDecodeBounds = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">return</span> BitmapFactory.decodeFile(filePath, opt);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>start_camera()方法是启动相机并返回图片的URI。</li>
<li>use_photo()方法是打开相册，获取到的图片URI在回到函数中获取。</li>
<li>get_path_from_URI()方法是把图片的URI转换成绝对路径。</li>
<li>getScaledMatrix()方法是把图片压缩成跟训练时的大小，并转换成预测需要用的数据格式浮点数组。</li>
<li>getScaleBitmap()方法是对图片进行等比例压缩，减少内存的支出。</li>
</ul>
<p>8、最后修改<code>MainActivity.java</code>，修改如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.Manifest;</span><br><span class="line"><span class="keyword">import</span> android.annotation.SuppressLint;</span><br><span class="line"><span class="keyword">import</span> android.app.Activity;</span><br><span class="line"><span class="keyword">import</span> android.content.Context;</span><br><span class="line"><span class="keyword">import</span> android.content.Intent;</span><br><span class="line"><span class="keyword">import</span> android.content.pm.PackageManager;</span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap;</span><br><span class="line"><span class="keyword">import</span> android.net.Uri;</span><br><span class="line"><span class="keyword">import</span> android.os.Bundle;</span><br><span class="line"><span class="keyword">import</span> android.os.Environment;</span><br><span class="line"><span class="keyword">import</span> android.support.annotation.NonNull;</span><br><span class="line"><span class="keyword">import</span> android.support.annotation.Nullable;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.app.ActivityCompat;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.content.ContextCompat;</span><br><span class="line"><span class="keyword">import</span> android.support.v7.app.AppCompatActivity;</span><br><span class="line"><span class="keyword">import</span> android.util.Log;</span><br><span class="line"><span class="keyword">import</span> android.view.View;</span><br><span class="line"><span class="keyword">import</span> android.widget.Button;</span><br><span class="line"><span class="keyword">import</span> android.widget.ImageView;</span><br><span class="line"><span class="keyword">import</span> android.widget.TextView;</span><br><span class="line"><span class="keyword">import</span> android.widget.Toast;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.Glide;</span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.load.engine.DiskCacheStrategy;</span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.request.RequestOptions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainActivity</span> <span class="keyword">extends</span> <span class="title">AppCompatActivity</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TAG = MainActivity<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>()</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> USE_PHOTO = <span class="number">1001</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> START_CAMERA = <span class="number">1002</span>;</span><br><span class="line">    <span class="keyword">private</span> Uri image_uri;</span><br><span class="line">    <span class="keyword">private</span> ImageView show_image;</span><br><span class="line">    <span class="keyword">private</span> TextView result_text;</span><br><span class="line">    <span class="keyword">private</span> String assets_path = <span class="string">"paddle_models"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> load_result = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[] ddims = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] PADDLE_MODEL = &#123;</span><br><span class="line">        <span class="string">"lenet"</span>,</span><br><span class="line">        <span class="string">"alexnet"</span>,</span><br><span class="line">        <span class="string">"vgg16"</span>,</span><br><span class="line">        <span class="string">"resnet"</span>,</span><br><span class="line">        <span class="string">"googlenet"</span>,</span><br><span class="line">        <span class="string">"mobilenet_v1"</span>,</span><br><span class="line">        <span class="string">"mobilenet_v2"</span>,</span><br><span class="line">        <span class="string">"inception_v1"</span>,</span><br><span class="line">        <span class="string">"inception_v2"</span>,</span><br><span class="line">        <span class="string">"squeezenet"</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load paddle-mobile api</span></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        System.loadLibrary(<span class="string">"paddle-mobile"</span>);</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (SecurityException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (UnsatisfiedLinkError e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (NullPointerException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.onCreate(savedInstanceState);</span><br><span class="line">    setContentView(R.layout.activity_main);</span><br><span class="line"></span><br><span class="line">    init();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// initialize view</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    request_permissions();</span><br><span class="line">    show_image = (ImageView) findViewById(R.id.show_image);</span><br><span class="line">    result_text = (TextView) findViewById(R.id.result_text);</span><br><span class="line">    Button use_photo = (Button) findViewById(R.id.use_photo);</span><br><span class="line">    Button start_photo = (Button) findViewById(R.id.start_camera);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// use photo click</span></span><br><span class="line">    use_photo.setOnClickListener(<span class="keyword">new</span> View.OnClickListener() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onClick</span><span class="params">(View view)</span> </span>&#123;</span><br><span class="line">            PhotoUtil.use_photo(MainActivity.<span class="keyword">this</span>, USE_PHOTO);</span><br><span class="line">            <span class="comment">//                load_model();</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">// start camera click</span></span><br><span class="line">    start_photo.setOnClickListener(<span class="keyword">new</span> View.OnClickListener() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onClick</span><span class="params">(View view)</span> </span>&#123;</span><br><span class="line">            image_uri = PhotoUtil.start_camera(MainActivity.<span class="keyword">this</span>, START_CAMERA);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy file from assets to sdcard</span></span><br><span class="line">    String sdcard_path = Environment.getExternalStorageDirectory()</span><br><span class="line">            + File.separator + assets_path;</span><br><span class="line">    copy_file_from_asset(<span class="keyword">this</span>, assets_path, sdcard_path);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// load model</span></span><br><span class="line">    load_model();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load infer model</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">load_model</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String model_path = Environment.getExternalStorageDirectory()</span><br><span class="line">            + File.separator + assets_path + File.separator + PADDLE_MODEL[<span class="number">4</span>];</span><br><span class="line">    Log.d(TAG, model_path);</span><br><span class="line">    load_result = ImageRecognition.load(model_path);</span><br><span class="line">    <span class="keyword">if</span> (load_result) &#123;</span><br><span class="line">        Log.d(TAG, <span class="string">"model load success"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        Log.d(TAG, <span class="string">"model load fail"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear infer model</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">clear_model</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ImageRecognition.clear();</span><br><span class="line">    Log.d(TAG, <span class="string">"model is clear"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy file from asset to sdcard</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copy_file_from_asset</span><span class="params">(Context context, String oldPath, String newPath)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        String[] fileNames = context.getAssets().list(oldPath);</span><br><span class="line">        <span class="keyword">if</span> (fileNames.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// directory</span></span><br><span class="line">            File file = <span class="keyword">new</span> File(newPath);</span><br><span class="line">            <span class="keyword">if</span> (!file.exists()) &#123;</span><br><span class="line">                file.mkdirs();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// copy recursivelyC</span></span><br><span class="line">            <span class="keyword">for</span> (String fileName : fileNames) &#123;</span><br><span class="line">                copy_file_from_asset(context, oldPath + <span class="string">"/"</span> + fileName, newPath + <span class="string">"/"</span> + fileName);</span><br><span class="line">            &#125;</span><br><span class="line">            Log.d(TAG, <span class="string">"copy files finish"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// file</span></span><br><span class="line">            File file = <span class="keyword">new</span> File(newPath);</span><br><span class="line">            <span class="comment">// if file exists will never copy</span></span><br><span class="line">            <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// copy file to new path</span></span><br><span class="line">            InputStream is = context.getAssets().open(oldPath);</span><br><span class="line">            FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(file);</span><br><span class="line">            <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">            <span class="keyword">int</span> byteCount;</span><br><span class="line">            <span class="keyword">while</span> ((byteCount = is.read(buffer)) != -<span class="number">1</span>) &#123;</span><br><span class="line">                fos.write(buffer, <span class="number">0</span>, byteCount);</span><br><span class="line">            &#125;</span><br><span class="line">            fos.flush();</span><br><span class="line">            is.close();</span><br><span class="line">            fos.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onActivityResult</span><span class="params">(<span class="keyword">int</span> requestCode, <span class="keyword">int</span> resultCode, @Nullable Intent data)</span> </span>&#123;</span><br><span class="line">    String image_path;</span><br><span class="line">    RequestOptions options = <span class="keyword">new</span> RequestOptions().skipMemoryCache(<span class="keyword">true</span>).diskCacheStrategy(DiskCacheStrategy.NONE);</span><br><span class="line">    <span class="keyword">if</span> (resultCode == Activity.RESULT_OK) &#123;</span><br><span class="line">        <span class="keyword">switch</span> (requestCode) &#123;</span><br><span class="line">            <span class="keyword">case</span> USE_PHOTO:</span><br><span class="line">                <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    Log.w(TAG, <span class="string">"user photo data is null"</span>);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                image_uri = data.getData();</span><br><span class="line">                Glide.with(MainActivity.<span class="keyword">this</span>).load(image_uri).apply(options).into(show_image);</span><br><span class="line">                <span class="comment">// get image path from uri</span></span><br><span class="line">                image_path = PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri);</span><br><span class="line">                <span class="comment">// show result</span></span><br><span class="line">                result_text.setText(image_path);</span><br><span class="line">                <span class="comment">// predict image</span></span><br><span class="line">                predict_image(PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri));</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> START_CAMERA:</span><br><span class="line">                <span class="comment">// show photo</span></span><br><span class="line">                Glide.with(MainActivity.<span class="keyword">this</span>).load(image_uri).apply(options).into(show_image);</span><br><span class="line">                <span class="comment">// get image path from uri</span></span><br><span class="line">                image_path = PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri);</span><br><span class="line">                <span class="comment">// show result</span></span><br><span class="line">                result_text.setText(image_path);</span><br><span class="line">                <span class="comment">// predict image</span></span><br><span class="line">                predict_image(PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri));</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SuppressLint</span>(<span class="string">"SetTextI18n"</span>)</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">predict_image</span><span class="params">(String image_path)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// picture to float array</span></span><br><span class="line">    Bitmap bmp = PhotoUtil.getScaleBitmap(image_path);</span><br><span class="line">    <span class="keyword">float</span>[] inputData = PhotoUtil.getScaledMatrix(bmp, ddims[<span class="number">2</span>], ddims[<span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">        <span class="comment">// get predict result</span></span><br><span class="line">        <span class="keyword">float</span>[] result = ImageRecognition.predictImage(inputData, ddims);</span><br><span class="line">        Log.d(TAG, <span class="string">"origin predict result:"</span> + Arrays.toString(result));</span><br><span class="line">        <span class="keyword">long</span> end = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">long</span> time = end - start;</span><br><span class="line">        Log.d(<span class="string">"result length"</span>, String.valueOf(result.length));</span><br><span class="line">        <span class="comment">// show predict result and time</span></span><br><span class="line">        <span class="keyword">int</span> r = get_max_result(result);</span><br><span class="line">        String show_text = <span class="string">"result："</span> + r + <span class="string">"\nprobability："</span> + result[r] + <span class="string">"\ntime："</span> + time + <span class="string">"ms"</span>;</span><br><span class="line">        result_text.setText(show_text);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">get_max_result</span><span class="params">(<span class="keyword">float</span>[] result)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span> probability = result[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> r = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; result.length; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (probability &lt; result[i]) &#123;</span><br><span class="line">            probability = result[i];</span><br><span class="line">            r = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> r;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// request permissions</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">request_permissions</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; permissionList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="keyword">this</span>, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) &#123;</span><br><span class="line">        permissionList.add(Manifest.permission.CAMERA);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="keyword">this</span>, Manifest.permission.WRITE_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) &#123;</span><br><span class="line">        permissionList.add(Manifest.permission.WRITE_EXTERNAL_STORAGE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="keyword">this</span>, Manifest.permission.READ_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) &#123;</span><br><span class="line">        permissionList.add(Manifest.permission.READ_EXTERNAL_STORAGE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if list is not empty will request permissions</span></span><br><span class="line">    <span class="keyword">if</span> (!permissionList.isEmpty()) &#123;</span><br><span class="line">        ActivityCompat.requestPermissions(<span class="keyword">this</span>, permissionList.toArray(<span class="keyword">new</span> String[permissionList.size()]), <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onRequestPermissionsResult</span><span class="params">(<span class="keyword">int</span> requestCode, @NonNull String[] permissions, @NonNull <span class="keyword">int</span>[] grantResults)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.onRequestPermissionsResult(requestCode, permissions, grantResults);</span><br><span class="line">    <span class="keyword">switch</span> (requestCode) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> (grantResults.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; grantResults.length; i++) &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">int</span> grantResult = grantResults[i];</span><br><span class="line">                    <span class="keyword">if</span> (grantResult == PackageManager.PERMISSION_DENIED) &#123;</span><br><span class="line">                        String s = permissions[i];</span><br><span class="line">                        Toast.makeText(<span class="keyword">this</span>, s + <span class="string">" permission was denied"</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onDestroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// clear model before destroy app</span></span><br><span class="line">    clear_model();</span><br><span class="line">    <span class="keyword">super</span>.onDestroy();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>load_model()方法是加载预测模型的。</li>
<li>clear_model()方法是清空预测模型的。</li>
<li>copy_file_from_asset()方法是把预测模型复制到内存卡上。</li>
<li>predict_image()方法是预测图片的。</li>
<li>get_max_result()方法是获取概率最大的预测结果。</li>
<li>request_permissions()方法是动态请求权限的。</li>
</ul>
<p>因为使用到图像加载框架Glide，所以要在<code>build.gradle</code>加入以下的引用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">implementation &#39;com.github.bumptech.glide:glide:4.3.1&#39;</span><br></pre></td></tr></table></figure>
<p>8、最后运行项目，选择图片预测就会得到结果。</p>
<h2 id="17-9-移动端开源框架部署疑难"><a href="#17-9-移动端开源框架部署疑难" class="headerlink" title="17.9 移动端开源框架部署疑难"></a>17.9 移动端开源框架部署疑难</h2><p>增加常见的几个问题</p>
<p>知识蒸馏（Distillation）相关论文阅读（1）——Distilling the Knowledge in a Neural Network（以及代码复现）</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0_%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/" class="post-title-link" itemprop="url">第十四章_超参数调整</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:35:00 / 修改时间：12:38:25" itemprop="dateCreated datePublished" datetime="2020-03-03T12:35:00+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p>﻿﻿﻿﻿﻿﻿﻿﻿﻿# 第十四章 超参数调整  </p>
<h2 id="14-1-写在前面"><a href="#14-1-写在前面" class="headerlink" title="14.1 写在前面"></a>14.1 写在前面</h2><p>​    关于训练深度学习模型最难的事情之一是你要处理的参数的数量。无论是从网络本身的层宽（宽度）、层数（深度）、连接方式，还是损失函数的超参数设计和调试，亦或者是学习率、批样本数量、优化器参数等等。这些大量的参数都会有网络模型最终的有效容限直接或者间接的影响。面对如此众多的参数，如果我们要一一对其优化调整，所需的无论是时间、资源都是不切实际。结果证实一些超参数比其它的更为重要，因此认识各个超参数的作用和其可能会造成的影响是深度学习训练中必不可少的一项重要技能。</p>
<p>​    超参数调整可以说是深度学习中理论和实际联系最重要的一个环节。目前，深度学习仍存在很多不可解释的部分，如何设计优化出好的网络可以为深度学习理论的探索提供重要的支持。超参数调整一般分为手动调整和自动优化超参数两种。读者可先浏览思维导图，本章节不会过多阐述所有超参数的详细原理，如果需要了解这部分，您可以翻阅前面的基础章节或者查阅相关文献资料。当然，下面会讲到的一些超参数优化的建议是根据笔者们的实践以及部分文献资料得到认知建议，并不是非常严格且一定有效的，很多研究者可能会很不同意某些的观点或有着不同的直觉，这都是可保留讨论的，因为这很依赖于数据本身情况。</p>
<p><img src="/img/ch14/思维导图.png" alt=""></p>
<p>​    </p>
<h2 id="14-2-超参数概念"><a href="#14-2-超参数概念" class="headerlink" title="14.2 超参数概念"></a>14.2 超参数概念</h2><h3 id="14-2-1-什么是超参数，参数和超参数的区别？"><a href="#14-2-1-什么是超参数，参数和超参数的区别？" class="headerlink" title="14.2.1 什么是超参数，参数和超参数的区别？"></a>14.2.1 什么是超参数，参数和超参数的区别？</h3><p>​    区分两者最大的一点就是是否通过数据来进行调整，模型参数通常是有数据来驱动调整，超参数则不需要数据来驱动，而是在训练前或者训练中人为的进行调整的参数。例如卷积核的具体核参数就是指模型参数，这是有数据驱动的。而学习率则是人为来进行调整的超参数。这里需要注意的是，通常情况下卷积核数量、卷积核尺寸这些也是超参数，注意与卷积核的核参数区分。</p>
<h3 id="14-2-2-神经网络中包含哪些超参数？"><a href="#14-2-2-神经网络中包含哪些超参数？" class="headerlink" title="14.2.2 神经网络中包含哪些超参数？"></a>14.2.2 神经网络中包含哪些超参数？</h3><p>　　 通常可以将超参数分为三类：网络参数、优化参数、正则化参数。</p>
<p>​    网络参数：可指网络层与层之间的交互方式（相加、相乘或者串接等）、卷积核数量和卷积核尺寸、网络层数（也称深度）和激活函数等。</p>
<p>​    优化参数：一般指学习率（learning rate）、批样本数量（batch size）、不同优化器的参数以及部分损失函数的可调参数。</p>
<p>​    正则化：权重衰减系数，丢弃比率（dropout）</p>
<h3 id="14-2-3-为什么要进行超参数调优？"><a href="#14-2-3-为什么要进行超参数调优？" class="headerlink" title="14.2.3 为什么要进行超参数调优？"></a>14.2.3 为什么要进行超参数调优？</h3><p>​    本质上，这是模型优化寻找最优解和正则项之间的关系。网络模型优化调整的目的是为了寻找到全局最优解（或者相比更好的局部最优解），而正则项又希望模型尽量拟合到最优。两者通常情况下，存在一定的对立，但两者的目标是一致的，即最小化期望风险。模型优化希望最小化经验风险，而容易陷入过拟合，正则项用来约束模型复杂度。所以如何平衡两者之间的关系，得到最优或者较优的解就是超参数调整优化的目的。</p>
<h3 id="14-2-4-超参数的重要性顺序"><a href="#14-2-4-超参数的重要性顺序" class="headerlink" title="14.2.4 超参数的重要性顺序"></a>14.2.4 超参数的重要性顺序</h3><ul>
<li><p>首先， <strong>学习率，损失函数上的可调参数</strong>。在网络参数、优化参数、正则化参数中最重要的超参数可能就是学习率了。学习率直接控制着训练中网络梯度更新的量级，直接影响着模型的<strong>有效容限能力</strong>；损失函数上的可调参数，这些参数通常情况下需要结合实际的损失函数来调整，大部分情况下这些参数也能很直接的影响到模型的的有效容限能力。这些损失一般可分成三类，第一类辅助损失结合常见的损失函数，起到辅助优化特征表达的作用。例如度量学习中的Center loss，通常结合交叉熵损失伴随一个权重完成一些特定的任务。这种情况下一般建议辅助损失值不高于或者不低于交叉熵损失值的两个数量级；第二类，多任务模型的多个损失函数，每个损失函数之间或独立或相关，用于各自任务，这种情况取决于任务之间本身的相关性，目前笔者并没有一个普适的经验由于提供参考；第三类，独立损失函数，这类损失通常会在特定的任务有显著性的效果。例如RetinaNet中的focal loss，其中的参数γ，α，对最终的效果会产生较大的影响。这类损失通常论文中会给出特定的建议值。</p>
</li>
<li><p>其次，<strong>批样本数量，动量优化器（Gradient Descent with Momentum）的动量参数<em>β</em></strong>。批样本决定了数量梯度下降的方向。过小的批数量，极端情况下，例如batch size为1，即每个样本都去修正一次梯度方向，样本之间的差异越大越难以收敛。若网络中存在批归一化（batchnorm），batch size过小则更难以收敛，甚至垮掉。这是因为数据样本越少，统计量越不具有代表性，噪声也相应的增加。而过大的batch size，会使得梯度方向基本稳定，容易陷入局部最优解，降低精度。一般参考范围会取在[1:1024]之间，当然这个不是绝对的，需要结合具体场景和样本情况；动量衰减参数<em>β</em>是计算梯度的指数加权平均数，并利用该值来更新参数，设置为 0.9 是一个常见且效果不错的选择；</p>
</li>
</ul>
<ul>
<li>最后，<strong>Adam优化器的超参数、权重衰减系数、丢弃法比率（dropout）和网络参数</strong>。在这里说明下，这些参数重要性放在最后<strong>并不等价于这些参数不重要</strong>。而是表示这些参数在大部分实践中<strong>不建议过多尝试</strong>，例如Adam优化器中的<em>β1，β2，ϵ</em>，常设为 0.9、0.999、10−8就会有不错的表现。权重衰减系数通常会有个建议值，例如0.0005 ，使用建议值即可，不必过多尝试。dropout通常会在全连接层之间使用防止过拟合，建议比率控制在[0.2,0.5]之间。使用dropout时需要特别注意两点：一、在RNN中，如果直接放在memory cell中,循环会放大噪声，扰乱学习。一般会建议放在输入和输出层；二、不建议dropout后直接跟上batchnorm，dropout很可能影响batchnorm计算统计量，导致方差偏移，这种情况下会使得推理阶段出现模型完全垮掉的极端情况；网络参数通常也属于超参数的范围内，通常情况下增加网络层数能增加模型的容限能力，但模型真正有效的容限能力还和样本数量和质量、层之间的关系等有关，所以一般情况下会选择先固定网络层数，调优到一定阶段或者有大量的硬件资源支持可以在网络深度上进行进一步调整。</li>
</ul>
<h3 id="14-2-5-部分超参数如何影响模型性能？"><a href="#14-2-5-部分超参数如何影响模型性能？" class="headerlink" title="14.2.5 部分超参数如何影响模型性能？"></a>14.2.5 部分超参数如何影响模型性能？</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">超参数</th>
<th style="text-align:center">如何影响模型容量</th>
<th style="text-align:center">原因</th>
<th style="text-align:center">注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">学习率</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">过高或者过低的学习率，都会由于优化失败而导致降低模型有效容限</td>
<td style="text-align:center">学习率最优点，在训练的不同时间点都可能变化，所以需要一套有效的学习率衰减策略</td>
</tr>
<tr>
<td style="text-align:center">损失函数部分超参数</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">损失函数超参数大部分情况都会可能影响优化，不合适的超参数会使即便是对目标优化非常合适的损失函数同样难以优化模型，降低模型有效容限。</td>
<td style="text-align:center">对于部分损失函数超参数其变化会对结果十分敏感，而有些则并不会太影响。在调整时，建议参考论文的推荐值，并在该推荐值数量级上进行最大最小值调试该参数对结果的影响。</td>
</tr>
<tr>
<td style="text-align:center">批样本数量</td>
<td style="text-align:center">过大过小，容易降低有效容量</td>
<td style="text-align:center">大部分情况下，选择适合自身硬件容量的批样本数量，并不会对模型容限造成。</td>
<td style="text-align:center">在一些特殊的目标函数的设计中，如何选择样本是很可能影响到模型的有效容限的，例如度量学习（metric learning）中的N-pair loss。这类损失因为需要样本的多样性，可能会依赖于批样本数量。</td>
</tr>
<tr>
<td style="text-align:center">丢弃法</td>
<td style="text-align:center">比率降低会提升模型的容量</td>
<td style="text-align:center">较少的丢弃参数意味着模型参数量的提升，参数间适应性提升，模型容量提升，但不一定能提升模型有效容限</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数</td>
<td style="text-align:center">调至最优，提升有效容量</td>
<td style="text-align:center">权重衰减可以有效的起到限制参数变化的幅度，起到一定的正则作用</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">优化器动量</td>
<td style="text-align:center">调至最优，可能提升有效容量</td>
<td style="text-align:center">动量参数通常用来加快训练，同时更容易跳出极值点，避免陷入局部最优解。</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">模型深度</td>
<td style="text-align:center">同条件下，深度增加，模型容量提升</td>
<td style="text-align:center">同条件，下增加深度意味着模型具有更多的参数，更强的拟合能力。</td>
<td style="text-align:center">同条件下，深度越深意味着参数越多，需要的时间和硬件资源也越高。</td>
</tr>
<tr>
<td style="text-align:center">卷积核尺寸</td>
<td style="text-align:center">尺寸增加，模型容量提升</td>
<td style="text-align:center">增加卷积核尺寸意味着参数量的增加，同条件下，模型参数也相应的增加。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="14-2-6-部分超参数合适的范围"><a href="#14-2-6-部分超参数合适的范围" class="headerlink" title="14.2.6 部分超参数合适的范围"></a>14.2.6 部分超参数合适的范围</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">超参数</th>
<th style="text-align:center">建议范围</th>
<th style="text-align:center">注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">初始学习率</td>
<td style="text-align:center">SGD: [1e-2, 1e-1]<br />momentum: [1e-3, 1e-2]<br />Adagrad: [1e-3, 1e-2]<br />Adadelta: [1e-2, 1e-1]<br />RMSprop: [1e-3, 1e-2]<br />Adam: [1e-3, 1e-2]<br />Adamax: [1e-3, 1e-2]<br />Nadam: [1e-3, 1e-2]</td>
<td style="text-align:center">这些范围通常是指从头开始训练的情况。若是微调，初始学习率可在降低一到两个数量级。</td>
</tr>
<tr>
<td style="text-align:center">损失函数部分超参数</td>
<td style="text-align:center">多个损失函数之间，损失值之间尽量相近，不建议超过或者低于两个数量级</td>
<td style="text-align:center">这是指多个损失组合的情况，不一定完全正确。单个损失超参数需结合实际情况。</td>
</tr>
<tr>
<td style="text-align:center">批样本数量</td>
<td style="text-align:center">[1:1024]</td>
<td style="text-align:center">当批样本数量过大(大于6000)或者等于1时，需要注意学习策略或者内部归一化方式的调整。</td>
</tr>
<tr>
<td style="text-align:center">丢弃法比率</td>
<td style="text-align:center">[0, 0.5]</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数</td>
<td style="text-align:center">[0, 1e-4]</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">卷积核尺寸</td>
<td style="text-align:center">[7x7],[5x5],[3x3],[1x1], [7x1,1x7]</td>
</tr>
</tbody>
</table>
</div>
<h2 id="14-3-网络训练中的超参调整策略"><a href="#14-3-网络训练中的超参调整策略" class="headerlink" title="14.3 网络训练中的超参调整策略"></a>14.3 网络训练中的超参调整策略</h2><h3 id="14-3-1-如何调试模型？"><a href="#14-3-1-如何调试模型？" class="headerlink" title="14.3.1 如何调试模型？"></a>14.3.1 如何调试模型？</h3><p>在讨论如何调试模型之前，我们先来纠正一个误区。通常理解如何调试模型的时候，我们想到一系列优秀的神经网络模型以及调试技巧。但这里需要指出的是数据才是模型的根本，如果有一批质量优秀的数据，或者说你能将数据质量处理的很好的时候，往往比挑选或者设计模型的收益来的更大。那在这之后才是模型的设计和挑选以及训练技巧上的事情。</p>
<p>1、探索和清洗数据。探索数据集是设计算法之前最为重要的一步，以图像分类为例，我们需要重点知道给定的数据集样本类别和各类别样本数量是否平衡，图像之间是否存在跨域问题（例如网上爬取的图像通常质量各异，存在噪声）。若是类别数远远超过类别样本数（比如类别10000，每个类别却只有10张图像），那通常的方法可能效果并不显著，这时候few-shot learning或者对数据集做进一步增强可能是你比较不错的选择。再如目标检测，待检测目标在数据集中的尺度范围是对检测器的性能有很大影响的部分。因此重点是检测大目标还是小目标、目标是否密集完全取决于数据集本身。所以，探索和进一步清洗数据集一直都是深度学习中最重要的一步。这是很多新手通常会忽略的一点。</p>
<p>2、探索模型结果。探索模型的结果，通常是需要对模型在验证集上的性能进行进一步的分析，这是如何进一步提升模型性能很重要的步骤。将模型在训练集和验证集都进行结果的验证和可视化，可直观的分析出模型是否存在较大偏差以及结果的正确性。以图像分类为例，若类别间样本数量很不平衡时，我们需要重点关注少样本类别在验证集的结果是否和训练集的出入较大，对出错类别可进一步进行模型数值分析以及可视化结果分析，进一步确认模型的行为。</p>
<p>3、监控训练和验证误差。首先很多情况下，我们忽略代码的规范性和算法撰写正确性验证，这点上容易产生致命的影响。在训练和验证都存在问题时，首先请确认自己的代码是否正确。其次，根据训练和验证误差进一步追踪模型的拟合状态。若训练数据集很小，此时监控误差则显得格外重要。确定了模型的拟合状态对进一步调整学习率的策略的选择或者其他有效超参数的选择则会更得心应手。</p>
<p>4、反向传播数值的计算，这种情况通常适合自己设计一个新操作的情况。目前大部分流行框架都已包含自动求导部分，但并不一定是完全符合你的要求的。验证求导是否正确的方式是比较自动求导的结果和有限差分计算结果是否一致。所谓有限差分即导数的定义，使用一个极小的值近似导数。</p>
<script type="math/tex; mode=display">
f^{'}(x_0) = \lim_{n\rightarrow0}\frac{\Delta y}{\Delta x} = \lim_{n\rightarrow0}\frac{f(x_0+\Delta x -f(x_0))}{\Delta x}</script><h3 id="14-3-2-为什么要做学习率调整"><a href="#14-3-2-为什么要做学习率调整" class="headerlink" title="14.3.2 为什么要做学习率调整?"></a>14.3.2 为什么要做学习率调整?</h3><p>​    学习率可以说是模型训练最为重要的超参数。通常情况下，一个或者一组优秀的学习率既能加速模型的训练，又能得到一个较优甚至最优的精度。过大或者过小的学习率会直接影响到模型的收敛。我们知道，当模型训练到一定程度的时候，损失将不再减少，这时候模型的一阶梯度接近零，对应Hessian 矩阵通常是两种情况，一、正定，即所有特征值均为正，此时通常可以得到一个局部极小值，若这个局部极小值接近全局最小则模型已经能得到不错的性能了，但若差距很大，则模型性能还有待于提升，通常情况下后者在训练初最常见。二，特征值有正有负，此时模型很可能陷入了鞍点，若陷入鞍点，模型性能表现就很差。以上两种情况在训练初期以及中期，此时若仍然以固定的学习率，会使模型陷入左右来回的震荡或者鞍点，无法继续优化。所以，学习率衰减或者增大能帮助模型有效的减少震荡或者逃离鞍点。</p>
<h3 id="14-3-3-学习率调整策略有哪些？"><a href="#14-3-3-学习率调整策略有哪些？" class="headerlink" title="14.3.3 学习率调整策略有哪些？"></a>14.3.3 学习率调整策略有哪些？</h3><p>通常情况下，大部分学习率调整策略都是衰减学习率，但有时若增大学习率也同样起到奇效。这里结合TensorFlow的内置方法来举例。</p>
<p>1、<strong>exponential_decay</strong>和<strong>natural_exp_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">exponential_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">natural_exp_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>指数衰减是最常用的衰减方式，这种方式简单直接，在训练初期衰减较大利于收敛，在后期衰减较小利于精调。以上两种均为指数衰减，区别在于后者使用以自然指数下降。</p>
<p><img src="/img/ch14/指数衰减.jpeg" alt="./"></p>
<p>2、<strong>piecewise_constant</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">piecewise_constant(x, boundaries, values, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>分段设置学习率法，跟指数型类似，区别在于每个阶段的衰减并不是按指数调整。可在不同阶段设置手动不同的学习率。这种学习率重点在有利于精调。</p>
<p>3、<strong>polynomial_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">polynomial_decay(learning_rate, global_step, decay_steps,</span><br><span class="line">                  end_learning_rate=<span class="number">0.0001</span>, power=<span class="number">1.0</span>,</span><br><span class="line">                  cycle=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>多项式衰减，计算如下：</p>
<script type="math/tex; mode=display">
global setp = min(global step, decay steps)</script><script type="math/tex; mode=display">
lr_{decayed} = (lr-lr_{end})*(1-{globalstep\over decaysteps})^{power} +lr_{end}</script><p>有别于上述两种，多项式衰减则是在每一步迭代上都会调整学习率。主要看Power参数，若Power为1，则是下图中的红色直线；若power小于1，则是开1/power次方，为蓝色线；绿色线为指数，power大于1。</p>
<p><img src=".\img\ch14\多项式衰减.jpeg" alt=""></p>
<p>此外，需要注意的是参数cycle，cycle对应的是一种周期循环调整的方式。这种cycle策略主要目的在后期防止在一个局部极小值震荡，若跳出该区域或许能得到更有的结果。这里说明cycle的方式不止可以在多项式中应用，可配合类似的周期函数进行衰减，如下图。</p>
<p><img src="/img/ch14/cycle衰减.jpeg" alt=""></p>
<p>4、<strong>inverse_time_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inverse_time_decay(learning_rate, global_step, decay_steps, decay_rate,</span><br><span class="line">                   staircase=<span class="literal">False</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>逆时衰减，这种方式和指数型类似。如图，<img src="/img/ch14/逆时衰减.jpeg" alt=""></p>
<p>5、<strong>cosine_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosine_decay(learning_rate, global_step, decay_steps, alpha=<span class="number">0.0</span>,</span><br><span class="line">                 name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>余弦衰减，即按余弦函数的方式衰减学习率，如图</p>
<p><img src=".\img\ch14\余弦衰减.jpeg" alt=""></p>
<p>6、<strong>cosine_decay_restarts</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cosine_decay_restarts(learning_rate, global_step, first_decay_steps,</span><br><span class="line">                           t_mul=<span class="number">2.0</span>, m_mul=<span class="number">1.0</span>, alpha=<span class="number">0.0</span>, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>余弦衰减，即余弦版本的cycle策略，作用与多项式衰减中的cycle相同。区别在于余弦重启衰减会重新回到初始学习率，拉长周期，而多项式版本则会逐周期衰减。</p>
<p><img src="/img/ch14/余弦cycle衰减.jpeg" alt=""></p>
<p>7、<strong>linear_cosine_decay</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear_cosine_decay(learning_rate, global_step, decay_steps,</span><br><span class="line">                        num_periods=<span class="number">0.5</span>, alpha=<span class="number">0.0</span>, beta=<span class="number">0.001</span>,</span><br><span class="line">                        name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>线性余弦衰减，主要应用于增强学习领域。</p>
<p><img src="/img/ch14/线性余弦衰减.jpeg" alt=""></p>
<p>8、<strong>noisy_linear_cosine_decay</strong></p>
<p>噪声线性余弦衰减，即在线性余弦衰减中加入随机噪声，增大寻优的随机性。</p>
<p><img src="/img/ch14/噪声线性余弦衰减.jpeg" alt=""></p>
<h3 id="14-3-4-极端批样本数量下，如何训练网络？"><a href="#14-3-4-极端批样本数量下，如何训练网络？" class="headerlink" title="14.3.4 极端批样本数量下，如何训练网络？"></a>14.3.4 极端批样本数量下，如何训练网络？</h3><p>​    极端批样本情况一般是指batch size为1或者batch size在6000以上的情况。这两种情况，在使用不合理的情况下都会导致模型最终性能无法达到最优甚至是崩溃的情况。</p>
<p>​    在目标检测、分割或者3D图像等输入图像尺寸较大的场景，通常batch size 会非常小。而在14.2.4中，我们已经讲到这种情况会导致梯度的不稳定以及batchnorm统计的不准确。针对梯度不稳定的问题，通常不会太致命，若训练中发现梯度不稳定导致性能的严重降低时可采用累计梯度的策略，即每次计算完不反向更新，而是累计多次的误差后进行一次更新，这是一种在内存有限情况下实现有效梯度更新的一个策略。batch size过小通常对batchnorm的影响是最大的，若网络模型中存在batchnorm，batch size若只为1或者2时会对训练结果产生非常大的影响。这时通常有两种策略，一、若模型使用了预训练网络，可冻结预训练网络中batchnorm的模型参数，有效降低batch size引起的统计量变化的影响。二、在网络不是过深或者过于复杂时可直接移除batchnorm或者使用groupnorm代替batchnorm，前者不多阐释，后者是有FAIR提出的一种用于减少batch对batchnorm影响，其主要策略是先将特征在通道上进行分组，然后在组内进行归一化。即归一化操作上完全与batch size无关。这种groupnorm的策略被证实在极小批量网络训练上能达到较优秀的性能。当然这里也引入里group这个超参数，一般情况下建议不宜取group为1或者各通道单独为组的group数量，可结合实际网络稍加调试。</p>
<p>​    为了降低训练时间的成本，多机多卡的分布式系统通常会使用超大的batch size进行网络训练。同样的在14.2.4中，我们提到了超大batch size会带来梯度方向过于一致而导致的精度大幅度降低的问题。这时通常可采用层自适应速率缩放（LARS）算法。从理论认知上将，batch size增大会减少反向传播的梯度更新次数，但为了达到相同的模型效果，需要增大学习率。但学习率一旦增大，又会引起模型的不收敛。为了解决这一矛盾，LARS算法就在各层上自适应的计算一个本地学习率用于更新本层的参数，这样能有效的提升训练的稳定性。目前利用LARS算法，腾讯公司使用65536的超大batch size能将ResNet50在ImageNet在4分钟完成训练，而谷歌使用32768的batch size使用TPU能将该时间缩短至2分钟。</p>
<h2 id="14-4-合理使用预训练网络"><a href="#14-4-合理使用预训练网络" class="headerlink" title="14.4 合理使用预训练网络"></a>14.4 合理使用预训练网络</h2><h3 id="14-4-1-什么是微调（fine-tune）"><a href="#14-4-1-什么是微调（fine-tune）" class="headerlink" title="14.4.1 什么是微调（fine-tune）"></a>14.4.1 什么是微调（fine-tune）</h3><p>​    微调（fine-tune），顾名思义指稍微调整参数即可得到优秀的性能，是迁移学习的一种实现方式。微调和从头训练（train from scratch）的本质区别在于模型参数的初始化，train from scratch通常指对网络各类参数进行随机初始化（当然随机初始化也存在一定技巧），随机初始化模型通常不具有任何预测能力，通常需要大量的数据或者特定域的数据进行从零开始的训练，这样需要训练到优秀的模型通常是稍困难的。而微调的网络，网络各类参数已经在其他数据集（例如ImageNet数据集）完成较好调整的，具备了较优秀的表达能力。因此，我们只需要以较小的学习速率在自己所需的数据集领域进行学习即可得到较为优秀的模型。微调通常情况下，无须再重新设计网络结构，预训练模型提供了优秀的结构，只需稍微修改部分层即可。在小数据集上，通常微调的效果比从头训练要好很多，原因在于数据量较小的前提下，训练更多参数容易导致过度拟合。</p>
<h3 id="14-4-2-微调有哪些不同方法？"><a href="#14-4-2-微调有哪些不同方法？" class="headerlink" title="14.4.2 微调有哪些不同方法？"></a>14.4.2 微调有哪些不同方法？</h3><p>​    以图像分类为例，通常情况下由于不同数据集需要的类别数不同，我们需要修改网络的输出顶层。这种情况下有两种微调方式：</p>
<ul>
<li><p>不冻结网络模型的任何层，对最后的改动层使用较大的学习率，对未改动层以较小的学习率进行训练全模型训练，进行多轮训练即可。即一步完成训练。</p>
</li>
<li><p>冻结除了顶部改动层以外的所有层参数，即不对冻结部分的层进行参数训练更新，进行若干轮的微调训练后，放开顶部层以下的若干层或者全部放开所有层的参数，再次进行若干轮训练即可。即分多步训练。</p>
<p>以上两种都属于微调。目前由于存在大量优秀的预训练模型，如何确定哪个模型适合自己的任务并能得到最佳性能需要花大量的时间探索。此时，上述的前者是种不错训练方式，你无须进行过多分步的操作。而当探索到一个比较适合的模型时，你不妨可以再次重新尝试下以第二种方式进行训练，或许能得到相比于前者稍高些的性能，因为小数据集上调整过多的参数过拟合的机率也会增大，当然这并不是绝对的。</p>
</li>
</ul>
<h3 id="14-4-3-微调先冻结底层，训练顶层的原因？"><a href="#14-4-3-微调先冻结底层，训练顶层的原因？" class="headerlink" title="14.4.3 微调先冻结底层，训练顶层的原因？"></a>14.4.3 微调先冻结底层，训练顶层的原因？</h3><p>​    14.12中第二种冻结多步训练的方式。首先冻结除了顶部改动层以外的所有层参数，对顶层进行训练，这个过程可以理解为顶层的域适应训练，主要用来训练适应模型的现有特征空间，防止顶层糟糕的初始化，对已经具备一定表达能力的层的干扰和破坏，影响最终的性能。之后，在很多深度学习框架教程中会使用放开顶层往下一半的层数，继续进行微调。这样的好处在于越底层的特征通常是越通用的特征，越往上其整体的高层次语义越完备，这通过感受野很容易理解。所以，若预训练模型的数据和微调训练的数据语义差异越大（例如ImageNet的预模型用于医学图像的训练），那越往顶层的特征语义差异就越大，因此通常也需要进行相应的调整。</p>
<h3 id="14-4-4-不同的数据集特性下如何微调？"><a href="#14-4-4-不同的数据集特性下如何微调？" class="headerlink" title="14.4.4 不同的数据集特性下如何微调？"></a>14.4.4 不同的数据集特性下如何微调？</h3><ul>
<li>数据集数据量少，数据和原数据集类似。这是通常做法只需修改最后的输出层，训练即可，训练过多参数容易过拟合。</li>
<li>数据集数据量少，数据和原数据集差异较大。由于数据差异较大，可以在完成输出顶层的微调后，微调顶层往下一半的层数，进行微调。</li>
<li>数据集数据量大，数据与原数据集差异较大。这种情况下，通常已经不需要用预训练模型进行微调，通常直接重新训练即可。</li>
<li>数据集数据量大，数据与原数据类似。这时预训练模型的参数是个很好的初始化，可利用预训练模型放开所有层以较小的学习率微调即可。</li>
</ul>
<h3 id="14-4-4-目标检测中使用预训练模型的优劣？"><a href="#14-4-4-目标检测中使用预训练模型的优劣？" class="headerlink" title="14.4.4 目标检测中使用预训练模型的优劣？"></a>14.4.4 目标检测中使用预训练模型的优劣？</h3><p>​    目标检测中无论是一阶段的YOLO、SSD或者RetinaNet 还是二阶段的Faster R-CNN、R-FCN 和 FPN都是基于ImageNet上预训练好的分类模型。</p>
<p>​    优势在于：</p>
<p>​    1、正如大部分微调的情况一样，使用预训练网络已拥有优秀的语义特征，能有效的加快训练速度；</p>
<p>​    2、其次，对于大部分二阶段的模型来说，并未实现严格意义上的完全端对端的训练，所以使用预训练模型能直接提取到语义特征，能使两个阶段的网络更容易实现模型的优化。</p>
<p>​    劣势在于，分类模型和检测模型之间仍然存在一定任务上的差异：</p>
<p>​    1、分类模型大部分训练于单目标数据，对同时进行多目标的捕捉能力稍弱，且不关注目标的位置，在一定程度上让模型损失部分空间信息，这对检测模型通常是不利的；</p>
<p>​    2、域适应问题，若预训练模型（ImageNet）和实际检测器的使用场景（医学图像，卫星图像）差异较大时，性能会受到影响；</p>
<p>​    3、使用预训练模型就意味着难以自由改变网络结构和参数限制了应用场合。</p>
<h3 id="14-4-5-目标检测中如何从零开始训练-train-from-scratch-？"><a href="#14-4-5-目标检测中如何从零开始训练-train-from-scratch-？" class="headerlink" title="14.4.5 目标检测中如何从零开始训练(train from scratch)？"></a>14.4.5 目标检测中如何从零开始训练(train from scratch)？</h3><p>​    结合FAIR相关的研究，我们可以了解目标检测和其他任务从零训练模型一样，只要拥有足够的数据以及充分而有效的训练，同样能训练出不亚于利用预训练模型的检测器。这里我们提供如下几点建议：</p>
<p>​    1、数据集不大时，同样需要进行数据集增强。</p>
<p>​    2、预训练模型拥有更好的初始化，train from scratch需要更多的迭代次数以及时间训练和优化检测器。而二阶段模型由于并不是严格的端对端训练，此时可能需要更多的迭代次数以及时间，而一阶段检测模型训练会相对更容易些（例如DSOD以ScratchDet及）。</p>
<p>​    3、目标检测中train from scratch最大的问题还是batch size过小。所以可采取的策略是增加GPU使用异步batchnorm增大batch size，若条件限制无法使用更多GPU时，可使用groupnorm代替batchnorm</p>
<p>​    4、由于分类模型存在对多目标的捕捉能力弱以及对物体空间位置信息不敏感等问题，可借鉴DetNet训练一个专属于目标检测的模型网络，增强对多目标、尺度和位置拥有更强的适应性。</p>
<h2 id="14-5-如何改善-GAN-的性能"><a href="#14-5-如何改善-GAN-的性能" class="headerlink" title="14.5 如何改善 GAN 的性能"></a>14.5 如何改善 GAN 的性能</h2><p>优化GAN性能通常需要在如下几个方面进行</p>
<ul>
<li>设计或选择更适合目的代价函数。</li>
<li>添加额外的惩罚。</li>
<li>避免判别器过度自信和生成器过度拟合。</li>
<li>更好的优化模型的方法。</li>
<li>添加标签明确优化目标。</li>
</ul>
<p>GAN常用训练技巧</p>
<ul>
<li><p>输入规范化到（-1，1）之间，最后一层的激活函数使用tanh（BEGAN除外）</p>
</li>
<li><p>使用wassertein GAN的损失函数，</p>
</li>
<li><p>如果有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好，另外使用标签平滑，单边标签平滑或者双边标签平滑</p>
</li>
<li><p>使用mini-batch norm， 如果不用batch norm 可以使用instance norm 或者weight norm</p>
</li>
<li><p>避免使用RELU和pooling层，减少稀疏梯度的可能性，可以使用leakrelu激活函数</p>
</li>
<li><p>优化器尽量选择ADAM，学习率不要设置太大，初始1e-4可以参考，另外可以随着训练进行不断缩小学习率，</p>
</li>
<li><p>给D的网络层增加高斯噪声，相当于是一种正则</p>
<h2 id="14-6-AutoML"><a href="#14-6-AutoML" class="headerlink" title="14.6 AutoML"></a>14.6 AutoML</h2></li>
</ul>
<h3 id="14-6-1-什么是AutoML？"><a href="#14-6-1-什么是AutoML？" class="headerlink" title="14.6.1 什么是AutoML？"></a>14.6.1 什么是AutoML？</h3><p>​    目前一个优秀的机器学习和深度学习模型，离不开这几个方面：</p>
<p>​    一、优秀的数据预处理；</p>
<p>​    二、合适的模型结构和功能；</p>
<p>​    三、优秀的训练策略和超参数；</p>
<p>​    四、合适的后处理操作；</p>
<p>​    五、严格的结果分析。</p>
<p>​    这几方面都对最终的结果有着举足轻重的影响，这也是目前的数据工程师和学者们的主要工作。但由于这每一方面都十分繁琐，尤其是在构建模型和训练模型上。而大部分情况下，这些工作有无须过深专业知识就能使用起来。所以AutoML主要的作用就是来帮助实现高效的模型构建和超参数调整。例如深度学习网络的架构搜索、超参数的重要性分析等等。当然AutoML并不简单的进行暴力或者随机的搜索，其仍然需要机器学习方面的知识，例如贝叶斯优化、强化学习、元学习以及迁移学习等等。目前也有些不错的AutoML工具包，例如Alex Honchar的Hyperopt、微软的NNI、Autokeras等。</p>
<p>目前AutoML已经成为最新的研究热点，有兴趣的可以参考<a href="https://www.automl.org/automl/literature-on-neural-architecture-search/" target="_blank" rel="noopener">AutoML literature</a>。</p>
<h3 id="14-6-2-自动化超参数搜索方法有哪些？"><a href="#14-6-2-自动化超参数搜索方法有哪些？" class="headerlink" title="14.6.2 自动化超参数搜索方法有哪些？"></a>14.6.2 自动化超参数搜索方法有哪些？</h3><p>​    目前自动化搜索主要包含网格搜索，随机搜索，基于模型的超参优化</p>
<p>​    网格搜索：</p>
<p>​        通常当超参数量较少的时候，可以使用网格搜索法。即列出每个超参数的大致候选集合。利用这些集合        进行逐项组合优化。在条件允许的情况下，重复进行网格搜索会当优秀，当然每次重复需要根据上一步得到的最优参数组合，进行进一步的细粒度的调整。网格搜索最大的问题就在于计算时间会随着超参数的数量指数级的增长。</p>
<p>​    随机搜索：</p>
<p>​        随机搜索，是一种用来替代网格搜索的搜索方式。随机搜索有别于网格搜索的一点在于，我们不需要设定一个离散的超参数集合，而是对每个超参数定义一个分布函数来生成随机超参数。随机搜索相比于网格搜索在一些不敏感超参上拥有明显优势。例如网格搜索对于批样本数量（batch size），在[16,32,64]这些范围内进行逐项调试，这样的调试显然收益更低下。当然随机搜索也可以进行细粒度范围内的重复的搜索优化。</p>
<p><img src="/img/ch14/14.14.png" alt=""></p>
<p>​    基于模型的超参优化：</p>
<p>​        有别于上述两种的搜索策略，基于模型的超参调优问题转化为了优化问题。直觉上会考虑是否进行一个可导建模，然后利用梯度下降进行优化。但不幸的是我们的超参数通常情况下是离散的，而且其计算代价依旧很高。</p>
<p>​        基于模型的搜索算法，最常见的就是贝叶斯超参优化。有别于的网格搜索和随机搜索独立于前几次搜索结果的搜索，贝叶斯则是利用历史的搜索结果进行优化搜索。其主要有四部分组成，1.目标函数，大部分情况下就是模型验证集上的损失。2、搜索空间，即各类待搜索的超参数。3、优化策略，建立的概率模型和选择超参数的方式。4、历史的搜索结果。首先对搜索空间进行一个先验性的假设猜想，即假设一种选择超参的方式，然后不断的优化更新概率模型，最终的目标是找到验证集上误差最小的一组超参数。</p>
<h3 id="14-6-3-什么是神经网络架构搜索（NAS）"><a href="#14-6-3-什么是神经网络架构搜索（NAS）" class="headerlink" title="14.6.3 什么是神经网络架构搜索（NAS）"></a>14.6.3 什么是神经网络架构搜索（NAS）</h3><p>2015至2017年间，是CNN网络设计最兴盛的阶段，大多都是由学者人工设计的网络结构。这个过程通常会很繁琐。其主要原因在于对不同模块组件的组成通常是个黑盒优化的问题，此外，在不同结构超参数以及训练超参数的选择优化上非凸优化问题，或者是个混合优化问题，既有离散空间又有连续空间。NAS（Neural Architecture Search）的出现就是为了解决如何通过机器策略和自动化的方式设计出优秀高效的网络。而这种策略通常不是统一的标准，不同的网络结合实际的需求通常会有不同的设计，比如移动端的模型会在效率和精度之间做平衡。目前，NAS也是AUTOML中最重要的部分。NAS通常会分为三个方面，搜索空间（在哪搜索），搜索策略（如何搜索）及评价预估。</p>
<ul>
<li><p>搜索空间，即在哪搜索，定义了优化问题所需变量。不同规模的搜索空间的变量其对于的难度也是不一样的。早期由于网络结构以及层数相对比较简单，参数量较少，因此会更多的使用遗传算法等进化算法对网络的超参数和权重进行优化。深度学习发展到目前，模型网络结构越来越复杂，参数量级越来越庞大，这些进化算法已经无法继续使用。但若我们先验给定一些网络结构和超参数，模型的性能已经被限制在给定的空间，此时搜索的空间已变得有限，所以只需对复杂模型的架构参数和对应的超参数进行优化即可。</p>
</li>
<li><p>搜索策略， 即如何搜索，定义了如何快速、准确找到最优的网络结构参数配置的策略。常见的搜索方法包括：随机搜索、贝叶斯优化、强化学习、进化算法以及基于模型的搜索算法。其中主要代表为2017年谷歌大脑的使用强化学习的搜索方法。</p>
</li>
<li><p>评价预估，定义了如何高效对搜索的评估策略。深度学习中，数据规模往往是庞大的，模型要在如此庞大的数据规模上进行搜索，这无疑是非常耗时的，对优化也会造成非常大的困难，所以需要一些高效的策略做近似的评估。 这里一般会有如下三种思路：</p>
<p>一、使用些低保真的训练集来训练模型。低保真在实际中可以用不同的理解，比如较少的迭代次数，用一小部分数据集或者保证结构的同时减少通道数等。这些方法都可以在测试优化结构时大大降低计算时间，当然也会存在一定的偏差。但架构搜索从来并不是要一组固定的参数，而是一种优秀的模型结构。最终选取时，只需在较优秀的几组结构中进行全集训练，进行择优选取即可。</p>
<p>二、使用代理模型。除了低保真的训练方式外，学者们提出了一种叫做代理模型的回归模型，采用例如插值等策略对已知的一些参数范围进行预测，目的是为了用尽可能少的点预测到最佳的结果。</p>
<p>三、参数级别的迁移。例如知识蒸馏等。用已训练好的模型权重参数对目标问题搜索，通常会让搜索拥有一个优秀的起点。由于积累了大量的历史寻优数据，对新问题的寻优将会起到很大的帮助。</p>
</li>
</ul>
<h3 id="14-6-4-NASNet的设计策略"><a href="#14-6-4-NASNet的设计策略" class="headerlink" title="14.6.4 NASNet的设计策略"></a>14.6.4 NASNet的设计策略</h3><p>NASNet是最早由google brain 通过网络架构搜索策略搜索并成功训练ImageNet的网络，其性能超越所有手动设计的网络模型。关于NASNet的搜索策略，首先需要参考google brain发表在ICLR2017的论文《Neural Architecture Search with Reinforcement Learning》。该论文是最早成功通过架构搜索策略在cifar-10数据集上取得比较不错效果的工作。NASNet很大程度上是沿用该搜索框架的设计思想。</p>
<p>NASNet的核心思想是利用强化学习对搜索空间内的结构进行反馈探索。架构搜索图如下，定义了一个以RNN为核心的搜索控制器。在搜索空间以概率p对模型进行搜索采样。得到网络模型A后，对该模型进行训练，待模型收敛得到设定的准确率R后，将梯度传递给控制器RNN进行梯度更新。</p>
<p><img src="/img/ch14/NAS搜索策略.png" alt=""></p>
<p>​                                    架构搜索策略流程</p>
<p>RNN控制器会对卷积层的滤波器的尺寸、数量以及滑动间隔进行预测。每次预测的结果都会作为下一级的输入，档层数达到设定的阈值时，会停止预测。而这个阈值也会随着训练的进行而增加。这里的控制器之预测了卷积，并没有对例如inception系列的分支结构或者ResNet的跳级结构等进行搜索。所以，控制器需要进一步扩展到预测这些跳级结构上，这样搜索空间相应的也会增大。为了预测这些结构，RNN控制器内每一层都增加了一个预测跳级结构的神经元，文中称为锚点，稍有不同的是该锚点的预测会由前面所有层的锚点状态决定。</p>
<p><img src="/img/ch14/RNN控制器.png" alt=""></p>
<p>​                                    RNN控制器</p>
<p>NASNet大体沿用了上述生成网络结构的机器，并在此基础上做了如下两点改进：</p>
<p>1、先验行地加入inception系列和ResNet的堆叠模块的思想。其定义了两种卷积模块，Normal Cell和Reduction Cell，前者不进行降采样，而后者是个降采样的模块。而由这两种模块组成的结构可以很方便的通过不同数量的模块堆叠将其从小数据集搜索到的架构迁移到大数据集上，大大提高了搜索效率。</p>
<p><img src="/img/ch14/NASNet的RNN控制器.png" alt=""></p>
<p>​                                    NASNet的RNN控制器</p>
<p>2、对RNN控制进行优化，先验性地将各种尺寸和类型的卷积和池化层加入到搜索空间内，用预测一个卷积模块代替原先预测一层卷积。如图，控制器RNN不在预测单个卷积内的超参数组成，而是对一个模块内的每一个部分进行搜索预测，搜索的空间则限定在如下这些操作中：</p>
<p>​                        • identity                      • 1x3 then 3x1 convolution<br>​                        • 1x7 then 7x1 convolution      • 3x3 dilated convolution<br>​                        • 3x3 average pooling               • 3x3 max pooling<br>​                        • 5x5 max pooling              • 7x7 max pooling<br>​                        • 1x1 convolution                  • 3x3 convolution<br>​                        • 3x3 depthwise-separable conv • 5x5 depthwise-seperable conv<br>​                        • 7x7 depthwise-separable conv</p>
<p>在模块内的连接方式上也提供了element-wise addition和concatenate两种方式。NASNet的搜索方式和过程对NAS的一些后续工作都具有非常好的参考借鉴意义。</p>
<h3 id="14-6-5-网络设计中，为什么卷积核设计尺寸都是奇数"><a href="#14-6-5-网络设计中，为什么卷积核设计尺寸都是奇数" class="headerlink" title="14.6.5 网络设计中，为什么卷积核设计尺寸都是奇数"></a>14.6.5 网络设计中，为什么卷积核设计尺寸都是奇数</h3><p>我们发现在很多大部分网络设计时都会使用例如3x3/5x5/7x7等奇数尺寸卷积核，主要原因有两点：</p>
<ul>
<li>保证像素点中心位置，避免位置信息偏移</li>
<li>填充边缘时能保证两边都能填充，原矩阵依然对称</li>
</ul>
<h3 id="14-6-6-网络设计中，权重共享的形式有哪些，为什么要权重共享"><a href="#14-6-6-网络设计中，权重共享的形式有哪些，为什么要权重共享" class="headerlink" title="14.6.6 网络设计中，权重共享的形式有哪些，为什么要权重共享"></a>14.6.6 网络设计中，权重共享的形式有哪些，为什么要权重共享</h3><p>权重共享的形式：</p>
<ul>
<li>深度学习中，权重共享最具代表性的就是卷积网络的卷积操作。卷积相比于全连接神经网络参数大大减少；</li>
<li>多任务网络中，通常为了降低每个任务的计算量，会共享一个骨干网络。</li>
<li>一些相同尺度下的结构化递归网络</li>
</ul>
<p>权重共享的好处：</p>
<p>​    权重共享一定程度上能增强参数之间的联系，获得更好的共性特征。同时很大程度上降低了网络的参数，节省计算量和计算所需内存（当然，结构化递归并不节省计算量）。此外权重共享能起到很好正则的作用。正则化的目的是为了降低模型复杂度，防止过拟合，而权重共享则正好降低了模型的参数和复杂度。</p>
<p>​    因此一个设计优秀的权重共享方式，在降低计算量的同时，通常会较独享网络有更好的效果。</p>
<p>权重共享不仅在人工设计（human-invented）的网络结构中有简化参数，降低模型复杂度的作用，在神经网络搜索（NAS）的网络结构中可以使得child model的计算效率提升，使得搜索过程可以在单卡GPU上复现，见Efficient NAS(<a href="https://arxiv.org/abs/1802.03268" target="_blank" rel="noopener">ENAS</a>)。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0_%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0_%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">第十三章_优化算法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:34:14 / 修改时间：12:34:32" itemprop="dateCreated datePublished" datetime="2020-03-03T12:34:14+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第一十三章-优化算法"><a href="#第一十三章-优化算法" class="headerlink" title="第一十三章 优化算法"></a>第一十三章 优化算法</h1><h2 id="13-1-如何解决训练样本少的问题"><a href="#13-1-如何解决训练样本少的问题" class="headerlink" title="13.1 如何解决训练样本少的问题"></a>13.1 如何解决训练样本少的问题</h2><p>目前大部分的深度学习模型仍然需要海量的数据支持。例如 ImageNet 数据就拥有1400多万的图片。而现实生产环境中，数据集通常较小，只有几万甚至几百个样本。这时候，如何在这种情况下应用深度学习呢?<br>（1）利用预训练模型进行迁移微调（fine-tuning），预训练模型通常在特征上拥有很好的语义表达。此时，只需将模型在小数据集上进行微调就能取得不错的效果。这也是目前大部分小数据集常用的训练方式。视觉领域内，通常会ImageNet上训练完成的模型。自然语言处理领域，也有BERT模型等预训练模型可以使用。<br>&emsp;&emsp;<br>（2）单样本或者少样本学习（one-shot，few-shot learning），这种方式适用于样本类别远远大于样本数量的情况等极端数据集。例如有1000个类别，每个类别只提供1-5个样本。少样本学习同样也需要借助预训练模型，但有别于微调的在于，微调通常仍然在学习不同类别的语义，而少样本学习通常需要学习样本之间的距离度量。例如孪生网络（Siamese Neural Networks）就是通过训练两个同种结构的网络来判别输入的两张图片是否属于同一类。<br>​    上述两种是常用训练小样本数据集的方式。此外，也有些常用的手段，例如数据集增强、正则或者半监督学习等方式来解决小样本数据集的训练问题。</p>
<h2 id="13-2-深度学习是否能胜任所有数据集"><a href="#13-2-深度学习是否能胜任所有数据集" class="headerlink" title="13.2 深度学习是否能胜任所有数据集?"></a>13.2 深度学习是否能胜任所有数据集?</h2><p>深度学习并不能胜任目前所有的数据环境，以下列举两种情况：</p>
<p>（1）深度学习能取得目前的成果，很大一部分原因依赖于海量的数据集以及高性能密集计算硬件。因此，当数据集过小时，需要考虑与传统机器学习相比，是否在性能和硬件资源效率更具有优势。<br>（2）深度学习目前在视觉，自然语言处理等领域都有取得不错的成果。这些领域最大的特点就是具有局部相关性。例如图像中，人的耳朵位于两侧，鼻子位于两眼之间，文本中单词组成句子。这些都是具有局部相关性的，一旦被打乱则会破坏语义或者有不同的语义。所以当数据不具备这种相关性的时候，深度学习就很难取得效果。</p>
<h2 id="13-3-有没有可能找到比已知算法更好的算法"><a href="#13-3-有没有可能找到比已知算法更好的算法" class="headerlink" title="13.3 有没有可能找到比已知算法更好的算法?"></a>13.3 有没有可能找到比已知算法更好的算法?</h2><p>在最优化理论发展中，有个没有免费午餐的定律，其主要含义在于，在不考虑具体背景和细节的情况下，任何算法和随机猜的效果期望是一样的。即，没有任何一种算法能优于其他一切算法，甚至不比随机猜好。深度学习作为机器学习领域的一个分支同样符合这个定律。所以，虽然目前深度学习取得了非常不错的成果，但是我们同样不能盲目崇拜。</p>
<p>优化算法本质上是在寻找和探索更符合数据集和问题的算法，这里数据集是算法的驱动力，而需要通过数据集解决的问题就是算法的核心，任何算法脱离了数据都会没有实际价值，任何算法的假设都不能脱离实际问题。因此，实际应用中，面对不同的场景和不同的问题，可以从多个角度针对问题进行分析，寻找更优的算法。</p>
<h2 id="13-4-什么是共线性，如何判断和解决共线性问题"><a href="#13-4-什么是共线性，如何判断和解决共线性问题" class="headerlink" title="13.4 什么是共线性，如何判断和解决共线性问题?"></a>13.4 什么是共线性，如何判断和解决共线性问题?</h2><p>对于回归算法，无论是一般回归还是逻辑回归，在使用多个变量进行预测分析时，都可能存在多变量相关的情况，这就是多重共线性。共线性的存在，使得特征之间存在冗余，导致过拟合。</p>
<p>常用判断是否存在共线性的方法有：</p>
<p>（1）相关性分析。当相关性系数高于0.8，表明存在多重共线性；但相关系数低，并不能表示不存在多重共线性；</p>
<p>（2）方差膨胀因子VIF。当VIF大于5或10时，代表模型存在严重的共线性问题；</p>
<p>（3）条件系数检验。 当条件数大于100、1000时，代表模型存在严重的共线性问题。</p>
<p>通常可通过PCA降维、逐步回归法和LASSO回归等方法消除共线性。</p>
<h2 id="13-5-权值初始化方法有哪些？"><a href="#13-5-权值初始化方法有哪些？" class="headerlink" title="13.5 权值初始化方法有哪些？"></a>13.5 权值初始化方法有哪些？</h2><p>在深度学习的模型中，从零开始训练时，权重的初始化有时候会对模型训练产生较大的影响。良好的初始化能让模型快速、有效的收敛，而糟糕的初始化会使得模型无法训练。</p>
<p>目前，大部分深度学习框架都提供了各类初始化方式，其中一般常用的会有如下几种：<br><strong>1. 常数初始化(constant)</strong> </p>
<p>​    把权值或者偏置初始化为一个常数。例如设置为0，偏置初始化为0较为常见，权重很少会初始化为0。TensorFlow中也有zeros_initializer、ones_initializer等特殊常数初始化函数。  </p>
<p><strong>2. 高斯初始化(gaussian)</strong> </p>
<p>​     给定一组均值和标准差，随机初始化的参数会满足给定均值和标准差的高斯分布。高斯初始化是很常用的初始化方式。特殊地，在TensorFlow中还有一种截断高斯分布初始化（truncated_normal_initializer），其主要为了将超过两个标准差的随机数重新随机，使得随机数更稳定。</p>
<p><strong>3. 均匀分布初始化(uniform)</strong> </p>
<p>​    给定最大最小的上下限，参数会在该范围内以均匀分布方式进行初始化，常用上下限为（0，1）。</p>
<p><strong>4. xavier 初始化(uniform)</strong>  </p>
<p>​    在batchnorm还未出现之前，要训练较深的网络，防止梯度弥散，需要依赖非常好的初始化方式。xavier 就是一种比较优秀的初始化方式，也是目前最常用的初始化方式之一。其目的是为了使得模型各层的激活值和梯度在传播过程中的方差保持一致。本质上xavier 还是属于均匀分布初始化，但与上述的均匀分布初始化有所不同，xavier 的上下限将在如下范围内进行均匀分布采样：</p>
<script type="math/tex; mode=display">
[-\sqrt{\frac{6}{n+m}},\sqrt{\frac{6}{n+m}}]</script><p>​    其中，n为所在层的输入维度，m为所在层的输出维度。</p>
<p><strong>6. kaiming初始化（msra 初始化）</strong> </p>
<p>​    kaiming初始化，在caffe中也叫msra 初始化。kaiming初始化和xavier 一样都是为了防止梯度弥散而使用的初始化方式。kaiming初始化的出现是因为xavier存在一个不成立的假设。xavier在推导中假设激活函数都是线性的，而在深度学习中常用的ReLu等都是非线性的激活函数。而kaiming初始化本质上是高斯分布初始化，与上述高斯分布初始化有所不同，其是个满足均值为0，方差为2/n的高斯分布：</p>
<script type="math/tex; mode=display">
[0,\sqrt{\frac{2}{n}}]</script><p>​    其中，n为所在层的输入维度。</p>
<p>除上述常见的初始化方式以外，不同深度学习框架下也会有不同的初始化方式，读者可自行查阅官方文档。</p>
<h2 id="13-5-如何防止梯度下降陷入局部最优解"><a href="#13-5-如何防止梯度下降陷入局部最优解" class="headerlink" title="13.5 如何防止梯度下降陷入局部最优解?"></a>13.5 如何防止梯度下降陷入局部最优解?</h2><p>梯度下降法(GD)及其一些变种算法是目前深度学习里最常用于求解凸优化问题的优化算法。神经网络很可能存在很多局部最优解，而非全局最优解。 为了防止陷入局部最优，通常会采用如下一些方法，当然，这并不能保证一定能找到全局最优解，或许能得到一个比目前更优的局部最优解也是不错的：</p>
<p><strong>（1）stochastic GD</strong> /<strong>Mini-Batch GD</strong> </p>
<p>​    在GD算法中，每次的梯度都是从所有样本中累计获取的，这种情况最容易导致梯度方向过于稳定一致，且更新次数过少，容易陷入局部最优。而stochastic GD是GD的另一种极端更新方式，其每次都只使用一个样本进行参数更新，这样更新次数大大增加也就不容易陷入局部最优。但引出的一个问题的在于其更新方向过多，导致不易于进一步优化。Mini-Batch GD便是两种极端的折中，即每次更新使用一小批样本进行参数更新。Mini-Batch GD是目前最常用的优化算法，严格意义上Mini-Batch GD也叫做stochastic GD，所以很多深度学习框架上都叫做SGD。<br><strong>（2）动量 </strong><br>​    动量也是GD中常用的方式之一，SGD的更新方式虽然有效，但每次只依赖于当前批样本的梯度方向，这样的梯度方向依然很可能很随机。动量就是用来减少随机，增加稳定性。其思想是模仿物理学的动量方式，每次更新前加入部分上一次的梯度量，这样整个梯度方向就不容易过于随机。一些常见情况时，如上次梯度过大，导致进入局部最小点时，下一次更新能很容易借助上次的大梯度跳出局部最小点。</p>
<p><strong>（3）自适应学习率 </strong> </p>
<p>​    无论是GD还是动量重点优化角度是梯度方向。而学习率则是用来直接控制梯度更新幅度的超参数。自适应学习率的优化方法有很多，例如Adagrad和RMSprop。两种自适应学习率的方式稍有差异，但主要思想都是基于历史的累计梯度去计算一个当前较优的学习率。</p>
<h2 id="13-7-为什么需要激活函数？"><a href="#13-7-为什么需要激活函数？" class="headerlink" title="13.7 为什么需要激活函数？"></a>13.7 为什么需要激活函数？</h2><p>（1）非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。</p>
<p>（2）几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。</p>
<p>（3）计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。</p>
<p>（4）非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。</p>
<p>（5）单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。</p>
<p>（6）输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。</p>
<p>（7）接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。</p>
<p>（8）参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。</p>
<p>（9）归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。</p>
<h2 id="13-6-常见的损失函数有哪些"><a href="#13-6-常见的损失函数有哪些" class="headerlink" title="13.6 常见的损失函数有哪些?"></a>13.6 常见的损失函数有哪些?</h2><p>机器学习通过对算法中的目标函数进行不断求解优化，得到最终想要的结果。分类和回归问题中，通常使用损失函数或代价函数作为目标函数。</p>
<p>损失函数用来评价预测值和真实值不一样的程度。通常损失函数越好，模型的性能也越好。</p>
<p>损失函数可分为<strong>经验风险损失</strong>和<strong>结构风险损失</strong>。经验风险损失是根据已知数据得到的损失。结构风险损失是为了防止模型被过度拟合已知数据而加入的惩罚项。</p>
<p>下面介绍常用的损失函数:<br><strong>（1）0-1 损失函数</strong><br>&emsp;&emsp;<br>如果预测值和目标值相等，值为 0，如果不相等，值为 1：</p>
<script type="math/tex; mode=display">
L(Y,f(x))=
\left\{
\begin{array}{}
1\;\;\;,\;\;Y\ne f(x), \\
0\;\;\;,\;\;Y=f(x).
\end{array}
\right.</script><p>&emsp;&emsp;<br>一般的在实际使用中，相等的条件过于严格，可适当放宽条件：</p>
<script type="math/tex; mode=display">
L(Y,f(x))=
\left\{
\begin{array}{}
1\;\;\;,\;\;|Y - f(x)| \ge T, \\
0\;\;\;,\;\;|Y-f(x)| < T.
\end{array}
\right.</script><p>&emsp;&emsp;<br><strong>（2）绝对值损失函数</strong><br>&emsp;&emsp;<br>和 0-1 损失函数相似，绝对值损失函数表示为：</p>
<script type="math/tex; mode=display">
L(Y,f(x))=|Y-f(x)|.</script><p>&emsp;&emsp;<br><strong>（3）平方损失函数</strong>  </p>
<script type="math/tex; mode=display">
L(Y|f(x))=\sum_{N}(Y-f(x))^2.</script><p>&emsp;&emsp;<br>这点可从最小二乘法和欧几里得距离角度理解。最小二乘法的原理是，最优拟合曲线应该 使所有点到回归直线的距离和最小。</p>
<p>&emsp;&emsp;<br><strong>（4）log 对数损失函数</strong>  </p>
<script type="math/tex; mode=display">
L(Y,P(Y|X))=-logP(Y|X).</script><p>&emsp;&emsp;<br>常见的逻辑回归使用的就是对数损失函数，有很多人认为逻辑回归的损失函数式平方损失， 其实不然。逻辑回归它假设样本服从伯努利分布，进而求得满足该分布的似然函数，接着取对 数求极值等。逻辑回归推导出的经验风险函数是最小化负的似然函数，从损失函数的角度看， 就是 log 损失函数。</p>
<p>&emsp;&emsp;<br><strong>（5）指数损失函数</strong><br>&emsp;&emsp;<br>指数损失函数的标准形式为：</p>
<script type="math/tex; mode=display">
L(Y|f(x))=exp[-yf(x)].</script><p>&emsp;&emsp;<br>例如 AdaBoost 就是以指数损失函数为损失函数。</p>
<p>&emsp;&emsp;<br><strong>（6）Hinge 损失函数</strong><br>&emsp;&emsp;<br>Hinge 损失函数的标准形式如下：</p>
<script type="math/tex; mode=display">
L(y)=max(0, 1-ty).</script><p>&emsp;&emsp;<br>其中 y 是预测值，范围为(-1,1), t 为目标值，其为-1 或 1。<br>&emsp;&emsp;<br>在线性支持向量机中，最优化问题可等价于：</p>
<script type="math/tex; mode=display">
\underset{w,b}{min}\sum_{i=1}^{N}(1-y_i(wx_i+b))+\lambda \lVert w^2 \rVert</script><p>&emsp;&emsp;</p>
<script type="math/tex; mode=display">
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i))+\lVert w^2 \rVert</script><p>&emsp;&emsp;<br>其中$l(wx_i+by_i))$是Hinge损失函数，$\lVert w^2 \rVert$可看做为正则化项。</p>
<h2 id="13-7-如何进行特征选择-feature-selection"><a href="#13-7-如何进行特征选择-feature-selection" class="headerlink" title="13.7 如何进行特征选择(feature selection)?"></a>13.7 如何进行特征选择(feature selection)?</h2><h3 id="13-7-1-特征类型有哪些？"><a href="#13-7-1-特征类型有哪些？" class="headerlink" title="13.7.1 特征类型有哪些？"></a>13.7.1 特征类型有哪些？</h3><p>对象本身会有许多属性。所谓特征，即能在某方面最能表征对象的一个或者一组属性。一般地，我们可以把特征分为如下三个类型：</p>
<p>（1）相关特征：对于特定的任务和场景具有一定帮助的属性，这些属性通常能有效提升算法性能；</p>
<p>（2）无关特征：在特定的任务和场景下完全无用的属性，这些属性对对象在本目标环境下完全无用；</p>
<p>（3）冗余特征：同样是在特定的任务和场景下具有一定帮助的属性，但这类属性已过多的存在，不具有产生任何新的信息的能力。</p>
<h3 id="13-7-2-如何考虑特征选择"><a href="#13-7-2-如何考虑特征选择" class="headerlink" title="13.7.2 如何考虑特征选择"></a>13.7.2 如何考虑特征选择</h3><p>当完成数据预处理之后，对特定的场景和目标而言很多维度上的特征都是不具有任何判别或者表征能力的，所以需要对数据在维度上进行筛选。一般地，可以从以下两个方面考虑来选择特征:</p>
<p>（1）特征是否具有发散性：某个特征若在所有样本上的都是一样的或者接近一致，即方差非常小。 也就是说所有样本的都具有一致的表现，那这些就不具有任何信息。</p>
<p>（2）特征与目标的相关性：与目标相关性高的特征，应当优选选择。</p>
<h3 id="13-7-3-特征选择方法分类"><a href="#13-7-3-特征选择方法分类" class="headerlink" title="13.7.3 特征选择方法分类"></a>13.7.3 特征选择方法分类</h3><p>根据特征选择的形式又可以将特征选择方法分为 3 种:<br>（1）过滤法：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。  </p>
<p>（2）包装法：根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。  </p>
<p>（3）嵌入法：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。</p>
<h3 id="13-7-4-特征选择目的"><a href="#13-7-4-特征选择目的" class="headerlink" title="13.7.4 特征选择目的"></a>13.7.4 特征选择目的</h3><p>（1）减少特征维度，使模型泛化能力更强，减少过拟合;   </p>
<p>（2）降低任务目标的学习难度；</p>
<p>（3）一组优秀的特征通常能有效的降低模型复杂度，提升模型效率 </p>
<h2 id="13-8-梯度消失-梯度爆炸原因，以及解决方法"><a href="#13-8-梯度消失-梯度爆炸原因，以及解决方法" class="headerlink" title="13.8 梯度消失/梯度爆炸原因，以及解决方法"></a>13.8 梯度消失/梯度爆炸原因，以及解决方法</h2><h3 id="13-8-1-为什么要使用梯度更新规则"><a href="#13-8-1-为什么要使用梯度更新规则" class="headerlink" title="13.8.1 为什么要使用梯度更新规则?"></a>13.8.1 为什么要使用梯度更新规则?</h3><p>目前深度学习的火热，其最大的功臣之一就是反向传播。反向传播，即根据损失评价函数计算的误差，计算得到梯度，通过梯度反向传播的方式，指导深度网络权值的更新优化。这样做的原因在于，深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数，因此整个深度网络可以视为是一个复合的非线性多元函数：</p>
<script type="math/tex; mode=display">
F(x)=f_n(\cdots f_3(f_2(f_1(x)*\theta_1+b)*\theta_2+b)\cdots)</script><p>我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，假设不同的输入，输出的最优解是g(x) ，那么，优化深度网络就是为了寻找到合适的权值，满足 Loss=L(g(x),F(x))取得极小值点，比如最简单的损失函数：</p>
<script type="math/tex; mode=display">
Loss = \lVert g(x)-f(x) \rVert^2_2.</script><p>假设损失函数的数据空间是下图这样的，我们最优的权值就是为了寻找下图中的最小值点， 对于这种数学寻找最小值问题，采用梯度下降的方法再适合不过了。</p>
<p><img src="/img/ch13/figure_13_15_1.png" alt=""></p>
<center>图 13.8.1 </center>

<h3 id="13-8-2-梯度消失-爆炸产生的原因"><a href="#13-8-2-梯度消失-爆炸产生的原因" class="headerlink" title="13.8.2 梯度消失/爆炸产生的原因?"></a>13.8.2 梯度消失/爆炸产生的原因?</h3><p>本质上，梯度消失和爆炸是一种情况。在深层网络中，由于网络过深，如果初始得到的梯度过小，或者传播途中在某一层上过小，则在之后的层上得到的梯度会越来越小，即产生了梯度消失。梯度爆炸也是同样的。一般地，不合理的初始化以及激活函数，如sigmoid等，都会导致梯度过大或者过小，从而引起消失/爆炸。</p>
<p>下面分别从网络深度角度以及激活函数角度进行解释：</p>
<p>（1）网络深度 </p>
<p>若在网络很深时，若权重初始化较小，各层上的相乘得到的数值都会0-1之间的小数，而激活函数梯度也是0-1之间的数。那么连乘后，结果数值就会变得非常小，导致<strong>梯度消失</strong>。若权重初始化较大，大到乘以激活函数的导数都大于1，那么连乘后，可能会导致求导的结果很大，形成<strong>梯度爆炸</strong>。</p>
<p>（2）激活函数<br>如果激活函数选择不合适，比如使用 sigmoid，梯度消失就会很明显了，原因看下图，左图是sigmoid的函数图，右边是其导数的图像，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25的，这样经过链式求导之后，很容易发生梯度消失。<br><img src="/img/ch13/figure_13_15_2.png" alt=""></p>
<center>图 13.8.2 sigmod函数与其导数</center>

<h3 id="13-8-3-梯度消失、爆炸的解决方案"><a href="#13-8-3-梯度消失、爆炸的解决方案" class="headerlink" title="13.8.3 梯度消失、爆炸的解决方案"></a>13.8.3 梯度消失、爆炸的解决方案</h3><p><strong>1、预训练加微调</strong><br>此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<p><strong>2、梯度剪切、正则</strong><br>梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。<br>另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是L1和L2正则。</p>
<p><strong>3、ReLu、leakReLu等激活函数</strong><br>（1）ReLu：其函数的导数在正数部分是恒等于1，这样在深层网络中，在激活函数部分就不存在导致梯度过大或者过小的问题，缓解了梯度消失或者爆炸。同时也方便计算。当然，其也存在存在一些缺点，例如过滤到了负数部分，导致部分信息的丢失，输出的数据分布不在以0为中心，改变了数据分布。<br>（2）leakrelu：就是为了解决relu的0区间带来的影响，其数学表达为：leakrelu=max(k*x,0)其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。</p>
<p><strong>4、batchnorm</strong><br>Batchnorm是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batchnorm本质上是解决反向传播过程中的梯度问题。Batchnorm全名是Batch Normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。  </p>
<p><strong>5、残差结构</strong><br>残差的方式，能使得深层的网络梯度通过跳级连接路径直接返回到浅层部分，使得网络无论多深都能将梯度进行有效的回传。</p>
<p><strong>6、LSTM</strong><br>LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)。在计算时，将过程中的梯度进行了抵消。</p>
<h2 id="13-9-深度学习为什么不用二阶优化？"><a href="#13-9-深度学习为什么不用二阶优化？" class="headerlink" title="13.9 深度学习为什么不用二阶优化？"></a>13.9 深度学习为什么不用二阶优化？</h2><p>目前深度学习中，反向传播主要是依靠一阶梯度。二阶梯度在理论和实际上都是可以应用都网络中的，但相比于一阶梯度，二阶优化会存在以下一些主要问题：<br>（1）计算量大，训练非常慢。<br>（2）二阶方法能够更快地求得更高精度的解，这在浅层模型是有益的。而在神经网络这类深层模型中对参数的精度要求不高，甚至不高的精度对模型还有益处，能够提高模型的泛化能力。<br>（3）稳定性。二阶方法能更快求高精度的解，同样对数据本身要的精度也会相应的变高，这就会导致稳定性上的问题。</p>
<h2 id="13-10-为什么要设置单一数字评估指标，设置指标的意义？"><a href="#13-10-为什么要设置单一数字评估指标，设置指标的意义？" class="headerlink" title="13.10 为什么要设置单一数字评估指标，设置指标的意义？"></a>13.10 为什么要设置单一数字评估指标，设置指标的意义？</h2><p>在训练模型时，无论是调整超参数，还是调整不同的模型算法，我们都需要一个有效的评价指标，这个评价标准能帮助我们快速了解新的尝试后模型的性能是否更优。例如在分类时，我们通常会选择选择准确率，当样本不平衡时，查准率和查全率又会是更好的评价指标。所以在训练模型时，如果设置了单一数字的评估指标通常能很快的反应出我们模型的改进是否直接产生了收益，从而加速我们的算法改进过程。若在训练过程中，发现优化目标进一步深入，现有指标无法完全反应进一步的目标时，就需要重新选择评估指标了。</p>
<h2 id="13-11训练-验证-测试集的定义及划分"><a href="#13-11训练-验证-测试集的定义及划分" class="headerlink" title="13.11训练/验证/测试集的定义及划分"></a>13.11训练/验证/测试集的定义及划分</h2><p>训练、验证、测试集在机器学习领域是非常重要的三个内容。三者共同组成了整个项目的性能的上限和走向。</p>
<p>训练集：用于模型训练的样本集合，样本占用量是最大的；</p>
<p>验证集：用于训练过程中的模型性能评价，跟着性能评价才能更好的调参；</p>
<p>测试集：用于最终模型的一次最终评价，直接反应了模型的性能。</p>
<p>在划分上，可以分两种情况：</p>
<p>1、在样本量有限的情况下，有时候会把验证集和测试集合并。实际中，若划分为三类，那么训练集：验证集：测试集=6:2:2；若是两类，则训练集：验证集=7:3。这里需要主要在数据量不够多的情况，验证集和测试集需要占的数据比例比较多，以充分了解模型的泛化性。</p>
<p>2、在海量样本的情况下，这种情况在目前深度学习中会比较常见。此时由于数据量巨大，我们不需要将过多的数据用于验证和测试集。例如拥有1百万样本时，我们按训练集：验证集：测试集=98:1:1的比例划分，1%的验证和1%的测试集都已经拥有了1万个样本。这已足够验证模型性能了。</p>
<p>此外，三个数据集的划分不是一次就可以的，若调试过程中发现，三者得到的性能评价差异很大时，可以重新划分以确定是数据集划分的问题导致还是由模型本身导致的。其次，若评价指标发生变化，而导致模型性能差异在三者上很大时，同样可重新划分确认排除数据问题，以方便进一步的优化。</p>
<h2 id="13-12-什么是TOP5错误率？"><a href="#13-12-什么是TOP5错误率？" class="headerlink" title="13.12 什么是TOP5错误率？"></a>13.12 什么是TOP5错误率？</h2><p>通常对于分类系统而言，系统会对某个未知样本进行所有已知样本的匹配，并给出该未知样本在每个已知类别上的概率。其中最大的概率就是系统系统判定最可能的一个类别。TOP5则就是在前五个最大概率的类别。TOP5错误率，即预测最可能的五类都不是该样本类别的错误率。  </p>
<p>TOP5错误率通常会用于在类别数量很多或者细粒度类别的模型系统。典型地，例如著名的ImageNet ，其包含了1000个类别。通常就会采用TOP5错误率。</p>
<h2 id="13-13-什么是泛化误差，如何理解方差和偏差？"><a href="#13-13-什么是泛化误差，如何理解方差和偏差？" class="headerlink" title="13.13 什么是泛化误差，如何理解方差和偏差？"></a>13.13 什么是泛化误差，如何理解方差和偏差？</h2><p>一般情况下，我们评价模型性能时都会使用泛化误差。泛化误差越低，模型性能越好。泛化误差可分解为方差、偏差和噪声三部分。这三部分中，噪声是个不可控因素，它的存在是算法一直无法解决的问题，很难约减，所以我们更多考虑的是方差和偏差。</p>
<p> 方差和偏差在泛化误差上可做如下分解，假设我们的预测值为g(x)，真实值为f(x)，则均方误差为</p>
<script type="math/tex; mode=display">
E((g(x)−f(x))2)</script><p>这里假设不考虑噪声，g来代表预测值，f代表真实值，g¯=E(g)代表算法的期望预测，则有如下表达：</p>
<script type="math/tex; mode=display">
\begin{align}
E(g-f)^2&=E(g^2-2gf+f^2)
\\&=E(g^2)-\bar g^2+(\bar g-f)^2
\\&=E(g^2)-2\bar g^2+\bar g^2+(\bar g-f)^2
\\&=E(g^2-2g\bar g^2+\bar g^2)+(\bar g-f)^2
\\&=\underbrace{E(g-\bar g)^2}_{var(x)}+\underbrace{(\bar g-f)^2}_{bias^2(x)}
\end{align}</script><p>有上述公式可知，方差描述是理论期望和预测值之间的关系，这里的理论期望通常是指所有适用于模型的各种不同分布类型的数据集；偏差描述为真实值和预测值之间的关系，这里的真实值通常指某一个特定分布的数据集合。</p>
<p>所以综上方差表现为模型在各类分布数据的适应能力，方差越大，说明数据分布越分散，而偏差则表现为在特定分布上的适应能力，偏差越大越偏离真实值。</p>
<h2 id="13-14-如何提升模型的稳定性？"><a href="#13-14-如何提升模型的稳定性？" class="headerlink" title="13.14 如何提升模型的稳定性？"></a>13.14 如何提升模型的稳定性？</h2><p>评价模型不仅要从模型的主要指标上的性能，也要注重模型的稳定性。模型的稳定性体现在对不同样本之间的体现的差异。如模型的方差很大，那可以从如下几个方面进行考虑：</p>
<p>（1）正则化（L2, L1, dropout）：模型方差大，很可能来自于过拟合。正则化能有效的降低模型的复杂度，增加对更多分布的适应性。</p>
<p>（2）提前停止训练：提前停止是指模型在验证集上取得不错的性能时停止训练。这种方式本质和正则化是一个道理，能减少方差的同时增加的偏差。目的为了平衡训练集和未知数据之间在模型的表现差异。</p>
<p>（3）扩充训练集：正则化通过控制模型复杂度，来增加更多样本的适应性。那增加训练集让模型适应不同类型的数据本身就是一种最简单直接的方式提升模型稳定的方法，也是最可靠的一种方式。  与正则有所不同的是，扩充数据集既可以减小偏差又能减小方差。</p>
<p>（4）特征选择：过高的特征维度会使模型过拟合，减少特征维度和正则一样可能会处理好方差问题，但是同时会增大偏差。但需要注意的是若过度删减特征，很可能会删除很多有用的特征，降低模型的性能。所以需要多注意删减的特征对模型的性能的影响。</p>
<h2 id="13-15-有哪些改善模型的思路"><a href="#13-15-有哪些改善模型的思路" class="headerlink" title="13.15 有哪些改善模型的思路"></a>13.15 有哪些改善模型的思路</h2><p>改善模型本质是如何优化模型，这本身是个很宽泛的问题。也是目前学界一直探索的目的，而从目前常规的手段上来说，一般可取如下几点。</p>
<h3 id="13-15-1-数据角度"><a href="#13-15-1-数据角度" class="headerlink" title="13.15.1 数据角度"></a>13.15.1 数据角度</h3><p>增强数据集。无论是有监督还是无监督学习，数据永远是最重要的驱动力。更多的类型数据对良好的模型能带来更好的稳定性和对未知数据的可预见性。对模型来说，“看到过的总比没看到的更具有判别的信心”。但增大数据并不是盲目的，模型容限能力不高的情况下即使增大数据也对模型毫无意义。而从数据获取的成本角度，对现有数据进行有效的扩充也是个非常有效且实际的方式。良好的数据处理，常见的处理方式如数据缩放、归一化和标准化等。</p>
<h3 id="13-15-2-模型角度"><a href="#13-15-2-模型角度" class="headerlink" title="13.15.2 模型角度"></a>13.15.2 模型角度</h3><p>模型的容限能力决定着模型可优化的空间。在数据量充足的前提下，对同类型的模型，增大模型规模来提升容限无疑是最直接和有效的手段。但越大的参数模型优化也会越难，所以需要在合理的范围内对模型进行参数规模的修改。而不同类型的模型，在不同数据上的优化成本都可能不一样，所以在探索模型时需要尽可能挑选优化简单，训练效率更高的模型进行训练。</p>
<h3 id="13-15-3-调参优化角度"><a href="#13-15-3-调参优化角度" class="headerlink" title="13.15.3 调参优化角度"></a>13.15.3 调参优化角度</h3><p>如果你知道模型的性能为什么不再提高了，那已经向提升性能跨出了一大步。 超参数调整本身是一个比较大的问题。一般可以包含模型初始化的配置，优化算法的选取、学习率的策略以及如何配置正则和损失函数等等。这里需要提出的是对于同一优化算法，相近参数规模的前提下，不同类型的模型总能表现出不同的性能。这实际上就是模型优化成本。从这个角度的反方向来考虑，同一模型也总能找到一种比较适合的优化算法。所以确定了模型后选择一个适合模型的优化算法也是非常重要的手段。</p>
<h3 id="13-15-4-训练角度"><a href="#13-15-4-训练角度" class="headerlink" title="13.15.4 训练角度"></a>13.15.4 训练角度</h3><p>很多时候我们会把优化和训练放一起。但这里我们分开来讲，主要是为了强调充分的训练。在越大规模的数据集或者模型上，诚然一个好的优化算法总能加速收敛。但你在未探索到模型的上限之前，永远不知道训练多久算训练完成。所以在改善模型上充分训练永远是最必要的过程。充分训练的含义不仅仅只是增大训练轮数。有效的学习率衰减和正则同样是充分训练中非常必要的手段。</p>
<h2 id="13-16-如何快速构建有效初始模型？"><a href="#13-16-如何快速构建有效初始模型？" class="headerlink" title="13.16 如何快速构建有效初始模型？"></a>13.16 如何快速构建有效初始模型？</h2><p>​    构建一个有效的初始模型能帮助我们快速了解数据的质量和确定模型构建的方向。构建一个良好的初始模型，一般需要注意如下几点：</p>
<p>​    1、了解”对手”。这里的“对手”通常是指数据，我们在得到数据时，第一步是需要了解数据特点和使用场合。了解数据特点能帮助我们快速定位如何进行建模。确定使用场合能帮助我们进一步确定模型需要优化的方向。数据特点一般需要了解例如数据集规模、训练集和验证集是否匹配、样本的分布是否均匀、数据是否存在缺失值等等。</p>
<p>​    2、站在巨人肩膀上。根据数据特点，我们通常能匹配到一个现有比较优秀的模型。这类模型都通常能在类似数据上表现出一个比较不错的性能。</p>
<p>​    3、一切从简。初始模型的作用在于迅速了解数据质量和特点，所以模型的性能通常不需要达到很高，模型复杂度也不需要很高。例如，做图像分类时，我们在使用预训练模型时，不需要一开始就使用例如ResNet152这类模型巨大，复杂度过高的模型。这在数据量较小时，很容易造成过拟合而导致出现我们对数据产生一些误导性的判断，此外也增加了额外训练构建时间。所以使用更小更简单的模型以及损失函数来试探数据是相比更明智的选择。</p>
<p>​    4、总比瞎猜强。构建模型的意义在于建立一个高效的模型，虽然初始模型我们不对性能做过高的要求。但前提在于必须要比随机猜测好，不然构建模型的意义就不存在了。</p>
<p>​    5、解剖模型。一旦确定了一个初始模型时，无论你对该模型多熟悉，当其面对一批新数据时，你永远需要重新去认识这个模型，因为你永远不确定模型内部到底发生了些什么。解剖模型一般需要在训练时注意误差变化、注意训练和验证集的差异；出现一些NAN或者INf等情况时，需要打印观察内部输出，确定问题出现的时间和位置；在完成训练后，需要测试模型的输出是否正确合理，以确认评价指标是否符合该数据场景。无论使用任何一种模型，我们都不能把它当做黑盒去看待。</p>
<h2 id="13-17-如何通过模型重新观察数据？"><a href="#13-17-如何通过模型重新观察数据？" class="headerlink" title="13.17 如何通过模型重新观察数据？"></a>13.17 如何通过模型重新观察数据？</h2><p>​    对于这个问题，与其说如何做，倒不如说这个问题是用来强调这样做的重要性。如何重新观察数据其实不难，而是很多读者，会忽略这一项过程的重要性。</p>
<p>​    通过模型重新观察数据，不仅能让我们了解模型情况，也能让我们对数据质量产生进一步的理解。目前深度学习在监督学习领域成就是非常显著的。监督学习需要依赖大量的人为标注，人为标注很难确定是否使用的数据中是否存在错误标注或者漏标注等问题。这无论是哪种情况都会影响我们对模型的判断。所以通过模型重新验证数据质量是非常重要的一步。很多初学者，通常会忽略这一点，而导致出现对模型的一些误判，严重时甚至会影响整个建模方向。此外，对于若出现一些过拟合的情况，我们也可以通过观察来了解模型。例如分类任务，样本严重不平衡时，模型全预测到了一边时，其正确率仍然很高，但显然模型已经出现了问题。</p>
<h2 id="13-18-如何解决数据不匹配问题？"><a href="#13-18-如何解决数据不匹配问题？" class="headerlink" title="13.18 如何解决数据不匹配问题？"></a>13.18 如何解决数据不匹配问题？</h2><h3 id="13-18-1-如何定位数据不匹配"><a href="#13-18-1-如何定位数据不匹配" class="headerlink" title="13.18.1 如何定位数据不匹配?"></a>13.18.1 如何定位数据不匹配?</h3><p>​    数据不匹配问题是个不容易定位和解决的问题。这个问题出现总会和模型过拟合表现很相似,即在训练集上能体现非常不错的性能,但在测试集上表现总是差强人意但区别在于如果遇到是数据不匹配的问题,通常在用一批和训<br>练集有看相同或者相似分布的数据上仍然能取得不错的结果。但很多时候,当测试集上结果表现很差时,很多初学<br>者可能会直接将问题定位在模型过拟合上,最后对模型尝试各种方法后,性能却始终不能得到有效提升。当遇到这<br>种情况时,建议先定位出是否存在数据不匹配的问题。最简单的验证方式就是可以从训练集中挑选出一部分数据作<br>为验证集,重新划分后训练和验证模型表现。</p>
<h3 id="13-18-2-举例常见几个数据不匹配的场景"><a href="#13-18-2-举例常见几个数据不匹配的场景" class="headerlink" title="13.18.2 举例常见几个数据不匹配的场景?"></a>13.18.2 举例常见几个数据不匹配的场景?</h3><p>​    例如设计款识别物体的app时,实际场景的图片均来自于手机拍摄,而训练集确是来自于网上各类抓取下来的图<br>片。例如在图像去噪、去模糊、去雾、超分辨率等图像处理场景时,由于大量数据的难以获取,因此都会采用人为<br>假设合成的图像进行训练,这时候应用到实际场景中也容易出现不匹配的问题</p>
<h3 id="13-18-3-如何解决数据不匹配问题"><a href="#13-18-3-如何解决数据不匹配问题" class="headerlink" title="13.18.3 如何解决数据不匹配问题?"></a>13.18.3 如何解决数据不匹配问题?</h3><p>​    数据不匹配是个很难有固定方法来解决的问题。这里提供几条供参考的途径：<br>​    1、收集更多符合实际场最需要的数据。这似乎是最简单但也最难方式<br>​    2、对结果做错误分析。找出数据集中出错的数据和正确数据之间的特点和区别,这对你无论是进行后续模型的分析或者是数据的处理提供非常有效的思路。注意,这里的数据集包括训练集和测试集<br>​    3、数据集增强。数据集增强并不意味看数据集越大越好,其目的是丰富数据的分布以适应更多的变化当遇到数<br>据不匹配时,对数据处理般可以有两种方式。其一,合成或处理更多接近需要的数据特点。其二,对所有数据包<br>括实际场景数据都进行处理,将所有数据都统一到另一个分布上,统一出一种新的特点。</p>
<h3 id="13-18-4-如何提高深度学习系统的性能"><a href="#13-18-4-如何提高深度学习系统的性能" class="headerlink" title="13.18.4 如何提高深度学习系统的性能"></a>13.18.4 如何提高深度学习系统的性能</h3><p>​    当我们要试图提高深度学习系统的性能时，目前我们大致可以从三方面考虑：</p>
<p>​    1、提高模型的结构，比如增加神经网络的层数，或者将简单的神经元单位换成复杂的 LSTM 神经元，比如在自然语言处理领域内，利用 LSTM 模型挖掘语法分析的优势。</p>
<p>​    2、改进模型的初始化方式，保证早期梯度具有某些有益的性质，或者具备大量的稀疏性，或者利用线性代数原理的优势。  </p>
<p>​    3、选择更强大的学习算法，比如对度梯度更新的方式，也可以是采用除以先前梯度 L2 范数来更新所有参数，甚至还可以选用计算代价较大的二阶算法。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 冯宇旭, 李裕梅. 深度学习优化器方法及学习率衰减方式综述[J]. 数据挖掘, 2018, 8(4): 186-200.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/" class="post-title-link" itemprop="url">第十二章_网络搭建及训练</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:32:46 / 修改时间：12:33:03" itemprop="dateCreated datePublished" datetime="2020-03-03T12:32:46+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第十二章-网络搭建及训练"><a href="#第十二章-网络搭建及训练" class="headerlink" title="第十二章 网络搭建及训练"></a>第十二章 网络搭建及训练</h1><h1 id="12-1-TensorFlow"><a href="#12-1-TensorFlow" class="headerlink" title="12.1 TensorFlow"></a>12.1 TensorFlow</h1><h2 id="12-1-1-TensorFlow是什么？"><a href="#12-1-1-TensorFlow是什么？" class="headerlink" title="12.1.1 TensorFlow是什么？"></a>12.1.1 TensorFlow是什么？</h2><p>&emsp;&emsp;TensorFlow支持各种异构平台，支持多CPU/GPU、服务器、移动设备，具有良好的跨平台的特性；TensorFlow架构灵活，能够支持各种网络模型，具有良好的通用性；此外，TensorFlow架构具有良好的可扩展性，对OP的扩展支持，Kernel特化方面表现出众。</p>
<p>&emsp;&emsp;TensorFlow最初由Google大脑的研究员和工程师开发出来，用于机器学习和神经网络方面的研究，于2015.10宣布开源，在众多深度学习框架中脱颖而出，在Github上获得了最多的Star量。</p>
<h2 id="12-1-2-TensorFlow的设计理念是什么？"><a href="#12-1-2-TensorFlow的设计理念是什么？" class="headerlink" title="12.1.2 TensorFlow的设计理念是什么？"></a>12.1.2 TensorFlow的设计理念是什么？</h2><p>TensorFlow的设计理念主要体现在两个方面：</p>
<p>（1）将图定义和图运算完全分开。<br>&emsp;&emsp;TensorFlow 被认为是一个“符号主义”的库。我们知道，编程模式通常分为命令式编程（imperative style programming）和符号式编程（symbolic style programming）。命令式编程就是编写我们理解的通常意义上的程序，很容易理解和调试，按照原有逻辑执行。符号式编程涉及很多的嵌入和优化，不容易理解和调试，但运行速度相对有所提升。现有的深度学习框架中，Torch 是典型的命令式的，Caffe、MXNet 采用了两种编程模式混合的方法，而 TensorFlow 完全采用符号式编程。</p>
<p>&emsp;&emsp;符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量间的计算关系，最后需要对据流图进行编译，但此时的数据流图还是一个空壳儿，里面没有任何实际数据，只有把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。</p>
<p>　　例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t &#x3D; 8 + 9</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在传统的程序操作中，定义了 t 的运算，在运行时就执行了，并输出 17。而在 TensorFlow中，数据流图中的节点，实际上对应的是 TensorFlow API 中的一个操作，并没有真正去运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">t &#x3D; tf.add(8,9)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line">#输出  Tensor&#123;&quot;Add_1:0&quot;,shape&#x3D;&#123;&#125;,dtype&#x3D;int32&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;（2）TensorFlow 中涉及的运算都要放在图中，而图的运行只发生在会话（session）中。开启会话后，就可以用数据去填充节点，进行运算；关闭会话后，就不能进行计算了。因此，会话提供了操作运行和 Tensor 求值的环境。</p>
<p>　　例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">#创建图</span><br><span class="line">a &#x3D; tf.constant([4.0,5.0])</span><br><span class="line">b &#x3D; tf.constant([6.0,7.0])</span><br><span class="line">c &#x3D; a * b</span><br><span class="line">#创建会话</span><br><span class="line">sess  &#x3D; tf.Session()</span><br><span class="line">#计算c</span><br><span class="line">print(sess.run(c))   #进行矩阵乘法，输出[24.,35.]</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<h2 id="12-1-3-TensorFlow特点有哪些？"><a href="#12-1-3-TensorFlow特点有哪些？" class="headerlink" title="12.1.3 TensorFlow特点有哪些？"></a>12.1.3 TensorFlow特点有哪些？</h2><h3 id="1-高度的灵活性"><a href="#1-高度的灵活性" class="headerlink" title="1.高度的灵活性"></a>1.高度的灵活性</h3><p>&emsp;&emsp;TensorFlow 并不仅仅是一个深度学习库，只要可以把你的计算过程表示称一个数据流图的过程，我们就可以使用 TensorFlow 来进行计算。TensorFlow 允许我们用计算图的方式建立计算网络，同时又可以很方便的对网络进行操作。用户可以基于 TensorFlow 的基础上用 python 编写自己的上层结构和库，如果TensorFlow没有提供我们需要的API的，我们也可以自己编写底层的 C++ 代码，通过自定义操作将新编写的功能添加到 TensorFlow 中。</p>
<h3 id="2-真正的可移植性"><a href="#2-真正的可移植性" class="headerlink" title="2.真正的可移植性"></a>2.真正的可移植性</h3><p>&emsp;&emsp;TensorFlow 可以在 CPU 和 GPU 上运行，可以在台式机、服务器、移动设备上运行。你想在你的笔记本上跑一下深度学习的训练，或者又不想修改代码，想把你的模型在多个CPU上运行， 亦或想将训练好的模型放到移动设备上跑一下，这些TensorFlow都可以帮你做到。</p>
<h3 id="3-多语言支持"><a href="#3-多语言支持" class="headerlink" title="3.多语言支持"></a>3.多语言支持</h3><p>&emsp;&emsp;TensorFlow采用非常易用的python来构建和执行我们的计算图，同时也支持 C++ 的语言。我们可以直接写python和C++的程序来执行TensorFlow，也可以采用交互式的ipython来方便的尝试我们的想法。当然，这只是一个开始，后续会支持更多流行的语言，比如Lua，JavaScript 或者R语言。</p>
<h3 id="4-丰富的算法库"><a href="#4-丰富的算法库" class="headerlink" title="4.丰富的算法库"></a>4.丰富的算法库</h3><p>&emsp;&emsp;TensorFlow提供了所有开源的深度学习框架里，最全的算法库，并且在不断的添加新的算法库。这些算法库基本上已经满足了大部分的需求，对于普通的应用，基本上不用自己再去自定义实现基本的算法库了。</p>
<h3 id="5-完善的文档"><a href="#5-完善的文档" class="headerlink" title="5.完善的文档"></a>5.完善的文档</h3><p>&emsp;&emsp;TensorFlow的官方网站，提供了非常详细的文档介绍，内容包括各种API的使用介绍和各种基础应用的使用例子，也包括一部分深度学习的基础理论。</p>
<p>&emsp;&emsp;自从宣布开源以来，大量人员对TensorFlow做出贡献，其中包括Google员工，外部研究人员和独立程序员，全球各地的工程师对TensorFlow的完善，已经让TensorFlow社区变成了Github上最活跃的深度学习框架。</p>
<h2 id="12-1-4-TensorFlow的系统架构是怎样的？"><a href="#12-1-4-TensorFlow的系统架构是怎样的？" class="headerlink" title="12.1.4 TensorFlow的系统架构是怎样的？"></a>12.1.4 TensorFlow的系统架构是怎样的？</h2><h3 id="emsp-emsp-整个系统从底层到上层可分为七层："><a href="#emsp-emsp-整个系统从底层到上层可分为七层：" class="headerlink" title="&emsp;&emsp;整个系统从底层到上层可分为七层："></a>&emsp;&emsp;整个系统从底层到上层可分为七层：</h3><p><img src=".\img\ch12\1.bmp" alt=""></p>
<p>&emsp;&emsp;设备层：硬件计算资源，支持CPU、GPU</p>
<p>&emsp;&emsp;网络层：支持两种通信协议</p>
<p>&emsp;&emsp;数值计算层：提供最基础的计算，有线性计算、卷积计算</p>
<p>&emsp;&emsp;高维计算层：数据的计算都是以数组的形式参与计算</p>
<p>&emsp;&emsp;计算图层：用来设计神经网络的结构</p>
<p>&emsp;&emsp;工作流层：提供轻量级的框架调用</p>
<p>&emsp;&emsp;构造层：最后构造的深度学习网络可以通过TensorBoard服务端可视化</p>
<h2 id="12-1-5-TensorFlow编程模型是怎样的？"><a href="#12-1-5-TensorFlow编程模型是怎样的？" class="headerlink" title="12.1.5 TensorFlow编程模型是怎样的？"></a>12.1.5 TensorFlow编程模型是怎样的？</h2><p>TensorFlow的编程模型：让向量数据在计算图里流动。那么在编程时至少有这几个过程：1.构建图，2.启动图，3.给图输入数据并获取结果。</p>
<h3 id="1-构建图"><a href="#1-构建图" class="headerlink" title="1.构建图"></a>1.构建图</h3><p>TensorFlow的图的类型是tf.Graph，它包含着计算节点和tensor的集合。</p>
<p>&emsp;&emsp;这里引用了两个新概念：tensor和计算节点。<br>&emsp;&emsp;我们先介绍tensor，一开始我们就介绍了，我们需要把数据输入给启动的图才能获取计算结果。那么问题来了，在构建图时用什么表示中间计算结果？这个时候tensor的概念就需要引入了。<br>&emsp;&emsp;类型是tf.Tensor，代表某个计算节点的输出，一定要看清楚是“代表”。它主要有两个作用：</p>
<p>1.构建不同计算节点之间的数据流</p>
<p>2.在启动图时，可以设置某些tensor的值，然后获取指定tensor的值。这样就完成了计算的输入输出功能。</p>
<p>如下代码所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inImage &#x3D; tf.placeholder(tf.float32,[32,32,3],&quot;inputImage&quot;)</span><br><span class="line">processedImage &#x3D; tf.image.per_image_standardization(inImage,&quot;processedImage&quot;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里inImage和processedImage都是tensor类型。它们代表着计算节点输出的数据，数据的值具体是多少在启动图的时候才知道。上面两个方法调用都传递了一个字符串，它是计算节点的名字，最好给节点命名，这样我们可以在图上调用get_tensor_by_name(name)获取对应的tensor对象，十分方便。（tensor名字为“&lt;计算节点名字&gt;:<tensor索引>”）</p>
<p>&emsp;&emsp;创建tensor时，需要指定类型和shape。对不同tensor进行计算时要求类型相同，可以使用 tf.cast 进行类型转换。同时也要求 shape (向量维度)满足运算的条件，我们可以使用 tf.reshape 改变shape。</p>
<p>&emsp;&emsp;现在了解计算节点的概念，其功能是对tensor进行计算、创建tensor或进行其他操作，类型是tf.Operation。获取节点对象的方法为get_operation_by_name(name)。</p>
<p>构建图，如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">g&#x3D;tf.Graph()</span><br><span class="line"></span><br><span class="line">with g.as_default():</span><br><span class="line">    input_data&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_data&quot;)</span><br><span class="line">    input_label&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_label&quot;)</span><br><span class="line"></span><br><span class="line">    W1&#x3D;tf.Variable(tf.truncated_normal([2,2]),name&#x3D;&quot;W1&quot;)</span><br><span class="line">    B1&#x3D;tf.Variable(tf.zeros([2]),name&#x3D;&quot;B1&quot;)</span><br><span class="line"></span><br><span class="line">    output&#x3D;tf.add(tf.matmul(input_data,W1),B1,name&#x3D;&quot;output&quot;)</span><br><span class="line">    cross_entropy&#x3D;tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;output,labels&#x3D;input_label)</span><br><span class="line"></span><br><span class="line">    train_step&#x3D;tf.train.AdamOptimizer().minimize(cross_entropy,name&#x3D;&quot;train_step&quot;)</span><br><span class="line"></span><br><span class="line">    initer&#x3D;tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面的代码中我们创建了一个图，并在上面添加了很多节点。我们可以通过调用get_default_graph()获取默认的图。</p>
<p>&emsp;&emsp;Input_data，input_label，W1，B1，output，cross_entropy都是tensor类型，train_step，initer，是节点类型。</p>
<p>有几类tensor或节点比较重要，下面介绍一下：</p>
<h4 id="1-placeholder"><a href="#1-placeholder" class="headerlink" title="1.placeholder"></a>1.placeholder</h4><p>&emsp;&emsp;Tensorflow，顾名思义， tensor代表张量数据，flow代表流，其最初的设计理念就是构建一张静态的数据流图。图是有各个计算节点连接而成，计算节点之间流动的便是中间的张量数据。要想让张量数据在我们构建的静态计算图中流动起来，就必须有最初的输入数据流。而placeholder，翻译过来叫做占位符，顾名思义，是给我们的输入数据提供一个接口，也就是说我们的一切输入数据，例如训练样本数据，超参数数据等都可以通过占位符接口输送到数据流图之中。使用实例如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x &#x3D; tf.placeholder(dtype&#x3D;tf.float32,shape&#x3D;[],name&#x3D;&#39;x&#39;)</span><br><span class="line">y &#x3D; tf.placeholder(dtpe&#x3D;tf.float32,shape&#x3D;[],nmae&#x3D;&#39;y&#39;)</span><br><span class="line">z &#x3D; x*y</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	prod &#x3D; sess.run(z,feed_dict&#x3D;&#123;x:1.,y:5.2&#125;)</span><br><span class="line">	print(prod)</span><br><span class="line">[out]:5.2</span><br></pre></td></tr></table></figure>
<h4 id="2-variable"><a href="#2-variable" class="headerlink" title="2. variable"></a>2. variable</h4><p>&emsp;&emsp;无论是传统的机器学习算法，例如线性支持向量机（Support Vector Machine, SVM)，其数学模型为y = <w,x> + b，还是更先进的深度学习算法，例如卷积神经网络（Convolutional Neural Network， CNN）单个神经元输出的模型y = w*x + b。可以看到，w和b就是我们要求的模型，模型的求解是通过优化算法（对于SVM，使用<br>SMO[1]算法，对于CNN，一般基于梯度下降法）来一步一步更新w和b的值直到满足停止条件。因此，大多数机器学习的模型中的w和b实际上是以变量的形式出现在代码中的，这就要求我们在代码中定义模型变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a &#x3D; tf.Variable(2.)</span><br><span class="line">b &#x3D; tf.Variable(3.)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer()) #变量初始化</span><br><span class="line">    print(sess.run(a*b))</span><br><span class="line">[out]:6.</span><br></pre></td></tr></table></figure>
<p>[1] Platt, John. “Sequential minimal optimization: A fast algorithm for training support vector machines.” (1998).</p>
<h4 id="3-initializer"><a href="#3-initializer" class="headerlink" title="3. initializer"></a>3. initializer</h4><p>&emsp;&emsp;由于tensorflow构建的是静态的计算流图，在开启会话之前，所有的操作都不会被执行。因此为了执行在计算图中所构建的赋值初始化计算节点，需要在开启会话之后，在会话环境下运行初始化。如果计算图中定义了变量，而会话环境下为执行初始化命令，则程序报错，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a &#x3D; tf.Variable(2.)</span><br><span class="line">b &#x3D; tf.Variable(3.)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	#sess.run(tf.global_variables_initializer()) #注释掉初始化命令</span><br><span class="line">    print(sess.run(a*b))</span><br><span class="line">[Error]: Attempting to use uninitialized value Variable</span><br></pre></td></tr></table></figure>
<h3 id="2-启动图"><a href="#2-启动图" class="headerlink" title="2.启动图"></a>2.启动图</h3><p>&emsp;&emsp;先了解session的概念，然后才能更好的理解图的启动。<br>&emsp;&emsp;图的每个运行实例都必须在一个session里，session为图的运行提供环境。Session的类型是tf.Session，在实例化session对象时我们需要给它传递一个图对象，如果不显示给出将使用默认的图。Session有一个graph属性，我们可以通过它获取session对应的图。</p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">numOfBatch&#x3D;5</span><br><span class="line">datas&#x3D;np.zeros([numOfBatch,2],np.float32)</span><br><span class="line">labels&#x3D;np.zeros([numOfBatch,2],np.float32)</span><br><span class="line"></span><br><span class="line">sess&#x3D;tf.Session(graph&#x3D;g)</span><br><span class="line">graph&#x3D;sess.graph</span><br><span class="line">sess.run([graph.get_operation_by_name(&quot;initer&quot;)])</span><br><span class="line"></span><br><span class="line">dataHolder&#x3D;graph.get_tensor_by_name(&quot;input_data:0&quot;)</span><br><span class="line">labelHolder&#x3D;graph.get_tensor_by_name(&quot;input_label:0&quot;)</span><br><span class="line">train&#x3D;graph.get_operation_by_name(&quot;train_step&quot;)</span><br><span class="line">out&#x3D;graph.get_tensor_by_name(&quot;output:0&quot;)</span><br><span class="line"></span><br><span class="line">for i inrange(200):</span><br><span class="line">   result&#x3D;sess.run([out,train],feed_dict&#x3D;&#123;dataHolder:datas,labelHolder:labels&#125;)</span><br><span class="line">   if i%100&#x3D;&#x3D;0:</span><br><span class="line">       saver.save(sess,&quot;.&#x2F;moules&quot;)</span><br><span class="line"></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>代码都比较简单，就不介绍了。不过要注意2点：1.别忘记运行初始化节点，2.别忘记close掉session对象以释放资源。</p>
<h4 id="3-给图输入数据并获取结果"><a href="#3-给图输入数据并获取结果" class="headerlink" title="3.给图输入数据并获取结果"></a>3.给图输入数据并获取结果</h4><p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for i inrange(200):</span><br><span class="line">    result&#x3D;sess.run([out,train],feed_dict&#x3D;&#123;dataHolder:datas,labelHolder:labels&#125;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里主要用到了session对象的run方法，它用来运行某个节点或tensor并获取对应的值。我们一般会一次传递一小部分数据进行mini-batch梯度下降来优化模型。</p>
<p>&emsp;&emsp;我们需要把我们需要运行的节点或tensor放入一个列表，然后作为第一个参数(不考虑self)传递给run方法，run方法会返回一个计算结果的列表，与我们传递的参数一一对应。</p>
<p>&emsp;&emsp;如果我们运行的节点依赖某个placeholder，那我们必须给这个placeholder指定值，怎么指定代码里面很清楚，给关键字参数feed_dict传递一个字典即可，字典里的元素的key是placeholder对象，value是我们指定的值。值的数据的类型必须和placeholder一致，包括shape。值本身的类型是numpy数组。</p>
<p>这里再解释一个细节，在定义placeholder时代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_data&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_data&quot;)</span><br><span class="line">input_label&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_label&quot;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;shape为[None,2]，说明数据第一个维度是不确定的，然后TensorFlow会根据我们传递的数据动态推断第一个维度，这样我们就可以在运行时改变batch的大小。比如一个数据是2维，一次传递10个数据对应的tensor的shape就是[10,2]。可不可以把多个维度指定为None？理论上不可以！</p>
<h2 id="12-1-6-如何基于tensorflow搭建VGG16"><a href="#12-1-6-如何基于tensorflow搭建VGG16" class="headerlink" title="12.1.6 如何基于tensorflow搭建VGG16"></a>12.1.6 如何基于tensorflow搭建VGG16</h2><p>​    介绍完关于tensorflow的基础知识，是时候来一波网络搭建实战了。虽然网上有很多相关教程，但我想从最标准的tensorflow代码和语法出发（而不是调用更高级的API，失去了原来的味道），向大家展示如何搭建其标准的VGG16网络架构。话不多说，上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(<span class="string">'weight'</span>, shape=shape, initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(<span class="string">'bias'</span>, shape=shape, initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, w, padding = <span class="string">'SAME'</span>, s=<span class="number">1</span>)</span>:</span></span><br><span class="line">    x = tf.nn.conv2d(x, w, strides=[<span class="number">1</span>, s, s, <span class="number">1</span>], padding = padding)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxPoolLayer</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_layer</span><span class="params">(x,in_chs, out_chs, ksize, layer_name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(layer_name):</span><br><span class="line">        w = get_weight_variable([ksize, ksize, in_chs, out_chs])</span><br><span class="line">        b = get_bias_variable([out_chs])</span><br><span class="line">        y = tf.nn.relu(tf.bias_add(conv2d(x,w,padding = <span class="string">'SAME'</span>, s=<span class="number">1</span>), b))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_layer</span><span class="params">(x,in_kernels, out_kernels, layer_name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(layer_name):</span><br><span class="line">        w = get_weight_variable([in_kernels,out_kernels])</span><br><span class="line">        b = get_bias_variable([out_kernels])</span><br><span class="line">        y = tf.nn.relu(tf.bias_add(tf.matmul(x,w),b))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VGG16</span><span class="params">(x)</span>:</span></span><br><span class="line">    conv1_1 = conv2d_layer(x,tf.get_shape(x).as_list()[<span class="number">-1</span>], <span class="number">64</span>, <span class="number">3</span>, <span class="string">'conv1_1'</span>)</span><br><span class="line">    conv1_2 = conv2d_layer(conv1_1,<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="string">'conv1_2'</span>)</span><br><span class="line">    pool_1 = maxPoolLayer(conv1_2)</span><br><span class="line">    </span><br><span class="line">    conv2_1 = conv2d_layer(pool1,<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="string">'conv2_1'</span>)</span><br><span class="line">    conv2_2 = conv2d_layer(conv2_1,<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="string">'conv2_2'</span>)</span><br><span class="line">    pool2 = maxPoolLayer(conv2_2)</span><br><span class="line">    </span><br><span class="line">	conv3_1 = conv2d_layer(pool2,<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_1'</span>)</span><br><span class="line">    conv3_2 = conv2d_layer(conv3_1,<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_2'</span>)</span><br><span class="line">	conv3_3 = conv2d_layer(conv3_2,<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_3'</span>)</span><br><span class="line">    pool3 = maxPoolLayer(conv3_3)</span><br><span class="line">    </span><br><span class="line">	conv4_1 = conv2d_layer(pool3,<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_1'</span>)</span><br><span class="line">    conv4_2 = conv2d_layer(conv4_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_2'</span>)</span><br><span class="line">	conv4_3 = conv2d_layer(conv4_2,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_3'</span>)</span><br><span class="line">    pool4 = maxPoolLayer(conv4_3)</span><br><span class="line">    </span><br><span class="line">	conv5_1 = conv2d_layer(pool4,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_1'</span>)</span><br><span class="line">    conv5_2 = conv2d_layer(conv5_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_2'</span>)</span><br><span class="line">	conv5_3 = conv2d_layer(conv5_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_3'</span>)</span><br><span class="line">    pool5 = maxPoolLayer(conv5_3)</span><br><span class="line">    </span><br><span class="line">	pool5_flatten_dims = int(np.prod(pool5.get_shape().as_list()[<span class="number">1</span>:]))</span><br><span class="line">    pool5_flatten = tf.reshape(pool5,[<span class="number">-1</span>,pool5_flatten_dims])</span><br><span class="line">    </span><br><span class="line">    fc_6 = fc_layer(pool5_flatten, pool5_flatten_dims, <span class="number">4096</span>, <span class="string">'fc6'</span>)</span><br><span class="line">	fc_7 = fc_layer(fc_6, <span class="number">4096</span>, <span class="number">4096</span>, <span class="string">'fc7'</span>)</span><br><span class="line">	fc_8 = fc_layer(fc_7, <span class="number">4096</span>, <span class="number">10</span>, <span class="string">'fc8'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> fc_8</span><br></pre></td></tr></table></figure>
<h1 id="12-2-Pytorch"><a href="#12-2-Pytorch" class="headerlink" title="12.2 Pytorch"></a>12.2 Pytorch</h1><h2 id="12-2-1-Pytorch是什么？"><a href="#12-2-1-Pytorch是什么？" class="headerlink" title="12.2.1 Pytorch是什么？"></a>12.2.1 Pytorch是什么？</h2><p>&emsp;&emsp;Pytorch是torch的python版本，是由Facebook开源的神经网络框架，专门针对 GPU 加速的深度神经网络（DNN）编程。Torch 是一个经典的对多维矩阵数据进行操作的张量（tensor ）库，在机器学习和其他数学密集型应用有广泛应用。与Tensorflow的静态计算图不同，pytorch的计算图是动态的，可以根据计算需要实时改变计算图。但由于Torch语言采用 Lua，导致在国内一直很小众，并逐渐被支持 Python 的 Tensorflow 抢走用户。作为经典机器学习库 Torch 的端口，PyTorch 为 Python 语言使用者提供了舒适的写代码选择。</p>
<h2 id="12-2-2-为什么选择-Pytorch？"><a href="#12-2-2-为什么选择-Pytorch？" class="headerlink" title="12.2.2 为什么选择 Pytorch？"></a>12.2.2 为什么选择 Pytorch？</h2><h3 id="1-简洁："><a href="#1-简洁：" class="headerlink" title="1.简洁："></a>1.简洁：</h3><p>&emsp;&emsp;PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像 TensorFlow 中充斥着session、graph、operation、name_scope、variable、tensor、layer等全新的概念，PyTorch 的设计遵循tensor→variable(autograd)→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。<br>简洁的设计带来的另外一个好处就是代码易于理解。PyTorch的源码只有TensorFlow的十分之一左右，更少的抽象、更直观的设计使得PyTorch的源码十分易于阅读。</p>
<h3 id="2-速度："><a href="#2-速度：" class="headerlink" title="2.速度："></a>2.速度：</h3><p>&emsp;&emsp;PyTorch 的灵活性不以速度为代价，在许多评测中，PyTorch 的速度表现胜过 TensorFlow和Keras 等框架。框架的运行速度和程序员的编码水平有极大关系，但同样的算法，使用PyTorch实现的那个更有可能快过用其他框架实现的。</p>
<h3 id="3-易用："><a href="#3-易用：" class="headerlink" title="3.易用："></a>3.易用：</h3><p>&emsp;&emsp;PyTorch 是所有的框架中面向对象设计的最优雅的一个。PyTorch的面向对象的接口设计来源于Torch，而Torch的接口设计以灵活易用而著称，Keras作者最初就是受Torch的启发才开发了Keras。PyTorch继承了Torch的衣钵，尤其是API的设计和模块的接口都与Torch高度一致。PyTorch的设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法，即所思即所得，不需要考虑太多关于框架本身的束缚。</p>
<h3 id="4-活跃的社区："><a href="#4-活跃的社区：" class="headerlink" title="4.活跃的社区："></a>4.活跃的社区：</h3><p>&emsp;&emsp;PyTorch 提供了完整的文档，循序渐进的指南，作者亲自维护的论坛 供用户交流和求教问题。Facebook 人工智能研究院对 PyTorch 提供了强力支持，作为当今排名前三的深度学习研究机构，FAIR的支持足以确保PyTorch获得持续的开发更新，不至于像许多由个人开发的框架那样昙花一现。</p>
<h2 id="12-2-3-PyTorch-的架构是怎样的？"><a href="#12-2-3-PyTorch-的架构是怎样的？" class="headerlink" title="12.2.3 PyTorch 的架构是怎样的？"></a>12.2.3 PyTorch 的架构是怎样的？</h2><p>&emsp;&emsp;PyTorch(Caffe2) 通过混合前端，分布式训练以及工具和库生态系统实现快速，灵活的实验和高效生产。PyTorch 和 TensorFlow 具有不同计算图实现形式，TensorFlow 采用静态图机制(预定义后再使用)，PyTorch采用动态图机制(运行时动态定义)。PyTorch 具有以下高级特征：</p>
<p>&emsp;&emsp;混合前端:新的混合前端在急切模式下提供易用性和灵活性，同时无缝转换到图形模式，以便在C ++运行时环境中实现速度，优化和功能。<br>&emsp;&emsp;分布式训练:通过利用本地支持集合操作的异步执行和可从Python和C ++访问的对等通信，优化了性能。<br>&emsp;&emsp;Python优先: PyTorch为了深入集成到Python中而构建的，因此它可以与流行的库和Cython和Numba等软件包一起使用。<br>&emsp;&emsp;丰富的工具和库:活跃的研究人员和开发人员社区建立了丰富的工具和库生态系统，用于扩展PyTorch并支持从计算机视觉到强化学习等领域的开发。<br>&emsp;&emsp;本机ONNX支持:以标准ONNX（开放式神经网络交换）格式导出模型，以便直接访问与ONNX兼容的平台，运行时，可视化工具等。<br>&emsp;&emsp;C++前端：C++前端是PyTorch的纯C++接口，它遵循已建立的Python前端的设计和体系结构。它旨在实现高性能，低延迟和裸机C++应用程序的研究。<br>使用GPU和CPU优化的深度学习张量库。</p>
<h2 id="12-2-4-Pytorch-与-tensorflow-之间的差异在哪里？"><a href="#12-2-4-Pytorch-与-tensorflow-之间的差异在哪里？" class="headerlink" title="12.2.4 Pytorch 与 tensorflow 之间的差异在哪里？"></a>12.2.4 Pytorch 与 tensorflow 之间的差异在哪里？</h2><p>&emsp;&emsp;上面也将了PyTorch 最大优势是建立的神经网络是动态的, 对比静态的 Tensorflow, 它能更有效地处理一些问题, 比如说 RNN 变化时间长度的输出。各有各的优势和劣势。两者都是大公司发布的, Tensorflow（Google）宣称在分布式训练上下了很大的功夫, 那就默认 Tensorflow 在分布式训练上要超出 Pytorch（Facebook），还有tensorboard可视化工具, 但是 Tensorflow 的静态计算图使得在 RNN 上有一点点被动 (虽然它用其他途径解决了), 不过用 PyTorch 的时候, 会对这种动态的 RNN 有更好的理解。而且 Tensorflow 的高度工业化, 它的底层代码很难看懂， Pytorch 好那么一点点, 如果深入 PytorchAPI, 至少能比看 Tensorflow 多看懂一点点 Pytorch 的底层在干啥。</p>
<h2 id="12-2-5-Pytorch有哪些常用工具包？"><a href="#12-2-5-Pytorch有哪些常用工具包？" class="headerlink" title="12.2.5 Pytorch有哪些常用工具包？"></a>12.2.5 Pytorch有哪些常用工具包？</h2><p>&emsp;&emsp;torch ：类似 NumPy 的张量库，强 GPU 支持 ；<br>&emsp;&emsp;torch.autograd ：基于 tape 的自动区别库，支持 torch 之中的所有可区分张量运行；<br>&emsp;&emsp;torch.nn ：为最大化灵活性未涉及、与 autograd 深度整合的神经网络库；<br>&emsp;&emsp;torch.optim：与 torch.nn 一起使用的优化包，包含 SGD、RMSProp、LBFGS、Adam 等标准优化方式；<br>&emsp;&emsp;torch.multiprocessing： python 多进程并发，进程之间 torch Tensors 的内存共享；<br>&emsp;&emsp;torch.utils：数据载入器。具有训练器和其他便利功能；<br>&emsp;&emsp;torch.legacy(.nn/.optim) ：处于向后兼容性考虑，从 Torch 移植来的 legacy 代码；</p>
<h1 id="12-3-Caffe"><a href="#12-3-Caffe" class="headerlink" title="12.3 Caffe"></a>12.3 Caffe</h1><h2 id="12-3-1-什么是-Caffe？"><a href="#12-3-1-什么是-Caffe？" class="headerlink" title="12.3.1 什么是 Caffe？"></a>12.3.1 什么是 Caffe？</h2><p>&emsp;&emsp;Caffe的全称应该是Convolutional Architecture for Fast Feature Embedding，它是一个清晰、高效的深度学习框架，它是开源的，核心语言是C++，它支持命令行、Python和Matlab接口，它既可以在CPU上运行也可以在GPU上运行。它的license是BSD 2-Clause。</p>
<h2 id="12-3-2-Caffe的特点是什么？"><a href="#12-3-2-Caffe的特点是什么？" class="headerlink" title="12.3.2 Caffe的特点是什么？"></a>12.3.2 Caffe的特点是什么？</h2><p>(1)、模块化：Caffe从一开始就设计得尽可能模块化，允许对新数据格式、网络层和损失函数进行扩展。</p>
<p>(2)、表示和实现分离：Caffe的模型(model)定义是用Protocol Buffer语言写进配置文件的。以任意有向无环图的形式，Caffe支持网络架构。Caffe会根据网络的需要来正确占用内存。通过一个函数调用，实现CPU和GPU之间的切换。</p>
<p>(3)、测试覆盖：在Caffe中，每一个单一的模块都对应一个测试。</p>
<p>(4)、python和Matlab接口：同时提供Python和Matlab接口。</p>
<p>(5)、预训练参考模型：针对视觉项目，Caffe提供了一些参考模型，这些模型仅应用在学术和非商业领域，它们的license不是BSD。</p>
<h2 id="12-3-3-Caffe的设计思想是怎样的？"><a href="#12-3-3-Caffe的设计思想是怎样的？" class="headerlink" title="12.3.3 Caffe的设计思想是怎样的？"></a>12.3.3 Caffe的设计思想是怎样的？</h2><p>&emsp;&emsp;基本上，Caffe 沿用了神经网络的一个简单假设——所有的计算都是以layer的形式表示的，layer做的事情就是take一些数据，然后输出一些计算以后的结果，比如说卷积，就是输入一个图像，然后和这一层的参数（filter）做卷积，然后输出卷积的结果。每一个layer需要做两个计算：forward是从输入计算输出，然后backward是从上面给的gradient来计算相对于输入的gradient，只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label），在training的时候，我们可以根据已有的label来计算loss和gradient，然后用gradient来update网络的参数，这个就是Caffe的一个基本流程。</p>
<p>&emsp;&emsp;基本上，最简单地用Caffe上手的方法就是先把数据写成Caffe的格式，然后设计一个网络，然后用Caffe提供的solver来做优化看效果如何，如果你的数据是图像的话，可以从现有的网络，比如说alexnet或者googlenet开始，然后做fine tuning，如果你的数据稍有不同，比如说是直接的float vector，你可能需要做一些custom的configuration，Caffe的logistic regression example兴许会很有帮助。</p>
<p>&emsp;&emsp;Fine tune方法：fine tuning的想法就是说，在imagenet那么大的数据集上train好一个很牛的网络了，那别的task上肯定也不错，所以我们可以把pretrain的网络拿过来，然后只重新train最后几层，重新train的意思是说，比如我以前需要classify imagenet的一千类，现在我只想识别是狗还是猫，或者是不是车牌，于是我就可以把最后一层softmax从一个4096<em>1000的分类器变成一个4096</em>2的分类器，这个strategy在应用中非常好使，所以我们经常会先在imagenet上pretrain一个网络，因为我们知道imagenet上training的大概过程会怎么样。</p>
<h2 id="12-3-4-Caffe架构是怎样的？"><a href="#12-3-4-Caffe架构是怎样的？" class="headerlink" title="12.3.4 Caffe架构是怎样的？"></a>12.3.4 Caffe架构是怎样的？</h2><p>&emsp;&emsp;Caffe的架构与其它的深度学习框架稍微不同，它没有根据算法实现过程的方式来进行编码，而是以系统级的抽象作为整体架构，逐层的封装实现细节，使得上层的架构变得很清晰。Caffe的整体架构如下：</p>
<h3 id="1-SyncedMem"><a href="#1-SyncedMem" class="headerlink" title="1. SyncedMem"></a>1. SyncedMem</h3><p>&emsp;&emsp;这个类的主要功能是封装CPU和GPU的数据交互操作。一般来说，数据的流动形式都是：硬盘-&gt;CPU内存-&gt;GPU内存-&gt;CPU内存-&gt;（硬盘），所以在写代码的过程中经常会写CPU/GPU之间数据传输的代码，同时还要维护CPU和GPU两个处理端的内存指针。这些事情处理起来不会很难，但是会很繁琐。因此SyncedMem的出现就是把CPU/GPU的数据传输操作封装起来，只需要调用简单的接口就可以获得两个处理端同步后的数据。</p>
<h3 id="2-Blob"><a href="#2-Blob" class="headerlink" title="2. Blob"></a>2. Blob</h3><p>&emsp;&emsp;Blob是用于存储数据的对象，在Caffe中各种数据(图像输入、模型参数)都是以Blob的形式在网络中传输的，Blob提供统一的存储操作接口，可用来保存训练数据、模型参数等，同时Blob还能在CPU和GPU之间进行同步以支持CPU/GPU的混合运算。<br>&emsp;&emsp;这个类做了两个封装：一个是操作数据的封装，使用Blob可以操纵高维的数据，快速访问其中的数据，变换数据的维度等；另一个是对原始数据和更新量的封装，每一个Blob中都有data和diff两个数据指针，data用于存储原始数据，diff 用于存储反向传播（Backpropagation）的梯度更新值。Blob使用了SyncedMem，这样便于访问不同的处理端。Blob基本实现了整个Caffe数据结构部分的封装，在Net类中可以看到所有的前后向数据和参数都用Blob来表示就足够了。数据的抽象到这个就可以了，接下来作层级的抽象。神经网络的前后向计算可以做到层与层之间完全独立，只要每个层按照一定的接口规则实现，就可以确保整个网络的正确性。</p>
<h3 id="3-Layer"><a href="#3-Layer" class="headerlink" title="3. Layer"></a>3. Layer</h3><p>&emsp;&emsp;Layer是网络Net的基本单元，也是Caffe中能在外部进行调整的最小网络结构单元，每个Layer都有输入Blob和输出Blob。Layer（层）是Caffe中最庞大最繁杂的模块，它是神经网络的基本计算单元。由于Caffe强调模块化设计，因此只允许每个layer完成一类特定的计算，例如convolution操作、pooling、非线性变换、内积运算，以及数据加载、归一化和损失计算等。Caffe中layer的种类有很多，具体的种类及功能请看官方文档。在创建一个Caffe模型的时候，也是以Layer为基础进行的。Layer是一个父类，它的下面还有各种实现特定功能的子类，例如data_layer，conv_layer，loss_layer等。Layer是通过LayFactory来创建的。</p>
<h3 id="4-Net"><a href="#4-Net" class="headerlink" title="4. Net"></a>4. Net</h3><p>&emsp;&emsp;Net是一个完整的深度网络，包含输入层、隐藏层、输出层，在Caffe中一般是一个卷积神经网络(Convolution Neural Networ，CNN)。通过定义不同类型的Layer，并用Blob将不同的Layer连接起来，就能产生一个Net。Net将数据Blob和层Layer组合起来做进一步的封装，对外提供了初始化和前后传播的接口，使得整体看上去和一个层的功能类似，但内部的组合可以是多种多样的。值得一提的是，每一层的输入输出数据统一保存在Net中，同时每个层内的参数指针也保存在Net中，不同的层可以通过WeightShare共享相同的参数，因此可以通过配置来实现多个神经网络层之间共享参数的功能。一个Net由多个Layer组成。一个典型的网络从data layer（从磁盘中载入数据）出发到loss layer结束。</p>
<h3 id="5-Solver"><a href="#5-Solver" class="headerlink" title="5. Solver"></a>5. Solver</h3><p>&emsp;&emsp;有了Net就可以进行神经网络的前后向传播计算了，但是还缺少神经网络的训练和预测功能，Solver类进一步封装了训练和预测相关的一些功能。它还提供了两个接口：一个是更新参数的接口，继承Solver可以实现不同的参数更新方法，如Momentum，Nesterov，Adagrad等，因此可以使用不同的优化算法。另一个接口是训练过程中每一轮特定状态下的可注入的一些回调函数，在代码中这个回调点的直接使用者就是多GPU训练算法。Solver定义了针对Net网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义Solver能够实现不同的神经网络求解方式。阅读Solver的代码可以了解网络的求解优化过程。Solver是一个父类，它下面还有实现不同优化方法的子类，例如sgd_solver，adagrad_sovler等，Solver是通过SolverFactory来创建的。</p>
<h3 id="6-Proto"><a href="#6-Proto" class="headerlink" title="6. Proto"></a>6. Proto</h3><p>&emsp;&emsp;caffe.proto位于…/src/caffe/proto目录下，在这个文件夹下还有一个.pb.cc和一个.pb.h文件，这两个文件都是由caffe.proto编译而来的。 在caffe.proto中定义了很多结构化数据，包括：<br>BlobProto、Datum、FillerParameter、NetParameter、SolverParameter、SolverState、LayerParameter、ConcatParameter、ConvolutionParameter、DataParameter、DropoutParameter、HDF5DataParameter、HDF5OutputParameter、ImageDataParameter、InfogainLossParameter、InnerProductParameter、LRNParameter、MemoryDataParameter、PoolingParameter、PowerParameter、WindowDataParameter、V0LayerParameter。</p>
<h3 id="7-IO"><a href="#7-IO" class="headerlink" title="7. IO"></a>7. IO</h3><p>&emsp;&emsp;除了上面的东西之外，还需要输入数据和参数。DataReader和DataTransformer帮助准备输入数据，Filler对参数进行初始化，一些Snapshot方法可以对模型进行持久化。</p>
<h2 id="12-3-5-Caffe的有哪些接口？"><a href="#12-3-5-Caffe的有哪些接口？" class="headerlink" title="12.3.5 Caffe的有哪些接口？"></a>12.3.5 Caffe的有哪些接口？</h2><p>&emsp;&emsp;Caffe深度学习框架支持多种编程接口，包括命令行、Python和Matlab,下面将介绍如何使用这些接口。</p>
<h3 id="1-Caffe-Python接口"><a href="#1-Caffe-Python接口" class="headerlink" title="1. Caffe Python接口"></a>1. Caffe Python接口</h3><p>&emsp;&emsp;Caffe提供 Python 接口，即Pycaffe，具体实现在caffe、python文件夹内。在Python代码中import caffe，可以load models（导入模型）、forward and backward （前向、反向迭代）、handle IO（数据输入输出）、visualize networks（绘制net）和instrument model solving（自定义优化方法)。所有的模型数据、计算参数都是暴露在外、可供读写的。<br>&emsp;&emsp;(1)caffe.Net 是主要接口，负责导入数据、校验数据、计算模型。<br>&emsp;&emsp;(2)caffe.Classsifier 用于图像分类。<br>&emsp;&emsp;(3)caffe.Detector 用于图像检测。<br>&emsp;&emsp;(4)caffe.SGDSolver 是露在外的 solver 的接口。<br>&emsp;&emsp;(5)caffe.io 处理输入输出，数据预处理。<br>&emsp;&emsp;(6)caffe.draw 可视化 net 的结构。<br>&emsp;&emsp;(7)caffe blobs 以 numpy ndarrys 的形式表示，方便而且高效。</p>
<h3 id="2-Caffe-MATLAB接口"><a href="#2-Caffe-MATLAB接口" class="headerlink" title="2. Caffe MATLAB接口"></a>2. Caffe MATLAB接口</h3><p>&emsp;&emsp;MATLAB接口（Matcaffe）在 caffe/matlab 目录的 caffe 软件包。在 matcaffe 的基础上，可将Caffe整合到MATLAB代码中。<br>&emsp;&emsp;MATLAB接口包括：<br>&emsp;&emsp;(1)MATLAB 中创建多个网络结构。<br>&emsp;&emsp;(2)网络的前向传播（Forward）与反向传播（Backward）计算。<br>&emsp;&emsp;(3)网络中的任意一层以及参数的存取。<br>&emsp;&emsp;(4)网络参数保存至文件或从文件夹加载。<br>&emsp;&emsp;(5)blob 和 network 形状调整。<br>&emsp;&emsp;(6)网络参数编辑和调整。<br>&emsp;&emsp;(7)创建多个 solvers 进行训练。<br>&emsp;&emsp;(8)从solver 快照（Snapshots）恢复并继续训练。<br>&emsp;&emsp;(9)访问训练网络（Train nets）和测试网络(Test nets)。<br>&emsp;&emsp;(10)迭代后网络交由 MATLAB 控制。<br>&emsp;&emsp;(11)MATLAB代码融合梯度算法。</p>
<h3 id="3-Caffe-命令行接口"><a href="#3-Caffe-命令行接口" class="headerlink" title="3. Caffe 命令行接口"></a>3. Caffe 命令行接口</h3><p>&emsp;&emsp;命令行接口 Cmdcaffe 是 Caffe 中用来训练模型、计算得分以及方法判断的工具。Cmdcaffe 存放在 caffe/build/tools 目录下。</p>
<h4 id="1-caffe-train"><a href="#1-caffe-train" class="headerlink" title="1. caffe train"></a>1. caffe train</h4><p>&emsp;&emsp;caffe train 命令用于模型学习，具体包括：<br>&emsp;&emsp;(1)caffe train 带 solver.prototxt 参数完成配置。<br>&emsp;&emsp;(2)caffe train 带 snapshot mode_iter_1000.solverstate 参数加载 solver snapshot。<br>&emsp;&emsp;(3)caffe train 带 weights 参数 model.caffemodel 完成 Fine-tuning 模型初始化。</p>
<h4 id="2-caffe-test"><a href="#2-caffe-test" class="headerlink" title="2. caffe test"></a>2. caffe test</h4><p>&emsp;&emsp;caffe test 命令用于测试运行模型的得分，并且用百分比表示网络输出的最终结果，比如 accuracyhuoloss 作为其结果。测试过程中，显示每个 batch 的得分，最后输出全部 batch 的平均得分值。</p>
<h4 id="3-caffe-time"><a href="#3-caffe-time" class="headerlink" title="3. caffe time"></a>3. caffe time</h4><p>&emsp;&emsp;caffe time 命令用来检测系统性能和测量模型相对执行时间，此命令通过逐层计时与同步，执行模型检测。</p>
<p>参考文献：<br>1.深度学习：Caffe之经典模型讲解与实战/ 乐毅，王斌</p>
<h3 id="10-4-网络搭建有什么原则？"><a href="#10-4-网络搭建有什么原则？" class="headerlink" title="10.4 网络搭建有什么原则？"></a>10.4 网络搭建有什么原则？</h3><h3 id="10-4-1新手原则。"><a href="#10-4-1新手原则。" class="headerlink" title="10.4.1新手原则。"></a>10.4.1新手原则。</h3><p>刚入门的新手不建议直接上来就开始搭建网络模型。比较建议的学习顺序如下：</p>
<ul>
<li>1.了解神经网络工作原理，熟悉基本概念及术语。</li>
<li>2.阅读经典网络模型论文+实现源码(深度学习框架视自己情况而定)。</li>
<li>3.找数据集动手跑一个网络，可以尝试更改已有的网络模型结构。</li>
<li>4.根据自己的项目需要设计网络。</li>
</ul>
<h3 id="10-4-2深度优先原则。"><a href="#10-4-2深度优先原则。" class="headerlink" title="10.4.2深度优先原则。"></a>10.4.2深度优先原则。</h3><p>通常增加网络深度可以提高准确率，但同时会牺牲一些速度和内存。但深度不是盲目堆起来的，一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率，如果浅层都学不到东西，深了也没效果。</p>
<h3 id="10-4-3卷积核size一般为奇数。"><a href="#10-4-3卷积核size一般为奇数。" class="headerlink" title="10.4.3卷积核size一般为奇数。"></a>10.4.3卷积核size一般为奇数。</h3><p>卷积核为奇数有以下好处：</p>
<ul>
<li>1 保证锚点刚好在中间，方便以 central pixel为标准进行滑动卷积，避免了位置信息发生偏移 。</li>
<li>2 保证在填充（Padding）时，在图像之间添加额外的零层，图像的两边仍然对称。</li>
</ul>
<h3 id="10-4-4卷积核不是越大越好。"><a href="#10-4-4卷积核不是越大越好。" class="headerlink" title="10.4.4卷积核不是越大越好。"></a>10.4.4卷积核不是越大越好。</h3><p>AlexNet中用到了一些非常大的卷积核，比如11×11、5×5卷积核，之前人们的观念是，卷积核越大，感受野越大，看到的图片信息越多，因此获得的特征越好。但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。于是在VGG、Inception网络中，利用2个3×3卷积核的组合比1个5×5卷积核的效果更佳，同时参数量（3×3×2+1=19&lt;26=5×5×1+1）被降低，因此后来3×3卷积核被广泛应用在各种模型中。</p>
<h2 id="10-5-有哪些经典的网络模型值得我们去学习的？"><a href="#10-5-有哪些经典的网络模型值得我们去学习的？" class="headerlink" title="10.5 有哪些经典的网络模型值得我们去学习的？"></a>10.5 有哪些经典的网络模型值得我们去学习的？</h2><p>提起经典的网络模型就不得不提起计算机视觉领域的经典比赛：ILSVRC .其全称是 ImageNet Large Scale Visual Recognition Challenge.正是因为ILSVRC 2012挑战赛上的AlexNet横空出世，使得全球范围内掀起了一波深度学习热潮。这一年也被称作“深度学习元年”。而在历年ILSVRC比赛中每次刷新比赛记录的那些神经网络也成为了人们心中的经典，成为学术界与工业届竞相学习与复现的对象，并在此基础上展开新的研究。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>年份</th>
<th>网络名称</th>
<th>获得荣誉</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2012</td>
<td>AlexNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>2</td>
<td>2014</td>
<td>VGGNet</td>
<td>ILSVRC图像分类亚军</td>
</tr>
<tr>
<td>3</td>
<td>2014</td>
<td>GoogLeNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>4</td>
<td>2015</td>
<td>ResNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>5</td>
<td>2017</td>
<td>SeNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li><p>1 AlexNet<br>论文:<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/alexnet.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.第一次使用非线性激活函数ReLU。</li>
<li>2.增加防加过拟合方法：Droupout层,提升了模型鲁棒性。</li>
<li>3.首次使用数据增强。  </li>
<li>4.首次使用GPU加速运算。</li>
</ul>
</blockquote>
</li>
<li><p>2 VGGNet<br>论文:<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/vgg.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.网络结构更深。</li>
<li>2.普遍使用小卷积核。</li>
</ul>
</blockquote>
</li>
<li><p>3 GoogLeNet<br>论文:<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/inception_v1.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.增强卷积模块功能。<br>主要的创新在于他的Inception，这是一种网中网（Network In Network）的结构，即原来的结点也是一个网络。Inception一直在不断发展，目前已经V2、V3、V4。其中1*1卷积主要用来降维，用了Inception之后整个网络结构的宽度和深度都可扩大，能够带来2-3倍的性能提升。</li>
<li>2.连续小卷积代替大卷积，保证感受野不变的同时，减少了参数数目。</li>
</ul>
</blockquote>
</li>
<li>4 ResNet<br>论文:<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/inception_v1.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点:<blockquote>
<p>解决了“退化”问题，即当模型的层次加深时，错误率却提高了。</p>
</blockquote>
</li>
<li>5 SeNet<br>论文:<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a><br>代码实现:<a href="https://github.com/ry/tensorflow-resnet" target="_blank" rel="noopener">tensorflow</a><br>主要特点:<blockquote>
<p>提出了feature recalibration，通过引入 attention 重新加权，可以得到抑制无效特征，提升有效特征的权重，并很容易地和现有网络结合，提升现有网络性能，而计算量不会增加太多。</p>
</blockquote>
</li>
</ul>
</blockquote>
<p><strong>CV领域网络结构演进历程：</strong><br><img src="/img/ch12/网络结构演进.png" alt="CV领域网络结构演进历程"></p>
<p><strong>ILSVRC挑战赛历年冠军:</strong><br><img src="/img/ch12/历年冠军.png" alt="ILSVRC挑战赛历年冠军"></p>
<p>此后，ILSVRC挑战赛的名次一直是衡量一个研究机构或企业技术水平的重要标尺。<br>ILSVRC 2017 已是最后一届举办.2018年起，将由WebVision竞赛（Challenge on Visual Understanding by Learning from Web Data）来接棒。因此，即使ILSVRC挑战赛停办了，但其对深度学习的深远影响和巨大贡献，将永载史册。</p>
<h2 id="10-6-网络训练有哪些技巧吗？"><a href="#10-6-网络训练有哪些技巧吗？" class="headerlink" title="10.6 网络训练有哪些技巧吗？"></a>10.6 网络训练有哪些技巧吗？</h2><h3 id="10-6-1-合适的数据集。"><a href="#10-6-1-合适的数据集。" class="headerlink" title="10.6.1.合适的数据集。"></a>10.6.1.合适的数据集。</h3><ul>
<li>1 没有明显脏数据(可以极大避免Loss输出为NaN)。</li>
<li>2 样本数据分布均匀。</li>
</ul>
<h3 id="10-6-2-合适的预处理方法。"><a href="#10-6-2-合适的预处理方法。" class="headerlink" title="10.6.2.合适的预处理方法。"></a>10.6.2.合适的预处理方法。</h3><p>关于数据预处理，在Batch Normalization未出现之前预处理的主要做法是减去均值，然后除去方差。在Batch Normalization出现之后，减均值除方差的做法已经没有必要了。对应的预处理方法主要是数据筛查、数据增强等。</p>
<h3 id="10-6-3-网络的初始化。"><a href="#10-6-3-网络的初始化。" class="headerlink" title="10.6.3.网络的初始化。"></a>10.6.3.网络的初始化。</h3><p>网络初始化最粗暴的做法是参数赋值为全0，这是绝对不可取的。因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的，这可能会直接导致模型失效，无法收敛。吴恩达视频中介绍的方法是将网络权重初始化均值为0、方差为1符合的正态分布的随机数据。</p>
<h3 id="10-6-4-小规模数据试练。"><a href="#10-6-4-小规模数据试练。" class="headerlink" title="10.6.4.小规模数据试练。"></a>10.6.4.小规模数据试练。</h3><p>在正式开始训练之前，可以先用小规模数据进行试练。原因如下：</p>
<ul>
<li>1 可以验证自己的训练流程对否。</li>
<li>2 可以观察收敛速度，帮助调整学习速率。</li>
<li>3 查看GPU显存占用情况，最大化batch_size(前提是进行了batch normalization，只要显卡不爆，尽量挑大的)。</li>
</ul>
<h3 id="10-6-5-设置合理Learning-Rate。"><a href="#10-6-5-设置合理Learning-Rate。" class="headerlink" title="10.6.5.设置合理Learning Rate。"></a>10.6.5.设置合理Learning Rate。</h3><ul>
<li>1 太大。Loss爆炸、输出NaN等。</li>
<li>2 太小。收敛速度过慢，训练时长大大延长。</li>
<li>3 可变的学习速率。比如当输出准确率到达某个阈值后，可以让Learning Rate减半继续训练。</li>
</ul>
<h3 id="10-6-6-损失函数"><a href="#10-6-6-损失函数" class="headerlink" title="10.6.6.损失函数"></a>10.6.6.损失函数</h3><p>损失函数主要分为两大类:分类损失和回归损失</p>
<blockquote>
<p>1.回归损失：</p>
<blockquote>
<ul>
<li>1 均方误差(MSE 二次损失 L2损失)<br>它是我们的目标变量与预测值变量差值平方。</li>
<li>2 平均绝对误差(MAE L1损失)<br>它是我们的目标变量与预测值变量差值绝对值。<br>关于MSE与MAE的比较。MSE更容易解决问题，但是MAE对于异常值更加鲁棒。更多关于MAE和MSE的性能，可以参考<a href="https://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/" target="_blank" rel="noopener">L1vs.L2 Loss Function</a></li>
</ul>
</blockquote>
<p>2.分类损失：</p>
<blockquote>
<ul>
<li>1 交叉熵损失函数。<br>是目前神经网络中最常用的分类目标损失函数。</li>
<li>2 合页损失函数<br>合页损失函数广泛在支持向量机中使用，有时也会在损失函数中使用。缺点:合页损失函数是对错误越大的样本施以更严重的惩罚，但是这样会导致损失函数对噪声敏感。</li>
</ul>
</blockquote>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">第十一章_迁移学习</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:29:59 / 修改时间：12:30:46" itemprop="dateCreated datePublished" datetime="2020-03-03T12:29:59+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第十一章-迁移学习"><a href="#第十一章-迁移学习" class="headerlink" title="第十一章 迁移学习"></a>第十一章 迁移学习</h1><p>​    本章主要简明地介绍了迁移学习的基本概念、迁移学习的必要性、研究领域和基本方法。重点介绍了几大类常用的迁移学习方法：数据分布自适应方法、特征选择方法、子空间学习方法、以及目前最热门的深度迁移学习方法。除此之外，我们也结合最近的一些研究成果对未来迁移学习进行了一些展望。并提供了一些迁移学习领域的常用学习资源，以方便感兴趣的读者快速开始学习。</p>
<h2 id="11-1-迁移学习基础知识"><a href="#11-1-迁移学习基础知识" class="headerlink" title="11.1 迁移学习基础知识"></a>11.1 迁移学习基础知识</h2><h3 id="11-1-1-什么是迁移学习？"><a href="#11-1-1-什么是迁移学习？" class="headerlink" title="11.1.1 什么是迁移学习？"></a>11.1.1 什么是迁移学习？</h3><p>找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。</p>
<h3 id="11-1-2-为什么需要迁移学习？"><a href="#11-1-2-为什么需要迁移学习？" class="headerlink" title="11.1.2 为什么需要迁移学习？"></a>11.1.2 为什么需要迁移学习？</h3><ol>
<li><strong>大数据与少标注的矛盾</strong>：虽然有大量的数据，但往往都是没有标注的，无法训练机器学习模型。人工进行数据标定太耗时。</li>
<li><strong>大数据与弱计算的矛盾</strong>：普通人无法拥有庞大的数据量与计算资源。因此需要借助于模型的迁移。</li>
<li><strong>普适化模型与个性化需求的矛盾</strong>：即使是在同一个任务上，一个模型也往往难以满足每个人的个性化需求，比如特定的隐私设置。这就需要在不同人之间做模型的适配。</li>
<li><strong>特定应用（如冷启动）的需求</strong>。</li>
</ol>
<h3 id="11-1-3-迁移学习的基本问题有哪些？"><a href="#11-1-3-迁移学习的基本问题有哪些？" class="headerlink" title="11.1.3 迁移学习的基本问题有哪些？"></a>11.1.3 迁移学习的基本问题有哪些？</h3><p>基本问题主要有3个：</p>
<ul>
<li><strong>How to transfer</strong>： 如何进行迁移学习？（设计迁移方法）</li>
<li><strong>What to transfer</strong>： 给定一个目标领域，如何找到相对应的源领域，然后进行迁移？（源领域选择）</li>
<li><strong>When to transfer</strong>： 什么时候可以进行迁移，什么时候不可以？（避免负迁移）</li>
</ul>
<h3 id="11-1-4-迁移学习有哪些常用概念？"><a href="#11-1-4-迁移学习有哪些常用概念？" class="headerlink" title="11.1.4 迁移学习有哪些常用概念？"></a>11.1.4 迁移学习有哪些常用概念？</h3><ul>
<li>基本定义<ul>
<li><strong>域(Domain)</strong>：数据特征和特征分布组成，是学习的主体<ul>
<li><strong>源域 (Source domain)</strong>：已有知识的域</li>
<li><strong>目标域 (Target domain)</strong>：要进行学习的域</li>
</ul>
</li>
<li><strong>任务 (Task)</strong>：由目标函数和学习结果组成，是学习的结果</li>
</ul>
</li>
<li>按特征空间分类<ul>
<li><strong>同构迁移学习（Homogeneous TL）</strong>： 源域和目标域的特征空间相同，$D_s=D_t$</li>
<li><strong>异构迁移学习（Heterogeneous TL）</strong>：源域和目标域的特征空间不同，$D_s\ne D_t$</li>
</ul>
</li>
<li>按迁移情景分类<ul>
<li><strong>归纳式迁移学习（Inductive TL）</strong>：源域和目标域的学习任务不同</li>
<li><strong>直推式迁移学习（Transductive TL)</strong>：源域和目标域不同，学习任务相同</li>
<li><strong>无监督迁移学习（Unsupervised TL)</strong>：源域和目标域均没有标签</li>
</ul>
</li>
<li>按迁移方法分类<ul>
<li><strong>基于实例的迁移 (Instance based TL)</strong>：通过权重重用源域和目标域的样例进行迁移</li>
<li><strong>基于特征的迁移 (Feature based TL)</strong>：将源域和目标域的特征变换到相同空间</li>
<li><strong>基于模型的迁移 (Parameter based TL)</strong>：利用源域和目标域的参数共享模型</li>
<li><strong>基于关系的迁移 (Relation based TL)</strong>：利用源域中的逻辑网络关系进行迁移</li>
</ul>
</li>
</ul>
<p><img src="/img/ch11/1542972502781.png" alt="1542972502781"></p>
<p><img src="/img/ch11/1542974131814.png" alt="1542974131814"></p>
<h3 id="11-1-5-迁移学习与传统机器学习有什么区别？"><a href="#11-1-5-迁移学习与传统机器学习有什么区别？" class="headerlink" title="11.1.5 迁移学习与传统机器学习有什么区别？"></a>11.1.5 迁移学习与传统机器学习有什么区别？</h3><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>迁移学习</th>
<th>传统机器学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据分布</td>
<td>训练和测试数据不需要同分布</td>
<td>训练和测试数据同分布</td>
</tr>
<tr>
<td>数据标签</td>
<td>不需要足够的数据标注</td>
<td>足够的数据标注</td>
</tr>
<tr>
<td>建模</td>
<td>可以重用之前的模型</td>
<td>每个任务分别建模</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/img/ch11/1542973960796.png" alt="1542973960796"></p>
<h3 id="11-1-6-迁移学习的核心及度量准则？"><a href="#11-1-6-迁移学习的核心及度量准则？" class="headerlink" title="11.1.6 迁移学习的核心及度量准则？"></a>11.1.6 迁移学习的核心及度量准则？</h3><p><strong>迁移学习的总体思路可以概括为</strong>：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。</p>
<p><strong>迁移学习的核心是</strong>：找到源领域和目标领域之间的相似性，并加以合理利用。这种相似性非常普遍。比如，不同人的身体构造是相似的；自行车和摩托车的骑行方式是相似的；国际象棋和中国象棋是相似的；羽毛球和网球的打球方式是相似的。这种相似性也可以理解为不变量。以不变应万变，才能立于不败之地。</p>
<p><strong>有了这种相似性后，下一步工作就是， 如何度量和利用这种相似性。</strong>度量工作的目标有两点：一是很好地度量两个领域的相似性，不仅定性地告诉我们它们是否相似，更定量地给出相似程度。二是以度量为准则，通过我们所要采用的学习手段，增大两个领域之间的相似性，从而完成迁移学习。</p>
<p><strong>一句话总结： 相似性是核心，度量准则是重要手段。</strong></p>
<h3 id="11-1-7-迁移学习与其他概念的区别？"><a href="#11-1-7-迁移学习与其他概念的区别？" class="headerlink" title="11.1.7 迁移学习与其他概念的区别？"></a>11.1.7 迁移学习与其他概念的区别？</h3><ol>
<li>迁移学习与多任务学习关系：<ul>
<li><strong>多任务学习</strong>：多个相关任务一起协同学习；</li>
<li><strong>迁移学习</strong>：强调信息复用，从一个领域(domain)迁移到另一个领域。</li>
</ul>
</li>
<li>迁移学习与领域自适应：<strong>领域自适应</strong>：使两个特征分布不一致的domain一致。</li>
<li>迁移学习与协方差漂移：<strong>协方差漂移</strong>：数据的条件概率分布发生变化。</li>
</ol>
<p>Reference： </p>
<ol>
<li><a href="https：//github.com/jindongwang/transferlearning-tutorial">王晋东，迁移学习简明手册</a></li>
<li>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175.</li>
<li>Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM.</li>
</ol>
<h3 id="11-1-8-什么是负迁移？产生负迁移的原因有哪些？"><a href="#11-1-8-什么是负迁移？产生负迁移的原因有哪些？" class="headerlink" title="11.1.8 什么是负迁移？产生负迁移的原因有哪些？"></a>11.1.8 什么是负迁移？产生负迁移的原因有哪些？</h3><p>负迁移(Negative Transfer)指的是，在源域上学习到的知识，对于目标域上的学习产生负面作用。</p>
<p>产生负迁移的原因主要有：</p>
<ul>
<li>数据问题：源域和目标域压根不相似，谈何迁移？</li>
<li>方法问题：源域和目标域是相似的，但是，迁移学习方法不够好，没找到可迁移的成分。</li>
</ul>
<p>负迁移给迁移学习的研究和应用带来了负面影响。在实际应用中，找到合理的相似性，并且选择或开发合理的迁移学习方法，能够避免负迁移现象。</p>
<h3 id="11-1-9-迁移学习的基本思路？"><a href="#11-1-9-迁移学习的基本思路？" class="headerlink" title="11.1.9 迁移学习的基本思路？"></a>11.1.9 迁移学习的基本思路？</h3><p>迁移学习的总体思路可以概括为：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。</p>
<ol>
<li>找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。</li>
<li>迁移学习，是指利用数据、任务、或模型之间的相似性，将在旧领域学习过的模型，应用于新领域的一种学习过程。</li>
<li>迁移学习<strong>最有用的场合</strong>是，如果你尝试优化任务B的性能，通常这个任务数据相对较少。<br>例如，在放射科中你知道很难收集很多射线扫描图来搭建一个性能良好的放射科诊断系统，所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用 1 百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务在放射科任务上做得更好，尽管任务没有这么多数据。</li>
<li>迁移学习什么时候是有意义的？它确实可以<strong>显著提高</strong>你的<strong>学习任务的性能</strong>，但我有时候也见过有些场合使用迁移学习时，任务实际上数据量比任务要少， 这种情况下增益可能不多。<blockquote>
<p>什么情况下可以使用迁移学习？</p>
<p>假如两个领域之间的区别特别的大，<strong>不可以直接采用迁移学习</strong>，因为在这种情况下效果不是很好。在这种情况下，推荐使用[3]的工作，在两个相似度很低的domain之间一步步迁移过去（踩着石头过河）。</p>
</blockquote>
</li>
</ol>
<blockquote>
<ol>
<li>迁移学习主要解决方案有哪些？</li>
<li>除直接看infer的结果的Accurancy以外，如何衡量迁移学习学习效果？</li>
<li>对抗网络是如何进行迁移的？</li>
</ol>
</blockquote>
<p>Reference： </p>
<ol>
<li><a href="https：//github.com/jindongwang/transferlearning-tutorial">王晋东，迁移学习简明手册</a></li>
<li>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175.</li>
<li>Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM.</li>
</ol>
<h2 id="11-2-迁移学习的基本思路有哪些？"><a href="#11-2-迁移学习的基本思路有哪些？" class="headerlink" title="11.2 迁移学习的基本思路有哪些？"></a>11.2 迁移学习的基本思路有哪些？</h2><p>​    迁移学习的基本方法可以分为四种。这四种基本的方法分别是：基于样本的迁移， 基于模型 的迁移， 基于特征的迁移，及基于关系的迁移。</p>
<h3 id="11-2-1-基于样本迁移"><a href="#11-2-1-基于样本迁移" class="headerlink" title="11.2.1 基于样本迁移"></a>11.2.1 基于样本迁移</h3><p>​    基于样本的迁移学习方法 (Instance based Transfer Learning) 根据一定的权重生成规则，对数据样本进行重用，来进行迁移学习。图<a href="#bookmark90">14</a>形象地表示了基于样本迁移方法的思想源域中存在不同种类的动物，如狗、鸟、猫等，目标域只有狗这一种类别。在迁移时，为了最大限度地和目标域相似，我们可以人为地提高源域中属于狗这个类别的样本权重。</p>
<p><img src="./media/631e5aab4e0680c374793804817bfbb6.jpg" alt=""></p>
<p><center>图 14: 基于样本的迁移学习方法示意图</p>
<p>​    在迁移学习中，对于源域D~s~和目标域D~t~，通常假定产生它们的概率分布是不同且未知的(P(X~s~) =P(X~t~))。另外，由于实例的维度和数量通常都非常大，因此，直接对 P(X~s~) 和P(X~t~) 进行估计是不可行的。因而，大量的研究工作 [<a href="#bookmark267">Khan and Heisterkamp,2016</a>, <a href="#bookmark319">Zadrozny, 2004</a>, <a href="#bookmark242">Cortes et al.,2008</a>, <a href="#bookmark243">Dai et al., 2007</a>, <a href="#bookmark302">Tan et al.,2015</a>, <a href="#bookmark303">Tan et al., 2017</a>] 着眼于对源域和目标域的分布比值进行估计(P(<strong>X</strong>t)/P(<strong>X</strong>s))。所估计得到的比值即为样本的权重。这些方法通常都假设P(<strong>x</strong>s) \&lt;并且源域和目标域的条件概率分布相同(P(y|x~s~)=<em>P</em>(y|x~t~))。特别地，上海交通大学Dai等人[<a href="#bookmark243">Dai et al.,2007</a>]提出了 TrAdaboost方法，将AdaBoost的思想应用于迁移学习中，提高有利于目标分类任务的实例权重、降低不利于目标分类任务的实例权重，并基于PAC理论推导了模型的泛化误差上界。TrAdaBoost方法是此方面的经典研究之一。文献 [<a href="#bookmark264">Huang et al., 2007</a>]提出核均值匹配方法 (Kernel Mean atching, KMM)对于概率分布进行估计，目标是使得加权后的源域和目标域的概率分布尽可能相近。在最新的研究成果中，香港科技大学的Tan等人扩展了实例迁移学习方法的应用场景，提出 了传递迁移学习方法(Transitive Transfer Learning, TTL) [<a href="#bookmark302">Tan etal., 2015</a>] 和远域迁移学习 (Distant Domain Transfer Learning,DDTL) [<a href="#bookmark303">Tan et al., 2017</a>]，利用联合矩阵分解和深度神经网络，将迁移学习应用于多个不相似的领域之间的知识共享，取得了良好的效果。</p>
<p>​    虽然实例权重法具有较好的理论支撑、容易推导泛化误差上界，但这类方法通常只在领域间分布差异较小时有效，因此对自然语言处理、计算机视觉等任务效果并不理想。而基于特征表示的迁移学习方法效果更好,是我们研究的重点。</p>
<h3 id="11-2-2-基于特征迁移"><a href="#11-2-2-基于特征迁移" class="headerlink" title="11.2.2 基于特征迁移"></a>11.2.2 基于特征迁移</h3><p>​    基于特征的迁移方法 (Feature based Transfer Learning) 是指将通过特征变换的方式互相迁移 [<a href="#bookmark272">Liu et al., 2011</a>, <a href="#bookmark327">Zheng et al.,2008</a>, <a href="#bookmark263">Hu and Yang, 2011</a>],来减少源域和目标域之间的差距；或者将源域和目标域的数据特征变换到统一特征空间中 [<a href="#bookmark288">Pan et al.,2011</a>, <a href="#bookmark278">Long et al., 2014b</a>, <a href="#bookmark248">Duan et al.,2012</a>],然后利用传统的机器学习方法进行分类识别。根据特征的同构和异构性,又可以分为同构和异构迁移学习。图<a href="#bookmark93">15</a>很形象地表示了两种基于特 征的迁移学习方法。</p>
<p><img src="./media/fa08900e89bfd53cc28345d21bc6aca0.jpg" alt=""></p>
<p><center>图 15: 基于特征的迁移学习方法示意图</p>
<p>​    基于特征的迁移学习方法是迁移学习领域中最热门的研究方法,这类方法通常假设源域和目标域间有一些交叉的特征。香港科技大学的 Pan 等人 [<a href="#bookmark288">Pan et al.,2011</a>] 提出的迁移 成分分析方法 (Transfer Component Analysis, TCA)是其中较为典型的一个方法。该方法的 核心内容是以最大均值差异 (Maximum MeanDiscrepancy, MMD) [<a href="#bookmark236">Borgwardt et al., 2006</a>]作为度量准则,将不同数据领域中的分布差异最小化。加州大学伯克利分校的 Blitzer 等人 [<a href="#bookmark235">Blitzer et al., 2006</a>] 提出了一种基于结构对应的学习方法(Structural Corresponding Learning,SCL),该算法可以通过映射将一个空间中独有的一些特征变换到其他所有空间中的轴特征上,然后在该特征上使用机器学习的算法进行分类预测。清华大学龙明盛等人[<a href="#bookmark278">Long et al.,2014b</a>]提出在最小化分布距离的同时，加入实例选择的迁移联合匹配(Tran-fer Joint Matching, TJM) 方法,将实例和特征迁移学习方法进行了有机的结合。澳大利亚卧龙岗大学的 Jing Zhang 等人 [<a href="#bookmark321">Zhang et al., 2017a</a>]提出对于源域和目标域各自训练不同 的变换矩阵,从而达到迁移学习的目标。</p>
<h3 id="11-2-3-基于模型迁移"><a href="#11-2-3-基于模型迁移" class="headerlink" title="11.2.3 基于模型迁移"></a>11.2.3 基于模型迁移</h3><p>​    基于模型的迁移方法 (Parameter/Model based Transfer Learning) 是指从源域和目标域中找到他们之间共享的参数信息,以实现迁移的方法。这种迁移方式要求的假设条件是： 源域中的数据与目标域中的数据可以共享一些模型的参数。其中的代表性工作主要有［<a href="#bookmark324">Zhao et al., 2010</a>, <a href="#bookmark325">Zhao et al., 2011</a>, <a href="#bookmark287">Panet al., 2008b</a>, <a href="#bookmark286">Pan et al., 2008a</a>］。图<a href="#bookmark96">16</a>形象地 表示了基于模型的迁移学习方法的基本思想。</p>
<p><img src="./media/602723a1d3ce0f3abe7c591a8e4bb6ec.jpg" alt=""></p>
<p><center>图 16: 基于模型的迁移学习方法示意图</p>
<p>​    其中，中科院计算所的Zhao等人[<a href="#bookmark325">Zhao et al., 2011</a>]提出了TransEMDT方法。该方法首先针对已有标记的数据，利用决策树构建鲁棒性的行为识别模型，然后针对无标定数据，利用K-Means聚类方法寻找最优化的标定参数。西安邮电大学的Deng等人[<a href="#bookmark245">Deng et al.,2014</a>] 也用超限学习机做了类似的工作。香港科技大学的Pan等人[<a href="#bookmark286">Pan etal., 2008a</a>]利用HMM，针对Wifi室内定位在不同设备、不同时间和不同空间下动态变化的特点，进行不同分布下的室内定位研究。另一部分研究人员对支持向量机 SVM 进行了改进研究 [<a href="#bookmark285">Nater et al.,2011</a>, <a href="#bookmark269">Li et al., 2012</a>]。这些方法假定 SVM中的权重向量 <strong>w</strong> 可以分成两个部分： <strong>w</strong> = <strong>wo</strong>+<strong>v</strong>， 其中 <strong>w</strong>0代表源域和目标域的共享部分， <strong>v</strong> 代表了对于不同领域的特定处理。在最新的研究成果中，香港科技大学的 Wei 等人 [<a href="#bookmark313">Wei et al., 2016b</a>]将社交信息加入迁移学习方法的 正则项中，对方法进行了改进。清华大学龙明盛等人[<a href="#bookmark275">Long et al., 2015a</a>, <a href="#bookmark276">Long et al., 2016</a>, <a href="#bookmark280">Long etal., 2017</a>]改进了深度网络结构，通过在网络中加入概率分布适配层，进一步提高了深度迁移学习网络对于大数据的泛化能力。</p>
<h3 id="11-2-4-基于关系迁移"><a href="#11-2-4-基于关系迁移" class="headerlink" title="11.2.4 基于关系迁移"></a>11.2.4 基于关系迁移</h3><p>​    基于关系的迁移学习方法 (Relation Based Transfer Learning) 与上述三种方法具有截然不同的思路。这种方法比较关注源域和目标域的样本之间的关系。图<a href="#bookmark82">17</a>形象地表示了不 同领域之间相似的关系。</p>
<p>​    就目前来说，基于关系的迁移学习方法的相关研究工作非常少，仅有几篇连贯式的文章讨论： [<a href="#bookmark283">Mihalkova et al., 2007</a>, <a href="#bookmark284">Mihalkova and Mooney,2008</a>, <a href="#bookmark244">Davis and Domingos, 2009</a>]。这些文章都借助于马尔科夫逻辑网络(Markov Logic Net)来挖掘不同领域之间的关系相似性。</p>
<p>​    我们将重点讨论基于特征和基于模型的迁移学习方法，这也是目前绝大多数研究工作的热点。</p>
<p><img src="./media/aa10d36f758430dd4ff72d2bf6a76a6c.jpg" alt=""></p>
<p><center>图 17: 基于关系的迁移学习方法示意图</p>
<p><img src="./media/1542812440636.png" alt="1542812440636"></p>
<p><center>图 18: 基于马尔科夫逻辑网的关系迁移</p>
<h2 id="11-3-迁移学习的常用方法"><a href="#11-3-迁移学习的常用方法" class="headerlink" title="11.3 迁移学习的常用方法"></a>11.3 迁移学习的常用方法</h2><h3 id="11-3-1-数据分布自适应"><a href="#11-3-1-数据分布自适应" class="headerlink" title="11.3.1 数据分布自适应"></a>11.3.1 数据分布自适应</h3><p>​    数据分布自适应 (Distribution Adaptation) 是一类最常用的迁移学习方法。这种方法的基本思想是,由于源域和目标域的数据概率分布不同,那么最直接的方式就是通过一些变换,将不同的数据分布的距离拉近。</p>
<p>​    图 <a href="#bookmark84">19</a>形象地表示了几种数据分布的情况。简单来说，数据的边缘分布不同，就是数据整体不相似。数据的条件分布不同，就是数据整体相似，但是具体到每个类里，都不太相似。</p>
<p><img src="./media/1542812748062.png" alt="1542812748062"></p>
<p><center>图 19: 不同数据分布的目标域数据</p>
<p>​    根据数据分布的性质,这类方法又可以分为边缘分布自适应、条件分布自适应、以及联合分布自适应。下面我们分别介绍每类方法的基本原理和代表性研究工作。介绍每类研究工作时,我们首先给出基本思路,然后介绍该类方法的核心,最后结合最近的相关工作介绍该类方法的扩展。</p>
<h3 id="11-3-2-边缘分布自适应"><a href="#11-3-2-边缘分布自适应" class="headerlink" title="11.3.2 边缘分布自适应"></a>11.3.2 边缘分布自适应</h3><p>​    边缘分布自适应方法 (Marginal Distribution Adaptation) 的目标是减小源域和目标域的边缘概率分布的距离,从而完成迁移学习。从形式上来说,边缘分布自适应方法是用P(X~s~)和 P(X~t~)之间的距离来近似两个领域之间的差异。即：</p>
<p>​    $DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert$ (6.1)</p>
<p>​    边缘分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark83">19(b)</a>的情形。</p>
<h3 id="11-3-3-条件分布自适应"><a href="#11-3-3-条件分布自适应" class="headerlink" title="11.3.3 条件分布自适应"></a>11.3.3 条件分布自适应</h3><p>​    条件分布自适应方法 (Conditional Distribution Adaptation) 的目标是减小源域和目标域的条件概率分布的距离，从而完成迁移学习。从形式上来说，条件分布自适应方法是用  P(y~s~|X~s~) 和 P (y~t~|X~t~) 之间的距离来近似两个领域之间的差异。即：</p>
<p>​    $DISTANCE(D~s~,D~t~)\approx\lVert P(y_s|X_s)-P(y_t|X_t)\Vert$(6.8)</p>
<p>​    条件分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark85">19(c)</a>的情形。</p>
<p>​    目前单独利用条件分布自适应的工作较少，这些工作主要可以在 [<a href="#bookmark292">Saito et al.,2017</a>] 中找到。最近，中科院计算所的 Wang 等人提出了 STL 方法(Stratified Transfer Learn­ing) [<a href="#bookmark309">Wang tal.,2018</a>]。作者提出了类内迁移 (Intra-class Transfer)的思想。指出现有的 绝大多数方法都只是学习一个全局的特征变换(Global DomainShift)，而忽略了类内的相 似性。类内迁移可以利用类内特征，实现更好的迁移效果。</p>
<p>​    STL 方法的基本思路如图所示。首先利用大多数投票的思想，对无标定的位置行为生成伪标；然后在再生核希尔伯特空间中，利用类内相关性进行自适应地空间降维，使得不同情境中的行为数据之间的相关性增大；最后，通过二次标定，实现对未知标定数据的精准标定。</p>
<p><img src="./media/1542817481582.png" alt="1542817481582"></p>
<p><center>图 21: STL 方法的示意图</p>
<h3 id="11-3-4-联合分布自适应"><a href="#11-3-4-联合分布自适应" class="headerlink" title="11.3.4 联合分布自适应"></a>11.3.4 联合分布自适应</h3><p>​    联合分布自适应方法 (Joint Distribution Adaptation) 的目标是减小源域和目标域的联合概率分布的距离，从而完成迁移学习。从形式上来说，联合分布自适应方法是用<em>P</em>(<strong>x</strong>s) 和P(<strong>x</strong>t)之间的距离、以及P(ys|<strong>x</strong>s)和P(yt|<strong>x</strong>t)之间的距离来近似两个领域之间的差异。即:</p>
<p>​    $DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert-\lVert P(y_s|X_s)-P(y_t|X_t)\Vert​$(6.10)</p>
<p>​    联合分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark83">19(b)</a>的情形、以及图<a href="#bookmark101">19(a)</a>迁移到<br>图<a href="#bookmark85">19(c)</a>的情形。</p>
<h3 id="11-3-4-概率分布自适应方法优劣性比较"><a href="#11-3-4-概率分布自适应方法优劣性比较" class="headerlink" title="11.3.4 概率分布自适应方法优劣性比较"></a>11.3.4 概率分布自适应方法优劣性比较</h3><p>综合上述三种概率分布自适应方法，我们可以得出如下的结论：</p>
<ol>
<li>精度比较： BDA >JDA >TCA >条件分布自适应。</li>
<li>将不同的概率分布自适应方法用于神经网络，是一个发展趋势。图<a href="#bookmark119">23</a>展示的结果表明将概率分布适配加入深度网络中，往往会取得比非深度方法更好的结果。</li>
</ol>
<p><img src="./media/1542823019007.png" alt="1542823019007"></p>
<p><center>图 22: BDA 方法的效果第二类方法：特征选择</p>
<h3 id="11-3-6-特征选择"><a href="#11-3-6-特征选择" class="headerlink" title="11.3.6 特征选择"></a>11.3.6 特征选择</h3><p>​    特征选择法的基本假设是：源域和目标域中均含有一部分公共的特征，在这部分公共的特征，源领域和目标领域的数据分布是一致的。因此，此类方法的目标就是，通过机器学习方法，选择出这部分共享的特征，即可依据这些特征构建模型。</p>
<p>​    图 <a href="#bookmark122">24</a>形象地表示了特征选择法的主要思路。</p>
<p><img src="./media/1542823210556.png" alt="1542823210556"></p>
<p><center>图 23: 不同分布自适应方法的精度比较</p>
<p><img src="./media/a3db84158d9b6454adff88dbe4fa5d28.jpg" alt=""></p>
<p><center>图 24: 特征选择法示意图</p>
<p>​    这这个领域比较经典的一个方法是发表在 2006 年的 ECML-PKDD 会议上,作者提出了一个叫做 SCL 的方法 (Structural Correspondence Learning) [<a href="#bookmark235">Blitzer et al.,2006</a>]。这个方法的目标就是我们说的,找到两个领域公共的那些特征。作者将这些公共的特征叫做Pivot feature。找出来这些Pivot feature,就完成了迁移学习的任务。</p>
<p><img src="./media/4abacd82901988c3e0a98bdb07b2abc6.jpg" alt=""></p>
<p><center>图 25: 特征选择法中的 Pivot feature 示意图</p>
<p>​    图 <a href="#bookmark124">25</a>形象地展示了 Pivot feature 的含义。 Pivot feature指的是在文本分类中,在不同领域中出现频次较高的那些词。总结起来：</p>
<ul>
<li>特征选择法从源域和目标域中选择提取共享的特征,建立统一模型</li>
<li>通常与分布自适应方法进行结合</li>
<li>通常采用稀疏表示 ||<strong>A</strong>||2,1 实现特征选择</li>
</ul>
<h3 id="11-3-5-统计特征对齐方法"><a href="#11-3-5-统计特征对齐方法" class="headerlink" title="11.3.5 统计特征对齐方法"></a>11.3.5 统计特征对齐方法</h3><p>​    统计特征对齐方法主要将数据的统计特征进行变换对齐。对齐后的数据，可以利用传统机器学习方法构建分类器进行学习。SA方法(Subspace Alignment，子空间对齐)[<a href="#bookmark249">Fernando et al.,2013</a>]是其中的代表性成果。SA方法直接寻求一个线性变换<strong>M</strong>，将不同的数据实现变换对齐。SA方法的优化目标如下：</p>
<p><img src="./media/1542823438846.png" alt="1542823438846"></p>
<p>则变换 <strong>M</strong> 的值为：</p>
<p><img src="./media/1542823455820.png" alt="1542823455820"></p>
<p>可以直接获得上述优化问题的闭式解：</p>
<p><img src="./media/1542823474720.png" alt="1542823474720"></p>
<p>​    SA 方法实现简单，计算过程高效，是子空间学习的代表性方法。</p>
<h3 id="11-3-6-流形学习方法"><a href="#11-3-6-流形学习方法" class="headerlink" title="11.3.6 流形学习方法"></a>11.3.6 流形学习方法</h3><p><strong>什么是流形学习</strong></p>
<p>​    流形学习自从 2000 年在 Science 上被提出来以后,就成为了机器学习和数据挖掘领域的热门问题。它的基本假设是,现有的数据是从一个高维空间中采样出来的,所以,它具有高维空间中的低维流形结构。流形就是是一种几何对象（就是我们能想像能观测到的）。通俗点说就是,我们无法从原始的数据表达形式明显看出数据所具有的结构特征,那我把它想像成是处在一个高维空间,在这个高维空间里它是有个形状的。一个很好的例子就是星座。满天星星怎么描述？我们想像它们在一个更高维的宇宙空间里是有形状的,这就有了各自星座,比如织女座、猎户座。流形学习的经典方法有Isomap、locally linear embedding、 laplacian eigenmap 等。</p>
<p>​    流形空间中的距离度量：两点之间什么最短？在二维上是直线（线段）,可在三维呢？地球上的两个点的最短距离可不是直线,它是把地球展开成二维平面后画的那条直线。那条线在三维的地球上就是一条曲线。这条曲线就表示了两个点之间的最短距离,我们叫它测地线。更通俗一点, 两点之间，测地线最短。在流形学习中,我们遇到测量距离的时候更多的时候用的就是这个测地线。在我们要介绍的 GFK 方法中,也是利用了这个测地线距离。比如在下面的图中,从 A 到 C 最短的距离在就是展开后的线段,但是在三维球体上看它却是一条曲线。</p>
<p><img src="./media/fcbe02803e45f6455a4602b645b472c5.jpg" alt=""></p>
<p><center>图 28: 三维空间中两点之间的距离示意图</p>
<p>​    由于在流形空间中的特征通常都有着很好的几何性质,可以避免特征扭曲,因此我们首先将原始空间下的特征变换到流形空间中。在众多已知的流形中, Grassmann 流形G（d） 可以通过将原始的 d 维子空间 （特征向量）看作它基础的元素,从而可以帮助学习分类 器。在 Grassmann流形中,特征变换和分布适配通常都有着有效的数值形式,因此在迁移学习问题中可以被很高效地表示和求解 [<a href="#bookmark260">Hamm and Lee,2008</a>]。因此,利用 Grassmann流形空间中来进行迁移学习是可行的。现存有很多方法可以将原始特征变换到流形空间 中[<a href="#bookmark257">Gopalan et al., 2011</a>, <a href="#bookmark230">Baktashmotlagh et al.,2014</a>]。</p>
<p>​    在众多的基于流形变换的迁移学习方法中，GFK(Geodesic Flow Kernel)方法[<a href="#bookmark255">Gong et<br>al., 2012</a>]是最为代表性的一个。GFK是在2011年发表在ICCV上的SGF方法[<a href="#bookmark257">Gopalan et al.,<br>2011</a>]发展起来的。我们首先介绍SGF方法。</p>
<p>​    SGF 方法从增量学习中得到启发：人类从一个点想到达另一个点，需要从这个点一步一步走到那一个点。那么，如果我们把源域和目标域都分别看成是高维空间中的两个点，由源域变换到目标域的过程不就完成了迁移学习吗？也就是说， 路是一步一步走出来的。</p>
<p>​    于是 SGF 就做了这个事情。它是怎么做的呢？把源域和目标域分别看成高维空间 (即Grassmann流形)中的两个点，在这两个点的测地线距离上取d个中间点，然后依次连接起来。这样，源域和目标域就构成了一条测地线的路径。我们只需要找到合适的每一步的变换，就能从源域变换到目标域了。图 <a href="#bookmark133">29</a>是 SGF 方法的示意图。</p>
<p><img src="./media/103de3658cbb97ad4c24bafe28f9d957.jpg" alt=""></p>
<p><center>图 29: SGF 流形迁移学习方法示意图</p>
<p>​    SGF 方法的主要贡献在于：提出了这种变换的计算及实现了相应的算法。但是它有很明显的缺点：到底需要找几个中间点？ SGF也没能给出答案，就是说这个参数d是没法估计的，没有一个好的方法。这个问题在 GFK 中被回答了。</p>
<p>​    GFK方法首先解决SGF的问题：如何确定中间点的个数d。它通过提出一种核学习的方法，利用路径上的无穷个点的积分，把这个问题解决了。这是第一个贡献。然后，它又解决了第二个问题：当有多个源域的时候，我们如何决定使用哪个源域跟目标域进行迁移？ GFK通过提出Rank of Domain度量，度量出跟目标域最近的源域，来解决这个问题。图 <a href="#bookmark134">30</a>是 GFK 方法的示意图。</p>
<p><img src="./media/e654d14df0b44ee4e8a0e505c654044b.jpg" alt=""></p>
<p><center>图 30: GFK 流形迁移学习方法示意图</p>
<p>​    用Ss和St分别表示源域和目标域经过主成分分析(PCA)之后的子空间，则G可以视为所有的d维子空间的集合。每一个d维的原始子空间都可以被看作G上的一个点。因此，在两点之间的测地线｛\$(t) :0 \&lt; t \&lt;1｝可以在两个子空间之间构成一条路径。如果我 们令Ss = \$(0)，St =\$(1)，则寻找一条从\$(0)到\$(1)的测地线就等同于将原始的特征变换到一个无穷维度的空间中，最终减小域之间的漂移现象。这种方法可以被看作是一种从\$(0)到\$(1)的増量式“行走”方法。</p>
<p>​    特别地，流形空间中的特征可以被表示为<strong>z</strong> =\$(t)T<strong>x</strong>。变换后的特征<strong>Z</strong>i和<strong>Z</strong>j的内积定义了一个半正定 (positive semidefinite) 的测地线流式核</p>
<p><img src="./media/1542823895008.png" alt="1542823895008"></p>
<p>​    GFK 方法详细的计算过程可以参考原始的文章，我们在这里不再赘述。</p>
<h3 id="11-3-7-什么是finetune？"><a href="#11-3-7-什么是finetune？" class="headerlink" title="11.3.7 什么是finetune？"></a>11.3.7 什么是finetune？</h3><p>​    深度网络的finetune也许是最简单的深度网络迁移方法。<strong>Finetune</strong>,也叫微调、fine-tuning, 是深度学习中的一个重要概念。简而言之，finetune就是利用别人己经训练好的网络，针对自己的任务再进行调整。从这个意思上看，我们不难理解finetune是迁移学习的一部分。</p>
<p><strong>为什么需要已经训练好的网络？</strong></p>
<p>​    在实际的应用中,我们通常不会针对一个新任务,就去从头开始训练一个神经网络。这样的操作显然是非常耗时的。尤其是，我们的训练数据不可能像ImageNet那么大，可以训练出泛化能力足够强的深度神经网络。即使有如此之多的训练数据,我们从头开始训练,其代价也是不可承受的。</p>
<p>​    那么怎么办呢？迁移学习告诉我们,利用之前己经训练好的模型,将它很好地迁移到自己的任务上即可。</p>
<p><strong>为什么需要 finetune？</strong></p>
<p>​    因为别人训练好的模型,可能并不是完全适用于我们自己的任务。可能别人的训练数据和我们的数据之间不服从同一个分布；可能别人的网络能做比我们的任务更多的事情；可能别人的网络比较复杂,我们的任务比较简单。</p>
<p>​    举一个例子来说,假如我们想训练一个猫狗图像二分类的神经网络,那么很有参考价值的就是在 CIFAR-100 上训练好的神经网络。但是 CIFAR-100 有 100 个类别,我们只需要 2个类别。此时,就需要针对我们自己的任务,固定原始网络的相关层,修改网络的输出层以使结果更符合我们的需要。</p>
<p>​    图<a href="#bookmark148">36</a>展示了一个简单的finetune过程。从图中我们可以看到，我们采用的预训练好的网络非常复杂,如果直接拿来从头开始训练,则时间成本会非常高昂。我们可以将此网络进行改造,固定前面若干层的参数,只针对我们的任务,微调后面若干层。这样,网络训练速度会极大地加快,而且对提高我们任务的表现也具有很大的促进作用。</p>
<p><img src="./media/b1630ca5d004d4b430672c8b8ce7fb90.jpg" alt=""></p>
<p><center>图 36: 一个简单的 finetune 示意图<br><strong>Finetune 的优势</strong></p>
<p>​    Finetune 的优势是显然的，包括:</p>
<ul>
<li>不需要针对新任务从头开始训练网络，节省了时间成本；</li>
<li>预训练好的模型通常都是在大数据集上进行的，无形中扩充了我们的训练数据，使得模型更鲁棒、泛化能力更好；</li>
<li>Finetune 实现简单，使得我们只关注自己的任务即可。</li>
</ul>
<p><strong>Finetune 的扩展</strong></p>
<p>​    在实际应用中，通常几乎没有人会针对自己的新任务从头开始训练一个神经网络。Fine-tune 是一个理想的选择。</p>
<p>​    Finetune 并不只是针对深度神经网络有促进作用，对传统的非深度学习也有很好的效果。例如， finetune对传统的人工提取特征方法就进行了很好的替代。我们可以使用深度网络对原始数据进行训练，依赖网络提取出更丰富更有表现力的特征。然后，将这些特征作为传统机器学习方法的输入。这样的好处是显然的: 既避免了繁复的手工特征提取，又能自动地提取出更有表现力的特征。</p>
<p>​    比如，图像领域的研究，一直是以 SIFT、SURF 等传统特征为依据的，直到 2014 年，伯克利的研究人员提出了 DeCAF特征提取方法［<a href="#bookmark246">Donahue et al.,2014</a>］，直接使用深度卷积神经网络进行特征提取。实验结果表明，该特征提取方法对比传统的图像特征，在精度上有着无可匹敌的优势。另外，也有研究人员用卷积神经网络提取的特征作为SVM分类器的输 入［<a href="#bookmark291">Razavian et al.,014</a>］，显著提升了图像分类的精度。</p>
<h3 id="11-3-8-finetune为什么有效？"><a href="#11-3-8-finetune为什么有效？" class="headerlink" title="11.3.8 finetune为什么有效？"></a>11.3.8 finetune为什么有效？</h3><p>​    随着 AlexNet [<a href="#bookmark268">Krizhevsky et al., 2012</a>] 在 2012 年的 ImageNet大赛上获得冠军，深度学习开始在机器学习的研究和应用领域大放异彩。尽管取得了很好的结果，但是神经网络本身就像一个黑箱子，看得见，摸不着，解释性不好。由于神经网络具有良好的层次结构很自然地就有人开始关注，能否通过这些层次结构来很好地解释网络？于是，有了我们熟知的例子：假设一个网络要识别一只猫，那么一开始它只能检测到一些边边角角的东西，和猫根本没有关系；然后可能会检测到一些线条和圆形；慢慢地，可以检测到有猫的区域；接着是猫腿、猫脸等等。图 <a href="#bookmark137">32</a>是一个简单的示例。</p>
<p><img src="./media/1542824195602.png" alt="1542824195602"></p>
<p><center>图 32: 深度神经网络进行特征提取到分类的简单示例</p>
<p>​    这表达了一个什么事实呢？概括来说就是：前面几层都学习到的是通用的特征（general feature）；随着网络层次的加深，后面的网络更偏重于学习任务特定的特征（specific feature）。<br>这非常好理解，我们也都很好接受。那么问题来了：如何得知哪些层能够学习到 general feature，哪些层能够学习到specific feature。更进一步：如果应用于迁移学习，如何决定该迁移哪些层、固定哪些层？</p>
<p>​    这个问题对于理解神经网络以及深度迁移学习都有着非常重要的意义。</p>
<p>​    来自康奈尔大学的 Jason Yosinski 等人 [<a href="#bookmark318">Yosinski et al., 2014</a>]率先进行了深度神经网络可迁移性的研究，将成果发表在2014年机器学习领域顶级会议NIPS上并做了口头汇报。该论文是一篇实验性质的文章（通篇没有一个公式）。其目的就是要探究上面我们提到的几个关键性问题。因此，文章的全部贡献都来自于实验及其结果。（别说为啥做实验也能发文章：都是高考，我只上了个普通一本，我高中同学就上了清华）</p>
<p>​    在ImageNet的1000类上，作者把1000类分成两份（A和B），每份500个类别。然后，分别对A和B基于Caffe训练了一个AlexNet网络。一个AlexNet网络一共有8层， 除去第8层是类别相关的网络无法迁移以外，作者在 1 到 7这 7层上逐层进行 finetune 实验，探索网络的可迁移性。</p>
<p>​    为了更好地说明 finetune 的结果，作者提出了有趣的概念： AnB 和 BnB。</p>
<p>​    迁移A网络的前n层到B （AnB） vs固定B网络的前n层（BnB）</p>
<p>​    简单说一下什么叫AnB:（所有实验都是针对数据B来说的）将A网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对B进行分类。</p>
<p>​    相应地，有BnB:把训练好的B网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对 B 进行分类。</p>
<p>​    <strong>实验结果</strong></p>
<p>​    实验结果如下图（图<a href="#bookmark145">33</a>） 所示:</p>
<p>​    这个图说明了什么呢？我们先看蓝色的BnB和BnB+（就是BnB加上finetune）。对 BnB而言，原训练好的 B 模型的前 3 层直接拿来就可以用而不会对模型精度有什么损失到了第4 和第5 层，精度略有下降，不过还是可以接受。然而到了第6 第第7层，精度居然奇迹般地回升了！这是为什么？原因如下:对于一开始精度下降的第4 第 5 层来说，确</p>
<p><img src="./media/1542824318155.png" alt="1542824318155"></p>
<p><center>图 33: 深度网络迁移实验结果 1</p>
<p>实是到了这一步，feature变得越来越specific,所以下降了。那对于第6第7层为什么精度又不变了？那是因为，整个网络就8层，我们固定了第6第7层，这个网络还能学什么呢？所以很自然地，精度和原来的 B 网络几乎一致！</p>
<p>​    对 BnB+ 来说，结果基本上都保持不变。说明 finetune 对模型结果有着很好的促进作用！</p>
<p>​    我们重点关注AnB和AnB+。对AnB来说，直接将A网络的前3层迁移到B,貌似不会有什么影响，再一次说明，网络的前3层学到的几乎都是general feature!往后，到了第4第5层的时候，精度开始下降，我们直接说：一定是feature不general 了！然而，到了第6第7层，精度出现了小小的提升后又下降，这又是为什么？作者在这里提出两点co-adaptation和feature representation。就是说，第4第5层精度下降的时候，主要是由于A和B两个数据集的差异比较大，所以会下降；至I」了第6第7层，由于网络几乎不迭代了，学习能力太差，此时 feature 学不到，所以精度下降得更厉害。</p>
<p>​    再看AnB+。加入了 finetune以后，AnB+的表现对于所有的n几乎都非常好，甚至 比baseB<br>（最初的B）还要好一些！这说明：finetune对于深度迁移有着非常好的促进作用!</p>
<p>​    把上面的结果合并就得到了下面一张图 （图<a href="#bookmark138">34</a>）：</p>
<p>​    至此， AnB 和 BnB 基本完成。作者又想，是不是我分 A 和 B 数据的时候，里面存在一些比较相似的类使结果好了？比如说A里有猫，B里有狮子，所以结果会好？为了排除这些影响，作者又分了一下数据集，这次使得A和B里几乎没有相似的类别。在这个条件下再做AnB,与原来精度比较（0%为基准）得到了下图（图<a href="#bookmark139">35</a>）:</p>
<p>​    这个图说明了什么呢？简单：随着可迁移层数的增加，模型性能下降。但是，前3层仍然还是可以迁移的！同时,与随机初始化所有权重比较,迁移学习的精度是很高的!总之：</p>
<ul>
<li>深度迁移网络要比随机初始化权重效果好；</li>
</ul>
<ul>
<li>网络层数的迁移可以加速网络的学习和优化。</li>
</ul>
<h3 id="11-3-9-什么是深度网络自适应？"><a href="#11-3-9-什么是深度网络自适应？" class="headerlink" title="11.3.9 什么是深度网络自适应？"></a>11.3.9 什么是深度网络自适应？</h3><p><strong>基本思路</strong></p>
<p>​    深度网络的 finetune 可以帮助我们节省训练时间，提高学习精度。但是 finetune 有它的先天不足:它无法处理训练数据和测试数据分布不同的情况。而这一现象在实际应用中比比皆是。因为 finetune 的基本假设也是训练数据和测试数据服从相同的数据分布。这在迁移学习中也是不成立的。因此，我们需要更进一步，针对深度网络开发出更好的方法使之更好地完成迁移学习任务。</p>
<p>​    以我们之前介绍过的数据分布自适应方法为参考，许多深度学习方法［<a href="#bookmark307">Tzeng et al.,2014</a>, <a href="#bookmark275">Long et al.,2015a</a>］都开发出了自适应层(AdaptationLayer)来完成源域和目标域数据的自适应。自适应能够使得源域和目标域的数据分布更加接近，从而使得网络的效果更好。</p>
<p>​    从上述的分析我们可以得出，深度网络的自适应主要完成两部分的工作:</p>
<p>​    一是哪些层可以自适应，这决定了网络的学习程度；</p>
<p>​    二是采用什么样的自适应方法 (度量准则)，这决定了网络的泛化能力。</p>
<p>​    深度网络中最重要的是网络损失的定义。绝大多数深度迁移学习方法都采用了以下的损失定义方式:</p>
<p><img src="./media/1542824918145.png" alt="1542824918145"></p>
<p>​    其中，I表示网络的最终损失，lc(Ds,<strong>y</strong>s)表示网络在有标注的数据(大部分是源域)上的常规分类损失(这与普通的深度网络完全一致)，Ia(Ds,Dt)表示网络的自适应损失。最后一部分是传统的深度网络所不具有的、迁移学习所独有的。此部分的表达与我们先前讨论过的源域和目标域的分布差异，在道理上是相同的。式中的A是权衡两部分的权重参数。</p>
<p>​    上述的分析指导我们设计深度迁移网络的基本准则：决定自适应层，然后在这些层加入自适应度量，最后对网络进行 finetune。</p>
<h3 id="11-3-10-GAN在迁移学习中的应用"><a href="#11-3-10-GAN在迁移学习中的应用" class="headerlink" title="11.3.10 GAN在迁移学习中的应用"></a>11.3.10 GAN在迁移学习中的应用</h3><p>生成对抗网络 GAN(Generative Adversarial Nets) [<a href="#bookmark256">Goodfellow et al.,2014</a>] 是目前人工智能领域最炙手可热的概念之一。其也被深度学习领军人物 Yann Lecun 评为近年来最令人欣喜的成就。由此发展而来的对抗网络，也成为了提升网络性能的利器。本小节介绍深度对抗网络用于解决迁移学习问题方面的基本思路以及代表性研究成果。</p>
<p><strong>基本思路</strong></p>
<p>​    GAN 受到自博弈论中的二人零和博弈 (two-player game) 思想的启发而提出。它一共包括两个部分：一部分为生成网络(Generative Network)，此部分负责生成尽可能地以假乱真的样本，这部分被成为生成器(Generator)；另一部分为判别网络(Discriminative Network), 此部分负责判断样本是真实的，还是由生成器生成的，这部分被成为判别器(Discriminator) 生成器和判别器的互相博弈，就完成了对抗训练。</p>
<p>​    GAN 的目标很明确：生成训练样本。这似乎与迁移学习的大目标有些许出入。然而，由于在迁移学习中，天然地存在一个源领域，一个目标领域，因此，我们可以免去生成样本的过程，而直接将其中一个领域的数据 (通常是目标域) 当作是生成的样本。此时，生成器的职能发生变化，不再生成新样本，而是扮演了特征提取的功能：不断学习领域数据的特征使得判别器无法对两个领域进行分辨。这样，原来的生成器也可以称为特征提取器<br>(Feature Extractor)。</p>
<p>​    通常用 Gf 来表示特征提取器，用 Gd 来表示判别器。正是基于这样的领域对抗的思想，深度对抗网络可以被很好地运用于迁移学习问题中。与深度网络自适应迁移方法类似，深度对抗网络的损失也由两部分构成：网络训练的损失lc*和领域判别损失Id：</p>
<p><img src="./media/1542826334834.png" alt="1542826334834"></p>
<p><strong>DANN</strong></p>
<p>Yaroslav Ganin 等人 [<a href="#bookmark251">Ganin et al., 2016</a>]首先在神经网络的训练中加入了对抗机制，作者将他们的网络称之为DANN(Domain-Adversarial Neural Network)。在此研宄中，网络的学习目标是：生成的特征尽可能帮助区分两个领域的特征，同时使得判别器无法对两个领域的差异进行判别。该方法的领域对抗损失函数表示为：</p>
<p><img src="./media/1542826461988.png" alt="1542826461988"></p>
<p>Id = max 其中的 Ld 表示为</p>
<p><img src="./media/1542826475517.png" alt="1542826475517"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>王晋东，迁移学习简明手册</p>
<p>[Baktashmotlagh et al., 2013] Baktashmotlagh, M., Harandi, M. T., Lovell, B. C.,and Salz- mann, M. (2013). Unsupervised domain adaptation by domain invariant projection. In <em>ICCV,</em> pages 769-776.</p>
<p>[Baktashmotlagh et al., 2014] Baktashmotlagh, M., Harandi, M. T., Lovell, B. C., and Salz- mann, M. (2014). Domain adaptation on the statistical manifold. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,pages 2481-2488.</p>
<p>[Ben-David et al., 2010] Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. (2010). A theory of learning from different domains. <em>Machine learning,</em> 79(1-2):151-175.</p>
<p>[Ben-David et al., 2007] Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2007). Analysis of representations for domain adaptation. In <em>NIPS</em>, pages 137-144.</p>
<p>[Blitzer et al., 2008] Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wortman, J. (2008). Learning bounds for domain adaptation. In <em>Advances in neural information processing systems</em>, pages 129-136.</p>
<p>[Blitzer et al., 2006] Blitzer, J., McDonald, R., and Pereira, F. (2006). Domain adaptation with structural correspondence learning. In <em>Proceedings of the 2006 conference on empiri­cal methods in natural language processing</em>, pages 120-128. Association for Computational Linguistics.</p>
<p>[Borgwardt et al., 2006] Borgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P., Scholkopf, B., and Smola, A. J. (2006). Integrating structured biological data by kernel maximum mean discrepancy. <em>Bioinformatics</em>, 22(14):e49-e57.</p>
<p>[Bousmalis et al., 2016] Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D. (2016). Domain separation networks. In <em>Advances in Neural Information Processing Systems</em>, pages 343-351.</p>
<p>[Cai et al., 2011] Cai, D., He, X., Han, J., and Huang, T. S. (2011). Graph regularized nonnegative matrix factorization for data representation. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 33(8):1548-1560.</p>
<p>[Cao et al., 2017] Cao, Z., Long, M., Wang, J., and Jordan, M. I. (2017). Partial transfer learning with selective adversarial networks. <em>arXiv preprint arXiv:1707.07901</em>.</p>
<p>[Carlucci et al., 2017] Carlucci, F. M., Porzi, L., Caputo, B., Ricci, E., and Bulo, S. R. (2017). Autodial: Automatic domain alignment layers. In International Conference on* Computer Vision.</p>
<p>[Cook et al., 2013] Cook, D., Feuz, K. D., and Krishnan, N. C. (2013). Transfer learning for activity recognition: A survey. <em>Knowledge and information systems</em>, 36(3):537-556.</p>
<p>[Cortes et al., 2008] Cortes, C., Mohri, M., Riley, M., and Rostamizadeh, A. (2008). Sample selection bias correction theory. In <em>International Conference on Algorithmic Learning Theory</em>, pages 38-53, Budapest, Hungary. Springer.</p>
<p>[Dai et al., 2007] Dai, W., Yang, Q., Xue, G.-R., and Yu, Y. (2007). Boosting for transfer learning. In <em>ICML</em>, pages 193-200. ACM.</p>
<p>[Davis and Domingos, 2009] Davis, J. and Domingos, P. (2009). Deep transfer via second- order markov logic. In <em>Proceedings of the 26th annual international conference on machine learning</em>, pages 217-224. ACM.</p>
<p>[Denget al., 2014] Deng,W.,Zheng,Q.,andWang,Z.(2014).Cross-personactivityrecog-nition using reduced kernel extreme learning machine. <em>Neural Networks,</em> 53:1-7.</p>
<p>[Donahue et al., 2014] Donahue, J., Jia, Y., et al. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In <em>ICML</em>, pages 647-655.</p>
<p>[Dorri and Ghodsi, 2012] Dorri, F. and Ghodsi, A. (2012). Adapting component analysis. In <em>Data Mining (ICDM), 2012 IEEE 12th International Conference on</em>, pages 846-851. IEEE.</p>
<p>[Duan et al., 2012] Duan, L., Tsang, I. W., and Xu, D. (2012). Domain transfer multi­ple kernel learning. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 34(3):465-479.</p>
<p>[Fernando et al., 2013] Fernando, B., Habrard, A., Sebban, M., and Tuytelaars, T. (2013). Unsupervised visual domain adaptation using subspace alignment. In ICCV*, pages 2960­2967.</p>
<p>[Fodor, 2002] Fodor, I. K. (2002). A survey of dimension reduction techniques. Center for Applied Scientific Computing, Lawrence Livermore National Laboratory*, 9:1-18.</p>
<p>[Ganin et al., 2016] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Lavi- olette, F., Marchand, M., and Lempitsky, V. (2016).Domain-adversarial training of neural networks. <em>Journal of Machine Learning<br>Research</em>, 17(59):1-35.</p>
<p>[Gao et al., 2012] Gao, C., Sang, N., and Huang, R. (2012). Online transfer boosting for object tracking. In <em>Pattern Recognition (ICPR), 2012 21st International Conference on</em>, pages 906-909. IEEE.</p>
<p>[Ghifary et al., 2017] Ghifary, M., Balduzzi, D., Kleijn, W. B., and Zhang, M. (2017). Scat­ter component analysis: A unified framework for domain adaptation and domain general­ization. <em>IEEE transactions on pattern analysis and machine intelligence</em>, 39(7):1414-1430.</p>
<p>[Ghifary et al., 2014] Ghifary, M., Kleijn, W. B., and Zhang, M. (2014). Domain adaptive neural networks for object recognition. In <em>PRICAI</em>, pages 898-904.</p>
<p>[Gong et al., 2012] Gong, B., Shi, Y., Sha, F., and Grauman, K. (2012). Geodesic flow kernel for unsupervised domain adaptation. In <em>CVPR</em>, pages 2066-2073.</p>
<p>[Goodfellow et al., 2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,  Warde- Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In <em>Advances in neural information processing systems</em>, pages 2672-2680.</p>
<p>[Gopalan et al., 2011] Gopalan, R., Li, R., and Chellappa, R. (2011). Domain adaptation for object recognition: An unsupervised approach. In <em>ICCV</em>, pages 999-1006. IEEE.</p>
<p>[Gretton et al., 2012] Gretton, A., Sejdinovic, D., Strathmann, H., Balakrishnan, S., Pontil, M., Fukumizu, K., and Sriperumbudur, B. K. (2012). Optimal kernel choice for large- scale two-sample tests. In <em>Advances in neural information processing systems</em>, pages 1205-1213.</p>
<p>[Gu et al., 2011] Gu, Q., Li, Z., Han, J., et al. (2011). Joint feature selection and subspace learning. In <em>IJCAI Proceedings-International Joint Conference on Artificial Intel ligence</em>, volume 22, page 1294.</p>
<p>[Hamm and Lee, 2008] Hamm, J. and Lee, D. D. (2008). Grassmann discriminant analysis: a unifying view on subspace-based learning. In <em>ICML</em>, pages 376-383. ACM.</p>
<p>[Hou et al., 2015] Hou, C.-A., Yeh, Y.-R., and Wang, Y.-C. F. (2015). An unsupervised domain adaptation approach for cross-domain visual classification. In <em>Advanced Video and Signal Based Surveil lance (AVSS), 2015 12th IEEE International Conference on</em>,pages 1-6. IEEE.</p>
<p>[Hsiao et al., 2016] Hsiao, P.-H., Chang, F.-J., and Lin, Y.-Y. (2016). Learning discrim­inatively reconstructed source data for object recognition with few examples. <em>IEEE</em>Transactions on Image Processing*, 25(8):3518-3532.</p>
<p>[Hu and Yang, 2011] Hu, D. H. and Yang, Q. (2011). Transfer learning for activity recog­nition via sensor mapping. In <em>IJCAI Proceedings-International Joint Conference on Artificial Intelligence</em>, volume 22, page 1962, Barcelona, Catalonia, Spain. IJCAI.</p>
<p>[Huang et al., 2007] Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M., Scholkopf, B., et al. (2007). Correcting sample selection bias by unlabeled  data. <em>Advances in neural information processing systems</em>, 19:601.</p>
<p>[Jaini et al., 2016] Jaini, P., Chen, Z., Carbajal, P., Law, E., Middleton, L., Regan, K., Schaekermann, M., Trimponias, G., Tung, J., and Poupart, P. (2016). Online bayesian transfer learning for sequential data modeling. In <em>ICLR 2017</em>.</p>
<p>[Kermany et al., 2018] Kermany, D. S., Goldbaum, M., Cai, W., Valentim, C. C., Liang, H., Baxter, S. L., McKeown, A., Yang, G., Wu, X., Yan, F., et al. (2018). Identifying medical diagnoses and treatable diseases by image-based deep learning. <em>Cell</em>, 172(5):1122-1131.</p>
<p>[Khan and Heisterkamp, 2016] Khan, M. N. A. and Heisterkamp, D. R. (2016). Adapting instance weights for unsupervised domain adaptation using quadratic mutual informa­tion and subspace learning. In <em>Pattern Recognition (ICPR), 2016 23rd International Conference on</em>, pages 1560-1565, Mexican City. IEEE.</p>
<p>[Krizhevsky et al., 2012] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems*, pages 1097-1105.</p>
<p>[Li et al., 2012] Li, H., Shi, Y., Liu, Y., Hauptmann, A. G., and Xiong, Z. (2012). Cross­domain video concept detection: A joint discriminative and generative active learning approach. <em>Expert Systems with Applications</em>,<br>39(15):12220-12228.</p>
<p>[Li et al., 2016] Li, J., Zhao, J., and Lu, K. (2016). Joint feature selection and structure preservation for domain adaptation. In <em>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</em>, pages<br>1697-1703. AAAI Press.</p>
<p>[Li et al., 2018] Li, Y., Wang, N., Shi, J., Hou, X., and Liu, J. (2018). Adaptive batch normalization for practical domain adaptation. <em>Pattern Recognition</em>, 80:109-117.</p>
<p>[Liu et al., 2011] Liu, J., Shah, M., Kuipers, B., and Savarese, S. (2011). Cross-view action recognition via view knowledge transfer. In <em>Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</em>, pages 3209-3216, Colorado Springs, CO, USA. IEEE.</p>
<p>[Liu and Tuzel, 2016] Liu, M.-Y. and Tuzel, O. (2016). Coupled generative adversarial networks. In <em>Advances in neural information processing systems</em>, pages 469-477.</p>
<p>[Liu et al., 2017] Liu, T., Yang, Q., and Tao, D. (2017). Understanding how  feature struc­ture transfers in transfer learning. In <em>IJCAI</em>.</p>
<p>[Long et al., 2015a] Long, M., Cao, Y., Wang, J., and Jordan, M. (2015a). Learning trans­ferable features with deep adaptation networks. In <em>ICML</em>, pages 97-105.</p>
<p>[Long et al., 2016] Long, M., Wang, J., Cao, Y., Sun, J., and Philip, S. Y. (2016). Deep learning of transferable representation for scalable domain  adaptation. <em>IEEE Transac­tions on Knowledge and Data Engineering</em>,<br>28(8):2027-2040.</p>
<p>[Long et al., 2014a] Long, M., Wang, J., Ding, G., Pan, S. J., and Yu, P. S. (2014a). Adaptation regularization: A general framework for transfer learning.*IEEE TKDE, 26(5):1076-1089.</p>
<p>[Long et al., 2014b] Long, M., Wang, J., Ding, G., Sun, J., and Yu, P. S. (2014b). Transfer joint matching for unsupervised domain adaptation. In *CVPR ,pages 1410-1417.</p>
<p>[Long et al., 2013] Long, M., Wang, J., et al. (2013). Transfer feature learning with joint distribution adaptation. In <em>ICCV</em>, pages 2200-2207.</p>
<p>[Long et al., 2017] Long, M., Wang, J., and Jordan, M. I. (2017). Deep transfer learning with joint adaptation networks. In <em>ICML</em>, pages 2208-2217.</p>
<p>[Long et al., 2015b] Long, M., Wang, J., Sun, J., and Philip, S. Y. (2015b). Domain invari­ant transfer kernel learning. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 27(6):1519-1532.</p>
<p>[Luo et al., 2017] Luo, Z., Zou, Y., Hoffman, J., and Fei-Fei, L. F. (2017). Label efficient learning of transferable representations acrosss domains and tasks. In <em>Advances in Neural Information Processing Systems</em>, pages 164-176.</p>
<p>[Mihalkova et al., 2007] Mihalkova, L., Huynh, T., and Mooney, R. J. (2007). Mapping and revising markov logic networks for transfer learning. In <em>AAAI</em>, volume 7, pages 608-614.</p>
<p>[Mihalkova and Mooney, 2008] Mihalkova, L. and Mooney, R. J. (2008). Transfer learning by mapping with minimal target data. In <em>Proceedings of the AAAI-08 workshop on transfer learning for complex tasks</em>.</p>
<p>[Nater et al., 2011] Nater, F., Tommasi, T., Grabner, H., Van Gool, L., and Caputo, B. (2011). Transferring activities: Updating human behavior analysis. In <em>Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on</em>, pages 1737­1744, Barcelona, Spain. IEEE.</p>
<p>[Pan et al., 2008a] Pan, S. J., Kwok, J. T., and Yang, Q. (2008a). Transfer learning via dimensionality reduction. In <em>Proceedings of the 23rd AAAI conference on Artificial in­telligence</em>, volume 8, pages 677-682.</p>
<p>[Pan et al., 2008b] Pan, S. J., Shen, D., Yang, Q., and Kwok, J. T. (2008b). Transferring localization models across space. In <em>Proceedings of the 23rd AAAI Conference on Artificial Intelligence</em>, pages 1383-1388.</p>
<p>[Pan et al., 2011] Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. (2011). Domain adaptation via transfer component analysis. <em>IEEE TNN</em>, 22(2):199-210.</p>
<p>[PanandYang, 2010] Pan,S.J.andYang,Q.(2010). A survey on transfer learning. IEEE TKDE*, 22(10):1345-1359.</p>
<p>[Patil and Phursule, 2013] Patil, D. M. and Phursule, R. (2013). Knowledge transfer using cost sensitive online learning classification. <em>International Journal of Science and Research</em>, pages 527-529.</p>
<p>[Razavian et al., 2014] Razavian, A. S., Azizpour, H., Sullivan, J., and Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In <em>Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on</em>, pages 512­519. IEEE.</p>
<p>[Saito et al., 2017] Saito, K., Ushiku, Y., and Harada, T. (2017). Asymmetric tri-training for unsupervised domain adaptation. In <em>International Conference on Machine Learning</em>.</p>
<p>[Sener et al., 2016] Sener, O., Song, H. O., Saxena, A., and Savarese, S. (2016). Learning transferrable representations for unsupervised domain adaptation. In <em>Advances in Neural Information Processing Systems</em>, pages 2110-2118.</p>
<p>[Shen et al., 2018] Shen, J., Qu, Y., Zhang, W., and Yu, Y. (2018). Wasserstein distance guided representation learning for domain adaptation. In <em>AAAI</em>.</p>
<p>[Si et al., 2010] Si, S., Tao, D., and Geng, B. (2010). Bregman divergence-based regu­larization for transfer subspace learning. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 22(7):929-942.</p>
<p>[Silver et al., 2017] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. <em>Nature</em>, 550(7676):354.</p>
<p>[Stewart and Ermon, 2017] Stewart, R. and Ermon, S. (2017). Label-free supervision of neural networks with physics and domain knowledge. In <em>AAAI</em>, pages 2576-2582.</p>
<p>[Sun et al., 2016] Sun, B., Feng, J., and Saenko, K. (2016). Return of frustratingly easy domain adaptation. In <em>AAAI</em>, volume 6, page 8.</p>
<p>[Sun and Saenko, 2015] Sun, B. and Saenko, K. (2015). Subspace distribution alignment for unsupervised domain adaptation. In <em>BMVC</em>, pages 24-1.</p>
<p>[Sun and Saenko, 2016] Sun, B. and Saenko, K. (2016). Deep coral: Correlation alignment for deep domain adaptation. In <em>European Conference on Computer Vision</em>, pages 443-450. Springer.</p>
<p>[Tahmoresnezhad and Hashemi, 2016] Tahmoresnezhad, J. and Hashemi, S. (2016). Visual domain adaptation via transfer feature learning. <em>Knowledge and Information Systems</em>, pages 1-21.</p>
<p>[Tan et al., 2015] Tan, B., Song, Y., Zhong, E., and Yang, Q. (2015). Transitive trans­fer learning. In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, pages 1155-1164. ACM.</p>
<p>[Tan et al., 2017] Tan, B., Zhang, Y., Pan, S. J., and Yang, Q. (2017). Distant domain transfer learning. In <em>Thirty-First AAAI Conference on Artificial Intelligence</em>.</p>
<p>[Taylor and Stone, 2009] Taylor, M. E. and Stone, P. (2009). Transfer learning for reinforce­ment learning domains: A survey. <em>Journal of Machine Learning Research</em>, 10(Jul):1633- 1685.</p>
<p>[Tzeng et al., 2015] Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. (2015). Simulta­neous deep transfer across domains and tasks. In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, pages 4068-4076, Santiago, Chile. IEEE.</p>
<p>[Tzeng et al., 2017] Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017). Adversarial discriminative domain adaptation. In <em>CVPR</em>, pages 2962-2971.</p>
<p>[Tzeng et al., 2014] Tzeng, E., Hoffman, J., Zhang, N., et al. (2014). Deep domain confu­sion: Maximizing for domain invariance. <em>arXiv preprint arXiv:1412.3474</em>.</p>
<p>[Wang et al., 2017] Wang, J., Chen, Y., Hao, S., et al. (2017). Balanced distribution adap­tation for transfer learning. In <em>ICDM</em>, pages 1129-1134.</p>
<p>[Wang et al., 2018] Wang, J., Chen, Y., Hu, L., Peng, X., and Yu, P. S. (2018). Strati­fied transfer learning for cross-domain activity recognition. In <em>2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)</em>.</p>
<p>[Wang et al., 2014] Wang, J., Zhao, P., Hoi, S. C., and Jin, R. (2014). Online feature selection and its applications. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 26(3):698-710.</p>
<p>[Wei et al., 2016a] Wei, P., Ke, Y., and Goh, C. K. (2016a). Deep nonlinearfeature coding for unsupervised domain adaptation. In <em>IJCAI</em>, pages 2189-2195.</p>
<p>[Wei et al., 2017] Wei, Y., Zhang, Y., and Yang, Q. (2017). Learning totransfer. <em>arXiv</em> preprint arXiv:1708.05629*.</p>
<p>[Wei et al., 2016b] Wei, Y., Zhu, Y., Leung, C. W.-k., Song, Y., and Yang, Q. (2016b). Instilling social to physical: Co-regularized heterogeneous transfer learning. In <em>Thirtieth</em> AAAI Conference on Artificial Intelligence*.</p>
<p>[Weiss et al., 2016] Weiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. <em>Journal of Big Data</em>, 3(1):1-40.</p>
<p>[Wu et al., 2017] Wu, Q., Zhou, X., Yan, Y., Wu, H., and Min, H. (2017). Online transfer learning by leveraging multiple source domains. <em>Knowledge and Information Systems</em>, 52(3):687-707.</p>
<p>[xinhua, 2016] xinhua (2016). <a href="http://mp.weixin.qq.com/s?__biz=MjM5ODYzNzAyMQ==&amp;" target="_blank" rel="noopener">http://mp.weixin.qq.com/s?__biz=MjM5ODYzNzAyMQ==&amp;</a> mid=2651933920&amp;idx=1\\&amp;sn=ae2866bd12000f1644eae1094497837e.</p>
<p>[Yan et al., 2017] Yan, Y., Wu, Q., Tan, M., Ng, M. K., Min, H., and Tsang, I. W. (2017). Online heterogeneous transfer by hedge ensemble of offline and online decisions. <em>IEEE transactions on neural networks and learning systems</em>.</p>
<p>[Yosinski et al., 2014] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In <em>Advances in neural information processing systems</em>, pages 3320-3328.</p>
<p>[Zadrozny, 2004] Zadrozny, B. (2004). Learning and evaluating classifiers under sample selection bias. In <em>Proceedings of the twenty-first international conference on Machine learning</em>, page 114, Alberta, Canada. ACM.</p>
<p>[Zellinger et al., 2017] Zellinger, W., Grubinger, T., Lughofer, E., Natschlager, T., and Saminger-Platz, S. (2017). Central moment discrepancy (cmd) for domain-invariant rep­resentation learning. <em>arXiv preprint arXiv:1702.08811</em>.</p>
<p>[Zhang et al., 2017a] Zhang, J., Li, W., and Ogunbona, P. (2017a). Joint geometrical and statistical alignment for visual domain adaptation. In <em>CVPR</em>.</p>
<p>[Zhang et al., 2017b] Zhang, X., Zhuang, Y., Wang, W., and Pedrycz, W. (2017b). On­line feature transformation learning for cross-domain object category recognition. <em>IEEE transactions on neural networks and learning systems</em>.</p>
<p>[Zhao and Hoi, 2010] Zhao, P. and Hoi, S. C. (2010). Otl: A framework of online transfer learning. In <em>Proceedings of the 27th international conference on machine learning (ICML- 10)</em>, pages 1231-1238.</p>
<p>[Zhao et al., 2010] Zhao, Z., Chen, Y., Liu, J., and Liu, M. (2010). Cross-mobile elm based activity recognition. <em>International Journal of Engineering and Industries</em>, 1(1):30-38.</p>
<p>[Zhao et al., 2011] Zhao, Z., Chen, Y., Liu, J., Shen, Z., and Liu, M. (2011). Cross-people mobile-phone based activity recognition. In <em>Proceedings of the Twenty-Second interna­tional joint conference on Artificial Intelligence (IJCAI)</em>, volume 11, pages 2545-2550. Citeseer.</p>
<p>[Zheng et al., 2009] Zheng, V. W., Hu, D. H., and Yang, Q. (2009). Cross-domain activity recognition. In <em>Proceedings of the 11th international conference on Ubiquitous computing</em>, pages 61-70. ACM.</p>
<p>[Zheng et al., 2008] Zheng, V. W., Pan, S. J., Yang, Q., and Pan, J. J. (2008). Transferring multi-device localization models using latent multi-task learning. In <em>AAAI</em>, volume 8, pages 1427-1432, Chicago, Illinois, USA. AAAI.</p>
<p>[Zhuang et al., 2015] Zhuang, F., Cheng, X., Luo, P., Pan, S. J., and He, Q. (2015). Su­pervised representation learning: Transfer learning with deep autoencoders. In <em>IJCAI</em>,pages 4119-4125.</p>
<p>[Zhuo et al., 2017] Zhuo, J., Wang, S., Zhang, W., and Huang, Q. (2017). Deep unsuper­vised convolutional domain adaptation. In <em>Proceedings of the 2017 ACM on Multimedia Conference</em>, pages 261-269. ACM.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E7%AB%A0_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E7%AB%A0_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">第十章_强化学习</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:28:54 / 修改时间：12:29:13" itemprop="dateCreated datePublished" datetime="2020-03-03T12:28:54+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/</a></p>
<h1 id="第十章-强化学习"><a href="#第十章-强化学习" class="headerlink" title="第十章 强化学习"></a>第十章 强化学习</h1><h2 id="10-1-强化学习的主要特点？"><a href="#10-1-强化学习的主要特点？" class="headerlink" title="10.1 强化学习的主要特点？"></a>10.1 强化学习的主要特点？</h2><p>其他许多机器学习算法中学习器都是学得怎样做，而RL是在尝试的过程中学习到在特定的情境下选择哪种行动可以得到最大的回报。在很多场景中，当前的行动不仅会影响当前的rewards，还会影响之后的状态和一系列的rewards。RL最重要的3个特定在于：<br>(1)    基本是以一种闭环的形式；<br>(2)    不会直接指示选择哪种行动（actions）；<br>(3)    一系列的actions和奖励信号（reward signals）都会影响之后较长的时间。 </p>
<h3 id="10-1-1-定义"><a href="#10-1-1-定义" class="headerlink" title="10.1.1 定义"></a>10.1.1 定义</h3><p>强化学习是机器学习的一个重要分支，是多学科多领域交叉的一个产物，它的本质是解决 decision making 问题，即自动进行决策，并且可以做连续决策。<br>它主要包含四个元素，agent，环境状态，行动，奖励, 强化学习的目标就是获得最多的累计奖励。<br>我们列举几个形象的例子：<br>小孩想要走路，但在这之前，他需要先站起来，站起来之后还要保持平衡，接下来还要先迈出一条腿，是左腿还是右腿，迈出一步后还要迈出下一步。<br>小孩就是 agent，他试图通过采取行动（即行走）来操纵环境（行走的表面），并且从一个状态转变到另一个状态（即他走的每一步），当他完成任务的子任务（即走了几步）时，孩子得到奖励（给巧克力吃），并且当他不能走路时，就不会给巧克力。</p>
<p><img src="/img/ch10/10-1.png" alt=""></p>
<p>上图中agent代表自身，如果是自动驾驶，agent就是车；如果你玩游戏它就是你当前控制的游戏角色，如马里奥，马里奥往前走时环境就一直在发生变化，有小怪物或者障碍物出现，它需要通过跳跃来进行躲避，就是要做action（如向前走和跳起的动作）；无人驾驶的action就是车左转、右转或刹车等等，它无时无刻都在与环境产生交互，action会反馈给环境，进而改变环境，如果自动驾驶的车行驶目标是100米，它向前开了10米，那环境就发生了变化，所以每次产生action都会导致环境改变，环境的改变会反馈给自身（agent），就是这样的一个循环；反馈又两种方式：1、做的好（reward）即正反馈，2、做得不好（punishment惩罚）即负反馈。Agent可能做得好，也可能做的不好，环境始终都会给它反馈，agent会尽量去做对自身有利的决策，通过反反复复这样的一个循环，agent会越来越做的好，就像孩子在成长过程中会逐渐明辨是非，这就是强化学习。</p>
<h2 id="10-2-强化学习应用实例"><a href="#10-2-强化学习应用实例" class="headerlink" title="10.2 强化学习应用实例"></a>10.2 强化学习应用实例</h2><p>（1）Manufacturing</p>
<p>例如一家日本公司 Fanuc，工厂机器人在拿起一个物体时，会捕捉这个过程的视频，记住它每次操作的行动，操作成功还是失败了，积累经验，下一次可以更快更准地采取行动。</p>
<p><img src="/img/ch10/10-2.png" alt=""></p>
<p>（2）Inventory Management</p>
<p>在库存管理中，因为库存量大，库存需求波动较大，库存补货速度缓慢等阻碍使得管理是个比较难的问题，可以通过建立强化学习算法来减少库存周转时间，提高空间利用率。</p>
<p>（3）Dynamic pricing</p>
<p>强化学习中的 Q-learning 可以用来处理动态定价问题。</p>
<p>（4）Customer Delivery</p>
<p>制造商在向各个客户运输时，想要在满足客户的所有需求的同时降低车队总成本。通过 multi-agents 系统和 Q-learning，可以降低时间，减少车辆数量。</p>
<p>（5）ECommerce Personalization</p>
<p>在电商中，也可以用强化学习算法来学习和分析顾客行为，定制产品和服务以满足客户的个性化需求。</p>
<p>（6）Ad Serving</p>
<p>例如算法 LinUCB （属于强化学习算法 bandit 的一种算法），会尝试投放更广范围的广告，尽管过去还没有被浏览很多，能够更好地估计真实的点击率。<br>再如双 11 推荐场景中，阿里巴巴使用了深度强化学习与自适应在线学习，通过持续机器学习和模型优化建立决策引擎，对海量用户行为以及百亿级商品特征进行实时分析，帮助每一个用户迅速发现宝贝，提高人和商品的配对效率。还有，利用强化学习将手机用户点击率提升了 10-20%。</p>
<p>（7）Financial Investment Decisions</p>
<p>例如这家公司 Pit.ai，应用强化学习来评价交易策略，可以帮助用户建立交易策略，并帮助他们实现其投资目标。</p>
<p>（8）Medical Industry</p>
<p>动态治疗方案（DTR）是医学研究的一个主题，是为了给患者找到有效的治疗方法。 例如癌症这种需要长期施药的治疗，强化学习算法可以将患者的各种临床指标作为输入 来制定治疗策略。</p>
<h2 id="10-3-强化学习和监督式学习、非监督式学习的区别"><a href="#10-3-强化学习和监督式学习、非监督式学习的区别" class="headerlink" title="10.3 强化学习和监督式学习、非监督式学习的区别"></a>10.3 强化学习和监督式学习、非监督式学习的区别</h2><p>在机器学习中，我们比较熟知的是监督式学习，非监督学习，此外还有一个大类就是强化学习：<br>当前的机器学习算法可以分为3种：有监督的学习（Supervised Learning）、无监督的学习（Unsupervised Learning）和强化学习（Reinforcement Learning），结构图如下所示：</p>
<p> <img src="/img/ch10/10-3.png" alt=""></p>
<h3 id="10-3-1-强化学习和监督式学习的区别："><a href="#10-3-1-强化学习和监督式学习的区别：" class="headerlink" title="10.3.1 强化学习和监督式学习的区别："></a>10.3.1 强化学习和监督式学习的区别：</h3><p>监督式学习就好比你在学习的时候，有一个导师在旁边指点，他知道怎么是对的怎么是错的，但在很多实际问题中，例如 chess，go，这种有成千上万种组合方式的情况，不可能有一个导师知道所有可能的结果。</p>
<p>而这时，强化学习会在没有任何标签的情况下，通过先尝试做出一些行为得到一个结果，通过这个结果是对还是错的反馈，调整之前的行为，就这样不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。</p>
<p>就好比你有一只还没有训练好的小狗，每当它把屋子弄乱后，就减少美味食物的数量（惩罚），每次表现不错时，就加倍美味食物的数量（奖励），那么小狗最终会学到一个知识，就是把客厅弄乱是不好的行为。</p>
<p>两种学习方式都会学习出输入到输出的一个映射，监督式学习出的是之间的关系，可以告诉算法什么样的输入对应着什么样的输出，强化学习出的是给机器的反馈 reward function，即用来判断这个行为是好是坏。<br>另外强化学习的结果反馈有延时，有时候可能需要走了很多步以后才知道以前的某一步的选择是好还是坏，而监督学习做了比较坏的选择会立刻反馈给算法。</p>
<p>而且强化学习面对的输入总是在变化，每当算法做出一个行为，它影响下一次决策的输入，而监督学习的输入是独立同分布的。</p>
<p>通过强化学习，一个 agent 可以在探索和开发（exploration and exploitation）之间做权衡，并且选择一个最大的回报。 </p>
<p>exploration 会尝试很多不同的事情，看它们是否比以前尝试过的更好。 </p>
<p>exploitation 会尝试过去经验中最有效的行为。</p>
<p>一般的监督学习算法不考虑这种平衡，就只是是 exploitative。</p>
<h3 id="10-3-2-强化学习和非监督式学习的区别："><a href="#10-3-2-强化学习和非监督式学习的区别：" class="headerlink" title="10.3.2 强化学习和非监督式学习的区别："></a>10.3.2 强化学习和非监督式学习的区别：</h3><p>非监督式不是学习输入到输出的映射，而是模式。例如在向用户推荐新闻文章的任务中，非监督式会找到用户先前已经阅读过类似的文章并向他们推荐其一，而强化学习将通过向用户先推荐少量的新闻，并不断获得来自用户的反馈，最后构建用户可能会喜欢的文章的“知识图”。</p>
<p>对非监督学习来说，它通过对没有概念标记的训练例进行学习，以发现训练例中隐藏的结构性知识。这里的训练例的概念标记是不知道的，因此训练样本的歧义性最高。对强化学习来说，它通过对没有概念标记、但与一个延迟奖赏或效用（可视为延迟的概念标记）相关联的训练例进行学习，以获得某种从状态到行动的映射。这里本来没有概念标记的概<br>念，但延迟奖赏可被视为一种延迟概念标记，因此其训练样本的歧义性介于监督学习和非监督学习之间。</p>
<p>需要注意的是，监督学习和非监督学习从一开始就是相对的，而强化学习在提出时并没有从训练样本歧义性的角度考虑其与监督学习和非监督学习的区别，因此，一些早期的研究中把强化学习视为一种特殊的非监督学习。事实上，对强化学习的定位到目前仍然是有争议的，有的学者甚至认为它是与“从例子中学习”同一级别的概念。</p>
<p>从训练样本歧义性角度进行的分类体系，在近几年可望有一些扩展，例如多示例学习（multi-instancelearning）等从训练样本歧义性方面来看很特殊的新的学习框架有可能会进入该体系。但到目前为止，没有任何新的框架得到了公认的地位。另外，半监督学习（semi-supervisedlearning）也有一定希望，它的障碍是半监督学习中的歧义性并不是与生俱来的，而是人为的，即用户期望用未标记的样本来辅助对已标记样本的学习。这与监督学习、非监督学习、强化学习等天生的歧义性完全不同。半监督学习中人为的歧义性在解决工程问题上是需要的、有用的（对大量样本进行标记的代价可能是极为昂贵的），但可能不太会导致方法学或对学习问题视点的大的改变。</p>
<p><strong>强化学习和前二者的本质区别</strong>:没有前两者具有的明确数据概念，它不知道结果，只有目标。数据概念就是大量的数据，有监督学习、无监督学习需要大量数据去训练优化你建立的模型，就像猫狗识别，用n多张猫狗图片去训练模型，经过训练优化后，你用一张崭新的猫狗图片让模型作出判断，这个模型就知道是猫还是狗。</p>
<h2 id="10-4-强化学习主要有哪些算法？"><a href="#10-4-强化学习主要有哪些算法？" class="headerlink" title="10.4 强化学习主要有哪些算法？"></a>10.4 强化学习主要有哪些算法？</h2><p>强化学习不需要监督信号,可以在模型未知的环境中平衡探索和利用, 其主要算法有蒙特卡罗强化学习, 时间差分(temporal difference: TD)学习, 策略梯度等。典型的深度强化学习算法特点及性能比较如下图所示：</p>
<p><img src="/img/ch10/10-4.png" alt=""></p>
<p>除了上述深度强化学习算法，还有深度迁移强化学习、分层深度强化学习、深度记忆强化学习以及多智能体强化学习等算法。</p>
<h2 id="10-5-深度迁移强化学习算法"><a href="#10-5-深度迁移强化学习算法" class="headerlink" title="10.5 深度迁移强化学习算法"></a>10.5 深度迁移强化学习算法</h2><p>传统深度强化学习算法每次只能解决一种游戏任务, 无法在一次训练中完成多种任务. 迁移学习和强化学习的结合也是深度强化学习的一种主要思路。</p>
<p>Parisotto等提出了一种基于行为模拟的深度迁移强化学习算法. 该算法通过监督信号的指导, 使得单一的策略网络学习各自的策略, 并将知识迁移到新任务中. Rusa等提出策略蒸馏(policy distillation)深度迁移强化学习算法. 策略蒸馏算法中分为学习网络和指导网络, 通过这两个网络Q值的偏差来确定目标函数,引导学习网络逼近指导网络的值函数空间. 此后,Rusa等又提出了一种基于渐进神经网络(progressive neural networks, PNN)的深度迁移强化学习算法.PNN是一种把神经网络和神经网络连起来的算法. 它在一系列序列任务中, 通过渐进的方式来存储知识和提取特征, 完成了对知识的迁移. PNN最终实现多个独立任务的训练, 通过迁移加速学习过程, 避免灾难性遗忘. Fernando 等提出了路径网络(PathNet)[45].PathNet可以说是PNN的进阶版. PathNet把网络中每一层都看作一个模块, 把构建一个网络看成搭积木,也就是复用积木. 它跟PNN非常类似, 只是这里不再有列, 而是不同的路径. PathNet将智能体嵌入到神经网络中, 其中智能体的任务是为新任务发现网络中可以复用的部分. 智能体是网络之中的路径, 其决定了反向传播过程中被使用和更新的参数范围. 在一系列的Atari强化学习任务上, PathNet都实现了正迁移, 这表明PathNet在训练神经网络上具有通用性应用能力.PathNet也可以显著提高A3C算法超参数选择的鲁棒性. Schaul等提出了一种通用值函数逼近器(universalvalue function approximators, UVFAs)来泛化状态和目标空间．UVFAs可以将学习到的知识迁移到环境动态特性相同但目标不同的新任务中.</p>
<h2 id="10-6-分层深度强化学习算法"><a href="#10-6-分层深度强化学习算法" class="headerlink" title="10.6 分层深度强化学习算法"></a>10.6 分层深度强化学习算法</h2><p>分层强化学习可以将最终目标分解为多个子任务来学习层次化的策略, 并通过组合多个子任务的策略形成有效的全局策略. Kulkarni等提出了分层DQN(hierarchical deep Q-network, h—DQN) 算法. h—DQN基于时空抽象和内在激励分层, 通过在不同的时空尺度上设置子目标对值函数进行层次化处理. 顶层的值函数用于确定宏观决策, 底层的值函数用于确定具体行动．Krishnamurthy等在h—DQN的基础上提出了基于内部选择的分层深度强化学习算法. 该模型结合时空抽象和深度神经网络, 自动地完成子目标的学习, 避免了特定的内在激励和人工设定中间目标,加速了智能体的学习进程, 同时也增强了模型的泛化能力. Kulkarni等基于后续状态表示法提出了深度后续强化学习(deep successor reinforcement learning,DSRL)．DSRL通过阶段性地分解子目标和学习子目标策略, 增强了对未知状态空间的探索, 使得智能体更加适应那些存在延迟反馈的任务．Vezhnevets等受封建(feudal)强化学习算法的启发, 提出一种分层深度强化学习的架构FeUdal网络(FuNs)[49]. FuNs框架使用一个管理员模块和一个工人模块. 管理员模块在较低的时间分辨率下工作, 设置抽象目标并传递给工人模块去执行. FuNs框架创造了一个稳定的自然层次结构, 并且允许两个模块以互补的方式学习. 实验证明, FuNs有助于处理长期信用分配和记忆任务,在Atari视频游戏和迷宫游戏中都取得了不错的效果。</p>
<h2 id="10-7-深度记忆强化学习算法"><a href="#10-7-深度记忆强化学习算法" class="headerlink" title="10.7 深度记忆强化学习算法"></a>10.7 深度记忆强化学习算法</h2><p>传统的深度强化学习模型不具备记忆、认知、推理等高层次的能力, 尤其是在面对状态部分可观察和延迟奖赏的情形时. Junhyuk等通过在传统的深度强化学习模型中加入外部的记忆网络部件和反馈控制机制, 提出反馈递归记忆Q网络(feedback recurrent memory Q-network, FRMQN)). FRMQN模型具备了一定的记忆与推理功能, 通过反馈控制机制,FRMQN整合过去存储的有价值的记忆和当前时刻的上下文状态, 评估动作值函数并做出决策. FRMQN初步模拟了人类的主动认知与推理能力, 并完成了一些高层次的认知任务. 在一些未经过训练的任务中,FRMQN模型表现出了很强的泛化能力．Blundell等设计出一种模型无关的情节控制算法(model-free episode control, MFEC). MFEC可以快速存储和回放状态转移序列, 并将回放的序列整合到结构化知识系统中, 使得智能体在面对一些复杂的决策任务时, 能快速达到人类玩家的水平．MFEC通过反向经验回放, 使智能体拥有初步的情节记忆. 实验表明, 基于MFEC算法的深度强化学习不仅可以在Atari游戏中学习到有效策略, 还可以处理一些三维场景的复杂任务. Pritzel等在MFEC的基础上进一步提出了神经情节控制(neural episodic control, NEC),有效提高了深度强化学习智能体的记忆能力和学习效率[53]. NEC能快速吸收新经验并依据新经验来采取行动. 价值函数包括价值函数渐变状态表示和价值函数快速更新估计两部分. 大量场景下的研究表明,NEC的学习速度明显快于目前最先进的通用深度强化学习智能体.</p>
<h2 id="10-8-多智能体深度强化学习算法"><a href="#10-8-多智能体深度强化学习算法" class="headerlink" title="10.8 多智能体深度强化学习算法"></a>10.8 多智能体深度强化学习算法</h2><p>在一些复杂场景中, 涉及到多智能体的感知决策问题, 这时需要将单一模型扩展为多个智能体之间相互合作、通信及竞争的多智能体深度强化学习系统.Foerster等提出了一种称为分布式深度递归Q网络(deep distributed recurrent Q-networks, DDRQN) 的模型, 解决了状态部分可观测状态下的多智能体通信与合作的挑战性难题[54]. 实验表明, 经过训练的DDRQN模型最终在多智能体之间达成了一致的通信协1536 控制理论与应用第34 卷议, 成功解决了经典的红蓝帽子问题.让智能体学会合作与竞争一直以来都是人工智能领域内的一项重要研究课题, 也是实现通用人工智能的必要条件. Lowe等提出了一种用于合作–竞争混合环境的多智能体actor-critic 算法(multi-agent deepdeterministic policy gradient, MADDPG)[55]. MADDPG对DDPG强化学习算法进行了延伸, 可实现多智能体的集中式学习和分布式执行, 让智能体学习彼此合作和竞争. 在多项测试任务中, MADDPG的表现都优于DDPG. </p>
<h2 id="10-9-强化学习开源框架"><a href="#10-9-强化学习开源框架" class="headerlink" title="10.9 强化学习开源框架"></a>10.9 强化学习开源框架</h2><p>谷歌TensorFlow Agents —-TensorFlow的加强版,它提供许多工具，通过强化学习可以实现各类智能应用程序的构建与训练。这个框架能够将OpoenAI Gym接口扩展至多个并行环境，并允许各代理立足TensorFlow之内实现以执行批量计算。其面向OpoenAI Gy环境的批量化接口可与TensorFlow实现全面集成，从而高效执行各类算法。该框架还结合有BatchPPO，一套经过优化的近端策略优化算法实现方案。其核心组件包括一个环境打包器，用于在外部过程中构建OpenAI Gym环境; 一套批量集成，用于实现TensorFlow图步并以强化学习运算的方式重置函数; 外加用于将TensorFlow图形批处理流程与强化学习算法纳入训练特内单一却步的组件。</p>
<p>Roboschool：Roboschool 提供开源软件以通过强化学习构建并训练机器人模拟。其有助于在同一环境当中对多个代理进行强化学习训练。通过多方训练机制，您可以训练同一代理分别作为两方玩家（因此能够自我对抗）、使用相同算法训练两套代理，或者设置两种算法进行彼此对抗。Roboschool由OpenAI开发完成，这一非营利性组织的背后赞助者包括Elon Musk、Sam Altman、Reid Hoffman以及Peter Thiel。其与OpenAI Gym相集成，后者是一套用于开发及评估强化学习算法的开源工具集。OpenAI Gym与TensorFlow、Theano以及其它多种深度学习库相兼容。OpenAI Gym当中包含用于数值计算、游戏以及物理引擎的相关代码。Roboschool基于Bullet物理引擎，这是一套开源许可物理库，并被其它多种仿真软件——例如Gazebo与Virtual Robot Experimentation Platform（简称V-REP）所广泛使用。其中包含多种强化学习算法，具体以怨报德 异步深度强化学习方法、Actor-Critic with Experience Replay、Actor- Critic using Kronecker-Factored Trust Region、深度确定性策略梯度、近端策略优化以及信任域策略优化等等。</p>
<p>Coach：英特尔公司的开源强化学习框架，可以对游戏、机器人以及其它基于代理的智能应用进行智能代理的建模、训练与评估。Coach 提供一套模块化沙箱、可复用组件以及用于组合新强化学习算法并在多种应用领域内训练新智能应用的Python API。该框架利用OpenAI Gym作为主工具，负责与不同强化学习环境进行交换。其还支持其它外部扩展，具体包括Roboschool、gym-extensions、PyBullet以及ViZDoom。Coach的环境打包器允许用户向其中添加自定义强化学习环境，从而解决其它学习问题。该框架能够在桌面计算机上高效训练强化学习代理，并利用多核CPU处理相关任务。其能够为一部分强化学习算法提供单线程与多线程实现能力，包括异步优势Actor-Critic、深度确定性策略梯度、近端策略优化、直接未来预测以及规范化优势函数。所有算法皆利用面向英特尔系统作出优化的TensorFLow完成，其中部分算法亦适用于英特尔的Neon深度学习框架。Coach 当中包含多种强化学习代理实现方案，具体包括从单线程实现到多线程实现的转换。其能够开发出支持单与多工作程序（同步或异步）强化学习实现方法的新代理。此外，其还支持连续与离散操作空间，以及视觉观察空间或仅包含原始测量指标的观察空间。</p>
<h2 id="10-10-深度强化学习算法小结"><a href="#10-10-深度强化学习算法小结" class="headerlink" title="10.10 深度强化学习算法小结"></a>10.10 深度强化学习算法小结</h2><p>基于值函数概念的DQN及其相应的扩展算法在离散状态、离散动作的控制任务中已经表现了卓越的性能, 但是受限于值函数离散型输出的影响, 在连续型控制任务上显得捉襟见肘. 基于策略梯度概念的,以DDPG, TRPO等为代表的策略型深度强化学习算法则更适用于处理基于连续状态空间的连续动作的控制输出任务, 并且算法在稳定性和可靠性上具有一定的理论保证, 理论完备性较强. 采用actor-critic架构的A3C算法及其扩展算法, 相比于传统DQN算法, 这类算法的数据利用效率更高, 学习速率更快, 通用性、可扩展应用性更强, 达到的表现性能更优, 但算法的稳定性无法得到保证. 而其他的如深度迁移强化学习、分层深度强化学习、深度记忆强化学习和多智能体深度强化学习等算法都是现在的研究热点, 通过这些算法能应对更为复杂的场景问题、系统环境及控制任务, 是目前深度强化学习算法研究的前沿领域.</p>
<p>展望未来，人工智能开发者们需要尽可能掌握上述框架以及其中所使用的各类强化学习算法。此外，还需要强化自身对于多代理强化学习架构的理解，因为其中多种框架都大量利用前沿博弈论研究成果。最后，还需要熟悉深度强化学习知识。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E4%B9%9D%E7%AB%A0_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E4%B9%9D%E7%AB%A0_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" class="post-title-link" itemprop="url">第九章_图像分割</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:28:18 / 修改时间：12:28:42" itemprop="dateCreated datePublished" datetime="2020-03-03T12:28:18+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第九章-图像分割"><a href="#第九章-图像分割" class="headerlink" title="第九章 图像分割"></a>第九章 图像分割</h1><h2 id="9-1-图像分割算法分类？"><a href="#9-1-图像分割算法分类？" class="headerlink" title="9.1 图像分割算法分类？"></a>9.1 图像分割算法分类？</h2><p>图像分割是预测图像中每一个像素所属的类别或者物体。基于深度学习的图像分割算法主要分为两类：</p>
<p><strong>1.语义分割</strong></p>
<p>为图像中的每个像素分配一个类别，如把画面中的所有物体都指出它们各自的类别。</p>
<p><img src="/img/ch9/Semantic-01.png" alt=""></p>
<p><strong>2.实例分割</strong></p>
<p>与语义分割不同，实例分割只对特定物体进行类别分配，这一点与目标检测有点相似，但目标检测输出的是边界框和类别，而实例分割输出的是掩膜（mask）和类别。</p>
<p><img src="/img/ch9/Instance-01.png" alt=""></p>
<h2 id="9-2-传统的基于CNN的分割方法缺点？"><a href="#9-2-传统的基于CNN的分割方法缺点？" class="headerlink" title="9.2 传统的基于CNN的分割方法缺点？"></a>9.2 传统的基于CNN的分割方法缺点？</h2><p>传统的基于CNN的分割方法：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入，用于训练与预测，这种方法主要有几个缺点：<br>1）存储开销大，例如，对每个像素使用15 * 15的图像块，然后不断滑动窗口，将图像块输入到CNN中进行类别判断，因此，需要的存储空间随滑动窗口的次数和大小急剧上升；<br>2）效率低下，相邻像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算有很大程度上的重复；<br>3）像素块的大小限制了感受区域的大小，通常像素块的大小比整幅图像的大小小很多，只能提取一些局部特征，从而导致分类性能受到限制。<br>而全卷积网络(FCN)则是从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。</p>
<h2 id="9-3-FCN"><a href="#9-3-FCN" class="headerlink" title="9.3 FCN"></a>9.3 FCN</h2><h3 id="9-3-1-FCN改变了什么"><a href="#9-3-1-FCN改变了什么" class="headerlink" title="9.3.1 FCN改变了什么?"></a>9.3.1 FCN改变了什么?</h3><p>​    对于一般的分类CNN网络，如VGG和Resnet，都会在网络的最后加入一些全连接层，经过softmax后就可以获得类别概率信息。但是这个概率信息是1维的，即只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。<br>​    而FCN提出可以把后面几个全连接都换成卷积，这样就可以获得一张2维的feature map，后接softmax层获得每个像素点的分类信息，从而解决了分割问题，如图4。</p>
<p><img src="/img/ch9/figure_9.1.1_2.jpg" alt=""></p>
<p><center>图 4</center></p>
<h3 id="9-3-2-FCN网络结构？"><a href="#9-3-2-FCN网络结构？" class="headerlink" title="9.3.2 FCN网络结构？"></a>9.3.2 FCN网络结构？</h3><p>​    FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax输出）不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。<br>​    下图是语义分割所采用的全卷积网络(FCN)的结构示意图：</p>
<p><img src="/img/ch9/figure_9.1.2_1.jpg" alt=""></p>
<h3 id="9-3-3-全卷积网络举例？"><a href="#9-3-3-全卷积网络举例？" class="headerlink" title="9.3.3 全卷积网络举例？"></a>9.3.3 全卷积网络举例？</h3><p>​    通常CNN网络在卷积层之后会接上若干个全连接层, 将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。以AlexNet为代表的经典CNN结构适合于图像级的分类和回归任务，因为它们最后都得到整个输入图像的一个概率向量。</p>
<p><img src="/img/ch9/figure_9.1.3_1.jpg" alt=""></p>
<p>&emsp;&emsp;<br>如上图所示：<br>&emsp;&emsp;<br>（1）在CNN中, 猫的图片输入到AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高, 用来做分类任务。<br>&emsp;&emsp;<br>（2）FCN与CNN的区别在于把CNN最后的全连接层转换成卷积层，输出的是一张已经带有标签的图片, 而这个图片就可以做语义分割。<br>&emsp;&emsp;<br>（3）CNN的强大之处在于它的多层结构能自动学习特征，并且可以学习到多个层次的特征: 较浅的卷积层感知域较小，学习到一些局部区域的特征；较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征。高层的抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高, 所以我们常常可以将卷积层看作是特征提取器。</p>
<h3 id="9-2-4-全连接层和卷积层如何相互转化？"><a href="#9-2-4-全连接层和卷积层如何相互转化？" class="headerlink" title="9.2.4 全连接层和卷积层如何相互转化？"></a>9.2.4 全连接层和卷积层如何相互转化？</h3><p>&emsp;&emsp;<br><strong>两者相互转换的可能性：</strong><br>&emsp;&emsp;<br>全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：<br>&emsp;&emsp;<br>（1）对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块，其余部分都是零。而在其中大部分块中，元素都是相等的。<br>&emsp;&emsp;<br>（2）任何全连接层都可以被转化为卷积层。比如VGG16中第一个全连接层是25088 <em> 4096的数据尺寸，将它转化为512 </em> 7 <em> 7 </em> 4096的数据尺寸，即一个K=4096的全连接层，输入数据体的尺寸是7 <em> 7 </em> 512，这个全连接层可以被等效地看做一个F=7, P=0, S=1, K=4096 的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致7 <em> 7, 这样输出就变为1 </em> 1 <em> 4096, 本质上和全连接层的输出是一样的。<br>&emsp;&emsp;<br><em>*输出激活数据体深度是由卷积核的数目决定的(K=4096)。</em></em><br>&emsp;&emsp;<br>在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是227x227x3的图像，一系列的卷积层和下采样层将图像数据变为尺寸为7x7x512的激活数据体, AlexNet的处理方式为使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：<br>&emsp;&emsp;<br>（1）第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7,K=4096，这样输出数据体就为[1x1x4096]。<br>&emsp;&emsp;<br>（2）第二个全连接层，令其滤波器尺寸为F=1,K=4096，这样输出数据体为[1x1x4096]。<br>&emsp;&emsp;<br>（3）最后一个全连接层也做类似的，令其F=1,K=1000，最终输出为[1x1x1000]。</p>
<h3 id="9-2-5-为什么传统CNN的输入图片是固定大小？"><a href="#9-2-5-为什么传统CNN的输入图片是固定大小？" class="headerlink" title="9.2.5 为什么传统CNN的输入图片是固定大小？"></a>9.2.5 为什么传统CNN的输入图片是固定大小？</h3><p>&emsp;&emsp;<br>对于CNN，一幅输入图片在经过卷积和pooling层时，这些层是不关心图片大小的。比如对于一个卷积层，outputsize = (inputsize - kernelsize) / stride + 1，它并不关心inputsize多大，对于一个inputsize大小的输入feature map，滑窗卷积，输出outputsize大小的feature map即可。pooling层同理。但是在进入全连接层时，feature map（假设大小为n×n）要拉成一条向量，而向量中每个元素（共n×n个）作为一个结点都要与下一个层的所有结点（假设4096个）全连接，这里的权值个数是4096×n×n，而我们知道神经网络结构一旦确定，它的权值个数都是固定的，所以这个n不能变化，n是conv5的outputsize，所以层层向回看，每个outputsize都要固定，那每个inputsize都要固定，因此输入图片大小要固定。</p>
<h3 id="9-2-6-把全连接层的权重W重塑成卷积层的滤波器有什么好处？"><a href="#9-2-6-把全连接层的权重W重塑成卷积层的滤波器有什么好处？" class="headerlink" title="9.2.6 把全连接层的权重W重塑成卷积层的滤波器有什么好处？"></a>9.2.6 把全连接层的权重W重塑成卷积层的滤波器有什么好处？</h3><p>&emsp;&emsp;<br>这样的转化可以在单个向前传播的过程中, 使得卷积网络在一张更大的输入图片上滑动，从而得到多个输出(可以理解为一个label map)。<br>&emsp;&emsp;<br>比如: 我们想让224×224尺寸的浮窗，以步长为32在384×384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6×6个位置的类别得分, 那么通过将全连接层转化为卷积层之后的运算过程为:<br>&emsp;&emsp;<br>如果224×224的输入图片经过卷积层和下采样层之后得到了[7x7x512]的数组，那么，384×384的大图片直接经过同样的卷积层和下采样层之后会得到[12x12x512]的数组, 然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出((12 – 7)/1 + 1 = 6), 这个结果正是浮窗在原图经停的6×6个位置的得分。<br>&emsp;&emsp;<br>一个确定的CNN网络结构之所以要固定输入图片大小，是因为全连接层权值数固定，而该权值数和feature map大小有关, 但是FCN在CNN的基础上把1000个结点的全连接层改为含有1000个1×1卷积核的卷积层，经过这一层，还是得到二维的feature map，同样我们也不关心这个feature map大小, 所以对于输入图片的size并没有限制。<br>&emsp;&emsp;<br>如下图所示，FCN将传统CNN中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层:</p>
<p><img src="/img/ch9/figure_9.1.7_1.png" alt=""></p>
<p><center>一个分类网络</center><br><img src="/img/ch9/figure_9.1.7_2.png" alt=""></p>
<p><center>变为全卷积网络</center><br><img src="/img/ch9/figure_9.1.7_3.png" alt=""></p>
<p><center>End-to-end, pixels-to pixels网络</center><br><img src="/img/ch9/figure_9.1.7_4.jpg" alt=""></p>
<p>（1）全连接层转化为全卷积层 : 在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个不同类别的概率。FCN将这3层表示为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)。看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前CNN已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。<br>&emsp;&emsp;<br>（2）CNN中输入的图像大小是统一固定成227x227大小的图像，第一层pooling后为55x55，第二层pooling后图像大小为27x27，第五层pooling后的图像大小为13x13, 而FCN输入的图像是H <em> W大小，第一层pooling后变为原图大小的1/2，第二层变为原图大小的1/4，第五层变为原图大小的1/8，第八层变为原图大小的1/16。<br>&emsp;&emsp;<br>（3）经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到H/32 </em> W/32的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是我们最重要的高维特征图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大几次到原图像的大小。<br>&emsp;&emsp;<br>相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算优化模型，然后再对36个位置做预测，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。</p>
<h3 id="9-2-7-反卷积层理解"><a href="#9-2-7-反卷积层理解" class="headerlink" title="9.2.7 反卷积层理解"></a>9.2.7 反卷积层理解</h3><p>&emsp;&emsp;<br>Upsampling的操作可以看成是反卷积(deconvolutional)，卷积运算的参数和CNN的参数一样是在训练FCN模型的过程中通过bp算法学习得到。反卷积层也是卷积层，不关心input大小，滑窗卷积后输出output。deconv并不是真正的deconvolution（卷积的逆变换），最近比较公认的叫法应该是transposed convolution，deconv的前向传播就是conv的反向传播。<br>&emsp;&emsp;<br>反卷积参数: 利用卷积过程filter的转置（实际上就是水平和竖直方向上翻转filter）作为计算卷积前的特征图。<br>&emsp;&emsp;<br>反卷积的运算如下所示:<br>&emsp;&emsp;<br>蓝色是反卷积层的input，绿色是反卷积层的outputFull padding, transposed Full padding, transposed。</p>
<p><img src="/img/ch9/figure_9.1.8_1.png" alt=""></p>
<p><center>上图中的反卷积，input是2×2, output是4×4。     Zero padding, non-unit strides, transposed。</center><br><img src="/img/ch9/figure_9.1.8_2.png" alt=""></p>
<p><center>上图中的反卷积，input feature map是3×3, 转化后是5×5, output是5×5</center></p>
<h3 id="9-2-8-跳级-skip-结构"><a href="#9-2-8-跳级-skip-结构" class="headerlink" title="9.2.8 跳级(skip)结构"></a>9.2.8 跳级(skip)结构</h3><p>&emsp;&emsp;<br>对CNN的结果做处理，得到了dense prediction，而作者在试验中发现，得到的分割结果比较粗糙，所以考虑加入更多前层的细节信息，也就是把倒数第几层的输出和最后的输出做一个fusion，实际上也就是加和：</p>
<p><img src="/img/ch9/figure_9.1.9_1.png" alt=""><br>&emsp;&emsp;<br>实验表明，这样的分割结果更细致更准确。在逐层fusion的过程中，做到第三行再往下，结果又会变差，所以作者做到这里就停了。</p>
<h3 id="9-2-9-模型训练"><a href="#9-2-9-模型训练" class="headerlink" title="9.2.9 模型训练"></a>9.2.9 模型训练</h3><p>&emsp;&emsp;<br>（1）用AlexNet，VGG16或者GoogleNet训练好的模型做初始化，在这个基础上做fine-tuning，全部都fine-tuning，只需在末尾加上upsampling，参数的学习还是利用CNN本身的反向传播原理。<br>&emsp;&emsp;<br>（2）采用whole image做训练，不进行patchwise sampling。实验证明直接用全图已经很effective and efficient。<br>&emsp;&emsp;<br>（3）对class score的卷积层做全零初始化。随机初始化在性能和收敛上没有优势。<br><em>举例：</em><br>&emsp;&emsp;<br><em>FCN例子: 输入可为任意尺寸图像彩色图像；输出与输入尺寸相同，深度为：20类目标+背景=21，模型基于AlexNet。</em><br>&emsp;&emsp;<br><em>蓝色：卷积层。</em><br>&emsp;&emsp;<br><em>绿色：Max Pooling层。</em><br>&emsp;&emsp;<br><em>黄色: 求和运算, 使用逐数据相加，把三个不同深度的预测结果进行融合：较浅的结果更为精细，较深的结果更为鲁棒。</em><br>&emsp;&emsp;<br><em>灰色: 裁剪, 在融合之前，使用裁剪层统一两者大小, 最后裁剪成和输入相同尺寸输出。</em><br>&emsp;&emsp;<br><em>对于不同尺寸的输入图像，各层数据的尺寸（height，width）相应变化，深度（channel）不变。</em></p>
<p><img src="/img/ch9/figure_9.1.10_1.png" alt=""><br>&emsp;&emsp;<br>（1）全卷积层部分进行特征提取, 提取卷积层（3个蓝色层）的输出来作为预测21个类别的特征。</p>
<p>&emsp;&emsp;<br>（2）图中虚线内是反卷积层的运算, 反卷积层（3个橙色层）可以把输入数据尺寸放大。和卷积层一样，升采样的具体参数经过训练确定。    </p>
<p>&emsp;&emsp;&emsp;&emsp;<br>1) 以经典的AlexNet分类网络为初始化。最后两级是全连接（红色），参数弃去不用。</p>
<p><img src="/img/ch9/figure_9.1.10_2.png" alt=""><br>&emsp;&emsp;&emsp;&emsp;<br>2) 从特征小图（）预测分割小图（），之后直接升采样为大图。</p>
<p><img src="/img/ch9/figure_9.1.10_3.png" alt=""></p>
<p><center>反卷积（橙色）的步长为32，这个网络称为FCN-32s</center><br>&emsp;&emsp;&emsp;&emsp;<br>3) 升采样分为两次完成（橙色×2）, 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）融合进来。使用跳级结构提升精确性。</p>
<p><img src="/img/ch9/figure_9.1.10_4.png" alt=""></p>
<p><center>第二次反卷积步长为16，这个网络称为FCN-16s</center><br>&emsp;&emsp;&emsp;&emsp;<br>4) 升采样分为三次完成（橙色×3）, 进一步融合了第3个pooling层的预测结果。</p>
<p><img src="/img/ch9/figure_9.1.10_5.png" alt=""></p>
<p><center>第三次反卷积步长为8，记为FCN-8s</center><br>其他参数:<br>&emsp;&emsp;<br>minibatch：20张图片。<br>&emsp;&emsp;<br>learning rate：0.001。<br>&emsp;&emsp;<br>初始化：分类网络之外的卷积层参数初始化为0。<br>&emsp;&emsp;<br>反卷积参数初始化为bilinear插值。<br>&emsp;&emsp;<br>最后一层反卷积固定位bilinear插值不做学习。</p>
<p><img src="/img/ch9/figure_9.1.10_6.png" alt=""></p>
<h3 id="9-2-10-FCN缺点"><a href="#9-2-10-FCN缺点" class="headerlink" title="9.2.10 FCN缺点"></a>9.2.10 FCN缺点</h3><p>&emsp;&emsp;<br>（1）得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。<br>&emsp;&emsp;<br>（2）对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。</p>
<h2 id="9-3-U-Net"><a href="#9-3-U-Net" class="headerlink" title="9.3 U-Net"></a>9.3 U-Net</h2><p>&emsp;&emsp;<br>卷积网络被大规模应用在分类任务中，输出的结果是整个图像的类标签。然而，在许多视觉任务，尤其是生物医学图像处理领域，目标输出应该包括目标类别的位置，并且每个像素都应该有类标签。另外，在生物医学图像往往缺少训练图片。所以，Ciresan等人训练了一个卷积神经网络，用滑动窗口提供像素的周围区域（patch）作为输入来预测每个像素的类标签。这个网络有两个优点：<br>第一，输出结果可以定位出目标类别的位置；<br>第二，由于输入的训练数据是patches，这样就相当于进行了数据增广，解决了生物医学图像数量少的问题。<br>&emsp;&emsp;<br>但是，这个方法也有两个很明显缺点。<br>&emsp;&emsp;<br>第一，它很慢，因为这个网络必须训练每个patch，并且因为patch间的重叠有很多的冗余(冗余会造成什么影响呢？卷积核里面的W，就是提取特征的权重，两个块如果重叠的部分太多，这个权重会被同一些特征训练两次，造成资源的浪费，减慢训练时间和效率，虽然说会有一些冗余，训练集大了，准确率不就高了吗？可是你这个是相同的图片啊，重叠的东西都是相同的，举个例子，我用一张相同的图片训练20次，按照这个意思也是增大了训练集啊，可是会出现什么结果呢，很显然，会导致过拟合，也就是对你这个图片识别很准，别的图片就不一定了)。<br>&emsp;&emsp;<br>第二，定位准确性和获取上下文信息不可兼得。大的patches需要更多的max-pooling层这样减小了定位准确性(为什么？因为你是对以这个像素为中心的点进行分类，如果patch太大，最后经过全连接层的前一层大小肯定是不变的，如果你patch大就需要更多的pooling达到这个大小，而pooling层越多，丢失信息的信息也越多；小的patches只能看到很小的局部信息，包含的背景信息不够。<br>&emsp;&emsp;<br>这篇论文建立了一个更好全卷积方法。我们定义和扩展了这个方法它使用更少的训练图片但产生更精确的分割。</p>
<p><img src="/img/ch9/figure_9.2_1.png" alt="">   </p>
<p>&emsp;&emsp;<br>(1)    使用全卷积神经网络。(全卷积神经网络就是卷积取代了全连接层，全连接层必须固定图像大小而卷积不用，所以这个策略使得，你可以输入任意尺寸的图片，而且输出也是图片，所以这是一个端到端的网络。)<br>&emsp;&emsp;<br>(2)    左边的网络是收缩路径：使用卷积和maxpooling。<br>&emsp;&emsp;<br>(3)    右边的网络是扩张路径:使用上采样产生的特征图与左侧收缩路径对应层产生的特征图进行concatenate操作。（pooling层会丢失图像信息和降低图像分辨率且是不可逆的操作，对图像分割任务有一些影响，对图像分类任务的影响不大，为什么要做上采样？因为上采样可以补足一些图片的信息，但是信息补充的肯定不完全，所以还需要与左边的分辨率比较高的图片相连接起来（直接复制过来再裁剪到与上采样图片一样大小），这就相当于在高分辨率和更抽象特征当中做一个折衷，因为随着卷积次数增多，提取的特征也更加有效，更加抽象，上采样的图片是经历多次卷积后的图片，肯定是比较高效和抽象的图片，然后把它与左边不怎么抽象但更高分辨率的特征图片进行连接）。<br>&emsp;&emsp;<br>(4)    最后再经过两次反卷积操作，生成特征图，再用两个1X1的卷积做分类得到最后的两张heatmap,例如第一张表示的是第一类的得分，第二张表示第二类的得分heatmap,然后作为softmax函数的输入，算出概率比较大的softmax类，选择它作为输入给交叉熵进行反向传播训练。</p>
<p>下面是U-Net模型的代码实现：（贡献者：黄钦建－华南理工大学）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def get_unet():</span><br><span class="line">    inputs &#x3D; Input((img_rows, img_cols, 1))</span><br><span class="line">    conv1 &#x3D; Conv2D(32, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(inputs)</span><br><span class="line">    conv1 &#x3D; Conv2D(32, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv1)</span><br><span class="line">    pool1 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2))(conv1)</span><br><span class="line">    # pool1 &#x3D; Dropout(0.25)(pool1)</span><br><span class="line">    # pool1 &#x3D; BatchNormalization()(pool1)</span><br><span class="line"></span><br><span class="line">    conv2 &#x3D; Conv2D(64, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(pool1)</span><br><span class="line">    conv2 &#x3D; Conv2D(64, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv2)</span><br><span class="line">    pool2 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2))(conv2)</span><br><span class="line">    # pool2 &#x3D; Dropout(0.5)(pool2)</span><br><span class="line">    # pool2 &#x3D; BatchNormalization()(pool2)</span><br><span class="line"></span><br><span class="line">    conv3 &#x3D; Conv2D(128, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(pool2)</span><br><span class="line">    conv3 &#x3D; Conv2D(128, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv3)</span><br><span class="line">    pool3 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2))(conv3)</span><br><span class="line">    # pool3 &#x3D; Dropout(0.5)(pool3)</span><br><span class="line">    # pool3 &#x3D; BatchNormalization()(pool3)</span><br><span class="line"></span><br><span class="line">    conv4 &#x3D; Conv2D(256, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(pool3)</span><br><span class="line">    conv4 &#x3D; Conv2D(256, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv4)</span><br><span class="line">    pool4 &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2))(conv4)</span><br><span class="line">    # pool4 &#x3D; Dropout(0.5)(pool4)</span><br><span class="line">    # pool4 &#x3D; BatchNormalization()(pool4)</span><br><span class="line"></span><br><span class="line">    conv5 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(pool4)</span><br><span class="line">    conv5 &#x3D; Conv2D(512, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv5)</span><br><span class="line"></span><br><span class="line">    up6 &#x3D; concatenate([Conv2DTranspose(256, (2, 2), strides&#x3D;(</span><br><span class="line">        2, 2), padding&#x3D;&#39;same&#39;)(conv5), conv4], axis&#x3D;3)</span><br><span class="line">    # up6 &#x3D; Dropout(0.5)(up6)</span><br><span class="line">    # up6 &#x3D; BatchNormalization()(up6)</span><br><span class="line">    conv6 &#x3D; Conv2D(256, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(up6)</span><br><span class="line">    conv6 &#x3D; Conv2D(256, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv6)</span><br><span class="line"></span><br><span class="line">    up7 &#x3D; concatenate([Conv2DTranspose(128, (2, 2), strides&#x3D;(</span><br><span class="line">        2, 2), padding&#x3D;&#39;same&#39;)(conv6), conv3], axis&#x3D;3)</span><br><span class="line">    # up7 &#x3D; Dropout(0.5)(up7)</span><br><span class="line">    # up7 &#x3D; BatchNormalization()(up7)</span><br><span class="line">    conv7 &#x3D; Conv2D(128, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(up7)</span><br><span class="line">    conv7 &#x3D; Conv2D(128, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv7)</span><br><span class="line"></span><br><span class="line">    up8 &#x3D; concatenate([Conv2DTranspose(64, (2, 2), strides&#x3D;(</span><br><span class="line">        2, 2), padding&#x3D;&#39;same&#39;)(conv7), conv2], axis&#x3D;3)</span><br><span class="line">    # up8 &#x3D; Dropout(0.5)(up8)</span><br><span class="line">    # up8 &#x3D; BatchNormalization()(up8)</span><br><span class="line">    conv8 &#x3D; Conv2D(64, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(up8)</span><br><span class="line">    conv8 &#x3D; Conv2D(64, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv8)</span><br><span class="line"></span><br><span class="line">    up9 &#x3D; concatenate([Conv2DTranspose(32, (2, 2), strides&#x3D;(</span><br><span class="line">        2, 2), padding&#x3D;&#39;same&#39;)(conv8), conv1], axis&#x3D;3)</span><br><span class="line">    # up9 &#x3D; Dropout(0.5)(up9)</span><br><span class="line">    # up9 &#x3D; BatchNormalization()(up9)</span><br><span class="line">    conv9 &#x3D; Conv2D(32, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(up9)</span><br><span class="line">    conv9 &#x3D; Conv2D(32, (3, 3), activation&#x3D;&#39;relu&#39;, padding&#x3D;&#39;same&#39;)(conv9)</span><br><span class="line"></span><br><span class="line">    # conv9 &#x3D; Dropout(0.5)(conv9)</span><br><span class="line"></span><br><span class="line">    conv10 &#x3D; Conv2D(1, (1, 1), activation&#x3D;&#39;sigmoid&#39;)(conv9)</span><br><span class="line"></span><br><span class="line">    model &#x3D; Model(inputs&#x3D;[inputs], outputs&#x3D;[conv10])</span><br><span class="line"></span><br><span class="line">    model.compile(optimizer&#x3D;Adam(lr&#x3D;1e-5),</span><br><span class="line">                  loss&#x3D;dice_coef_loss, metrics&#x3D;[dice_coef])</span><br><span class="line"></span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>
<h2 id="9-4-SegNet"><a href="#9-4-SegNet" class="headerlink" title="9.4 SegNet"></a>9.4 SegNet</h2><p>&emsp;&emsp;<br>可训练的图像分割引擎，包含一个encoder网络，一个对应的decoder网络，衔接像素级分类层，解码网络与VGG16的13层卷积层相同。解码网络是将低分辨率的编码特征图映射到全分辨率的特征图。解码网络使用最大池化层的池化索引进行非线性上采样，上采样过程就不需要学习。上采样得到的稀疏图与可训练的滤波器卷积得到致密的特征图。<br>&emsp;&emsp;<br>使用池化层索引进行上采样的优势：<br>&emsp;&emsp;<br>1）提升边缘刻画度；<br>&emsp;&emsp;<br>2）减少训练的参数；<br>&emsp;&emsp;<br>3）这种上采样模式可以包含到任何编码-解码网络中。<br>&emsp;&emsp;<br>SegNet网络的结构如下图所示：</p>
<p><img src="/img/ch9/figure_9.3_1.jpg" alt="">  </p>
<p>&emsp;&emsp;<br>SegNet网络结构如图1所示，Input为输入图片，Output为输出分割的图像，不同颜色代表不同的分类。语义分割的重要性就在于不仅告诉你图片中某个东西是什么，而且告知你他在图片的位置。我们可以看到是一个对称网络，由中间绿色pooling层与红色upsampling层作为分割，左边是卷积提取高维特征，并通过pooling使图片变小，SegNet作者称为Encoder，右边是反卷积（在这里反卷积与卷积没有区别）与upsampling，通过反卷积使得图像分类后特征得以重现，upsampling使图像变大，SegNet作者称为Decoder，最后通过Softmax，输出不同分类的最大值。这就是大致的SegNet过程，下面对这个过程里面使用到的方法进行介绍。<br>&emsp;&emsp;<br>编码网络与滤波器族卷积得到特征图，进行BN，ReLU，最大池化。最大池化是为了获得空间小位移的平移不变。最大池化和下采样损失了边缘细节，因此，在编码过程中保存边缘信息很重要。考虑到内存原因，只保存最大池化索引，如最大特征值的位置。<br>&emsp;&emsp;<br>SegNet解码技术如下图所示：</p>
<p><img src="/img/ch9/figure_9.3_2.jpg" alt="">  </p>
<p>&emsp;&emsp;<br>解码网络使用保存的最大池化索引上采样，得到稀疏的特征图，将特征图与可训练的解码滤波器族卷积得到致密的特征图。之后进行BN。高维的特征图输入soft-max层，对每个像素进行分类，得到每个像素属于K类的概率。  图3中右边是FCN的解码技术，FCN对编码的特征图进行降维，降维后输入到解码网络，解码网络中，上采样使用反卷积实现，上采样的特征图与降维的编码图进行element-wise add得到最终的解码特征图。FCN解码模型需要存储编码特征图，在嵌入式设备中内存紧张。<br>&emsp;&emsp;<br>SegNet的Encoder过程中，卷积的作用是提取特征，SegNet使用的卷积为same卷积（详见卷积神经网络CNN（1）)，即卷积后不改变图片大小；在Decoder过程中，同样使用same卷积，不过卷积的作用是为upsampling变大的图像丰富信息，使得在Pooling过程丢失的信息可以通过学习在Decoder得到。SegNet中的卷积与传统CNN的卷积并没有区别。</p>
<h2 id="9-5-空洞卷积-Dilated-Convolutions"><a href="#9-5-空洞卷积-Dilated-Convolutions" class="headerlink" title="9.5 空洞卷积(Dilated Convolutions)"></a>9.5 空洞卷积(Dilated Convolutions)</h2><p>&emsp;&emsp;<br>在图像分割领域，图像输入到CNN（典型的网络比如FCN[3]）中，FCN先像传统的CNN那样对图像做卷积再pooling，降低图像尺寸的同时增大感受野，但是由于图像分割预测是pixel-wise的输出，所以要将pooling后较小的图像尺寸upsampling到原始的图像尺寸进行预测（upsampling一般采用deconv反卷积操作，deconv可参见知乎答案如何理解深度学习中的deconvolution networks？），之前的pooling操作使得每个pixel预测都能看到较大感受野信息。因此图像分割FCN中有两个关键，一个是pooling减小图像尺寸增大感受野，另一个是upsampling扩大图像尺寸。在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？答案就是dilated conv。<br>&emsp;&emsp;<br>以前的CNN主要问题总结：<br>&emsp;&emsp;<br>（1）Up-sampling / pooling layer<br>&emsp;&emsp;<br>（2）内部数据结构丢失；空间层级化信息丢失。<br>&emsp;&emsp;<br>（3）小物体信息无法重建 (假设有四个pooling layer 则 任何小于 2^4 = 16 pixel 的物体信息将理论上无法重建。)<br>&emsp;&emsp;<br>举例如下：</p>
<p><img src="/img/ch9/figure_9.3_3.png" alt=""></p>
<p><center>Dilated Convolution with a 3 x 3 kernel and dilation rate 2</center><br>&emsp;&emsp;<br>下面看一下dilated conv原始论文[4]中的示意图</p>
<p><img src="/img/ch9/figure_9.3_4.jpg" alt="">  </p>
<p>&emsp;&emsp;<br>(a)    图对应3x3的1-dilated conv，和普通的卷积操作一样，(b)图对应3x3的2-dilated conv，实际的卷积kernel size还是3x3，但是空洞为1，也就是对于一个7x7的图像patch，只有9个红色的点和3x3的kernel发生卷积操作，其余的点略过。也可以理解为kernel的size为7x7，但是只有图中的9个点的权重不为0，其余都为0。 可以看到虽然kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7（如果考虑到这个2-dilated conv的前一层是一个1-dilated conv的话，那么每个红点就是1-dilated的卷积输出，所以感受野为3x3，所以1-dilated和2-dilated合起来就能达到7x7的conv）,(c)图是4-dilated conv操作，同理跟在两个1-dilated和2-dilated conv的后面，能达到15x15的感受野。对比传统的conv操作，3层3x3的卷积加起来，stride为1的话，只能达到(kernel-1) * layer+1=7的感受野，也就是和层数layer成线性关系，而dilated conv的感受野是指数级的增长。<br>&emsp;&emsp;<br>dilated的好处是不做pooling损失信息的情况下，加大了感受野，让每个卷积输出都包含较大范围的信息。在图像需要全局信息或者语音文本需要较长的sequence信息依赖的问题中，都能很好的应用dilated conv，比如图像分割、语音合成WaveNet、机器翻译ByteNet中。</p>
<h2 id="9-6-RefineNet"><a href="#9-6-RefineNet" class="headerlink" title="9.6 RefineNet"></a>9.6 RefineNet</h2><p>&emsp;&emsp;<br>网络结构：<br>&emsp;&emsp;<br>RefineNet block的作用就是把不同resolution level的feature map进行融合。网络结构如下：</p>
<p><img src="/img/ch9/figure_9.4_1.png" alt=""><br>&emsp;&emsp;<br>最左边一栏就是FCN的encoder部分(文中是用的ResNet)，先把pretrained ResNet按feature map的分辨率分成四个ResNet blocks，然后向右把四个blocks分别作为4个path通过RefineNet block进行融合refine，最后得到一个refined feature map(接softmax再双线性插值输出)。<br>注意除了RefineNet-4，所有的RefineNet block都是二输入的，用于融合不同level做refine，而单输入的RefineNet-4可以看作是先对ResNet的一个task adaptation。  </p>
<p>&emsp;&emsp;<br><strong>RefineNet Block</strong><br>&emsp;&emsp;<br>接下来仔细看一下RefineNet block，可以看到主要组成部分是Residual convolution unit, Multi-resolution fusion, Chained residual pooling, Output convolutions. 切记这个block作用是融合多个level的feature map输出单个level的feature map，但具体的实现应该是和输入个数、shape无关的。</p>
<p><img src="/img/ch9/figure_9.4_2.png" alt=""> </p>
<p>&emsp;&emsp;<br>Residual convolution unit就是普通的去除了BN的residual unit；  </p>
<p>&emsp;&emsp;<br>Multi-resolution fusion是先对多输入的feature map都用一个卷积层进行adaptation(都化到最小的feature map的shape)，再上采样再做element-wise的相加。注意如果是像RefineNet-4那样的单输入block这一部分就直接pass了；</p>
<p>&emsp;&emsp;<br>Chained residual pooling中的ReLU对接下来池化的有效性很重要，还可以使模型对学习率的变化没这么敏感。这个链式结构能从很大范围区域上获取背景context。另外，这个结构中大量使用了identity mapping这样的连接，无论长距离或者短距离的，这样的结构允许梯度从一个block直接向其他任一block传播。</p>
<p>&emsp;&emsp;<br>Output convolutions就是输出前再加一个RCU。</p>
<h2 id="9-7-PSPNet"><a href="#9-7-PSPNet" class="headerlink" title="9.7 PSPNet"></a>9.7 PSPNet</h2><p>&emsp;&emsp;<br>场景解析对于无限制的开放词汇和不同场景来说是具有挑战性的.本文使用文中的pyramid pooling module实现基于不同区域的上下文集成，提出了PSPNet，实现利用上下文信息的能力进行场景解析。<br>&emsp;&emsp;<br>作者认为，FCN存在的主要问题是没有采取合适的策略来用全局的信息，本文的做法就是借鉴SPPNet来设计了PSPNet解决这个问题。<br>&emsp;&emsp;<br>很多State-of-the-art的场景解析框架都是基于FCN的.基于CNN的方法能够增强动态物体的理解，但是在无限制词汇和不同场景中仍然面临挑战.举个例子，如下图.</p>
<p><img src="/img/ch9/figure_9.6_1.png" alt=""><br>&emsp;&emsp;<br>FCN认为右侧框中是汽车，但是实际上是船，如果参考上下文的先验知识，就会发现左边是一个船屋，进而推断是框中是船.FCN存在的主要问题就是不能利用好全局的场景线索。  </p>
<p>&emsp;&emsp;<br>对于尤其复杂的场景理解，之前都是采用空间金字塔池化来做的，和之前方法不同（为什么不同，需要参考一下经典的金字塔算法），本文提出了pyramid scene parsing network(PSPNet)。<br>&emsp;&emsp;<br>本文的主要贡献如下:<br>&emsp;&emsp;<br>(1)    提出了PSPNet在基于FCN的框架中集成困难的上下文特征<br>&emsp;&emsp;<br>(2)    通过基于深度监督误差开发了针对ResNet的高效优化策略<br>&emsp;&emsp;<br>(3)    构建了一个用于state-of-the-art的场景解析和语义分割的实践系统（具体是什么？）<br>&emsp;&emsp;<br>通过观察FCN的结果，发现了如下问题：<br>&emsp;&emsp;<br>(1)    关系不匹配（Mismatched Relationship）<br>&emsp;&emsp;<br>(2)    易混淆的类别（Confusion Categories）<br>&emsp;&emsp;<br>(3)    不显眼的类别（Inconspicuous Classes）<br>&emsp;&emsp;<br>总结以上结果发现，以上问题部分或者全部与上下文关系和全局信息有关系，因此本文提出了PSPNet.框架如下:</p>
<p><img src="/img/ch9/figure_9.6_2.png" alt=""><br>&emsp;&emsp;<br>并且加入额外的深度监督 Loss</p>
<p><img src="/img/ch9/figure_9.6_3.png" alt=""></p>
<h2 id="9-8-DeepLab系列"><a href="#9-8-DeepLab系列" class="headerlink" title="9.8 DeepLab系列"></a>9.8 DeepLab系列</h2><h3 id="9-8-1-DeepLabv1"><a href="#9-8-1-DeepLabv1" class="headerlink" title="9.8.1 DeepLabv1"></a>9.8.1 DeepLabv1</h3><p>&emsp;&emsp;<br>DeepLab 是结合了深度卷积神经网络（DCNNs）和概率图模型（DenseCRFs）的方法。<br>&emsp;&emsp;<br>在实验中发现 DCNNs 做语义分割时精准度不够的问题，根本原因是 DCNNs 的高级特征的平移不变性，即高层次特征映射，根源于重复的池化和下采样。<br>&emsp;&emsp;<br>针对信号下采样或池化降低分辨率，DeepLab 是采用的 atrous（带孔）算法扩展感受野，获取更多的上下文信息。<br>&emsp;&emsp;<br>分类器获取以对象中心的决策是需要空间变换的不变性，这天然地限制了 DCNN 的定位精度，DeepLab 采用完全连接的条件随机场（CRF）提高模型捕获细节的能力。<br>&emsp;&emsp;<br>除空洞卷积和 CRFs 之外，论文使用的 tricks 还有 Multi-Scale features。其实就是 U-Net 和 FPN 的思想，在输入图像和前四个最大池化层的输出上附加了两层的 MLP，第一层是 128 个 3×3 卷积，第二层是 128 个 1×1 卷积。最终输出的特征与主干网的最后一层特征图融合，特征图增加 5×128=640 个通道。<br>&emsp;&emsp;<br>实验表示多尺度有助于提升预测结果，但是效果不如 CRF 明显。<br>&emsp;&emsp;<br>论文模型基于 VGG16，在 Titan GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在 PASCAL VOC-2012 达到 71.6% IOU accuracy。</p>
<h3 id="9-8-2-DeepLabv2"><a href="#9-8-2-DeepLabv2" class="headerlink" title="9.8.2 DeepLabv2"></a>9.8.2 DeepLabv2</h3><p>&emsp;&emsp;<br>DeepLabv2 是相对于 DeepLabv1 基础上的优化。DeepLabv1 在三个方向努力解决，但是问题依然存在：特征分辨率的降低、物体存在多尺度，DCNN 的平移不变性。<br>&emsp;&emsp;<br>因 DCNN 连续池化和下采样造成分辨率降低，DeepLabv2 在最后几个最大池化层中去除下采样，取而代之的是使用空洞卷积，以更高的采样密度计算特征映射。<br>&emsp;&emsp;<br>物体存在多尺度的问题，DeepLabv1 中是用多个 MLP 结合多尺度特征解决，虽然可以提供系统的性能，但是增加特征计算量和存储空间。<br>&emsp;&emsp;<br>论文受到 Spatial Pyramid Pooling (SPP) 的启发，提出了一个类似的结构，在给定的输入上以不同采样率的空洞卷积并行采样，相当于以多个比例捕捉图像的上下文，称为 ASPP (atrous spatial pyramid pooling) 模块。<br>&emsp;&emsp;<br>DCNN 的分类不变形影响空间精度。DeepLabv2 是采样全连接的 CRF 在增强模型捕捉细节的能力。<br>&emsp;&emsp;<br>论文模型基于 ResNet，在 NVidia Titan X GPU 上运行速度达到了 8FPS，全连接 CRF 平均推断需要 0.5s ，在耗时方面和 DeepLabv1 无差异，但在 PASCAL VOC-2012 达到 79.7 mIOU。</p>
<h3 id="9-8-3-DeepLabv3"><a href="#9-8-3-DeepLabv3" class="headerlink" title="9.8.3 DeepLabv3"></a>9.8.3 DeepLabv3</h3><p>&emsp;&emsp;<br>好的论文不止说明怎么做，还告诉为什么。DeepLab 延续到 DeepLabv3 系列，依然是在空洞卷积做文章，但是探讨不同结构的方向。<br>&emsp;&emsp;<br>DeepLabv3 论文比较了多种捕获多尺度信息的方式：</p>
<p><img src="/img/ch9/figure_9.6_4.png" alt=""> </p>
<p>&emsp;&emsp;<br>1.Image Pyramid：将输入图片放缩成不同比例，分别应用在 DCNN 上，将预测结果融合得到最终输出。<br>&emsp;&emsp;<br>2.Encoder-Decoder：利用 Encoder 阶段的多尺度特征，运用到 Decoder 阶段上恢复空间分辨率，代表工作有 FCN、SegNet、PSPNet 等工。<br>&emsp;&emsp;<br>3.Deeper w. Atrous Convolution：在原始模型的顶端增加额外的模块，例如 DenseCRF，捕捉像素间长距离信息。<br>&emsp;&emsp;<br>4.Spatial Pyramid Pooling：空间金字塔池化具有不同采样率和多种视野的卷积核，能够以多尺度捕捉对象。<br>&emsp;&emsp;<br>DeepLabv1-v2 都是使用带孔卷积提取密集特征来进行语义分割。但是为了解决分割对象的多尺度问题，DeepLabv3 设计采用多比例的带孔卷积级联或并行来捕获多尺度背景。<br>&emsp;&emsp;<br>此外，DeepLabv3 将修改之前提出的带孔空间金字塔池化模块，该模块用于探索多尺度卷积特征，将全局背景基于图像层次进行编码获得特征，取得 state-of-art 性能，在 PASCAL VOC-2012 达到 86.9 mIOU。</p>
<h3 id="9-8-4-DeepLabv3"><a href="#9-8-4-DeepLabv3" class="headerlink" title="9.8.4 DeepLabv3+"></a>9.8.4 DeepLabv3+</h3><p>&emsp;&emsp;<br>语义分割关注的问题:<br>&emsp;&emsp;<br>1、 实例对象多尺度问题。<br>&emsp;&emsp;<br>2、 因为深度网络存在stride=2的层，会导致feature分辨率下降，从而导致预测精度降低，而造成的边界信息丢失问题。<br>&emsp;&emsp;<br>deeplab V3新设计的aspp结构解决了问题1，deeplab v3+主要目的在于解决问题2。<br>&emsp;&emsp;<br>问题2 可以使用空洞卷积替代更多的pooling层来获取分辨率更高的feature。但是feature分辨率更高会极大增加运算量。以deeplab v3使用的resnet101为例，stride=16将造成后面9层feature变大，后面9层的计算量变为原来的2*2=4倍大。stride=8则更为恐怖，后面78层的计算量都会变大很多。<br>&emsp;&emsp;<br>解决方案：1、编解码器结构；2 Modified Aligned Xception</p>
<p><img src="/img/ch9/figure_9.6_5.png" alt="">   </p>
<p>&emsp;&emsp;<br>在deeplabv3基础上加入解码器。A是aspp结构，其中8x的上采样可以看做是一个解码器。B是编解码结构，它集合了高层和底层的特征。C就是本文采取的结构。<br>&emsp;&emsp;<br>方法：<br>&emsp;&emsp;<br>（1）Encoder-Decoder with Atrous Convolution</p>
<p><img src="/img/ch9/figure_9.6_6.png" alt="">  </p>
<p>&emsp;&emsp;<br>编码器采用deeplabv3。<br>&emsp;&emsp;<br>解码器部分：先从低层级选一个feature，将低层级的feature用1 <em> 1的卷积进行通道压缩（原本为256通道，或者512通道），目的在于减少低层级的比重。作者认为编码器得到的feature具有更丰富的信息，所以编码器的feature应该有更高的比重。 这样做有利于训练。<br>&emsp;&emsp;<br>再将编码器的输出上采样，使其分辨率与低层级feature一致。举个例子，如果采用resnet conv2 输出的feature，则这里要</em> 4上采样。将两种feature连接后，再进行一次3 * 3的卷积（细化作用），然后再次上采样就得到了像素级的预测。后面的实验结果表明这种结构在 stride=16 时既有很高的精度速度又很快。stride=8相对来说只获得了一点点精度的提升，但增加了很多的计算量。<br>&emsp;&emsp;<br>（2）Modified Aligned Xception<br>&emsp;&emsp;<br>Xception主要采用了deepwish seperable convolution来替换原来的卷积层。简单的说就是这种结构能在更少参数更少计算量的情况下学到同样的信息。这边则是考虑将原来的resnet-101骨架网换成xception。</p>
<p><img src="/img/ch9/figure_9.6_7.png" alt="">  </p>
<p>&emsp;&emsp;<br><strong>红色部分为修改</strong><br>&emsp;&emsp;<br>更多层：重复8次改为16次（基于MSRA目标检测的工作）。<br>&emsp;&emsp;<br>将原来简单的pool层改成了stride为2的deepwish seperable convolution。<br>&emsp;&emsp;<br>额外的RELU层和归一化操作添加在每个 3 × 3 depthwise convolution之后（原来只在1 * 1卷积之后）</p>
<h2 id="9-9-Mask-R-CNN"><a href="#9-9-Mask-R-CNN" class="headerlink" title="9.9 Mask-R-CNN"></a>9.9 Mask-R-CNN</h2><h3 id="9-9-1-Mask-RCNN-的网络结构示意图"><a href="#9-9-1-Mask-RCNN-的网络结构示意图" class="headerlink" title="9.9.1 Mask-RCNN 的网络结构示意图"></a>9.9.1 Mask-RCNN 的网络结构示意图</h3><p><img src="/img/ch9/figure_9.8_1.png" alt=""></p>
<p>&emsp;&emsp;<br>其中黑色部分为原来的Faster-RCNN，红色部分为在Faster网络上的修改：<br>&emsp;&emsp;<br>1）将ROI Pooling层替换成了ROIAlign；<br>&emsp;&emsp;<br>2）添加并列的FCN层（Mask层）；<br>&emsp;&emsp;<br>先来概述一下Mask-RCNN的几个特点（来自于Paper<a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noopener">Mask R-CNN</a>的Abstract）：<br>&emsp;&emsp;<br>1）在边框识别的基础上添加分支网络，用于语义Mask识别；<br>&emsp;&emsp;<br>2）训练简单，相对于Faster仅增加一个小的Overhead，可以跑到5FPS；<br>&emsp;&emsp;<br>3）可以方便的扩展到其他任务，比如人的姿态估计等；<br>&emsp;&emsp;<br>4）不借助Trick，在每个任务上，效果优于目前所有的 single-model entries；包括 COCO 2016 的Winners。</p>
<h3 id="9-9-2-RCNN行人检测框架"><a href="#9-9-2-RCNN行人检测框架" class="headerlink" title="9.9.2 RCNN行人检测框架"></a>9.9.2 RCNN行人检测框架</h3><p>&emsp;&emsp;<br>来看下后面两种RCNN方法与Mask结合的示意图:</p>
<p><img src="/img/ch9/figure_9.8_2.png" alt=""><br>&emsp;&emsp;<br>图中灰色部分是原来的RCNN结合ResNet or FPN的网络，下面黑色部分为新添加的并联Mask层，这个图本身与上面的图也没有什么区别，旨在说明作者所提出的Mask RCNN方法的泛化适应能力：可以和多种RCNN框架结合，表现都不错。</p>
<h3 id="9-9-3-Mask-RCNN-技术要点"><a href="#9-9-3-Mask-RCNN-技术要点" class="headerlink" title="9.9.3 Mask-RCNN 技术要点"></a>9.9.3 Mask-RCNN 技术要点</h3><p>&emsp;&emsp;<br><strong>1.技术要点1 - 强化的基础网络</strong><br>&emsp;&emsp;<br>通过ResNeXt-101+FPN用作特征提取网络，达到state-of-the-art的效果。<br>&emsp;&emsp;<br><strong>2.技术要点2 - ROIAlign</strong><br>&emsp;&emsp;<br>采用ROIAlign替代RoiPooling（改进池化操作）。引入了一个插值过程，先通过双线性插值到14<em>14，再pooling到7</em>7，很大程度上解决了仅通过Pooling直接采样带来的Misalignment对齐问题。<br>&emsp;&emsp;<br>PS： 虽然 Misalignment 在分类问题上影响并不大，但在 Pixel 级别的 Mask 上会存在较大误差。<br>&emsp;&emsp;<br>后面我们把结果对比贴出来（Table2 c &amp; d），能够看到 ROIAlign 带来较大的改进，可以看到，Stride 越大改进越明显。<br>&emsp;&emsp;<br><strong>3.技术要点3 - Loss Function</strong><br>&emsp;&emsp;<br>每个ROIAlign对应K <em> m^2维度的输出。K对应类别个数，即输出K个mask，m对应池化分辨率（7 </em> 7）。Loss函数定义：</p>
<script type="math/tex; mode=display">
Lmask(Cls_k)=Sigmoid(Cls_k)</script><p>&emsp;&emsp;<br>$Lmask(Cls_k) = Sigmoid (Cls_k)$，平均二值交叉熵 （average binary cross-entropy）Loss，通过逐像素的 Sigmoid 计算得到。<br>&emsp;&emsp;<br>Why K个mask？通过对每个 Class 对应一个Mask可以有效避免类间竞争（其他Class不贡献Loss）。</p>
<p><img src="/img/ch9/figure_9.8_3.png" alt=""><br>&emsp;&emsp;<br>通过结果对比来看（Table2 b），也就是作者所说的 Decouple 解耦，要比多分类的Softmax效果好很多。<br>&emsp;&emsp;<br>另外，作者给出了很多实验分割效果，就不都列了，只贴一张和FCIS的对比图（FCIS出现了Overlap的问题）</p>
<p><img src="/img/ch9/figure_9.8_4.png" alt=""></p>
<h2 id="9-10-CNN在基于弱监督学习的图像分割中的应用"><a href="#9-10-CNN在基于弱监督学习的图像分割中的应用" class="headerlink" title="9.10 CNN在基于弱监督学习的图像分割中的应用"></a>9.10 CNN在基于弱监督学习的图像分割中的应用</h2><p>&emsp;&emsp;<br>答案来源：<a href="https://zhuanlan.zhihu.com/p/23811946" target="_blank" rel="noopener">CNN在基于弱监督学习的图像分割中的应用</a>  </p>
<p>&emsp;&emsp;<br>最近基于深度学习的图像分割技术一般依赖于卷积神经网络CNN的训练，训练过程中需要非常大量的标记图像，即一般要求训练图像中都要有精确的分割结果。<br>&emsp;&emsp;<br>对于图像分割而言，要得到大量的完整标记过的图像非常困难，比如在ImageNet数据集上，有1400万张图有类别标记，有50万张图给出了bounding box,但是只有4460张图像有像素级别的分割结果。对训练图像中的每个像素做标记非常耗时，特别是对医学图像而言，完成对一个三维的CT或者MRI图像中各组织的标记过程需要数小时。<br>&emsp;&emsp;<br>如果学习算法能通过对一些初略标记过的数据集的学习就能完成好的分割结果，那么对训练数据的标记过程就很简单，这可以大大降低花在训练数据标记上的时间。这些初略标记可以是：<br>&emsp;&emsp;<br>1、只给出一张图像里面包含哪些物体，<br>&emsp;&emsp;<br>2、给出某个物体的边界框，<br>&emsp;&emsp;<br>3、对图像中的物体区域做部分像素的标记，例如画一些线条、涂鸦等（scribbles)。</p>
<h3 id="9-10-1-Scribble标记"><a href="#9-10-1-Scribble标记" class="headerlink" title="9.10.1 Scribble标记"></a>9.10.1 Scribble标记</h3><p>&emsp;&emsp;<br>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_ScribbleSup_Scribble-Supervised_Convolutional_CVPR_2016_paper.pdf" target="_blank" rel="noopener">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation (CVPR 2016)</a><br>&emsp;&emsp;<br>香港中文大学的Di Lin提出了一个基于Scribble标记的弱监督学习方法。Scribble是一个很方便使用的标记方法，因此被用得比较广泛。如下图，只需要画五条线就能完成对一副图像的标记工作。</p>
<p><img src="/img/ch9/figure_9.9_1.png" alt=""><br>&emsp;&emsp;<br>ScribbleSup分为两步，第一步将像素的类别信息从scribbles传播到其他未标记的像素，自动完成所有的训练图像的标记工作； 第二步使用这些标记图像训练CNN。在第一步中，该方法先生成super-pxels, 然后基于graph cut的方法对所有的super-pixel进行标记。</p>
<p><img src="/img/ch9/figure_9.9_2.png" alt="">  </p>
<p>&emsp;&emsp;<br>Graph Cut的能量函数为：</p>
<script type="math/tex; mode=display">
\sum_{i}\psi _i\left(y_i|X,S\right)+\sum_{i,j}\psi_{ij}\left(y_i,y_j,X\right)</script><p>&emsp;&emsp;<br>在这个graph中，每个super-pixel是graph中的一个节点，相接壤的super-pixel之间有一条连接的边。这个能量函数中的一元项包括两种情况，一个是来自于scribble的，一个是来自CNN对该super-pixel预测的概率。整个最优化过程实际上是求graph cut能量函数和CNN参数联合最优值的过程：</p>
<script type="math/tex; mode=display">
\sum_{i}\psi _i^{scr}\left(y_i|X,S\right)+\sum _i-logP\left(y_i| X,\theta\right)+\sum_{i,j}\psi _{ij}\left(y_i,y_j|X\right)</script><p>&emsp;&emsp;<br>上式的最优化是通过交替求 $Y$ 和 $\theta$ 的最优值来实现的。文章中发现通过三次迭代就能得到比较好的结果。</p>
<p><img src="/img/ch9/figure_9.9_3.png" alt=""></p>
<h3 id="9-10-2-图像级别标记"><a href="#9-10-2-图像级别标记" class="headerlink" title="9.10.2 图像级别标记"></a>9.10.2 图像级别标记</h3><p>&emsp;&emsp;<br>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pathak_Constrained_Convolutional_Neural_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Constrained Convolutional Neural Networks for Weakly Supervised Segmentation （ICCV 2015）</a><br>&emsp;&emsp;<br>UC Berkeley的Deepak Pathak使用了一个具有图像级别标记的训练数据来做弱监督学习。训练数据中只给出图像中包含某种物体，但是没有其位置信息和所包含的像素信息。该文章的方法将image tags转化为对CNN输出的label分布的限制条件，因此称为 Constrained convolutional neural network (CCNN).</p>
<p><img src="/img/ch9/figure_9.9_4.png" alt=""><br>&emsp;&emsp;</p>
<p>该方法把训练过程看作是有线性限制条件的最优化过程：</p>
<script type="math/tex; mode=display">
\underset{\theta ,P}{minimize}\qquad D(P(X)||Q(X|\theta ))\\
subject\to\qquad A\overrightarrow{P} \geqslant \overrightarrow{b},\sum_{X}^{ }P(X)=1</script><p>&emsp;&emsp;</p>
<p>其中的线性限制条件来自于训练数据上的标记，例如一幅图像中前景类别像素个数期望值的上界或者下界（物体大小）、某个类别的像素个数在某图像中为0，或者至少为1等。该目标函数可以转化为为一个loss function，然后通过SGD进行训练。</p>
<p><img src="/img/ch9/figure_9.9_5.png" alt=""><br>&emsp;&emsp;<br>实验中发现单纯使用Image tags作为限制条件得到的分割结果还比较差，在PASCAL VOC 2012 test数据集上得到的mIoU为35.6%，加上物体大小的限制条件后能达到45.1%，如果再使用bounding box做限制，可以达到54%。FCN-8s可以达到62.2%，可见弱监督学习要取得好的结果还是比较难。     </p>
<h3 id="9-10-3-DeepLab-bounding-box-image-level-labels"><a href="#9-10-3-DeepLab-bounding-box-image-level-labels" class="headerlink" title="9.10.3 DeepLab+bounding box+image-level labels**"></a>9.10.3 DeepLab+bounding box+image-level labels**</h3><p>&emsp;&emsp;<br>论文地址：<a href="https://arxiv.org/pdf/1502.02734.pdf" target="_blank" rel="noopener">Weakly-and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation</a><br>&emsp;&emsp;<br>Google的George Papandreou 和UCLA的Liang-Chieh Chen等在DeepLab的基础上进一步研究了使用bounding box和image-level labels作为标记的训练数据。使用了期望值最大化算法（EM）来估计未标记的像素的类别和CNN的参数。</p>
<p><img src="/img/ch9/figure_9.9_6.png" alt=""><br>&emsp;&emsp;<br>对于image-level标记的数据，我们可以观测到图像的像素值和图像级别的标记 ,但是不知道每个像素的标号,因此把$y$当做隐变量。使用如下的概率图模式：</p>
<script type="math/tex; mode=display">
P\left ( x,y,z;\theta \right ) = P\left ( x \right )\left (\prod_{m=1}^{M} P\left ( y_m|x;\theta \right )\right )P\left ( z|y \right )</script><p>&emsp;&emsp;<br>这篇论文是通过EM算法来学习模型的参数$\theta$，具体推导过程可参考原论文。</p>
<p><img src="/img/ch9/figure_9.9_7.png" alt=""><br>&emsp;&emsp;<br>对于给出bounding box标记的训练图像，该方法先使用CRF对该训练图像做自动分割，然后在分割的基础上做全监督学习。通过实验发现，单纯使用图像级别的标记得到的分割效果较差，但是使用bounding box的训练数据可以得到较好的结果，在VOC2012 test数据集上得到mIoU 62.2%。另外如果使用少量的全标记图像和大量的弱标记图像进行结合，可以得到与全监督学习(70.3%)接近的分割结果(69.0%)。     </p>
<h3 id="9-10-4-统一的框架"><a href="#9-10-4-统一的框架" class="headerlink" title="9.10.4 统一的框架"></a>9.10.4 统一的框架</h3><p>&emsp;&emsp;<br>论文地址：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xu_Learning_to_Segment_2015_CVPR_paper.pdf" target="_blank" rel="noopener">Learning to Segment Under Various Forms of Weak Supervision (CVPR 2015)</a> </p>
<p>&emsp;&emsp;<br>Wisconsin-Madison大学的Jia Xu提出了一个统一的框架来处理各种不同类型的弱标记：图像级别的标记、bounding box和部分像素标记如scribbles。该方法把所有的训练图像分成共计$n$个super-pixel，对每个super-pixel提取一个$d$维特征向量。因为不知道每个super-pixel所属的类别，相当于无监督学习，因此该方法对所有的super-pixel做聚类，使用的是最大间隔聚类方法(max-margin clustering, MMC),该过程的最优化目标函数是：</p>
<script type="math/tex; mode=display">
\underset{W,H}{min} \qquad  \frac{1}{2}tr\left ( W^TW \right ) + \lambda\sum_{p=1}^{n}\sum_{c=1}^{C}\xi \left ( w_c;x_p;h_p^c \right)</script><p>&emsp;&emsp;<br>在这个目标函数的基础上，根据不同的弱标记方式，可以给出不同的限制条件，因此该方法就是在相应的限制条件下求最大间隔聚类。</p>
<p><img src="/img/ch9/figure_9.9_8.png" alt="">  </p>
<p>&emsp;&emsp;<br>该方法在Siftflow数据集上得到了比较好的结果，比state-of-the-art的结果提高了10%以上。</p>
<p>&emsp;&emsp;<br>小结：在弱标记的数据集上训练图像分割算法可以减少对大量全标记数据的依赖，在大多数应用中会更加贴合实际情况。弱标记可以是图像级别的标记、边框和部分像素的标记等。训练的方法一般看做是限制条件下的最优化方法。另外EM算法可以用于CNN参数和像素类别的联合求优。</p>
<h3 id="9-10-5-弱监督分割最新进展（贡献者：明奇-北京理工大学）"><a href="#9-10-5-弱监督分割最新进展（贡献者：明奇-北京理工大学）" class="headerlink" title="9.10.5 弱监督分割最新进展（贡献者：明奇-北京理工大学）"></a>9.10.5 弱监督分割最新进展（贡献者：明奇-北京理工大学）</h3><ul>
<li><strong>bbox监督</strong></li>
</ul>
<ol>
<li><p>Learning to Segment via Cut-and-Paste（ECCV 2018）</p>
<p>利用GAN对抗学习的思想，在cut-paste思想指导下利用bbox弱监督进行实例分割。<br><img src="/img/ch9/9.10.5-1.png" alt=""><br>采用对抗学习的思想，网络主体分为两大部分：mask生成器和合成图像判别器。具体过程为：（1）在图像上截取gt，经过特征提取后预测一个bbox内gt的mask；（2）在原图上随机cut一个背景图像，将bbox内按照生成的mask提取出物体分割结果，然后paste到原图裁剪的背景上去；（3）合成的图像经过判别器进行真假判断。<br>通过生成器生成更好mask来使得判别器更难判别，在对抗学习中提升两者的性能，逐渐获得更好的结果 .</p>
</li>
<li><p>Simple Does It: Weakly Supervised Instance and Semantic Segmentation（CVPR2017）<br>本文做的是bbox弱监督语义/实例分割任务，能达到全监督分割效果(DeepLabv1)的95%。主要工作为：讨论了使用弱监督语义标签进行迭代训练的方法，以及其限制和不足之处；证明了通过类似GrabCut的算法能通过bbox生成分割训练标签方法的可行性，可以避免像上面的迭代方法重新调整网络训练策略；在VOC数据集上逼近监督学习的分割任务效果。<br>作者的启发是：将bbox level的mask送入网络训练后得到分割mask的比输入的bbox mask要好（这是很好的insight）。因此启发的操作是：将bbox level标注作为初始mask输入优化，每次得到的标注作为gt进行下一轮的迭代，从而不断获得更好的效果。效果图如下：<br><img src="/img/ch9/9.10.5-3.png" alt=""><br>在此基础上，再加上优化的GrabCut+算法，以及部分区域的筛选，以及BSDS500的边界预测信息整合到一起，能够达到很好的弱监督迭代分割效果。</p>
</li>
</ol>
<ul>
<li><strong>分类监督</strong></li>
</ul>
<ol>
<li><p>Weakly Supervised Learning of Instance Segmentation with Inter-pixel Relations(CVPR2019)<br>使用分类标注作为弱监督信息，在CAM提取到特征的基础上，进一步设计IRNet学习额外的特征约束，从而到达更好的弱监督实例分割效果。为了解决CAM应用到实例分割的上述局限，设计IRNet。其组成为两部分：（1）不分类别的实例响应图  （2）pairwise semantic affinitie。其中通过不分类别的实例响应图和CAM结合，约束后得到instance-wise CAMS；另一个分支预先预测物体的边界然后得到pairwise semantic affinitie（关于这个的论文参考Related Work的对应部分，有相应的方法，暂时不深究）进行融合和处理得到最终的分割。整体流程如下：<br><img src="/img/ch9/9.10.5-2.png" alt=""> </p>
</li>
<li><p>Weakly Supervised Instance Segmentation using Class Peak Response（CVPR2018）<br>本文使用图像级的类别标注监督信息，通过探索类别响应峰值使分类网络能够很好地提取实例分割mask。本工作是使用图像级标注进行弱监督实例分割的首个工作。<br>在分类监督信息之下，CNN网络会产生一个类别响应图，每个位置是类别置信度分数。其局部极大值往往具有实例很强视觉语义线索。首先将类别峰值响应图的信息进行整合，然后反向传播将其映射到物体实例信息量较大的区域如边界。上述从类别极值响应图产生的映射图称为Peak Response Maps (PRMs)，该图提供了实例物体的详细表征，可以很好地用作分割监督信息。<br>具体流程如图：<br><img src="/img/ch9/9.10.5-4.png" alt=""><br>首先将图片经过正常的分类网络训练，其中在类别预测响应图上提取出局部响应极值点，进行增强卷积后预测出PRM。然后结合多种信息进行推断生成mask。</p>
</li>
<li><p>Weakly Supervised Semantic Segmentation Using Superpixel Pooling Network（AAAI 2017）<br>本文介绍通过类别标注的标签实现弱监督语义分割的方法。该方法在语义分割mask生成和使用生成mask学习分割生成网络之间反复交替。要实现这种交替迭代学习，关键点就是如何利用类别标注得到较准确的初始分割。为了解决这一问题,提出了Superpixel Pooling Network (SPN)，将输入图像的超像素分割结果作为低阶结构的表征，辅助语义分割的推断。<br><img src="/img/ch9/9.10.5-5.png" alt=""><br>首先是SPN生成初始mask，然后用另一个网络DecoupledNet来学习每个像素的mask标注。其中，该分割网络将语义分割任务解耦为分类和分割两个子任务，并且能够从类别标注中学习形状先验知识用于辅助分割。</p>
</li>
</ol>
<h2 id="9-11-DenseNet（贡献者：黄钦建－华南理工大学）"><a href="#9-11-DenseNet（贡献者：黄钦建－华南理工大学）" class="headerlink" title="9.11 DenseNet（贡献者：黄钦建－华南理工大学）"></a>9.11 DenseNet（贡献者：黄钦建－华南理工大学）</h2><p>&emsp;&emsp;<br>这篇论文是CVPR2017年的最佳论文。</p>
<p>&emsp;&emsp;<br>卷积神经网络结构的设计主要朝着两个方向发展，一个是更宽的网络（代表：GoogleNet、VGG），一个是更深的网络（代表：ResNet）。但是随着层数的加深会出现一个问题——梯度消失，这将会导致网络停止训练。到目前为止解决这个问题的思路基本都是在前后层之间加一个identity connections(short path)。</p>
<p><img src="/img/ch9/9-10-3.png" alt=""></p>
<p>&emsp;&emsp;<br>由上图中可知Resnet是做值的相加（也就是add操作），通道数是不变的。而DenseNet是做通道的合并（也就是Concatenation操作），就像Inception那样。从这两个公式就可以看出这两个网络的本质不同。此外DensetNet的前面一层输出也是后面所有层的输入，这也不同于ResNet残差网络。</p>
<p><img src="/img/ch9/9-10-1.png" alt=""></p>
<p>&emsp;&emsp;<br>DenseNet的Block结构如上图所示。</p>
<p>&emsp;&emsp;<br>1*1卷积核的目的：减少输入的特征图数量，这样既能降维减少计算量，又能融合各个通道的特征。我们将使用BottleNeck Layers的DenseNet表示为DenseNet-B。(在论文的实验里，将1×1×n小卷积里的n设置为4k，k为每个H产生的特征图数量)</p>
<p><img src="/img/ch9/9-10-2.png" alt=""></p>
<p>&emsp;&emsp;<br>上图是DenseNet网络的整体网络结构示意图。其中1*1卷积核的目的是进一步压缩参数，并且在Transition Layer层有个参数Reduction（范围是0到1），表示将这些输出缩小到原来的多少倍，默认是0.5，这样传给下一个Dense Block的时候channel数量就会减少一半。当Reduction的值小于1的时候，我们就把带有这种层的网络称为DenseNet-C。</p>
<p>&emsp;&emsp;<br>DenseNet网络的优点包括：</p>
<ul>
<li>减轻了梯度消失</li>
<li>加强了feature的传递</li>
<li>更有效地利用了feature </li>
<li>一定程度上较少了参数数量</li>
<li>一定程度上减轻了过拟合</li>
</ul>
<h2 id="9-12-图像分割的常用数据集"><a href="#9-12-图像分割的常用数据集" class="headerlink" title="9.12 图像分割的常用数据集"></a>9.12 图像分割的常用数据集</h2><h3 id="9-12-1-PASCAL-VOC"><a href="#9-12-1-PASCAL-VOC" class="headerlink" title="9.12.1 PASCAL VOC"></a>9.12.1 PASCAL VOC</h3><p>VOC 数据集分为20类，包括背景为21类，分别如下： </p>
<ul>
<li>Person: person </li>
<li>Animal: bird, cat, cow, dog, horse, sheep </li>
<li>Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train </li>
<li>Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor</li>
</ul>
<p>VOC 数据集中用于分割比赛的图片实例如下，包含原图以及图像分类分割和图像物体分割两种图（PNG格式）。图像分类分割是在20种物体中，ground-turth图片上每个物体的轮廓填充都有一个特定的颜色，一共20种颜色。</p>
<p><img src="/img/ch9/VOC-01.png" alt=""></p>
<h3 id="9-12-2-MS-COCO"><a href="#9-12-2-MS-COCO" class="headerlink" title="9.12.2 MS COCO"></a>9.12.2 MS COCO</h3><p>MS COCO 是最大图像分割数据集，提供的类别有 80 类，有超过 33 万张图片，其中 20 万张有标注，整个数据集中个体的数目超过 150 万个。MS COCO是目前难度最大，挑战最高的图像分割数据集。</p>
<p><img src="/img/ch9/COCO-01.png" alt=""></p>
<h3 id="9-12-3-Cityscapes"><a href="#9-12-3-Cityscapes" class="headerlink" title="9.12.3 Cityscapes"></a>9.12.3 Cityscapes</h3><p>Cityscapes 是驾驶领域进行效果和性能测试的图像分割数据集，它包含了5000张精细标注的图像和20000张粗略标注的图像，这些图像包含50个城市的不同场景、不同背景、不同街景，以及30类涵盖地面、建筑、交通标志、自然、天空、人和车辆等的物体标注。Cityscapes评测集有两项任务：像素级（Pixel-level）图像场景分割（以下简称语义分割）与实例级（Instance-level）图像场景分割（以下简称实例分割）。</p>
<p><img src="/img/ch9/Cityscapes-01.png" alt=""></p>
<h2 id="9-13-全景分割（贡献者：北京理工大学—明奇）"><a href="#9-13-全景分割（贡献者：北京理工大学—明奇）" class="headerlink" title="9.13 全景分割（贡献者：北京理工大学—明奇）"></a>9.13 全景分割（贡献者：北京理工大学—明奇）</h2><p>全景分割的开山之作：何恺明的<em>Panoptic Segmentation</em></p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>&emsp;&emsp;语义分割通过带孔全卷积网络，根据不同的stuff进行划分；实例分割则是在目标检测的基础上基于检测框进行物体的分割。缺少一种框架可以将两者进行融合实现既能分割背景又能分割实例，而这在自动驾驶和AR技术中大有作为。由此提出的全景分割任务能将两者进行结合。        </p>
<p>&emsp;&emsp;全景分割的思路很直观：为图像的每个像素分配语义label和类内实例id，前者用于区分语义信息，后者用于分割实例（因此stuff不具有实例id）。提出全景分割时，只是启发式地将语意分割和实例分割两种任务的输出进行后处理的融合（如NMS），并以此建立PS任务的baseline。为了评价全景分割的质量，提出panoptic quality (PQ) 标准，将背景和物体的评价纳入一个完整的框架下。示意图如下：<br><img src="/img/ch9/9.13-1.png" alt=""></p>
<ol>
<li><strong>Panoptic Segmentation</strong> </li>
</ol>
<ul>
<li><p><strong>Task format</strong><br>全景分割的标注方法：<br>像素级的标注，标出类别label和类内实例id。如果某像素的这两个信息都能匹配，则可以将该像素匹配到某个类别和实例中去；类外的像素可以分配空标签，即并不是所有的像素都要有语义类别。</p>
</li>
<li><p><strong>Stuff and thing labels</strong><br>对于stuff和thing（背景填充和物体实例）的标签，交集是空集，并集是所有可能的label空间。这两者是互相独立不相关的（很好理解，像素属于那个类和它属于哪个物体不具有相关性）。</p>
</li>
<li><p><strong>Relationship</strong><br>都是像素级的label，需要为每个像素分配对应的标签。但是实例分割基于region的，允许重叠的segmentation，而全景分割和语义分割一样是像素级的label，不允许重叠标签的出现。</p>
</li>
<li><p><strong>Confidence scores</strong><br>这一点上更像语义分割而不是实例分割，对于PS不需要置信分数评价分割质量。提到这个，作者认为语义分割和全景分割可以直接利用人工标注的label进行对比从而评价当前mask的质量；而实例分割在选择mask时评价的是分类置信度，这个并没有人工标注进行参考，因此难以把握。   </p>
</li>
</ul>
<ol>
<li><strong>Panoptic Segmentation Metric</strong><br>&emsp;&emsp;用于衡量全景分割效果的指标应具有：完备性；可解释性；简洁性。由是提出了PQ指标，可分为两步：分割匹配、在匹配上进行计算PQ。    </li>
</ol>
<p>3.1  <strong>Segment Matching</strong><br>&emsp;&emsp;定义match：预测的segmentation和gt的iou大于0.5，说明两者can match。再结合全景分割的不可重叠性，不难得到：最多只有一个预测的segmentation可以match gt。    </p>
<p>3.2  <strong>PQ Computation</strong><br>&emsp;&emsp;PQ的计算类似mAP，也是类内求取，然后求类间的平均值，以便不敏感类别不平衡。对于每一类，可以根据gt与预测的segmentation分为三类（下图描述）：<br><img src="/img/ch9/9.13-2.png" alt=""></p>
<p>TP: 预测为正，实际为正，描述match较好的<br>FP: 预测为正，实际为负，描述match错的<br>FN: 预测为负，实际为正，描述没match出来的gt<br>&emsp;&emsp;通过上述三类可以计算得到PQ值公式：<br><img src="/img/ch9/9.13-3.png" alt=""></p>
<p>式中出去FP与FN后，剩下的式子描述的是match的segmentation的平均IoU，加上FP与FN是为了惩罚match失败的分割实例。<br>有意思的是，对上述式子进行简单的恒等变化：<br><img src="/img/ch9/9.13-4.png" alt=""></p>
<p>第一项评价的是match分割的质量，第二项类似于F1得分。因此可以PQ分解为：  </p>
<script type="math/tex; mode=display">PQ=SQ*RQ</script><ul>
<li><p><strong>Void labels</strong><br>gt中可能出现两种像素标注为空的情况：超出类别的像素和模糊不清的像素（难以分类）。在评估结果时，这些空的标签不予以评估。具体而言：<br>（1）在matching部分，预测出为void的像素会被移出prediction并不参与IoU计算；<br>（2）matching后，unmatched prediction按照一般情况会计算FP FN，但是对于空标签情况，如果该prediction含有的void像素块超过一定匹配阈值就会被移除，并不算作FP计算得分。   </p>
</li>
<li><p><strong>Group labels</strong><br>有时区分相同语义类别的实例个体标注比较困难，因此有提出组标签的标注方法。但对于PQ计算而言：<br>（1）matching部分不使用组标签，而是严格区分实例<br>（2）matching后，对于包含一部分相同类别像素点的unmatched predicted segments，这一部分将被去除并不视作false positives    </p>
</li>
</ul>
<p>3.3  <strong>Comparison to Existing Metrics</strong></p>
<ul>
<li><p><strong>Semantic segmentation metrics</strong><br>衡量语义分割的标准有像素级精度，平均精度，IoU。但是其只专注于像素级的划分，不能反映物体实例级别的分割性能。</p>
</li>
<li><p><strong>Instance segmentation metrics</strong><br>度量为AP，主要是引入了置信度分数confidence score对检测目标进行打分。（两者不是完全的隔绝，实例分割也有用IoU监督的，而confidence score是否能够反映mask的真实质量也有存疑过，这个标准也不是固定的）</p>
</li>
<li><p><strong>Panoptic quality</strong><br>PQ的度量可以分解成SQ和RQ，SQ反映了语义分割的像素级IoU性能，RQ专注于检测识别的效果，因此将两者统一到一个框架下。</p>
</li>
</ul>
<p>分割效果：<br><img src="/img/ch9/9.13-5.png" alt=""></p>
<p><br><br><br></p>
<p><hr /><br>TODO</p>
<ul>
<li>[ ] 图像分割数据集标注工具</li>
<li>[ ] 图像分割评价标准</li>
<li>[x] 全景分割</li>
<li>[ ] UNet++</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%85%AB%E7%AB%A0_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%85%AB%E7%AB%A0_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" class="post-title-link" itemprop="url">第八章_目标检测</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:27:39 / 修改时间：12:28:02" itemprop="dateCreated datePublished" datetime="2020-03-03T12:27:39+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第八章-目标检测"><a href="#第八章-目标检测" class="headerlink" title="第八章  目标检测"></a>第八章  目标检测</h1><h2 id="8-1-基本概念"><a href="#8-1-基本概念" class="headerlink" title="8.1 基本概念"></a>8.1 基本概念</h2><h3 id="8-1-1-什么是目标检测？"><a href="#8-1-1-什么是目标检测？" class="headerlink" title="8.1.1 什么是目标检测？"></a>8.1.1 什么是目标检测？</h3><p>​    目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。</p>
<p>​    计算机视觉中关于图像识别有四大类任务：</p>
<p><strong>分类-Classification</strong>：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。</p>
<p><strong>定位-Location</strong>：解决“在哪里？”的问题，即定位出这个目标的的位置。</p>
<p><strong>检测-Detection</strong>：解决“是什么？在哪里？”的问题，即定位出这个目标的的位置并且知道目标物是什么。</p>
<p><strong>分割-Segmentation</strong>：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。</p>
<p><img src="/img/ch8/8.1.1.png" alt="图像识别四大类任务，图像来源于cs231n 2016课件Lecture 8"></p>
<h3 id="8-1-2-目标检测要解决的核心问题？"><a href="#8-1-2-目标检测要解决的核心问题？" class="headerlink" title="8.1.2 目标检测要解决的核心问题？"></a>8.1.2 目标检测要解决的核心问题？</h3><p>除了图像分类之外，目标检测要解决的核心问题是：</p>
<p>1.目标可能出现在图像的任何位置。</p>
<p>2.目标有各种不同的大小。</p>
<p>3.目标可能有各种不同的形状。</p>
<h3 id="8-1-3-目标检测算法分类？"><a href="#8-1-3-目标检测算法分类？" class="headerlink" title="8.1.3 目标检测算法分类？"></a>8.1.3 目标检测算法分类？</h3><p>基于深度学习的目标检测算法主要分为两类：</p>
<p><strong>1.Two stage目标检测算法</strong></p>
<p>​    先进行区域生成（region proposal，RP）（一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。</p>
<p>​    任务：特征提取—&gt;生成RP—&gt;分类/定位回归。</p>
<p>​    常见的two stage目标检测算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN和R-FCN等。</p>
<p><strong>2.One stage目标检测算法</strong></p>
<p>​    不用RP，直接在网络中提取特征来预测物体分类和位置。</p>
<p>​    任务：特征提取—&gt;分类/定位回归。</p>
<p>​    常见的one stage目标检测算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD和RetinaNet等。</p>
<p><img src="/img/ch8/8.1.2.png" alt=""></p>
<h3 id="8-1-4-目标检测有哪些应用？"><a href="#8-1-4-目标检测有哪些应用？" class="headerlink" title="8.1.4 目标检测有哪些应用？"></a>8.1.4 目标检测有哪些应用？</h3><p>​    目标检测具有巨大的实用价值和应用前景。应用领域包括人脸检测、行人检测、车辆检测、飞机航拍或卫星图像中道路的检测、车载摄像机图像中的障碍物检测、医学影像在的病灶检测等。还有在安防领域中，可以实现比如安全帽、安全带等动态检测，移动侦测、区域入侵检测、物品看护等功能。</p>
<h2 id="8-2-Two-Stage目标检测算法"><a href="#8-2-Two-Stage目标检测算法" class="headerlink" title="8.2 Two Stage目标检测算法"></a>8.2 Two Stage目标检测算法</h2><h3 id="8-2-1-R-CNN"><a href="#8-2-1-R-CNN" class="headerlink" title="8.2.1 R-CNN"></a>8.2.1 R-CNN</h3><p><strong>R-CNN有哪些创新点？</strong></p>
<ol>
<li>使用CNN（ConvNet）对 region proposals 计算 feature vectors。从经验驱动特征（SIFT、HOG）到数据驱动特征（CNN feature map），提高特征对样本的表示能力。</li>
<li>采用大样本下（ILSVRC）有监督预训练和小样本（PASCAL）微调（fine-tuning）的方法解决小样本难以训练甚至过拟合等问题。</li>
</ol>
<p>注：ILSVRC其实就是众所周知的ImageNet的挑战赛，数据量极大；PASCAL数据集（包含目标检测和图像分割等），相对较小。</p>
<p><strong>R-CNN 介绍</strong></p>
<p>​    R-CNN作为R-CNN系列的第一代算法，其实没有过多的使用“深度学习”思想，而是将“深度学习”和传统的“计算机视觉”的知识相结合。比如R-CNN pipeline中的第二步和第四步其实就属于传统的“计算机视觉”技术。使用selective search提取region proposals，使用SVM实现分类。</p>
<p><img src="/img/ch8/8.2.1-1.png" alt=""></p>
<p>原论文中R-CNN pipeline只有4个步骤，光看上图无法深刻理解R-CNN处理机制，下面结合图示补充相应文字</p>
<ol>
<li><p>预训练模型。选择一个预训练 （pre-trained）神经网络（如AlexNet、VGG）。</p>
</li>
<li><p>重新训练全连接层。使用需要检测的目标重新训练（re-train）最后全连接层（connected layer）。</p>
</li>
<li><p>提取 proposals并计算CNN 特征。利用选择性搜索（Selective Search）算法提取所有proposals（大约2000幅images），调整（resize/warp）它们成固定大小，以满足 CNN输入要求（因为全连接层的限制），然后将feature map 保存到本地磁盘。</p>
<p><img src="/img/ch8/8.1.4.png" alt=""></p>
</li>
<li><p>训练SVM。利用feature map 训练SVM来对目标和背景进行分类（每个类一个二进制SVM）</p>
</li>
<li><p>边界框回归（Bounding boxes Regression）。训练将输出一些校正因子的线性回归分类器</p>
</li>
</ol>
<p><img src="/img/ch8/8.1.5.png" alt=""></p>
<p><strong>R-CNN 实验结果</strong></p>
<p>R-CNN在VOC 2007测试集上mAP达到58.5%，打败当时所有的目标检测算法。</p>
<p><img src="/img/ch8/8.1.6.png" alt=""></p>
<h3 id="8-2-2-Fast-R-CNN"><a href="#8-2-2-Fast-R-CNN" class="headerlink" title="8.2.2 Fast R-CNN"></a>8.2.2 Fast R-CNN</h3><p><strong>Fast R-CNN有哪些创新点？</strong></p>
<ol>
<li>只对整幅图像进行一次特征提取，避免R-CNN中的冗余特征提取</li>
<li>用RoI pooling层替换最后一层的max pooling层，同时引入建议框数据，提取相应建议框特征</li>
<li>Fast R-CNN网络末尾采用并行的不同的全连接层，可同时输出分类结果和窗口回归结果，实现了end-to-end的多任务训练【建议框提取除外】，也不需要额外的特征存储空间【R-CNN中的特征需要保持到本地，来供SVM和Bounding-box regression进行训练】</li>
<li>采用SVD对Fast R-CNN网络末尾并行的全连接层进行分解，减少计算复杂度，加快检测速度。</li>
</ol>
<p><strong>Fast R-CNN 介绍</strong></p>
<p>​    Fast R-CNN是基于R-CNN和SPPnets进行的改进。SPPnets，其创新点在于计算整幅图像的the shared feature map，然后根据object proposal在shared feature map上映射到对应的feature vector（就是不用重复计算feature map了）。当然，SPPnets也有缺点：和R-CNN一样，训练是多阶段（multiple-stage pipeline）的，速度还是不够”快”，特征还要保存到本地磁盘中。</p>
<p>将候选区域直接应用于特征图，并使用RoI池化将其转化为固定大小的特征图块。以下是Fast R-CNN的流程图</p>
<p><img src="/img/ch8/8.2.2-1.png" alt=""></p>
<p><strong>RoI Pooling层详解</strong></p>
<p>因为Fast R-CNN使用全连接层，所以应用RoI Pooling将不同大小的ROI转换为固定大小。</p>
<p>RoI Pooling 是Pooling层的一种，而且是针对RoI的Pooling，其特点是输入特征图尺寸不固定，但是输出特征图尺寸固定（如7x7）。</p>
<p><strong>什么是RoI呢？</strong></p>
<p>RoI是Region of Interest的简写，一般是指图像上的区域框，但这里指的是由Selective Search提取的候选框。</p>
<p><img src="/img/ch8/8.2.2-2.png" alt=""></p>
<p>往往经过RPN后输出的不止一个矩形框，所以这里我们是对多个RoI进行Pooling。</p>
<p><strong>RoI Pooling的输入</strong></p>
<p>输入有两部分组成： </p>
<ol>
<li>特征图（feature map）：指的是上面所示的特征图，在Fast RCNN中，它位于RoI Pooling之前，在Faster RCNN中，它是与RPN共享那个特征图，通常我们常常称之为“share_conv”； </li>
<li>RoIs，其表示所有RoI的N*5的矩阵。其中N表示RoI的数量，第一列表示图像index，其余四列表示其余的左上角和右下角坐标。</li>
</ol>
<p>在Fast RCNN中，指的是Selective Search的输出；在Faster RCNN中指的是RPN的输出，一堆矩形候选框，形状为1x5x1x1（4个坐标+索引index），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）。其实关于ROI的坐标理解一直很混乱，到底是根据谁的坐标来。其实很好理解，我们已知原图的大小和由Selective Search算法提取的候选框坐标，那么根据”映射关系”可以得出特征图（featurwe map）的大小和候选框在feature map上的映射坐标。至于如何计算，其实就是比值问题，下面会介绍。所以这里把ROI理解为原图上各个候选框（region proposals），也是可以的。</p>
<p>注：说句题外话，由Selective Search算法提取的一系列可能含有object的bounding box，这些通常称为region proposals或者region of interest（ROI）。</p>
<p><strong>RoI的具体操作</strong></p>
<ol>
<li><p>根据输入image，将ROI映射到feature map对应位置</p>
<p>注：映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”，得到了feature map上的box坐标</p>
</li>
<li><p>将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）</p>
</li>
<li><p>对每个sections进行max pooling操作</p>
</li>
</ol>
<p>这样我们就可以从不同大小的方框得到固定大小的相应 的feature maps。值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。RoI Pooling 最大的好处就在于极大地提高了处理速度。</p>
<p><strong>RoI Pooling的输出</strong></p>
<p>输出是batch个vector，其中batch的值等于RoI的个数，vector的大小为channel <em> w </em> h；RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框。</p>
<p><strong>RoI Pooling示例</strong></p>
<p><img src="/img/ch8/8.1.11.gif" alt=""></p>
<h3 id="8-2-3-Faster-R-CNN"><a href="#8-2-3-Faster-R-CNN" class="headerlink" title="8.2.3 Faster R-CNN"></a>8.2.3 Faster R-CNN</h3><p><strong>Faster R-CNN有哪些创新点？</strong></p>
<p>Fast R-CNN依赖于外部候选区域方法，如选择性搜索。但这些算法在CPU上运行且速度很慢。在测试中，Fast R-CNN需要2.3秒来进行预测，其中2秒用于生成2000个ROI。Faster R-CNN采用与Fast R-CNN相同的设计，只是它用内部深层网络代替了候选区域方法。新的候选区域网络（RPN）在生成ROI时效率更高，并且以每幅图像10毫秒的速度运行。<br><img src="/img/ch8/8.2.3-1.png" alt=""> </p>
<p>图8.1.13 Faster R-CNN的流程图<br>Faster R-CNN的流程图与Fast R-CNN相同，采用外部候选区域方法代替了内部深层网络。<br><img src="/img/ch8/8.2.3-2.png" alt=""> </p>
<p>图8.1.14<br><strong>候选区域网络</strong></p>
<p>候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个3×3的卷积核，以使用卷积网络（如下所示的ZF网络）构建与类别无关的候选区域。其他深度网络（如VGG或ResNet）可用于更全面的特征提取，但这需要以速度为代价。ZF网络最后会输出256个值，它们将馈送到两个独立的全连接层，以预测边界框和两个objectness分数，这两个objectness分数度量了边界框是否包含目标。我们其实可以使用回归器计算单个objectness分数，但为简洁起见，Faster R-CNN使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。<br><img src="/img/ch8/8.2.3-3.png" alt=""> </p>
<p>图8.1.15<br>对于特征图中的每一个位置，RPN会做k次预测。因此，RPN将输出4×k个坐标和每个位置上2×k个得分。下图展示了8×8的特征图，且有一个3×3的卷积核执行运算，它最后输出8×8×3个ROI（其中k=3）。下图（右）展示了单个位置的3个候选区域。<br><img src="/img/ch8/8.2.3-4.png" alt=""></p>
<p>图8.1.16<br>假设最好涵盖不同的形状和大小。因此，Faster R-CNN不会创建随机边界框。相反，它会预测一些与左上角名为锚点的参考框相关的偏移量（如x, y）。我们限制这些偏移量的值，因此我们的猜想仍然类似于锚点。<br><img src="/img/ch8/8.1.17.png" alt=""> </p>
<p>图8.1.17<br>要对每个位置进行k个预测，我们需要以每个位置为中心的k个锚点。每个预测与特定锚点相关联，但不同位置共享相同形状的锚点。<br><img src="/img/ch8/8.2.3-6.png" alt=""> </p>
<p>图8.1.18<br>这些锚点是精心挑选的，因此它们是多样的，且覆盖具有不同比例和宽高比的现实目标。这使得我们可以用更好的猜想来指导初始训练，并允许每个预测专门用于特定的形状。该策略使早期训练更加稳定和简便。<br><img src="/img/ch8/8.2.3-7.png" alt=""></p>
<p>图8.1.19<br>Faster R-CNN使用更多的锚点。它部署9个锚点框：3个不同宽高比的3个不同大小的锚点（Anchor）框。每一个位置使用9个锚点，每个位置会生成2×9个objectness分数和4×9个坐标。</p>
<h3 id="8-2-4-R-FCN"><a href="#8-2-4-R-FCN" class="headerlink" title="8.2.4 R-FCN"></a>8.2.4 R-FCN</h3><p><strong>R-FCN有哪些创新点？</strong></p>
<p>R-FCN 仍属于two-stage 目标检测算法：RPN+R-FCN</p>
<ol>
<li>Fully convolutional</li>
<li>位置敏感得分图（position-sentive score maps）</li>
</ol>
<blockquote>
<p>our region-based detector is <strong>fully convolutional</strong> with almost all computation shared on the entire image. To achieve this goal, we propose <strong>position-sensitive score maps</strong> to address a dilemma between translation-invariance in image classification and translation-variance in object detection.</p>
</blockquote>
<p>R-FCN backbone：ResNet</p>
<p>ResNet-101+R-FCN：83.6% in PASCAL VOC 2007 test datasets</p>
<p>既提高了mAP，又加快了检测速度</p>
<pre><code>假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。现在我们回顾一下所有问题。在Faster R-CNN中，检测器使用了多个全连接层进行预测。如果有2000个ROI，那么成本非常高。R-FCN通过减少每个ROI所需的工作量实现加速。上面基于区域的特征图与ROI是独立的，可以在每个ROI之外单独计算。剩下的工作就比较简单了，因此R-FCN的速度比Faster R-CNN快。
</code></pre><p><img src="/img/ch8/8.2.4-1.png" alt=""></p>
<pre><code>图8.2.1 人脸检测
现在我们来看一下5×5的特征图M，内部包含一个蓝色方块。我们将方块平均分成3×3个区域。现在，我们在M中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元[2,2]处于激活状态。在左侧创建一个新的特征图，用于检测目标的左上角。
</code></pre><p><img src="/img/ch8/8.2.4-2.png" alt=""> </p>
<pre><code>图8.2.2 检测示例
我们将方块分成9个部分，由此创建了9个特征图，每个用来检测对应的目标区域。这些特征图叫做位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。
</code></pre><p><img src="/img/ch8/8.2.4-3.png" alt=""></p>
<pre><code>图8.2.3生成9个得分图
下图中红色虚线矩形是建议的ROI。我们将其分割成3×3个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角ROI区域包含左眼的概率。我们将结果存储成3×3 vote数组，如下图（右）所示。例如，vote_array[0][0]包含左上角区域是否包含目标对应部分的得分。
</code></pre><p><img src="/img/ch8/8.2.4-4.png" alt=""> </p>
<pre><code>图8.2.4
将ROI应用到特征图上，输出一个3x3数组。将得分图和ROI映射到vote数组的过程叫做位置敏感ROI池化（position-sensitive ROI-pool）。该过程与前面讨论过的ROI池化非常接近。
</code></pre><p><img src="/img/ch8/8.2.4-5.png" alt=""> </p>
<pre><code>图8.2.5
将ROI的一部分叠加到对应的得分图上，计算V[i][j]。在计算出位置敏感ROI池化的所有值后，类别得分是其所有元素得分的平均值。
</code></pre><p><img src="/img/ch8/8.2.6.png" alt=""></p>
<pre><code>图8.2.6 ROI池化
假如我们有C个类别要检测。我们将其扩展为C+1个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有3×3个得分图，因此一共有(C+1)×3×3个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。以下是数据流图，在本案例中，k=3。
</code></pre><p><img src="/img/ch8/8.2.7.png" alt=""> </p>
<pre><code>图8.2.7
</code></pre><h3 id="8-2-5-FPN"><a href="#8-2-5-FPN" class="headerlink" title="8.2.5 FPN"></a>8.2.5 FPN</h3><p><strong>FPN有哪些创新点？</strong></p>
<ol>
<li>多层特征</li>
<li>特征融合</li>
</ol>
<p>解决目标检测中的多尺度问题，通过简单的网络连接改变，在基本不增加原有模型计算量的情况下，大幅度提升小物体（small object）检测的性能。</p>
<p>在物体检测里面，有限计算量情况下，网络的深度（对应到感受野）与 stride 通常是一对矛盾的东西，常用的网络结构对应的 stride 一般会比较大（如 32），而图像中的小物体甚至会小于 stride 的大小，造成的结果就是小物体的检测性能急剧下降。传统解决这个问题的思路包括：</p>
<ol>
<li>图像金字塔（image pyramid），即多尺度训练和测试。但该方法计算量大，耗时较久。</li>
<li>特征分层，即每层分别预测对应的scale分辨率的检测结果，如SSD算法。该方法强行让不同层学习同样的语义信息，但实际上不同深度对应于不同层次的语义特征，浅层网络分辨率高，学到更多是细节特征，深层网络分辨率低，学到更多是语义特征。</li>
</ol>
<p>因而，目前多尺度的物体检测主要面临的挑战为：</p>
<ol>
<li>如何学习具有强语义信息的多尺度特征表示？</li>
<li>如何设计通用的特征表示来解决物体检测中的多个子问题？如 object proposal, box localization, instance segmentation.</li>
<li>如何高效计算多尺度的特征表示？</li>
</ol>
<p>FPN网络直接在Faster R-CNN单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义（rich semantic）特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。作者接下来实验了将 FPN 应用在 Faster RCNN 上的性能，在 COCO 上达到了 state-of-the-art 的单模型精度。在RPN上，FPN增加了8.0个点的平均召回率（average recall，AR）；在后面目标检测上，对于COCO数据集，FPN增加了2.3个点的平均精确率（average precision，AP），对于VOC数据集，FPN增加了3.8个点的AP。</p>
<p><img src="img/ch8/FPN-01.png" alt=""></p>
<p>FPN算法主要由三个模块组成，分别是：</p>
<ol>
<li>Bottom-up pathway（自底向上线路）</li>
<li>Lareral connections（横向链接）</li>
<li>Top-down path（自顶向下线路）</li>
</ol>
<p><img src="img/ch8/FPN-02.png" alt=""></p>
<p><strong>Bottom-up pathway</strong></p>
<p>FPN是基于Faster R-CNN进行改进，其backbone是ResNet-101，FPN主要应用在Faster R-CNN中的RPN（用于bouding box proposal generation）和Fast R-CNN（用于object detection）两个模块中。</p>
<p>其中 RPN 和 Fast RCNN 分别关注的是召回率（recall）和精确率（precision），在这里对比的指标分别为 Average Recall(AR) 和 Average Precision(AP)。</p>
<p>注：Bottom-up可以理解为自底向上，Top-down可以理解为自顶向下。这里的下是指low-level，上是指high-level，分别对应于提取的低级（浅层）特征和高级语义（高层）特征。</p>
<p>Bottom-up pathway 是卷积网络的前向传播过程。在前向传播过程中，feature map的大小可以在某些层发生改变。一些尺度（scale）因子为2，所以后一层feature map的大小是前一层feature map大小的二分之一，根据此关系进而构成了feature pyramid（hierarchy）。</p>
<p>然而还有很多层输出的feature map是一样的大小（即不进行缩放的卷积），作者将这些层归为同一 stage。对于feature pyramid，作者为每个stage定义一个pyramid level。</p>
<p>作者将每个stage的最后一层的输出作为feature map，然后不同stage进行同一操作，便构成了feature pyramid。</p>
<p>具体来说，对于ResNets-101，作者使用了每个stage的最后一个残差结构的特征激活输出。将这些残差模块输出表示为{C2, C3, C4, C5}，对应于conv2，conv3，conv4和conv5的输出，并且注意它们相对于输入图像具有{4, 8, 16, 32}像素的步长。考虑到内存占用，没有将conv1包含在金字塔中。</p>
<p><img src="img/ch8/FPN-03.png" alt=""></p>
<p><strong>Top-down pathway and lateral connections</strong></p>
<p>Top-town pathway是上采样（upsampling）过程。而later connection（横向连接）是将上采样的结果和bottom-up pathway生成的相同大小的feature map进行融合（merge）。</p>
<p>注：上采样尺度因子为2，因为为了和之前下采样卷积的尺度因子=2一样。上采样是放大，下采样是缩小。</p>
<p>具体操作如下图所示，上采样（2x up）feature map与相同大小的bottom-up feature map进行逐像素相加融合（element-wise addition），其中bottom-up feature先要经过1x1卷积层，目的是为了减少通道维度（reduce channel dimensions）。</p>
<p>注：减少通道维度是为了将bottom-up feature map的通道数量与top-down feature map的通道数量保持一致，又因为两者feature map大小一致，所以可以进行对应位置像素的叠加（element-wise addition）。</p>
<p><img src="img/ch8/FPN-04.png" alt=""></p>
<h3 id="8-2-6-Mask-R-CNN"><a href="#8-2-6-Mask-R-CNN" class="headerlink" title="8.2.6 Mask R-CNN"></a>8.2.6 Mask R-CNN</h3><p><strong>Mask R-CNN有哪些创新点？</strong></p>
<ol>
<li>Backbone：ResNeXt-101+FPN</li>
<li>RoI Align替换RoI Pooling</li>
</ol>
<p>Mask R-CNN是一个实例分割（Instance segmentation）算法，主要是在目标检测的基础上再进行分割。Mask R-CNN算法主要是Faster R-CNN+FCN，更具体一点就是ResNeXt+RPN+RoI Align+Fast R-CNN+FCN。</p>
<p><img src="img/ch8/Mask R-CNN-01.png" alt=""></p>
<p><strong>Mask R-CNN算法步骤</strong></p>
<ol>
<li>输入一幅你想处理的图片，然后进行对应的预处理操作，或者预处理后的图片；</li>
<li>将其输入到一个预训练好的神经网络中（ResNeXt等）获得对应的feature map；</li>
<li>对这个feature map中的每一点设定预定个的RoI，从而获得多个候选RoI；</li>
<li>将这些候选的RoI送入RPN网络进行二值分类（前景或背景）和BB回归，过滤掉一部分候选的RoI；</li>
<li>对这些剩下的RoI进行RoI Align操作（即先将原图和feature map的pixel对应起来，然后将feature map和固定的feature对应起来）；</li>
<li>对这些RoI进行分类（N类别分类）、BB回归和MASK生成（在每一个RoI里面进行FCN操作）。</li>
</ol>
<p><strong>RoI Pooling和RoI Align有哪些不同？</strong></p>
<p>ROI Align 是在Mask-RCNN中提出的一种区域特征聚集方式，很好地解决了RoI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。实验显示，在检测测任务中将 RoI Pooling 替换为 RoI Align 可以提升检测模型的准确性。</p>
<p>在常见的两级检测框架（比如Fast-RCNN，Faster-RCNN，RFCN）中，RoI Pooling 的作用是根据预选框的位置坐标在特征图中将相应区域池化为固定尺寸的特征图，以便进行后续的分类和包围框回归操作。由于预选框的位置通常是由模型回归得到的，一般来讲是浮点数，而池化后的特征图要求尺寸固定。故RoI Pooling这一操作存在两次量化的过程。</p>
<ul>
<li>将候选框边界量化为整数点坐标值。</li>
<li>将量化后的边界区域平均分割成 $k\times k$ 个单元(bin),对每一个单元的边界进行量化。</li>
</ul>
<p>事实上，经过上述两次量化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度。在论文里，作者把它总结为“不匹配问题（misalignment）”。</p>
<p>下面我们用直观的例子具体分析一下上述区域不匹配问题。如下图所示，这是一个Faster-RCNN检测框架。输入一张$800\times 800$的图片，图片上有一个$665\times 665$的包围框（框着一只狗）。图片经过主干网络提取特征后，特征图缩放步长（stride）为32。因此，图像和包围框的边长都是输入时的1/32。800正好可以被32整除变为25。但665除以32以后得到20.78，带有小数，于是RoI Pooling 直接将它量化成20。接下来需要把框内的特征池化$7\times 7$的大小，因此将上述包围框平均分割成$7\times 7$个矩形区域。显然，每个矩形区域的边长为2.86，又含有小数。于是ROI Pooling 再次把它量化到2。经过这两次量化，候选区域已经出现了较明显的偏差（如图中绿色部分所示）。更重要的是，该层特征图上0.1个像素的偏差，缩放到原图就是3.2个像素。那么0.8的偏差，在原图上就是接近30个像素点的差别，这一差别不容小觑。</p>
<p><img src="img/ch8/Mask R-CNN-02.png" alt=""></p>
<p>为了解决RoI Pooling的上述缺点，作者提出了RoI Align这一改进的方法(如图2)。</p>
<p><img src="img/ch8/Mask R-CNN-03.png" alt=""></p>
<p>RoI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值，从而将整个特征聚集过程转化为一个连续的操作。值得注意的是，在具体的算法操作上，RoI Align并不是简单地补充出候选区域边界上的坐标点，然后将这些坐标点进行池化，而是重新设计了一套比较优雅的流程，如下图所示：</p>
<ol>
<li><p>遍历每一个候选区域，保持浮点数边界不做量化。</p>
</li>
<li><p>将候选区域分割成$k\times k$个单元，每个单元的边界也不做量化。</p>
</li>
<li><p>在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。</p>
</li>
</ol>
<p>这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果采样点数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。在相关实验中，作者发现将采样点设为4会获得最佳性能，甚至直接设为1在性能上也相差无几。事实上，RoI Align 在遍历取样点的数量上没有RoI Pooling那么多，但却可以获得更好的性能，这主要归功于解决了mis alignment的问题。值得一提的是，我在实验时发现，RoI Align在VOC 2007数据集上的提升效果并不如在COCO上明显。经过分析，造成这种区别的原因是COCO上小目标的数量更多，而小目标受mis alignment问题的影响更大（比如，同样是0.5个像素点的偏差，对于较大的目标而言显得微不足道，但是对于小目标，误差的影响就要高很多）。</p>
<p><img src="img/ch8/Mask R-CNN-04.png" alt=""></p>
<h3 id="8-2-7-DetNet（贡献者：北京理工大学—明奇）"><a href="#8-2-7-DetNet（贡献者：北京理工大学—明奇）" class="headerlink" title="8.2.7  DetNet（贡献者：北京理工大学—明奇）"></a>8.2.7  DetNet（贡献者：北京理工大学—明奇）</h3><p>DetNet是发表在ECCV2018的论文，比较新，出发点是现有的检测任务backbone都是从分类任务衍生而来的，因此作者想针对检测专用的backbone做一些讨论和研究而设计了DetNet，思路比较新奇。</p>
<ol>
<li><strong>Introduction</strong><br>&emsp;&emsp;很多backbone的提出都是用于挑战ImageNet分类任务后被应用到检测上来，而鲜有单独<u>针对检测任务设计的backbone</u>。          </li>
</ol>
<p>&emsp;&emsp;<strong>检测和分类有明显的区别</strong>：（1）不仅需要分类，还需要精确的定位 （2）最近的检测器都是基于类似FPN结构，在分类网络基础上加额外多尺度特征进行检测，应对不同尺度变化的目标。这两点又是相互补充，共同协助网络完成分类到检测任务的转变。例如分类任务是检测的一环所以必不可少，但是传统分类采用的最高级特征定位细节不够，因此很多最近网络设法用类似FPN的结构去处理尺度变化的问题，就将分类较好地过渡到检测任务上了。</p>
<ol>
<li><p><strong>DetNet</strong>  </p>
<p>2.1 <strong>Motivation</strong><br>&emsp;&emsp;主要着眼点是<strong>分辨率</strong>，从大目标和小目标分别阐述保持分辨率的重要性。所以DetNet也是从分辨率的保持着手，解决多尺度物体的识别问题。</p>
</li>
</ol>
<ul>
<li><p>Weak visibility of large objects<br>&emsp;&emsp;网络在较深层如P6（FPN）P7（RetinaNet）大目标的边界不明确使精确定位困难。</p>
</li>
<li><p>Invisibility of small objects<br>&emsp;&emsp;小目标就很惨了，降采样容易丢。这个就不赘述了，所以只要避开降采样就能防止目标丢失，但是这种方法又会导致抽象能力不够 </p>
</li>
</ul>
<p>​    2.2  <strong>DetNet Design</strong><br>&emsp;&emsp;保持分辨率有两个麻烦的问题：（1）内存消耗大，计算大 （2）降采样减少导致高层的抽象特征不足以很好地进行分类任务。下面设计时会同时考虑时间和高层抽象信息两点。<br>&emsp;&emsp;先放出DetNet的多尺度各stage的尺寸如下图， 可以看到，相比前两种方式，DetNet在P4之后就不再进一步降采样了，进行分辨率的保持。</p>
<p><img src="img/ch8/DetNet-1.png" alt=""></p>
<p>&emsp;&emsp;实现细节如下图：</p>
<p><img src="img/ch8/DetNet-2.png" alt=""></p>
<ul>
<li>采用的backbone是ResNet-50，改进设计了DetNet-59。</li>
<li>对bottlenecks进行了改进，传统的其实不止C，也包含两种，即将AB的膨胀卷积换成普通卷积。AB是新的基础模块。</li>
<li>为了减少分辨率保持带来的时间和内存成本消耗，通道数固定为256（思考：降采样和膨胀卷积都会有信息丢失，这里可以想想）。</li>
<li>DetNet也可以加FPN结构，方法类似。</li>
</ul>
<ol>
<li><p><strong>Experiments</strong><br> &emsp;&emsp;检测和训练的细节配置就不看了。</p>
<p>3.1 <strong>Main Results</strong></p>
</li>
</ol>
<p><img src="img/ch8/DetNet-3.png" alt=""></p>
<ul>
<li>在FPN基础上明显有大物体涨点，同时由于高分辨率，小物体也有不错的提升。</li>
<li>膨胀卷积提供的大感受野使得分类也不逊色<br><img src="img/ch8/DetNet-4.png" alt=""></li>
</ul>
<p>​    3.2  <strong>Results analysis</strong><br><img src="img/ch8/DetNet-5.png" alt=""></p>
<ul>
<li>从AP50看出，高好1.7；从AP80看出，高了3.7。由此可以看出确实提高了检测性能。（</li>
<li><p>从定位性能来看，大物体的提升比小物体更多。作者认为是高分辨率解决了大物体边界模糊的问题。其实有一种解释：小目标没有大目标明显，因为膨胀卷积核降采样都会丢失小目标，只是膨胀卷积可能离散采样不至于像降采样直接给到后面没了，但是没有根本性的解决，所以小目标不大。<br><img src="img/ch8/DetNet-6.png" alt=""></p>
</li>
<li><p>AR指标也有类似结论</p>
</li>
<li>AR50体现了小目标的查全率更好，这也印证上面分析的：相对降采样，膨胀卷积丢失会好点。此下大目标效果虽然提升不大但是也很高了，作者表示DetNet擅长找到更精确的定位目标，在AR85的高指标就能看出。</li>
<li>AR85看大目标丢失少，说明能够像 VGG一样对大目标效果优良。关于小目标的效果平平，作者认为没有必要太高，因为FPN结构对小目标已经利用地很充分了，这里即使不高也没事。</li>
</ul>
<p>3.3 <strong>Discussion</strong></p>
<ul>
<li>关于stage<br>&emsp;&emsp;为了研究backbone对检测的影响，首先研究stage的作用。前4个还好说，和ResNet一样，但是P5 P6就不同，没有尺度的变化，和传统意义的stage不一样了，需要重新定义。这里DetNet也是类似ResNet的方法，虽然没有尺度变化，但是AB模块的位置还是保持了，B开启一个stage（<del>听上去有点牵强</del>）。如下图，认为新加的仍属于P5。<br><img src="img/ch8/DetNet-7.png" alt=""></li>
</ul>
<p>&emsp;&emsp;验证方法是做了实验，将P6开始的block换成上图所示的A模块对比效果如下图。 发现还是加了B效果更好。（但是这个stage和传统意义很不一样，所以很多性质不能相提并论，只是B模块的改变也不好判定什么）<br><img src="img/ch8/DetNet-8.png" alt=""></p>
<h3 id="8-2-8-CBNet"><a href="#8-2-8-CBNet" class="headerlink" title="8.2.8  CBNet"></a>8.2.8  CBNet</h3><p>本部分介绍一篇在COCO数据集达到最高单模型性能——mAP 53.3的网络，论文于2019.9.3发布在ArXiv，全名是<em>CBNet: A Novel Composite Backbone Network Architecture for Object Detection</em></p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>&emsp;&emsp;名义上是单模型，实际是多模型的特征融合，只是和真正的多模型策略略有不同。作者的起点是，设计新的模型往往需要在ImageNet上进行预训练，比较麻烦。因而提出的Composite Backbone Network (CBNet)，采用经典网络的多重组合的方式构建网络，一方面可以提取到更有效的特征，另一方面也能够直接用现成的预训练参数（如ResNet，ResNeXt等）比较简单高效。</p>
<ol>
<li><strong>Proposed method</strong><br><img src="img/ch8/CBNet-1.png" alt=""><br>2.1  <strong>Architecture of CBNet</strong><br><img src="img/ch8/CBNet-2.png" alt=""></li>
</ol>
<p>&emsp;&emsp;如上图，模型中采用K个（K&gt;1）相同的结构进行紧密联结。其中两个相同backbone的叫Dual-Backbone (DB)，三个叫Triple- Backbone (TB)；L代表backbone的stage数目，这里统一设置为L=5。其中，和前任工作不同的地方在于，这里将不同的stage信息进行复用回传，以便获取更好的特征（为什么work不好说）。        </p>
<p>2.2  <strong>Other possible composite styles</strong><br><img src="img/ch8/CBNet-3.png" alt=""></p>
<p>&emsp;&emsp;相关工作的其他类似结构，大同小异。要么是前面backbone的stage往后传播，要么是往前一个传播，每个都有一篇论文，应该都会给出不同的解释；第四个结构不太一样，是类似densnet的结构，但是密集连接+多backbone assemble的内存消耗不出意外会非常大。但是脱离这些体系来看，多backbone的结构类似多模型的assemble，和单模型有点不公平。</p>
<ol>
<li><strong>Experiment</strong></li>
</ol>
<ul>
<li><strong>result</strong><br><img src="img/ch8/CBNet-4.png" alt=""></li>
</ul>
<p>COCO数据集上的结果。看来提升还是有的。但是也能看出，大趋势上，三阶级联效果不如两阶的提升大，也是这部分的特征提升空间有限的缘故，到底哪部分在work不好说。下图的研究就更说明这一点了，斜率逐渐减小。</p>
<ul>
<li><strong>Comparisons of different composite styles</strong><br><img src="img/ch8/CBNet-5.png" alt=""></li>
</ul>
<p>他的级联网络相比，作者的阐述点只落脚于特征的利用情况，但是这个东西本身就很玄乎，不好说到底怎么算利用得好。硬要说这种做法的解释性，大概就是将backbone方向的后面高级语义特征传播回前面进行加强，相当于横向的FPN传播。</p>
<ul>
<li><strong>Number of backbones in CBNet</strong><br><img src="img/ch8/CBNet-6.png" alt=""></li>
</ul>
<p>速度慢是必然的，FPN+ResNeXt为8fps，加上两个backboen后为5.5FPS；如果减去backbone的前两个stage，可以节省部分参数达到6.9FPS，而精度下降不大（整体速度太低，这个实验意义不大）</p>
<ul>
<li><strong>Sharing weights for CBNet</strong><br><img src="img/ch8/CBNet-7.png" alt=""></li>
<li>从中可以看出其实权重是否share区别不大， 不到一个点的降幅，参数量减少。</li>
</ul>
<ul>
<li><strong>Effectiveness of basic feature enhancement by CBNet</strong><br><img src="img/ch8/CBNet-8.png" alt=""></li>
</ul>
<p>从中可以看出激活响应效果更好，确实是能够提取到更为有效的特征，对物体的响应更加敏感。</p>
<h2 id="8-3-One-Stage目标检测算法"><a href="#8-3-One-Stage目标检测算法" class="headerlink" title="8.3 One Stage目标检测算法"></a>8.3 One Stage目标检测算法</h2><p>我们将对单次目标检测器（包括SSD系列和YOLO系列等算法）进行综述。我们将分析FPN以理解多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。然后我们将分析Focal loss和RetinaNet，看看它们是如何解决训练过程中的类别不平衡问题的。</p>
<h3 id="8-3-1-SSD"><a href="#8-3-1-SSD" class="headerlink" title="8.3.1 SSD"></a>8.3.1 SSD</h3><p><strong>SSD有哪些创新点？</strong></p>
<ol>
<li>基于Faster R-CNN中的Anchor，提出了相似的先验框（Prior box）</li>
<li>从不同比例的特征图（多尺度特征）中产生不同比例的预测，并明确地按长宽比分离预测。</li>
</ol>
<p>不同于前面的R-CNN系列，SSD属于one-stage方法。SSD使用 VGG16 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样），将后面的全连接层替换成卷积层，并在之后添加自定义卷积层，并在最后直接采用卷积进行检测。在多个特征图上设置不同缩放比例和不同宽高比的先验框以融合多尺度特征图进行检测，靠前的大尺度特征图可以捕捉到小物体的信息，而靠后的小尺度特征图能捕捉到大物体的信息，从而提高检测的准确性和定位的准确性。如下图是SSD的网络结构图。</p>
<p><img src="/img/ch8/SSD-01.png" alt=""></p>
<p><strong>1. 怎样设置default boxes？</strong><br>SSD中default box的概念有点类似于Faster R-CNN中的anchor。不同于Faster R-CNN只在最后一个特征层取anchor, SSD在多个特征层上取default box，可以得到不同尺度的default box。在特征图的每个单元上取不同宽高比的default box,一般宽高比在{1,2,3,1/2,1/3}中选取，有时还会额外增加一个宽高比为1但具有特殊尺度的box。如下图所示，在8x8的feature map和4x4的feature map上的每个单元取4个不同的default box。原文对于300x300的输入，分别在conv4_3, conv7,conv8_2,conv9_2,conv10_2,conv11_2的特征图上的每个单元取4,6,6,6,4,4个default box. 由于以上特征图的大小分别是38x38,19x19,10x10,5x5,3x3,1x1，所以一共得到38x38x4+19x19x6+10x10x6+5x5x6+<br>3x3x4+1x1x4=8732个default box.对一张300x300的图片输入网络将会针对这8732个default box预测8732个边界框。</p>
<p><img src="/img/ch8/SSD-02.png" alt=""></p>
<p><strong>2. 怎样对先验框进行匹配？</strong><br>SSD在训练的时候只需要输入图像和图像中每个目标对应的ground truth. 先验框与ground truth 的匹配遵循两个原则：</p>
<p>（1）对图片中的每个ground truth, 在先验框中找到与其IOU最大的先验框，则该先验框对应的预测边界框与ground truth 匹配。</p>
<p>（2）对于（1）中每个剩下的没有与任何ground truth匹配到的先验框，找到与其IOU最大的ground truth，若其与该ground truth的IOU值大于某个阈值（一般设为0.5），则该先验框对应的预测边界框与该ground truth匹配。</p>
<p>按照这两个原则进行匹配，匹配到ground truth的先验框对应的预测边界框作为正样本，没有匹配到ground truth的先验框对应的预测边界框作为负样本。尽管一个ground truth可以与多个先验框匹配，但是ground truth的数量相对先验框还是很少，按照上面的原则进行匹配还是会造成负样本远多于正样本的情况。为了使正负样本尽量均衡（一般保证正负样本比例约为1：3），SSD采用hard negative mining, 即对负样本按照其预测背景类的置信度进行降序排列，选取置信度较小的top-k作为训练的负样本。</p>
<p><strong>3. 怎样得到预测的检测结果？</strong></p>
<p>最后分别在所选的特征层上使用3x3卷积核预测不同default boxes所属的类别分数及其预测的边界框location。由于对于每个box需要预测该box属于每个类别的置信度（假设有c类，包括背景，例如20class的数据集合，c=21）和该box对应的预测边界框的location(包含4个值，即该box的中心坐标和宽高)，则每个box需要预测c+4个值。所以对于某个所选的特征层，该层的卷积核个数为（c+4）x 该层的default box个数.最后将每个层得到的卷积结果进行拼接。对于得到的每个预测框，取其类别置信度的最大值，若该最大值大于置信度阈值，则最大值所对应的类别即为该预测框的类别，否则过滤掉此框。对于保留的预测框根据它对应的先验框进行解码得到其真实的位置参数（这里还需注意要防止预测框位置超出图片），然后根据所属类别置信度进行降序排列，取top-k个预测框，最后进行NMS，过滤掉重叠度较大的预测框，最后得到检测结果。</p>
<p>SSD优势是速度比较快，整个过程只需要一步，首先在图片不同位置按照不同尺度和宽高比进行密集抽样，然后利用CNN提取特征后直接进行分类与回归，所以速度比较快，但均匀密集采样会造成正负样本不均衡的情况使得训练比较困难，导致模型准确度有所降低。另外，SSD对小目标的检测没有大目标好，因为随着网络的加深，在高层特征图中小目标的信息丢失掉了，适当增大输入图片的尺寸可以提升小目标的检测效果。</p>
<h3 id="8-3-2-DSSD"><a href="#8-3-2-DSSD" class="headerlink" title="8.3.2 DSSD"></a>8.3.2 DSSD</h3><p><strong>DSSD有哪些创新点？</strong></p>
<ol>
<li>Backbone：将ResNet替换SSD中的VGG网络，增强了特征提取能力</li>
<li>添加了Deconvolution层，增加了大量上下文信息</li>
</ol>
<p>为了解决SSD算法检测小目标困难的问题，DSSD算法将SSD算法基础网络从VGG-16更改为ResNet-101，增强网络特征提取能力，其次参考FPN算法思路利用去Deconvolution结构将图像深层特征从高维空间传递出来，与浅层信息融合，联系不同层级之间的图像语义关系，设计预测模块结构，通过不同层级特征之间融合特征输出预测物体类别信息。</p>
<p>DSSD算法中有两个特殊的结构：Prediction模块；Deconvolution模块。前者利用提升每个子任务的表现来提高准确性，并且防止梯度直接流入ResNet主网络。后者则增加了三个Batch Normalization层和三个3×3卷积层，其中卷积层起到了缓冲的作用，防止梯度对主网络影响太剧烈，保证网络的稳定性。</p>
<p>SSD和DSSD的网络模型如下图所示：</p>
<p><img src="img/ch8/DSSD-01.png" alt=""></p>
<p><strong>Prediction Module</strong></p>
<p>SSD直接从多个卷积层中单独引出预测函数，预测量多达7000多，梯度计算量也很大。MS-CNN方法指出，改进每个任务的子网可以提高准确性。根据这一思想，DSSD在每一个预测层后增加残差模块，并且对于多种方案进行了对比，如下图所示。结果表明，增加残差预测模块后，高分辨率图片的检测精度比原始SSD提升明显。</p>
<p><img src="img/ch8/DSSD-02.png" alt=""></p>
<p><strong>Deconvolution模块</strong></p>
<p>为了整合浅层特征图和deconvolution层的信息，作者引入deconvolution模块，如下图所示。作者受到论文Learning to Refine Object Segments的启发，认为用于精细网络的deconvolution模块的分解结构达到的精度可以和复杂网络一样，并且更有效率。作者对其进行了一定的修改：其一，在每个卷积层后添加批归一化（batch normalization）层；其二，使用基于学习的deconvolution层而不是简单地双线性上采样；其三，作者测试了不同的结合方式，元素求和（element-wise sum）与元素点积（element-wise product）方式，实验证明元素点积计算能得到更好的精度。</p>
<p><img src="img/ch8/DSSD-03.png" alt=""></p>
<h3 id="8-3-3-YOLOv1"><a href="#8-3-3-YOLOv1" class="headerlink" title="8.3.3 YOLOv1"></a>8.3.3 YOLOv1</h3><p><strong>YOLOv1有哪些创新点？</strong></p>
<ol>
<li>将整张图作为网络的输入，直接在输出层回归bounding box的位置和所属的类别</li>
<li>速度快，one stage detection的开山之作</li>
</ol>
<p><strong>YOLOv1介绍</strong></p>
<p>YOLO（You Only Look Once: Unified, Real-Time Object Detection）是one-stage detection的开山之作。之前的物体检测方法首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。</p>
<p>YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。下图展示了各物体检测系统的流程图。</p>
<p><img src="/img/ch8/YOLOv1-01.png" alt=""></p>
<p>事实上，YOLO也并没有真正的去掉候选区，而是直接将输入图片划分成7x7=49个网格，每个网格预测两个边界框，一共预测49x2=98个边界框。可以近似理解为在输入图片上粗略的选取98个候选区，这98个候选区覆盖了图片的整个区域，进而用回归预测这98个候选框对应的边界框。</p>
<p><strong>1. 网络结构是怎样的？</strong></p>
<p>YOLO网络借鉴了GoogLeNet分类网络结构，不同的是YOLO使用1x1卷积层和3x3卷积层替代inception module。如下图所示，整个检测网络包括24个卷积层和2个全连接层。其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。</p>
<p><img src="/img/ch8/YOLOv1-02.png" alt=""></p>
<p><strong>2. YOLO的输入、输出、损失函数分别是什么？</strong></p>
<p>前面说到YOLO将输入图像分成7x7的网格，最后输出是7x7xk的张量。YOLO网络最后接了两个全连接层，全连接层要求输入是固定大小的，所以YOLO要求输入图像有固定大小，论文中作者设计的输入尺寸是448x448。</p>
<p>YOLO将输入图像分成7x7的网格，每个网格预测2个边界框。若某物体的ground truth的中心落在该网格，则该网格中与这个ground truth IOU最大的边界框负责预测该物体。对每个边界框会预测5个值，分别是边界框的中心x,y（相对于所属网格的边界），边界框的宽高w,h（相对于原始输入图像的宽高的比例），以及这些边界框的confidencescores（边界框与ground truth box的IOU值）。同时每个网格还需要预测c个类条件概率 （是一个c维向量，表示某个物体object在这个网格中，且该object分别属于各个类别的概率，这里的c类物体不包含背景）。论文中的c=20，则每个网格需要预测2x5+20=30个值，这些值被映射到一个30维的向量。<br>为了让边界框坐标损失、分类损失达到很好的平衡，损失函数设计如下图所示。</p>
<p><img src="/img/ch8/YOLOv1-03.png" alt=""></p>
<p>如上图所示，损失函数分为坐标预测（蓝色框）、含有物体的边界框的confidence预测（红色框）、不含有物体的边界框的confidence预测（黄色框）、分类预测（紫色框）四个部分。</p>
<p>由于不同大小的边界框对预测偏差的敏感度不同，小的边界框对预测偏差的敏感度更大。为了均衡不同尺寸边界框对预测偏差的敏感度的差异。作者巧妙的对边界框的w,h取均值再求L2 loss. YOLO中更重视坐标预测，赋予坐标损失更大的权重，记为 coord，在pascal voc训练中coodd=5 ，classification error部分的权重取1。</p>
<p>某边界框的置信度定义为：某边界框的confidence = 该边界框存在某类对象的概率pr(object)*该边界框与该对象的ground truth的IOU值 ，若该边界框存在某个对象pr(object)=1 ，否则pr(object)=0 。由于一幅图中大部分网格中是没有物体的，这些网格中的边界框的confidence置为0，相比于有物体的网格，这些不包含物体的网格更多，对梯度更新的贡献更大，会导致网络不稳定。为了平衡上述问题，YOLO损失函数中对没有物体的边界框的confidence error赋予较小的权重，记为 noobj，对有物体的边界框的confidence error赋予较大的权重。在pascal VOC训练中noobj=0.5 ，有物体的边界框的confidence error的权重设为1.</p>
<p><strong>3. YOLO怎样预测？</strong></p>
<p>YOLO最后采用非极大值抑制（NMS）算法从输出结果中提取最有可能的对象和其对应的边界框。</p>
<p>输入一张图片到YOLO网络将输出一个7<em>7</em>30的张量表示图片中每个网格对应的可能的两个边界框以及每个边界框的置信度和包含的对象属于各个类别的概率。由此可以计算某对象i属于类别 同时在第j个边界框中的得分：</p>
<p><img src="/img/ch8/YOLOv1-04.png" alt=""></p>
<p>每个网格有20个类条件概率，2个边界框置信度，相当于每个网格有40个得分，7x7个网格有1960个得分，每类对象有1960/20=98个得分，即98个候选框。</p>
<p><strong>NMS步骤如下：</strong></p>
<p>1.设置一个Score的阈值，一个IOU的阈值；</p>
<p>2.对于每类对象，遍历属于该类的所有候选框，</p>
<p>①过滤掉Score低于Score阈值的候选框；</p>
<p>②找到剩下的候选框中最大Score对应的候选框，添加到输出列表；</p>
<p>③进一步计算剩下的候选框与②中输出列表中每个候选框的IOU，若该IOU大于设置的IOU阈值，将该候选框过滤掉，否则加入输出列表中；</p>
<p>④最后输出列表中的候选框即为图片中该类对象预测的所有边界框</p>
<p>3.返回步骤2继续处理下一类对象。</p>
<p>YOLO将识别与定位合二为一，结构简便，检测速度快，更快的Fast YOLO可以达到155FPS。相对于R-CNN系列, YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。同时YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上做训练的YOLO，在艺术图片上可以得到较好的测试结果。</p>
<p>由于YOLO网格设置比较稀疏，且每个网格只预测2个边界框，其总体预测精度不高，略低于Fast RCNN。其对小物体的检测效果较差，尤其是对密集的小物体表现比较差。</p>
<h3 id="8-3-4-YOLOv2"><a href="#8-3-4-YOLOv2" class="headerlink" title="8.3.4 YOLOv2"></a>8.3.4 YOLOv2</h3><p><strong>YOLOv2 有哪些创新点？</strong></p>
<p>YOLOv1虽然检测速度快，但在定位方面不够准确，并且召回率较低。为了提升定位准确度，改善召回率，YOLOv2在YOLOv1的基础上提出了几种改进策略，如下图所示，可以看到，一些改进方法能有效提高模型的mAP。</p>
<ol>
<li>大尺度预训练分类</li>
<li>New Network：Darknet-19</li>
<li>加入anchor</li>
</ol>
<p><img src="/img/ch8/YOLOv2-01.png" alt=""></p>
<p><strong>YOLOv2 介绍</strong></p>
<p><strong>（1）Batch Normalization</strong></p>
<p>YOLOv2中在每个卷积层后加Batch Normalization(BN)层，去掉dropout. BN层可以起到一定的正则化效果，能提升模型收敛速度，防止模型过拟合。YOLOv2通过使用BN层使得mAP提高了2%。<br><strong>（2）High Resolution Classifier</strong></p>
<p>目前的大部分检测模型都会使用主流分类网络（如vgg、resnet）在ImageNet上的预训练模型作为特征提取器,<br>而这些分类网络大部分都是以小于256x256的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2将输入图片的分辨率提升至448x448，为了使网络适应新的分辨率，YOLOv2先在ImageNet上以448x448的分辨率对网络进行10个epoch的微调，让网络适应高分辨率的输入。通过使用高分辨率的输入，YOLOv2的mAP提升了约4%。</p>
<p><strong>（3）Convolutional With Anchor Boxes</strong></p>
<p>YOLOv1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLOv2去掉了YOLOv1中的全连接层，使用Anchor Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLOv2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416x416的输入，模型下采样的总步长为32，最后得到13x13的特征图，然后对13x13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。使用anchor<br>boxes之后，YOLOv2可以预测13x13x5=845个边界框，模型的召回率由原来的81%提升到88%，mAP由原来的69.5%降低到69.2%.召回率提升了7%，准确率下降了0.3%。</p>
<p><strong>（4）Dimension Clusters</strong></p>
<p>在Faster R-CNN和SSD中，先验框都是手动设定的，带有一定的主观性。YOLOv2采用k-means聚类算法对训练集中的边界框做了聚类分析，选用boxes之间的IOU值作为聚类指标。综合考虑模型复杂度和召回率，最终选择5个聚类中心，得到5个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均IOU值，这使得模型更容易训练学习。</p>
<p><strong>（5）New Network：Darknet-19</strong></p>
<p>YOLOv2采用Darknet-19，其网络结构如下图所示，包括19个卷积层和5个max pooling层，主要采用3x3卷积和1x1卷积，这里1x1卷积可以压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用BN层以加快模型收敛同时防止过拟合。最终采用global avg pool 做预测。采用YOLOv2，模型的mAP值没有显著提升，但计算量减少了。</p>
<p><img src="/img/ch8/YOLOv2-02.png" alt=""></p>
<p><strong>（6）Direct location prediction</strong></p>
<p>Faster R-CNN使用anchor boxes预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。YOLOv2沿用YOLOv1的方法，根据所在网格单元的位置来预测坐标,则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。设一个网格相对于图片左上角的偏移量是cx，cy。先验框的宽度和高度分别是pw和ph，则预测的边界框相对于特征图的中心坐标(bx，by)和宽高bw、bh的计算公式如下图所示。</p>
<p><img src="/img/ch8/YOLOv2-03.png" alt=""></p>
<p>YOLOv2结合Dimention Clusters, 通过对边界框的位置预测进行约束，使模型更容易稳定训练，这种方式使得模型的mAP值提升了约5%。</p>
<p><strong>（7）Fine-Grained Features</strong></p>
<p>YOLOv2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。YOLOv2提取Darknet-19最后一个max pool层的输入，得到26x26x512的特征图。经过1x1x64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%.</p>
<p><strong>（8）Multi-Scale Training</strong></p>
<p>YOLOv2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数{320,352,…,608}。采用Multi-Scale Training, 可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p>
<p>YOLOv2借鉴了很多其它目标检测方法的一些技巧，如Faster R-CNN的anchor boxes, SSD中的多尺度检测。除此之外，YOLOv2在网络设计上做了很多tricks,使它能在保证速度的同时提高检测准确率，Multi-Scale Training更使得同一个模型适应不同大小的输入，从而可以在速度和精度上进行自由权衡。</p>
<p><strong>YOLOv2的训练</strong></p>
<p>YOLOv2的训练主要包括三个阶段。<br>第一阶段：先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为$224\times 224$,共训练160个epochs。<br>第二阶段：将网络的输入调整为$448\times 448$,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。<br>第三个阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。<br>网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个$3\times 3 \times 2014$卷积层，同时增加了一个passthrough层，最后使用$1\times 1$卷积层输出预测结果。</p>
<h3 id="8-3-5-YOLO9000"><a href="#8-3-5-YOLO9000" class="headerlink" title="8.3.5 YOLO9000"></a>8.3.5 YOLO9000</h3><p>github：<a href="http://pjreddie.com/yolo9000/" target="_blank" rel="noopener">http://pjreddie.com/yolo9000/</a></p>
<p>YOLO9000是在YOLOv2的基础上提出的一种联合训练方法，可以检测超过9000个类别的模型。YOLOv2混合目标检测数据集和分类数据集，用目标检测数据集及其类别标记信息和位置标注信息训练模型学习预测目标定位和分类，用分类数据集及其类别标记信息进一步扩充模型所能识别的物体类别同时能增强模型鲁棒性。</p>
<p><strong>1. YOLO9000是怎么组织数据的？</strong></p>
<p>YOLO9000根据各个类别之间的从属关系建立一种树结WordTree, 将COCO数据集和ImageNet数据集组织起来。</p>
<p>WordTree的生成方式如下：</p>
<p>①首先遍历ImageNet中的类别名词。</p>
<p>②对每个名词，在WordNet(一种结构化概念及概念之间关系的语言数据库)上找到从它所在位置到根节点（设根节点为实体对象physical object）的最短路径，由于在WordNet中大多数同义词只有一个路径，所以先把将该路径上的词全都加到树中。</p>
<p>③迭代地检查剩下的名词，取它到根节点的最短路径，将该最短路径上的还没出现在层次树中的词加入到树中。<br>混合后的数据集形成一个有9418类的WordTree.生成的WordTree模型如下图所示。另外考虑到COCO数据集相对于ImageNet数据集数据量太少了，为了平衡两个数据集，作者进一步对COCO数据集过采样，使COCO数据集与ImageNet数据集的数据量比例接近1：4。</p>
<p><img src="/img/ch8/YOLOv2-04.png" alt=""></p>
<p>对于物体的标签，采用one-hot编码的形式，数据集中的每个物体的类别标签被组织成1个长度为9418的向量，向量中除在WordTree中从该物体对应的名词到根节点的路径上出现的词对应的类别标号处为1，其余位置为0。</p>
<p><strong>2. YOLO9000是怎么进行联合训练的？</strong></p>
<p>YOLO9000采用YOLOv2的结构，anchorbox由原来的5调整到3，对每个anchorbox预测其对应的边界框的位置信息x,y,w,h和置信度以及所包含的物体分别属于9418类的概率，所以每个anchorbox需要预测4+1+9418=9423个值。每个网格需要预测3x9423=28269个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的YOLOv2loss进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的loss进行反向传播。</p>
<p><strong>3. YOLO9000是怎么预测的？</strong></p>
<p>WordTree中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次softmax处理，以得到同义词集合中的每个词的下义词的概率。当需要预测属于某个类别的概率时，需要预测该类别节点的条件概率。即在WordTree上找到该类别名词到根节点的路径，计算路径上每个节点的概率之积。预测时，YOLOv2得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。</p>
<p>YOLO9000使用WordTree混合目标检测数据集和分类数据集，并在其上进行联合训练，使之能实时检测出超过9000个类别的物体，其强大令人赞叹不已。YOLO9000尤其对动物的识别效果很好，但是对衣服或者设备等类别的识别效果不是很好，可能的原因是与目标检测数据集中的数据偏向有关。</p>
<h3 id="8-3-6-YOLOv3"><a href="#8-3-6-YOLOv3" class="headerlink" title="8.3.6 YOLOv3"></a>8.3.6 YOLOv3</h3><p>YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。</p>
<p><strong>YOLOv3有哪些创新点？</strong></p>
<ol>
<li>新网络结构：DarkNet-53</li>
<li>融合FPN</li>
<li>用逻辑回归替代softmax作为分类器</li>
</ol>
<p><strong>1. YOLOv3对网络结构做了哪些改进？</strong></p>
<p>YOLOv3在之前Darknet-19的基础上引入了残差块，并进一步加深了网络，改进后的网络有53个卷积层，取名为Darknet-53，网络结构如下图所示（以256*256的输入为例）。</p>
<p><img src="/img/ch8/YOLOv3-01.png" alt=""></p>
<p>为了比较Darknet-53与其它网络结构的性能，作者在TitanX上，采用相同的实验设置，将256x256的图片分别输入以Darknet-19，ResNet-101，ResNet-152和Darknet-53为基础网络的分类模型中，实验得到的结果如下图所示。可以看到Darknet-53比ResNet-101的性能更好，而且速度是其1.5倍，Darknet-53与ResNet-152性能相似但速度几乎是其2倍。注意到，Darknet-53相比于其它网络结构实现了每秒最高的浮点计算量，说明其网络结构能更好的利用GPU。</p>
<p><img src="/img/ch8/YOLOv3-02.png" alt=""></p>
<p><strong>2.YOLOv3中怎样实现多尺度检测？</strong></p>
<p>YOLOv3借鉴了FPN的思想，从不同尺度提取特征。相比YOLOv2，YOLOv3提取最后3层特征图，不仅在每个特征图上分别独立做预测，同时通过将小特征图上采样到与大的特征图相同大小，然后与大的特征图拼接做进一步预测。用维度聚类的思想聚类出9种尺度的anchor box，将9种尺度的anchor box均匀的分配给3种尺度的特征图.如下图是在网络结构图的基础上加上多尺度特征提取部分的示意图（以在COCO数据集(80类)上256x256的输入为例）：</p>
<p><img src="/img/ch8/YOLOv3-03.png" alt=""></p>
<p>从YOLOv1到YOLOv2再到YOLO9000、YOLOv3, YOLO经历三代变革，在保持速度优势的同时，不断改进网络结构，同时汲取其它优秀的目标检测算法的各种trick，先后引入anchor box机制、引入FPN实现多尺度检测等。</p>
<h3 id="8-3-7-RetinaNet"><a href="#8-3-7-RetinaNet" class="headerlink" title="8.3.7 RetinaNet"></a>8.3.7 RetinaNet</h3><p><strong>研究背景</strong></p>
<ul>
<li>Two-Stage检测器（如Faster R-CNN、FPN）效果好，但速度相对慢</li>
<li>One-Stage检测器（如YOLO、SSD）速度快，但效果一般</li>
</ul>
<p><img src="/img/ch8/RetinaNet-01.png" alt=""></p>
<p>作者对one-stage检测器准确率不高的问题进行探究，发现主要问题在于正负类别不均衡（简单-难分类别不均衡）。</p>
<blockquote>
<p>We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause.</p>
</blockquote>
<p>作者建议通过重新设计标准的交叉熵损失（cross entropy loss）来解决这种类别不平衡（class inbalance）问题，即提出Focal Loss。</p>
<blockquote>
<p>We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.</p>
</blockquote>
<p>结合Focal Loss的one-stage检测器称为RetinaNet，该检测器在COCO上mAP可以和特征金字塔网络（feature pyramid network，FPN）或者Mask R-CNN接近，</p>
<p><strong>问：什么是类别不均衡（class imbalance）？</strong></p>
<p>答：负样本的数量极大于正样本的数量，比如包含物体的区域（正样本）很少，而不包含物体的区域（负样本）很多。比如检测算法在早期会生成一大波的bbox。而一幅常规的图片中，顶多就那么几个object。这意味着，绝大多数的bbox属于background。</p>
<p><strong>问：样本的类别不均衡会带来什么问题？</strong></p>
<p>答：由于大多数都是简单易分的负样本（属于背景的样本），使得训练过程不能充分学习到属于那些有类别样本的信息；其次简单易分的负样本太多，可能掩盖了其他有类别样本的作用（这些简单易分的负样本仍产生一定幅度的loss，见下图蓝色曲线，数量多会对loss起主要贡献作用，因此就主导了梯度的更新方向，掩盖了重要的信息）</p>
<blockquote>
<p>This imbalance causes two problems: (1) training is inefficient as most locations are easy negatives that contribute no useful learning signal; (2) en masse, the easy negatives can overwhelm training and lead to degenerate models.</p>
</blockquote>
<p>简单来说，因为bbox数量爆炸。 正是因为bbox中属于background的bbox太多了，所以如果分类器无脑地把所有bbox统一归类为background，accuracy也可以刷得很高。于是乎，分类器的训练就失败了。分类器训练失败，检测精度自然就低了。</p>
<p><strong>问：为什么在two-stage检测器中，没有出现类别不均衡（class imbalamce）问题呢？</strong></p>
<p>答：因为通过RPN阶段可以减少候选目标区域，而在分类阶段，可以固定前景与背景比值（foreground-to-background ratio）为1:3，或者使用OHEM（online hard example mining）使得前景和背景的数量达到均衡。</p>
<p><strong>RetinaNet有哪些创新点？</strong></p>
<p><strong>概述：</strong></p>
<ul>
<li>New loss：提出Focal Loss函数解决class imbalance</li>
</ul>
<script type="math/tex; mode=display">
FL(p_t) = -(1-p_t)^\gamma \log(p_t)FL(pt)=−(1−pt)γlog(pt)</script><ul>
<li>New detector：RetinaNet = ResNet + FPN + Two sub-networks + Focal Loss</li>
</ul>
<p>Focal Loss更加聚焦在困难样本（hard examples）上的训练。</p>
<p><img src="/img/ch8/RetinaNet-02.png" alt=""></p>
<p>将Focal Loss与ResNet-101-FPN backbone结合提出RetinaNet（one-stage检测器），RetinaNet在COCO test-dev上达到39.1mAP，速度为5FPS。</p>
<p>RetinaNet检测器与当时最佳的其它检测器进行比较，无论是速度上还是准确率上都是最佳：</p>
<p><img src="/img/ch8/RetinaNet-03.png" alt=""></p>
<p><strong>详解：</strong></p>
<p>作者提出一种新的损失函数，思路是希望那些hard examples对损失的贡献变大，使网络更倾向于从这些样本上学习。</p>
<p>作者以二分类为例进行说明：</p>
<p><strong>交叉熵函数CE</strong></p>
<p>首先是我们常使用的交叉熵损失函数：</p>
<p><img src="/img/ch8/RetinaNet-04.png" alt=""></p>
<p>上式中，y=+1或者y=-1。p∈[0,1]是y=+1的估计概率。作者定义pt为：</p>
<p><img src="/img/ch8/RetinaNet-05.png" alt=""></p>
<p><img src="/img/ch8/RetinaNet-06.png" alt=""></p>
<p>注：对交叉熵函数不了解的，可以参考<a href="https://blog.csdn.net/chaipp0607/article/details/73392175" target="_blank" rel="noopener">理解交叉熵作为损失函数在神经网络中的作用</a></p>
<p><strong>均衡交叉熵函数</strong></p>
<p>要对类别不均衡问题对loss的贡献进行一个控制，即加上一个控制权重即可，最初作者的想法即如下这样，对于属于少数类别的样本，增大α即可</p>
<p><img src="/img/ch8/RetinaNet-07.png" alt=""></p>
<p>但这样有一个问题，它仅仅解决了正负样本之间的平衡问题，并没有区分易分/难分样本，按作者的话说：</p>
<blockquote>
<p>While α balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. Instead, we propose to reshape the loss function to down-weight easy examples and thus focus training on hard negatives.</p>
</blockquote>
<p>问：为什么公式(3)只解决正负样本不均衡问题？</p>
<p>答：增加了一个系数αt，跟pt的定义类似，当label=1的时候，αt=a；当label=-1的时候，αt=1-a，a的范围也是0到1。因此可以通过设定a的值（一般而言假如1这个类的样本数比-1这个类的样本数多很多，那么a会取0到0.5来增加-1这个类的样本的权重）来控制正负样本对总的loss的共享权重。</p>
<p><strong>Focal Loss</strong></p>
<p>作者一开始给交叉熵损失函数添加modulating factor：</p>
<script type="math/tex; mode=display">
(1-pt)^γ(1−pt)γ</script><p><img src="/img/ch8/RetinaNet-08.png" alt=""></p>
<p>显然，样本越易分，pt就越大（pt—&gt;1），modulating factor趋近于0，则贡献的loss就越小，同样地，样本越难分，其pt就越小，modulating factor接近于1，则贡献的loss不受影响。</p>
<p>问：为什么pt越大，FL值越小？</p>
<p>答：根据公式（4）可知，FL与log(pt)中的pt成反比，与1-pt成正比，因此FL与pt的关系成反比。这是交叉熵函数的基本性质。当pt很大时（接近于1），FL值很小；而当pt很小时（接近于0），FL值会很大。</p>
<p>注：这里有个超参数—focusing parameter γ。</p>
<p>γ 放大了modulating factor的作用。</p>
<p>举原文中的一个例子，当pt=0.9时，带有modulating factor的focal loss是CE loss的100分之一，即进一步减小了正确分类的损失。</p>
<blockquote>
<p>For instance, with γ = 2, an example classified with pt = 0.9 would have 100× lower loss compared with CE and with pt ≈ 0.968 it would have 1000× lower loss. This in turn increases the importance of correcting misclassified examples (whose loss is scaled down by at most 4× for pt ≤ .5 and γ = 2).</p>
</blockquote>
<p>在实际中，作者采用如下公式，即综合了公式(3)和公式(4)的形式，这样机能调整正负样本的权重，又能控制难易分类样本的权重：</p>
<p><img src="/img/ch8/RetinaNet-09.png" alt=""></p>
<p>这里的两个参数 α和γ 来控制，在实验中a的选择范围也很广，一般而言当γ增加的时候，a需要减小一点，本文作者采用α=0.25，γ=2效果最好。</p>
<p><strong>RetinaNet Detector</strong></p>
<p>RetinaNet是由backbone网络和两个特殊任务的子网络（subnet）组成（属于one-stage检测器）。Backbone用来计算feature map；第一个子网络用来object classification，第二个子网络用来bounding box regression。</p>
<p><strong>Feature Pyramid Network Backbone</strong></p>
<p><img src="/img/ch8/RetinaNet-10.png" alt=""></p>
<p><strong>Anchor</strong></p>
<p><strong>Classification Subnet</strong></p>
<p><strong>Box Regression Subnet</strong></p>
<p><img src="/img/ch8/RetinaNet-11.png" alt=""></p>
<p><img src="/img/ch8/RetinaNet-12.png" alt=""></p>
<p>RetinaNet结构注意内容：</p>
<ol>
<li>训练时FPN每一级的所有example都被用于计算Focal Loss，loss值加到一起用来训练；</li>
<li>测试时FPN每一级只选取score最大的1000个example来做nms；</li>
<li>整个结构不同层的head部分(上图中的c和d部分)共享参数，但分类和回归分支间的参数不共享；</li>
<li>分类分支的最后一级卷积的bias初始化成前面提到的-log((1-π)/π);</li>
</ol>
<p>作者：张磊_0503 链接：<a href="https://www.jianshu.com/p/204d9ad9507f" target="_blank" rel="noopener">https://www.jianshu.com/p/204d9ad9507f</a> 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。</p>
<p><strong>实验结果</strong></p>
<p>Table1是关于RetinaNet和Focal Loss的一些实验结果。（a）是在交叉熵的基础上加上参数a，a=0.5就表示传统的交叉熵，可以看出当a=0.75的时候效果最好，AP值提升了0.9。（b）是对比不同的参数γ和a的实验结果，可以看出随着γ的增加，AP提升比较明显。（d）通过和OHEM的对比可以看出最好的Focal Loss比最好的OHEM提高了3.2AP。这里OHEM1:3表示在通过OHEM得到的minibatch上强制positive和negative样本的比例为1:3，通过对比可以看出这种强制的操作并没有提升AP。（e）加入了运算时间的对比，可以和前面的Figure2结合起来看，速度方面也有优势！注意这里RetinaNet-101-800的AP是37.8，当把训练时间扩大1.5倍同时采用scale jitter，AP可以提高到39.1，这就是全文和table2中的最高的39.1AP的由来。</p>
<p><img src="/img/ch8/RetinaNet-13.png" alt=""></p>
<p><img src="/img/ch8/RetinaNet-14.png" alt=""></p>
<h3 id="8-3-8-RFBNet"><a href="#8-3-8-RFBNet" class="headerlink" title="8.3.8 RFBNet"></a>8.3.8 RFBNet</h3><p><strong>RFBNet有哪些创新点？</strong></p>
<ol>
<li>提出RF block（RFB）模块</li>
</ol>
<p>RFBNet主要想利用一些技巧使得轻量级模型在速度和精度上达到很好的trade-off的检测器。灵感来自人类视觉的感受野结构Receptive Fields (RFs) ，提出了新奇的RF block（RFB）模块，来验证感受野尺寸和方向性的对提高有鉴别鲁棒特征的关系。RFBNet是以主干网络（backbone）为VGG16的SSD来构建的，主要是在Inception的基础上加入了dilated卷积层（dilated convolution），从而有效增大了感受野（receptive field）。整体上因为是基于SSD网络进行改进，所以检测速度还是比较快，同时精度也有一定的保证。</p>
<p><strong>RFB介绍</strong></p>
<p>RFB是一个类似Inception模块的多分支卷积模块，它的内部结构可分为两个组件：多分支卷积层和dilated卷积层。如下图：</p>
<p><img src="img/ch8/RFBNet-01.png" alt=""></p>
<p><strong>1.多分支卷积层</strong><br>​      根据RF的定义，用多种尺寸的卷积核来实现比固定尺寸更好。具体设计：1.瓶颈结构，1x1-s2卷积减少通道特征，然后加上一个nxn卷积。2.替换5x5卷积为两个3x3卷积去减少参数，然后是更深的非线性层。有些例子，使用1xn和nx1代替nxn卷积；shortcut直连设计来自于ResNet和Inception ResNet V2。3.为了输出，卷积经常有stride=2或者是减少通道，所以直连层用一个不带非线性激活的1x1卷积层。</p>
<p><strong>2.Dilated 卷积层</strong></p>
<p>设计灵感来自Deeplab，在保持参数量和同样感受野的情况下，用来获取更高分辨率的特征。下图展示两种RFB结构：RFB和RFB-s。每个分支都是一个正常卷积后面加一个dilated卷积，主要是尺寸和dilated因子不同。（a）RFB。整体结构上借鉴了Inception的思想，主要不同点在于引入3个dilated卷积层（比如3x3conv，rate=1），这也是RFBNet增大感受野的主要方式之一；（b）RFB-s。RFB-s和RFB相比主要有两个改进，一方面用3x3卷积层代替5x5卷积层，另一方面用1x3和3x1卷积层代替3x3卷积层，主要目的应该是为了减少计算量，类似Inception后期版本对Inception结构的改进。 </p>
<p><img src="img/ch8/RFBNet-02.png" alt=""></p>
<p>RFBNet300的整体结构如下图所示，基本上和SSD类似。RFBNet和SSD不同的是：1、主干网上用两个RFB结构替换原来新增的两层。2、conv4_3和conv7_fc在接预测层之前分别接RFB-s和RFB结构。 </p>
<p><img src="img/ch8/RFBNet-03.png" alt=""></p>
<h3 id="8-3-9-M2Det"><a href="#8-3-9-M2Det" class="headerlink" title="8.3.9 M2Det"></a>8.3.9 M2Det</h3><p><strong>M2Det有哪些创新点？</strong></p>
<ol>
<li>提出了多层次特征金字塔网络（MLFPN）来构建更有效的特征金字塔，用于检测不同尺度的对象。</li>
</ol>
<p>M2Det的整体架构如下所示。M2Det使用backbone和多级特征金字塔网络（MLFPN）从输入图像中提取特征，然后类似于SSD，根据学习的特征生成密集的边界框和类别分数，最后是非最大抑制（NMS）操作以产生最终结果。 MLFPN由三个模块组成：特征融合模块（FFM），简化的U形模块（TUM）和按基于尺度的特征聚合模块（SFAM）。 FFMv1通过融合骨干网络的特征图，将语义信息丰富为基本特征。每个TUM生成一组多尺度特征，然后交替连接的TUM和FFMv2提取多级多尺度特征。此外，SFAM通过按比例缩放的特征连接操作和自适应注意机制将特征聚合到多级特征金字塔中。下面介绍有关M2Det中三个核心模块和网络配置的更多详细信息。</p>
<p><img src="img/ch8/M2Det-01.png" alt=""></p>
<p><strong>FFMs</strong></p>
<p>FFM融合了M2Det中不同层次的特征，这对于构建最终的多级特征金字塔至关重要。它们使用1x1卷积层来压缩输入特征的通道，并使用连接操作来聚合这些特征图。特别是，由于FFMv1以backbone中不同比例的两个特征图作为输入，因此它采用一个上采样操作，在连接操作之前将深度特征重新缩放到相同的尺度。同时，FFMv2采用基本特征和前一个TUM的最大输出特征图 - 这两个具有相同的比例 - 作为输入，并产生下一个TUM的融合特征。 FFMv1和FFMv2的结构细节分别如下图（a）和（b）所示。</p>
<p><img src="img/ch8/M2Det-02.png" alt=""></p>
<p><strong>TUMs</strong> </p>
<p>TUM不同于FPN和RetinaNet，TUM采用简化的U形结构，如上图（c）所示。编码器是一系列3x3，步长为2的卷积层.并且解码器将这些层的输出作为其参考特征集，而原始FPN选择ResNet主干网络中每个阶段的最后一层的输出。此外，在解码器分支的上采样层后添加1x1卷积层和按元素求和的操作，以增强学习能力并保持特征的平滑性。每个TUM的解码器中的所有输出形成当前级别的多尺度特征。整体而言，堆叠TUM的输出形成多层次多尺度特征，而前TUM主要提供浅层特征，中间TUM提供中等特征，后TUM提供深层特征。</p>
<p><strong>SFAM</strong></p>
<p>SFAM旨在将由TUM生成的多级多尺度特征聚合成多级特征金字塔，如下图所示。SFAM的第一阶段是沿着信道维度将等效尺度的特征连接在一起。聚合特征金字塔可以表示为$X = [X_1,X_2,…,X_i,…,X_L]$，其中</p>
<script type="math/tex; mode=display">X_i = Concat(X_{1i}, X_{2i}, ...., X_{Li}) \in R^{W_i \times H_i \times C}</script><p>指的是尺度第i个最大的特征。这里，聚合金字塔中的每个比例都包含来自多级深度的特征。但是，简单的连接操作不太适合。在第二阶段，引入了通道注意模块，以促使特征集中在最有益的通道。在SE区块之后，使用全局平均池化来在挤压步骤中生成通道统计z∈RC。</p>
<p><img src="img/ch8/M2Det-03.png" alt=""></p>
<h2 id="8-4-人脸检测"><a href="#8-4-人脸检测" class="headerlink" title="8.4 人脸检测"></a>8.4 人脸检测</h2><p>在目标检测领域可以划分为了人脸检测与通用目标检测，往往人脸这方面会有专门的算法（包括人脸检测、人脸识别、人脸其他属性的识别等等），并且和通用目标检测（识别）会有一定的差别，着主要来源于人脸的特殊性（有时候目标比较小、人脸之间特征不明显、遮挡问题等），下面将从人脸检测和通用目标检测两个方面来讲解目标检测。</p>
<h3 id="8-4-1-目前主要有人脸检测方法分类？"><a href="#8-4-1-目前主要有人脸检测方法分类？" class="headerlink" title="8.4.1 目前主要有人脸检测方法分类？"></a>8.4.1 目前主要有人脸检测方法分类？</h3><p>目前人脸检测方法主要包含两个区域：传统人脸检测算法和基于深度学习的人脸检测算法。传统人脸检测算法主要可以分为4类：</p>
<p>（1）基于知识的人脸检测方法；</p>
<p>（2）基于模型的人脸检测方法；</p>
<p>（3）基于特征的人脸检测方法；</p>
<p>（4）基于外观的人脸检测方法。</p>
<p>由于本书着重关注深度学习，下面会着重介绍基于深度学习的人脸检测方法。</p>
<p>2006年Hinton首次提出深度学习（Deep Learning）的概念，它是通过组合低层的特征形成更高层的抽象特征。随后研究者将深度学习应用在人脸检测领域，主要集中在基于卷积神经网络（CNN）的人脸检测研究，如基于级联卷积神经网络的人脸检测（cascade cnn）、 基于多任务卷积神经网络的人脸检测（MTCNN）、Facebox等，很大程度上提高了人脸检测的鲁棒性。当然通用目标检测算法像Faster-rcnn、yolo、ssd等也有用在人脸检测领域，也可以实现比较不错的结果，但是和专门人脸检测算法比还是有差别。下面部分主要介绍基于深度学习的的人脸检测算法，基于深度学习的通用目标检测算法将在第二大节介绍。</p>
<h3 id="8-4-2-如何检测图片中不同大小的人脸？"><a href="#8-4-2-如何检测图片中不同大小的人脸？" class="headerlink" title="8.4.2 如何检测图片中不同大小的人脸？"></a>8.4.2 如何检测图片中不同大小的人脸？</h3><p>传统人脸检测算法中针对不同大小人脸主要有两个策略：</p>
<p>（1）缩放图片的大小（图像金字塔如图8.4.1所示）；</p>
<p>（2）缩放滑动窗的大小（如图8.4.2所示）。</p>
<p><img src="/img/ch8/8.4.1.png" alt=""></p>
<p>图 8.1 图像金字塔           </p>
<p>​      <img src="/img/ch8/8.4.2.png" alt=""></p>
<p> 图 8.2 缩放滑动窗口</p>
<p>​    基于深度学习的人脸检测算法中针对不同大小人脸主要也有两个策略，但和传统人脸检测算法有点区别，主要包括:</p>
<p>（1）缩放图片大小。（不过也可以通过缩放滑动窗的方式，基于深度学习的滑动窗人脸检测方式效率会很慢存在多次重复卷积，所以要采用全卷积神经网络（FCN），用FCN将不能用滑动窗的方法。）</p>
<p>（2）通过anchor box的方法（如图8.3所示，不要和图8.2混淆，这里是通过特征图预测原图的anchor box区域，具体在facebox中有描述）。</p>
<p><img src="/img/ch8/8.4.3.png" alt=""></p>
<p>图 8.3 anchor box</p>
<h3 id="8-4-3-如何设定算法检测最小人脸尺寸"><a href="#8-4-3-如何设定算法检测最小人脸尺寸" class="headerlink" title="8.4.3 如何设定算法检测最小人脸尺寸?"></a>8.4.3 如何设定算法检测最小人脸尺寸?</h3><p>主要是看滑动窗的最小窗口和anchorbox的最小窗口。</p>
<p>（1）滑动窗的方法 </p>
<p>假设通过12×12的滑动窗，不对原图做缩放的话，就可以检测原图中12×12的最小人脸。但是往往通常给定最小人脸a=40、或者a=80，以这么大的输入训练CNN进行人脸检测不太现实，速度会很慢，并且下一次需求最小人脸a=30*30又要去重新训练，通常还会是12×12的输入，为满足最小人脸框a，只需要在检测的时候对原图进行缩放即可：w=w×12/a。</p>
<p>（2）anchorbox的方法</p>
<p>原理类似，这里主要看anchorbox的最小box，通过可以通过缩放输入图片实现最小人脸的设定。</p>
<h3 id="8-4-4-如何定位人脸的位置？"><a href="#8-4-4-如何定位人脸的位置？" class="headerlink" title="8.4.4 如何定位人脸的位置？"></a>8.4.4 如何定位人脸的位置？</h3><p>（1）滑动窗的方式：</p>
<p>滑动窗的方式是基于分类器识别为人脸的框的位置确定最终的人脸，</p>
<p><img src="/img/ch8/8.4.4.png" alt=""></p>
<p>图 8.4 滑动窗</p>
<p>（2）FCN的方式：</p>
<p>​    FCN的方式通过特征图映射到原图的方式确定最终识别为人脸的位置，特征图映射到原图人脸框是要看特征图相比较于原图有多少次缩放（缩放主要查看卷积的步长和池化层），假设特征图上(2,3)的点，可粗略计算缩放比例为8倍，原图中的点应该是(16,24)；如果训练的FCN为12*12的输入，对于原图框位置应该是(16,24,12,12),当然这只是估计位置，具体的再构建网络时要加入回归框的预测，主要是相对于原图框的一个平移与缩放。</p>
<p>（3）通过anchor box的方式：</p>
<p>​    通过特征图映射到图的窗口，通过特征图映射到原图到多个框的方式确定最终识别为人脸的位置。</p>
<h3 id="8-4-5-如何通过一个人脸的多个框确定最终人脸框位置？"><a href="#8-4-5-如何通过一个人脸的多个框确定最终人脸框位置？" class="headerlink" title="8.4.5 如何通过一个人脸的多个框确定最终人脸框位置？"></a>8.4.5 如何通过一个人脸的多个框确定最终人脸框位置？</h3><p><img src="/img/ch8/8.4.5.png" alt=""></p>
<p>图 8.5 通过NMS得到最终的人脸位置</p>
<p>NMS改进版本有很多，最原始的NMS就是判断两个框的交集，如果交集大于设定的阈值，将删除其中一个框，那么两个框应该怎么选择删除哪一个呢？ 因为模型输出有概率值，一般会优选选择概率小的框删除。</p>
<h3 id="8-4-6-基于级联卷积神经网络的人脸检测（Cascade-CNN）"><a href="#8-4-6-基于级联卷积神经网络的人脸检测（Cascade-CNN）" class="headerlink" title="8.4.6 基于级联卷积神经网络的人脸检测（Cascade CNN）"></a>8.4.6 基于级联卷积神经网络的人脸检测（Cascade CNN）</h3><ol>
<li><p>cascade cnn的框架结构是什么？</p>
<p><img src="/img/ch8/8.4.6.png" alt=""></p>
</li>
</ol>
<p>级联结构中有6个CNN，3个CNN用于人脸非人脸二分类，另外3个CNN用于人脸区域的边框校正。给定一幅图像，12-net密集扫描整幅图片，拒绝90%以上的窗口。剩余的窗口输入到12-calibration-net中调整大小和位置，以接近真实目标。接着输入到NMS中，消除高度重叠窗口。下面网络与上面类似。</p>
<ol>
<li>cascade cnn人脸校验模块原理是什么？ </li>
</ol>
<p>该网络用于窗口校正，使用三个偏移变量：Xn:水平平移量，Yn:垂直平移量，Sn:宽高比缩放。候选框口(x,y,w,h)中，(x,y)表示左上点坐标，(w,h)表示宽和高。</p>
<p>我们要将窗口的控制坐标调整为：</p>
<script type="math/tex; mode=display">
（x-{x_nw}/{s_n},y-{y_nh}/{s_n},{w}/{s_n},{h}/{s_n}）</script><p>这项工作中，我们有$N=5×3×3=45$种模式。偏移向量三个参数包括以下值：</p>
<script type="math/tex; mode=display">
Sn：(0.83,0.91,1.0,1.10,1.21)</script><script type="math/tex; mode=display">
Xn：(-0.17,0,0.17)</script><script type="math/tex; mode=display">
Yn：(-0.17,0,0.17)</script><p>同时对偏移向量三个参数进行校正。</p>
<p><img src="/img/ch8/8.4.8.png" alt=""></p>
<p>3、训练样本应该如何准备？</p>
<p>人脸样本：</p>
<p>非人脸样本：</p>
<ol>
<li>级联的好处</li>
</ol>
<p>级联的工作原理和好处：</p>
<ul>
<li>最初阶段的网络可以比较简单，判别阈值可以设得宽松一点，这样就可以在保持较高召回率的同时排除掉大量的非人脸窗口；</li>
<li>最后阶段网络为了保证足够的性能，因此一般设计的比较复杂，但由于只需要处理前面剩下的窗口，因此可以保证足够的效率；</li>
<li>级联的思想可以帮助我们去组合利用性能较差的分类器，同时又可以获得一定的效率保证。</li>
</ul>
<h3 id="8-4-7-基于多任务卷积神经网络的人脸检测（MTCNN）"><a href="#8-4-7-基于多任务卷积神经网络的人脸检测（MTCNN）" class="headerlink" title="8.4.7 基于多任务卷积神经网络的人脸检测（MTCNN）"></a>8.4.7 基于多任务卷积神经网络的人脸检测（MTCNN）</h3><p><img src="/img/ch8/8.4.9.png" alt=""></p>
<p><img src="/img/ch8/8.4.10.png" alt=""></p>
<p><img src="/img/ch8/8.4.11.png" alt=""></p>
<p><img src="/img/ch8/8.4.12.png" alt=""></p>
<p>1.MTCNN模型有三个子网络。分别是P-Net,R-Net,O-Net.我想问一下，1.模型中的三个input size是指的是同一张图resize到不同尺度下喂给不同模型，还是同一张图，依次经过三个模型，然后是不同的输入尺寸？（这部分能给我讲一下吗）2.每个模型它都有对应三个结果（face classification;bounding box;facial landmark）这三个在网络上是如何对应的呢？</p>
<p>为了检测不同大小的人脸，开始需要构建图像金字塔，先经过pNet模型，输出人脸类别和边界框（边界框的预测为了对特征图映射到原图的框平移和缩放得到更准确的框），将识别为人脸的框映射到原图框位置可以获取patch，之后每一个patch通过resize的方式输入到rNet，识别为人脸的框并且预测更准确的人脸框，最后rNet识别为人脸的的每一个patch通过resize的方式输入到oNet，跟rNet类似，关键点是为了在训练集有限情况下使模型更鲁棒。</p>
<p>还要注意一点构建图像金字塔的的缩放比例要保留，为了将边界框映射到最开始原图上的</p>
<p>还要注意一点：如何从featureMap映射回原图</p>
<h3 id="8-4-8-Facebox"><a href="#8-4-8-Facebox" class="headerlink" title="8.4.8 Facebox"></a>8.4.8 Facebox</h3><p><img src="/img/ch8/8.4.13.png" alt=""></p>
<p><strong>（1）Rapidly Digested Convolutional Layers(RDCL)</strong></p>
<p>在网络前期，使用RDCL快速的缩小feature map的大小。 主要设计原则如下：</p>
<ul>
<li>Conv1, Pool1, Conv2 和 Pool2 的stride分别是4, 2, 2 和 2。这样整个RDCL的stride就是32，可以很快把feature map的尺寸变小。</li>
<li>卷积(或pooling)核太大速度就慢，太小覆盖信息又不足。文章权衡之后，将Conv1, Pool1, Conv2 和 Pool2 的核大小分别设为7x7,3x3,5x5,3x3</li>
<li>使用CReLU来保证输出维度不变的情况下，减少卷积核数量。</li>
</ul>
<p><strong>（2）Multiple Scale Convolutional Layers(MSCL)</strong></p>
<p>在网络后期，使用MSCL更好地检测不同尺度的人脸。 主要设计原则有：</p>
<ul>
<li>类似于SSD，在网络的不同层进行检测；</li>
<li>采用Inception模块。由于Inception包含多个不同的卷积分支，因此可以进一步使得感受野多样化。</li>
</ul>
<p><strong>（3）Anchor densification strategy</strong></p>
<p>为了anchor密度均衡，可以对密度不足的anchor以中心进行偏移加倍，如下图所示：</p>
<p><img src="/img/ch8/8.4.14.png" alt=""></p>
<h2 id="8-5-目标检测的技巧汇总"><a href="#8-5-目标检测的技巧汇总" class="headerlink" title="8.5 目标检测的技巧汇总"></a>8.5 目标检测的技巧汇总</h2><h3 id="8-5-1-Data-Augmentation（贡献者：北京理工大学—明奇）"><a href="#8-5-1-Data-Augmentation（贡献者：北京理工大学—明奇）" class="headerlink" title="8.5.1 Data Augmentation（贡献者：北京理工大学—明奇）"></a>8.5.1 Data Augmentation（贡献者：北京理工大学—明奇）</h3><p>介绍一篇发表在Big Data上的数据增强相关的文献综述。</p>
<ol>
<li><strong>Introduction</strong>  </li>
</ol>
<ul>
<li>数据增强与过拟合<br>验证是否过拟合的方法：画出loss曲线，如果训练集loss持续减小但是验证集loss增大，就说明是过拟合了。</li>
</ul>
<p><img src="/img/ch8/8.5.1-1.png" alt=""></p>
<ul>
<li><p>数据增强目的<br>通过数据增强实现数据更复杂的表征，从而减小验证集和训练集以及最终测试集的差距，让网络更好地学习迁移数据集上的数据分布。这也说明网络不是真正地理解数据，而是记忆数据分布。</p>
</li>
<li><p>数据增强的方法<br>（1）数据变换增强<br>包括几何变换、色彩空间变换，随机擦除，对抗训练，神经风格迁移等<br>（2）重采样增强<br>主要侧重于新的实例合成。如图像混合（mixup），特征空间的增强，GAN生成图片。一张图看明白：</p>
</li>
</ul>
<p><img src="/img/ch8/8.5.1-2.png" alt=""></p>
<ol>
<li><strong>Image Data Augmentation techniques</strong></li>
</ol>
<p>2.1 <strong>Data Augmentations based on basic image manipulations</strong>  </p>
<ul>
<li><p>Geometric transformations<br>&emsp;&emsp;如果数据集潜在的表征能够被观察和分离，那么简单的几何变换就能取得很好的效果。对于复杂的数据集如医学影像，数据小而且训练集和测试集的偏差大，几何变换等增强的合理运用就很关键。</p>
<ul>
<li><p>Flipping<br>作者提到了要衡量普遍性的观点。但是这种变换对于数字数据集不具有安全性。</p>
</li>
<li><p>Color space<br>主要提及的识别RGB通道上的变换，将三通道图进行分离，以及直方图变换增强等。（颜色空间更多增强方式可以参考A Preliminary Study on Data Augmentation of Deep Learning for Image Classification）</p>
</li>
<li><p>Cropping<br>通常在输入图片的尺寸不一时会进行按中心的裁剪操作。裁剪某种程度上和平移操作有相似性。根据裁剪幅度变化，该操作具有一定的不安全性。</p>
</li>
<li><p>Rotation<br>大幅度的旋转对数字集会有不安全性的考虑。</p>
</li>
<li><p>Translation<br>平移也需要合理设计。如车站人脸检测，只需要中心检测时，就可以加合适的平移增强。平移后空出部分填0或者255，或用高斯分布噪声。</p>
</li>
<li><p>Noise injection<br>在像素上叠加高斯分布的随机噪声。</p>
</li>
</ul>
</li>
<li><p>Color space transformations<br>&emsp;&emsp;由于实际图像中一定存在光线偏差，所以光线的增强十分有必要（但是IJCV的光流文章指出，3D建模的灯光增强实在是很难学习到，所以对于光线增强的效果不如几何也可能因为<strong>光线的复杂度更高，数据样本远远不够</strong>）。色彩变换十分多样，如像素限制、像素矩阵变换、像素值颠倒等；灰度图和彩图相比，计算时间成本大大较少，但是据实验效果会下降一些，很明显因为特征的维度被降维了；还有尝试将RGB映射到其他的色彩空间进行学习，YUV,CMY.HSV等。<br>&emsp;&emsp;除了计算大内存消耗和时间长等缺点，色彩变换也面临不安全性，比如识别人脸的关键信息是黄白黑，但是大量增强出红绿蓝，会丢信息。颜色变换的增强方法是从色彩空间角度拟合偏置，效果有限的可能性是多样的：1. 真实几何多样性比颜色更简单  2. 色彩的变化多样性更多，导致增强不够反而学不好，颜色空间的欠拟合 3. <strong>变换不安全</strong></p>
</li>
</ul>
<ul>
<li>Experiment<br><img src="/img/ch8/8.5.1-3.png" alt=""></li>
</ul>
<p><strong>随机裁剪</strong>效果最好。        </p>
<p>2.2  <strong>Geometric versus photometric transformations</strong> </p>
<ul>
<li><p>Kernel filter<br>滤波器核在图像处理用的比较广，这里提到用这种方法来增强。还提到了一种正则化增强方法PatchShuffle，在一个patch内随机交换像素值，使得对噪声的抵抗更强以及避免过拟合。<br>文章指出关于应用滤波器增强的工作尚且不多，因为这种方法其实和CNN的机制是一样的，这么做也许还不如直接在原始CNN上加层加深网络。</p>
</li>
<li><p>Mixing images<br><del>就是那篇被ICLR拒稿的采样方法</del>直接均值相加混合。  </p>
</li>
</ul>
<p><img src="/img/ch8/8.5.1-4.png" alt=""></p>
<p>&emsp;&emsp;还有非线性的mixup裁剪如下：  </p>
<p><img src="/img/ch8/8.5.1-5.png" alt=""></p>
<p>&emsp;&emsp;以及随机裁剪的图像混合：  </p>
<p><img src="/img/ch8/8.5.1-6.png" alt=""></p>
<p>&emsp;&emsp;这些混合方式是十分反人类直觉的，因此可解释性不强。只能说是可能增强了对底层低级特征如线条边缘等的鲁棒性。其实有点没有抓住关键点。</p>
<ul>
<li>Random erasing<br>随机擦除就是类似cutout的思想，通过mask的遮挡使得网络能够提高遮挡情况的鲁棒性。需要手工设计的部分包括mask的大小以及生成方式。是一种比较有效的方法。这种方式也需要考量增强的安全性，比如MNIST数据集8cutout后可能出问题。</li>
</ul>
<p><img src="/img/ch8/8.5.1-7.png" alt=""></p>
<ul>
<li>A note on combining augmentations<br>组合的增强方式往往是连续变化的，导致数据集的容量会迅速扩大，这对于小数据集领域来说容易发生过拟合 ，所以需要设计合理的搜索算法设计恰当的训练数据集。        </li>
</ul>
<p>2.3  <strong>Data Augmentations based on Deep Learning</strong></p>
<ul>
<li><p>Feature space augmentation<br>之前刚看的基于SMOTE类别不平衡的过采样法来进行特征空间的插值操作进行数据增强，就实验效果而言不算特别出众。</p>
</li>
<li><p>Adversarial training<br>对抗样本训练可以提高鲁棒性，但是实际应用中其实提高不一定明显，因为自然对抗样本的数目没有那么多。而NIPS的对抗攻击大赛很多从神经网络的学习策略下手，进行梯度攻击，更加偏向于人为的攻击了，对于普适的检测性能提高意义反而不大，更强调安全需求高的场合。</p>
</li>
<li><p>GAN‑based Data Augmentation</p>
</li>
<li><p>Neural Style Transfer</p>
</li>
</ul>
<p>不觉得这个效果会普遍很好，应该来说是针对特定域会有效（如白天黑夜），实际效果应该有限。</p>
<ul>
<li>Meta learning Data Augmentations <ul>
<li>Neural augmentation</li>
<li>Smart Augmentation<br>两个东西差不多，就是上次看到SmartAugment方法。随机采样类内图片进行通道叠加然后输出融合图像，学通过梯度下降使得输出图像的类内差距减小（没考虑类间关系，可能也不便处理）。</li>
</ul>
</li>
</ul>
<p><img src="/img/ch8/8.5.1-8.png" alt=""></p>
<ul>
<li>AutoAugment<br>谷歌最早做的自学习增强方法，走的NAS的思路RL+RNN搜索增强空间，还有后来最近发的检测增强也是大同小异，基本就是换汤不换药，问题在于<strong>搜索空间太大</strong>，复现搜索过于依赖硬件条件（<del>普通实验室玩不起</del>）</li>
</ul>
<ol>
<li><strong>Design considerations for image Data Augmentation</strong></li>
</ol>
<p>3.1  <strong>Test-time augmentation</strong><br>&emsp;&emsp;许多都论文指出在检测阶段进行同等的数据增强能够获得较好的效果。归结可以认为是训练检测阶段的一致性。当然，这种手段时间成本太高，只在如医学影像等追求精度的关键领域可以使用。        </p>
<p>3.2  <strong>Curriculum learning</strong><br>&emsp;&emsp;Bengio团队早年在ICML提出的观点，确实合理，一开始就进行大量的增强容易导致网络不收敛。<br>从一个数据集学习到的数据增强也可以迁移到其他数据集。</p>
<p>3.3  <strong>Resolution impact</strong><br>高清（1920×1080×3）或4K（3840×2160×3）等高分辨率图像需要更多的处理和内存来训练深度CNN。然而下一代模型更倾向于使用这样更高分辨率的图像。因为模型中常用的下采样会造成图像中信息的丢失，使图像识别更困难。<br>研究人员发现，高分辨率图像和低分辨率图像一起训练的模型集合，比单独的任何一个模型都要好。<br>某个实验（这里就不注明引用了）在256×256图像和512×512图像上训练的模型分别获得7.96%和7.42%的top-5 error。汇总后，他们的top-5 error变低，为6.97%。<br>随着超分辨率网络的发展，将图像放大到更高的分辨率后训练模型，能够得到更好更健壮的图像分类器。</p>
<p>3.4  <strong>Final dataset size</strong><br>&emsp;&emsp;数据增强的形式可以分为在线和离线增强。前者是在加载数据时增强，可能造成额外的内存消耗（现在都是数据容量不变的随机增强）。<br>&emsp;&emsp;此外作者提到了一个比较有意思的点：当前数据集尤其是进行增广后是十分庞大的，明显能够在一定程度上缩小数据集但是保持性能下降不多的子集效率会高得多。</p>
<p>3.5 <strong>Alleviating class imbalance with Data Augmentation</strong><br>&emsp;&emsp;这也是值得借鉴的一点。通过增强在一定程度上解决类别不平衡问题。但增强需要仔细设计，否则会面对已经学习较好的类别或者场景造成过拟合等问题。</p>
<h3 id="8-5-2-OHEM"><a href="#8-5-2-OHEM" class="headerlink" title="8.5.2  OHEM"></a>8.5.2  OHEM</h3><h3 id="8-5-3-NMS：Soft-NMS-Polygon-NMS-Inclined-NMS-ConvNMS-Yes-Net-NMS-Softer-NMS"><a href="#8-5-3-NMS：Soft-NMS-Polygon-NMS-Inclined-NMS-ConvNMS-Yes-Net-NMS-Softer-NMS" class="headerlink" title="8.5.3  NMS：Soft NMS/ Polygon NMS/ Inclined NMS/ ConvNMS/ Yes-Net NMS/ Softer NMS"></a>8.5.3  NMS：Soft NMS/ Polygon NMS/ Inclined NMS/ ConvNMS/ Yes-Net NMS/ Softer NMS</h3><h3 id="8-5-4-Multi-Scale-Training-Testing"><a href="#8-5-4-Multi-Scale-Training-Testing" class="headerlink" title="8.5.4  Multi Scale Training/Testing"></a>8.5.4  Multi Scale Training/Testing</h3><h3 id="8-5-5-建立小物体与context的关系"><a href="#8-5-5-建立小物体与context的关系" class="headerlink" title="8.5.5  建立小物体与context的关系"></a>8.5.5  建立小物体与context的关系</h3><h3 id="8-5-6-参考relation-network"><a href="#8-5-6-参考relation-network" class="headerlink" title="8.5.6  参考relation network"></a>8.5.6  参考relation network</h3><h3 id="8-5-7-结合GAN"><a href="#8-5-7-结合GAN" class="headerlink" title="8.5.7  结合GAN"></a>8.5.7  结合GAN</h3><h3 id="8-5-8-结合attention"><a href="#8-5-8-结合attention" class="headerlink" title="8.5.8  结合attention"></a>8.5.8  结合attention</h3><h3 id="8-5-9-训练tricks（贡献者：北京理工大学—明奇）"><a href="#8-5-9-训练tricks（贡献者：北京理工大学—明奇）" class="headerlink" title="8.5.9  训练tricks（贡献者：北京理工大学—明奇）"></a>8.5.9  训练tricks（贡献者：北京理工大学—明奇）</h3><p>介绍一篇2019.2.4亚马逊挂在ArXiv的目标检测训练tricks的文章（之前亚马逊发了篇分类的tricks在CVPR上）</p>
<ol>
<li><strong>Introduction</strong></li>
</ol>
<p>&emsp;&emsp;上次亚马逊发了个分类的训练trick在CVPR上，这次是检测的，还没发表。就没什么多说的了，下面直接介绍。先看效果如下，其实摘要声称的5%是单阶段的yolov3的提升，说明：单阶段没有RoIPooling阶段很多性质确实不如两阶段，因此采用trick很有必要；相反，两阶段本身结构优于单阶段所以外加的trick提供的如不变性等网络自身能够学习和适应就不起作用了。 </p>
<p><img src="/img/ch8/8.5.9-1.png" alt=""></p>
<ol>
<li><strong>Bag of Freebies</strong> </li>
</ol>
<p>&emsp;&emsp;提出了一种基于mixup的视觉联系图像混合方法，以及一些数据处理和训练策略。        </p>
<p>2.1  <strong>Visually Coherent Image Mixup for Object Detection</strong><br>&emsp;&emsp;先介绍图像分类中的mixup方法，作用是提供了训练的正则化，应用到图像上如下图，将图像作简单的像素值输入mixup的凸函数中得到合成图；然后将one-hot编码类似处理得到新的label。       </p>
<p><img src="/img/ch8/8.5.9-2.png" alt=""></p>
<p>&emsp;&emsp;技术细节：         </p>
<ul>
<li>相比于分类的resize，为了保证检测图像不畸变影响效果，作者选择直接叠加，取最大的宽高，空白进行灰度填充，不进行缩放。        </li>
<li>选择ab较大（如1.5,1.5）的Beta分布作为系数来混合图像，作者说是相干性视觉图像的更强；loss是两张图像物体的loss之和，loss计算权重分别是beta分布的系数 </li>
</ul>
<p><img src="/img/ch8/8.5.9-3.png" alt=""></p>
<p>2.2  <strong>Classification Head Label Smoothing</strong><br>&emsp;&emsp;标签平滑在检测的分类任务常有用到，最早是Inceptionv2中提出。<br>&emsp;&emsp;如果标签中有的是错的，或者不准，会导致网络过分信任标签而一起错下去。为了提高网络泛化能力，避免这种错误，在one-hot的label进行计算loss时，真实类别位置乘以一个系数（1-e），e很小如0.05，以0.95的概率送进去；非标注的类别原来为0，现在改为e=0.05送进去计算loss。网络的优化方向不变，但是相比0-1label会更加平滑。<br>（标签平滑这个讲的不错：<a href="https://juejin.im/post/5a29fd4051882534af25dc92）" target="_blank" rel="noopener">https://juejin.im/post/5a29fd4051882534af25dc92）</a></p>
<p><img src="/img/ch8/8.5.9-4.png" alt=""></p>
<p>&emsp;&emsp;这里进一步改进了一下label smooth的公式而已，在原来基础上除了个类别数。        </p>
<p>2.3  <strong>Data Preprocessing</strong><br>&emsp;&emsp;就是数据增强，没什么其他的。至于分类也是几何变换和色彩变换。这么分区别其实是是否变换label。但是将真实世界就这么简单地分解过于粗糙了。好不容易谷歌的增强考虑到了如何学习一下检测任务的增强，但是也只是加了bbox_only的增强，就效果而言，一般；而且就实际来说，合理性和有效性有待商榷。<br>&emsp;&emsp;作者认为，两阶段网络的RPN生成就是对输入的任意裁剪，所以这个增强就够了；这老哥膨胀了，two-stage就不用裁剪的增强，虽然两阶段能提供一些不变性，但是用了一般来说都是更好的。 </p>
<p>2.4  <strong>Training Schedule Revamping</strong><br>训练策略上：余弦学习率调整+warmup      </p>
<p>2.5  <strong>Synchronized Batch Normalization</strong><br>跨多卡同步正则化，土豪专区，穷人退避     </p>
<p>2.6  <strong>Random shapes training for single-stage object detection networks</strong><br>多尺度训练，每经过一定的iteration更换一种尺度。举例是yolov3的尺度范围。</p>
<h2 id="8-6-目标检测的常用数据集"><a href="#8-6-目标检测的常用数据集" class="headerlink" title="8.6 目标检测的常用数据集"></a>8.6 目标检测的常用数据集</h2><h3 id="8-6-1-PASCAL-VOC"><a href="#8-6-1-PASCAL-VOC" class="headerlink" title="8.6.1 PASCAL VOC"></a>8.6.1 PASCAL VOC</h3><p>​    VOC数据集是目标检测经常用的一个数据集，自2005年起每年举办一次比赛，最开始只有4类，到2007年扩充为20个类，共有两个常用的版本：2007和2012。学术界常用5k的train/val 2007和16k的train/val 2012作为训练集，test 2007作为测试集，用10k的train/val 2007+test 2007和16k的train/val 2012作为训练集，test2012作为测试集，分别汇报结果。</p>
<h3 id="8-6-2-MS-COCO"><a href="#8-6-2-MS-COCO" class="headerlink" title="8.6.2 MS COCO"></a>8.6.2 MS COCO</h3><p>​    COCO数据集是微软团队发布的一个可以用来图像recognition+segmentation+captioning 数据集，该数据集收集了大量包含常见物体的日常场景图片，并提供像素级的实例标注以更精确地评估检测和分割算法的效果，致力于推动场景理解的研究进展。依托这一数据集，每年举办一次比赛，现已涵盖检测、分割、关键点识别、注释等机器视觉的中心任务，是继ImageNet Chanllenge以来最有影响力的学术竞赛之一。</p>
<p>相比ImageNet，COCO更加偏好目标与其场景共同出现的图片，即non-iconic images。这样的图片能够反映视觉上的语义，更符合图像理解的任务要求。而相对的iconic images则更适合浅语义的图像分类等任务。</p>
<p>​    COCO的检测任务共含有80个类，在2014年发布的数据规模分train/val/test分别为80k/40k/40k，学术界较为通用的划分是使用train和35k的val子集作为训练集（trainval35k），使用剩余的val作为测试集（minival），同时向官方的evaluation server提交结果（test-dev）。除此之外，COCO官方也保留一部分test数据作为比赛的评测集。</p>
<h3 id="8-6-3-Google-Open-Image"><a href="#8-6-3-Google-Open-Image" class="headerlink" title="8.6.3 Google Open Image"></a>8.6.3 Google Open Image</h3><p>​    Open Image是谷歌团队发布的数据集。最新发布的Open Images V4包含190万图像、600个种类，1540万个bounding-box标注，是当前最大的带物体位置标注信息的数据集。这些边界框大部分都是由专业注释人员手动绘制的，确保了它们的准确性和一致性。另外，这些图像是非常多样化的，并且通常包含有多个对象的复杂场景（平均每个图像 8 个）。</p>
<h3 id="8-6-4-ImageNet"><a href="#8-6-4-ImageNet" class="headerlink" title="8.6.4 ImageNet"></a>8.6.4 ImageNet</h3><p>​    ImageNet是一个计算机视觉系统识别项目， 是目前世界上图像识别最大的数据库。ImageNet是美国斯坦福的计算机科学家，模拟人类的识别系统建立的。能够从图片识别物体。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。Imagenet数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。</p>
<h3 id="8-6-5-DOTA"><a href="#8-6-5-DOTA" class="headerlink" title="8.6.5 DOTA"></a>8.6.5 DOTA</h3><p>​    DOTA是遥感航空图像检测的常用数据集，包含2806张航空图像，尺寸大约为4kx4k，包含15个类别共计188282个实例，其中14个主类，small vehicle 和 large vehicle都是vehicle的子类。其标注方式为四点确定的任意形状和方向的四边形。航空图像区别于传统数据集，有其自己的特点，如：尺度变化性更大；密集的小物体检测；检测目标的不确定性。数据划分为1/6验证集，1/3测试集，1/2训练集。目前发布了训练集和验证集，图像尺寸从800x800到4000x4000不等。</p>
<h2 id="8-7-目标检测常用标注工具"><a href="#8-7-目标检测常用标注工具" class="headerlink" title="8.7 目标检测常用标注工具"></a>8.7 目标检测常用标注工具</h2><h3 id="8-7-1-LabelImg"><a href="#8-7-1-LabelImg" class="headerlink" title="8.7.1 LabelImg"></a>8.7.1 LabelImg</h3><p>​    LabelImg 是一款开源的图像标注工具，标签可用于分类和目标检测，它是用 Python 编写的，并使用Qt作为其图形界面，简单好用。注释以 PASCAL VOC 格式保存为 XML 文件，这是 ImageNet 使用的格式。 此外，它还支持 COCO 数据集格式。</p>
<h3 id="8-7-2-labelme"><a href="#8-7-2-labelme" class="headerlink" title="8.7.2 labelme"></a>8.7.2 labelme</h3><p>​    labelme 是一款开源的图像/视频标注工具，标签可用于目标检测、分割和分类。灵感是来自于 MIT 开源的一款标注工具 LabelMe。labelme 具有的特点是：</p>
<ul>
<li>支持图像的标注的组件有：矩形框，多边形，圆，线，点（rectangle, polygons, circle, lines, points）</li>
<li>支持视频标注</li>
<li>GUI 自定义</li>
<li>支持导出 VOC 格式用于 semantic/instance segmentation</li>
<li>支出导出 COCO 格式用于 instance segmentation</li>
</ul>
<h3 id="8-7-3-Labelbox"><a href="#8-7-3-Labelbox" class="headerlink" title="8.7.3 Labelbox"></a>8.7.3 Labelbox</h3><p>​    Labelbox 是一家为机器学习应用程序创建、管理和维护数据集的服务提供商，其中包含一款部分免费的数据标签工具，包含图像分类和分割，文本，音频和视频注释的接口，其中图像视频标注具有的功能如下：</p>
<ul>
<li>可用于标注的组件有：矩形框，多边形，线，点，画笔，超像素等（bounding box, polygons, lines, points，brush, subpixels）</li>
<li>标签可用于分类，分割，目标检测等</li>
<li>以 JSON / CSV / WKT / COCO / Pascal VOC 等格式导出数据</li>
<li>支持 Tiled Imagery (Maps)</li>
<li>支持视频标注 （快要更新）</li>
</ul>
<h3 id="8-7-4-RectLabel"><a href="#8-7-4-RectLabel" class="headerlink" title="8.7.4 RectLabel"></a>8.7.4 RectLabel</h3><p>​    RectLabel 是一款在线免费图像标注工具，标签可用于目标检测、分割和分类。具有的功能或特点：</p>
<ul>
<li>可用的组件：矩形框，多边形，三次贝塞尔曲线，直线和点，画笔，超像素</li>
<li>可只标记整张图像而不绘制</li>
<li>可使用画笔和超像素</li>
<li>导出为YOLO，KITTI，COCO JSON和CSV格式</li>
<li>以PASCAL VOC XML格式读写</li>
<li>使用Core ML模型自动标记图像</li>
<li>将视频转换为图像帧</li>
</ul>
<h3 id="8-7-5-CVAT"><a href="#8-7-5-CVAT" class="headerlink" title="8.7.5 CVAT"></a>8.7.5 CVAT</h3><p>​    CVAT 是一款开源的基于网络的交互式视频/图像标注工具，是对加州视频标注工具（Video Annotation Tool） 项目的重新设计和实现。OpenCV团队正在使用该工具来标注不同属性的数百万个对象，许多 UI 和 UX 的决策都基于专业数据标注团队的反馈。具有的功能</p>
<ul>
<li>关键帧之间的边界框插值</li>
<li>自动标注（使用TensorFlow OD API 和 Intel OpenVINO IR格式的深度学习模型）</li>
</ul>
<h3 id="8-7-6-VIA"><a href="#8-7-6-VIA" class="headerlink" title="8.7.6 VIA"></a>8.7.6 VIA</h3><p>​    VGG Image Annotator（VIA）是一款简单独立的手动注释软件，适用于图像，音频和视频。 VIA 在 Web 浏览器中运行，不需要任何安装或设置。 页面可在大多数现代Web浏览器中作为离线应用程序运行。</p>
<ul>
<li>支持标注的区域组件有：矩形，圆形，椭圆形，多边形，点和折线</li>
</ul>
<h3 id="8-7-6-其他标注工具"><a href="#8-7-6-其他标注工具" class="headerlink" title="8.7.6 其他标注工具"></a>8.7.6 其他标注工具</h3><p>​    liblabel，一个用 MATLAB 写的轻量级 语义/示例(semantic/instance) 标注工具。<br>ImageTagger：一个开源的图像标注平台。<br>Anno-Mage：一个利用深度学习模型半自动图像标注工具，预训练模型是基于MS COCO数据集，用 RetinaNet 训练的。</p>
<p>&lt;/br&gt;<br>​    当然还有一些数据标注公司，可能包含更多标注功能，例如对三维目标检测的标注（3D Bounding box Labelling），激光雷达点云的标注（LIDAR 3D Point Cloud Labeling）等。</p>
<h2 id="8-8-目标检测工具和框架（贡献者：北京理工大学—明奇）"><a href="#8-8-目标检测工具和框架（贡献者：北京理工大学—明奇）" class="headerlink" title="8.8 目标检测工具和框架（贡献者：北京理工大学—明奇）"></a>8.8 目标检测工具和框架（贡献者：北京理工大学—明奇）</h2><p>各种不同的算法虽然部分官方会有公布代码，或者github上有人复现，但是囿于安装环境不一，实现的框架（pytorch、C++、Caffee、tensorflow、MXNet等）不同，每次更换算法都需要重新安装环境，并且代码之间的迁移性差，十分不方便。所以为了方便将不同的算法统一在一个代码库中，不同的大厂都提出了自己的解决方案。如facebook的Detectron、商汤科技的mmdetection、SimpleDet等。其中Detectron最早，所以用户量最大，其次是国内近段时间崛起的mmdetection，下面介绍该目标检测工具箱。</p>
<ol>
<li><strong>Introduction</strong><br>MMdetection的特点：</li>
</ol>
<ul>
<li>模块化设计：将不同网络的部分进行切割，模块之间具有很高的复用性和独立性（十分便利，可以任意组合）</li>
<li>高效的内存使用</li>
<li>SOTA</li>
</ul>
<ol>
<li><strong>Support Frameworks</strong>  </li>
</ol>
<ul>
<li><p>单阶段检测器<br>SSD、RetinaNet、FCOS、FSAF</p>
</li>
<li><p>两阶段检测器<br>Faster R-CNN、R-FCN、Mask R-CNN、Mask Scoring R-CNN、Grid R-CNN</p>
</li>
<li><p>多阶段检测器<br>Cascade R-CNN、Hybrid Task Cascade</p>
</li>
<li><p>通用模块和方法<br>soft-NMS、DCN、OHEN、Train from Scratch 、M2Det 、GN 、HRNet 、Libra R-CNN</p>
</li>
</ul>
<ol>
<li><strong>Architecture</strong></li>
</ol>
<p>模型表征：划分为以下几个模块：<br>Backbone（ResNet等）、Neck（FPN）、DenseHead（AnchorHead）、RoIExtractor、RoIHead（BBoxHead/MaskHead）<br>结构图如下：<br><img src="/img/ch8/mmdetection.png" alt=""></p>
<ol>
<li><strong>Notice</strong></li>
</ol>
<ul>
<li>1x代表12epoch的COCO训练，2x类似推导</li>
<li>由于batch-size一般比较小（1/2这样的量级），所以大多数地方默认冻结BN层。可以使用GN代替。</li>
</ul>
<ol>
<li><strong>参考链接</strong><br>mmdetection代码高度模块化，十分好用和便利，更详细的文档直接参见官方文档：<br><a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener">https://github.com/open-mmlab/mmdetection</a></li>
</ol>
<p>注释版的mmdetection代码（更新至v1.0.0）：<a href="https://github.com/ming71/mmdetection-annotated" target="_blank" rel="noopener">https://github.com/ming71/mmdetection-annotated</a></p>
<p>使用方法简介：<br>安装记录（可能过时，以官方文档为准）：<a href="https://ming71.github.io/mmdetection-memo.html" target="_blank" rel="noopener">https://ming71.github.io/mmdetection-memo.html</a><br>使用方法（截止更新日期，如果过时以官方为准）：<a href="https://ming71.github.io/mmdetection-instruction.html" target="_blank" rel="noopener">https://ming71.github.io/mmdetection-instruction.html</a></p>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul>
<li>[ ] 目标检测基础知识：mAP、IoU和NMS等</li>
<li>[ ] 目标检测评测指标</li>
<li>[ ] 目标检测常见标注工具</li>
<li>[ ] 完善目标检测的技巧汇总</li>
<li>[ ] 目标检测的现在难点和未来发展</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">https://github.com/amusi/awesome-object-detection</a></p>
<p><a href="https://github.com/hoya012/deep_learning_object_detection" target="_blank" rel="noopener">https://github.com/hoya012/deep_learning_object_detection</a></p>
<p><a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html" target="_blank" rel="noopener">https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html</a></p>
<p><a href="https://www.zhihu.com/question/272322209/answer/482922713" target="_blank" rel="noopener">https://www.zhihu.com/question/272322209/answer/482922713</a></p>
<p><a href="http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b" target="_blank" rel="noopener">http://blog.leanote.com/post/afanti.deng@gmail.com/b5f4f526490b</a></p>
<p><a href="https://blog.csdn.net/hw5226349/article/details/78987385" target="_blank" rel="noopener">https://blog.csdn.net/hw5226349/article/details/78987385</a></p>
<p>[1] Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 580-587.</p>
<p>[2] Girshick R. Fast r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448.</p>
<p>[3] He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2015, 37(9): 1904-1916.</p>
<p>[4] Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99.</p>
<p>[5] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2117-2125.</p>
<p>[6] He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969.</p>
<p>[7] Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector[C]//European conference on computer vision. Springer, Cham, 2016: 21-37.</p>
<p>[8] Fu C Y, Liu W, Ranga A, et al. Dssd: Deconvolutional single shot detector[J]. arXiv preprint arXiv:1701.06659, 2017.</p>
<p>[9] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 779-788.</p>
<p>[10] Redmon J, Farhadi A. YOLO9000: better, faster, stronger[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7263-7271.</p>
<p>[11] Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. arXiv preprint arXiv:1804.02767, 2018.</p>
<p>[12] Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988.</p>
<p>[13] Liu S, Huang D. Receptive field block net for accurate and fast object detection[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 385-400.</p>
<p>[14] Zhao Q, Sheng T, Wang Y, et al. M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network[J]. arXiv preprint arXiv:1811.04533, 2018.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%85%AD%E7%AB%A0_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(RNN)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%85%AD%E7%AB%A0_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(RNN)/" class="post-title-link" itemprop="url">第六章_循环神经网络(RNN)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:25:17 / 修改时间：12:25:36" itemprop="dateCreated datePublished" datetime="2020-03-03T12:25:17+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第六章-循环神经网络-RNN"><a href="#第六章-循环神经网络-RNN" class="headerlink" title="第六章 循环神经网络(RNN)"></a>第六章 循环神经网络(RNN)</h1><h2 id="6-1-为什么需要RNN？"><a href="#6-1-为什么需要RNN？" class="headerlink" title="6.1 为什么需要RNN？"></a>6.1 为什么需要RNN？</h2><p>​    时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。一般的神经网络，在训练数据足够、算法模型优越的情况下，给定特定的x，就能得到期望y。其一般处理单个的输入，前一个输入和后一个输入完全无关，但实际应用中，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如：</p>
<p>​    当我们在理解一句话意思时，孤立的理解这句话的每个词不足以理解整体意思，我们通常需要处理这些词连接起来的整个序列； 当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就由此诞生了。</p>
<h2 id="6-2-图解RNN基本结构"><a href="#6-2-图解RNN基本结构" class="headerlink" title="6.2 图解RNN基本结构"></a>6.2 图解RNN基本结构</h2><h3 id="6-2-1-基本的单层网络结构"><a href="#6-2-1-基本的单层网络结构" class="headerlink" title="6.2.1 基本的单层网络结构"></a>6.2.1 基本的单层网络结构</h3><p>​    在进一步了解RNN之前，先给出最基本的单层网络结构，输入是<code>$x$</code>，经过变换<code>Wx+b</code>和激活函数<code>f</code>得到输出<code>y</code>：</p>
<p><img src="/img/ch6/6.1.jpg" alt=""></p>
<h3 id="6-2-2-图解经典RNN结构"><a href="#6-2-2-图解经典RNN结构" class="headerlink" title="6.2.2 图解经典RNN结构"></a>6.2.2 图解经典RNN结构</h3><p>​    在实际应用中，我们还会遇到很多序列形的数据，如：</p>
<ul>
<li><p>自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。</p>
</li>
<li><p>语音处理。此时，x1、x2、x3……是每帧的声音信号。</p>
</li>
<li><p>时间序列问题。例如每天的股票价格等等。</p>
<p>其单个序列如下图所示：</p>
<p><img src="/img/ch6/6.2.jpg" alt=""></p>
<p>前面介绍了诸如此类的序列数据用原始的神经网络难以建模，基于此，RNN引入了隐状态$h$（hidden state），$h​$可对序列数据提取特征，接着再转换为输出。</p>
<p>为了便于理解，先计算$h_1​$：</p>
<p><img src="/img/ch6/6.3.jpg" alt=""></p>
<p>注：图中的圆圈表示向量，箭头表示对向量做变换。</p>
<p>RNN中，每个步骤使用的参数<code>$U,W,b$</code>​相同，<code>$h_2$</code>的计算方式和<code>$h_1​$</code>类似，其计算结果如下：</p>
<p><img src="/img/ch6/6.4.jpg" alt=""></p>
<p>计算$h_3$,$h_4​$也相似，可得：</p>
<p><img src="/img/ch6/6.5.jpg" alt=""></p>
<p>接下来，计算RNN的输出$y_1$，采用$Softmax$作为激活函数，根据$y_n=f(Wx+b)$，得$y_1​$:</p>
<p><img src="/img/ch6/6.6.jpg" alt=""></p>
<p>使用和$y_1​$相同的参数$V,c​$，得到$y_1,y_2,y_3,y_4​$的输出结构：</p>
<p><img src="/img/ch6/6.7.jpg" alt=""></p>
<p>以上即为最经典的RNN结构，其输入为$x_1,x_2,x_3,x_4$，输出为$y_1,y_2,y_3,y_4$，当然实际中最大值为$y_n$，这里为了便于理解和展示，只计算4个输入和输出。从以上结构可看出，RNN结构的输入和输出等长。</p>
</li>
</ul>
<h3 id="6-2-3-vector-to-sequence结构"><a href="#6-2-3-vector-to-sequence结构" class="headerlink" title="6.2.3 vector-to-sequence结构"></a>6.2.3 vector-to-sequence结构</h3><p>​    有时我们要处理的问题输入是一个单独的值，输出是一个序列。此时，有两种主要建模方式：</p>
<p>​    方式一：可只在其中的某一个序列进行计算，比如序列第一个进行输入计算，其建模方式如下：</p>
<p><img src="/img/ch6/6.9.jpg" alt=""></p>
<p>​    方式二：把输入信息X作为每个阶段的输入，其建模方式如下：</p>
<p><img src="/img/ch6/6.10.jpg" alt=""></p>
<h3 id="6-2-4-sequence-to-vector结构"><a href="#6-2-4-sequence-to-vector结构" class="headerlink" title="6.2.4 sequence-to-vector结构"></a>6.2.4 sequence-to-vector结构</h3><p>​    有时我们要处理的问题输入是一个序列，输出是一个单独的值，此时通常在最后的一个序列上进行输出变换，其建模如下所示：</p>
<p>  <img src="/img/ch6/6.8.jpg" alt=""></p>
<h3 id="6-2-5-Encoder-Decoder结构"><a href="#6-2-5-Encoder-Decoder结构" class="headerlink" title="6.2.5 Encoder-Decoder结构"></a>6.2.5 Encoder-Decoder结构</h3><p>​    原始的sequence-to-sequence结构的RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。</p>
<p>​    其建模步骤如下：</p>
<p>​    <strong>步骤一</strong>：将输入数据编码成一个上下文向量$c$，这部分称为Encoder，得到$c$有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给$c$，还可以对最后的隐状态做一个变换得到$c$，也可以对所有的隐状态做变换。其示意如下所示：</p>
<p>  <img src="/img/ch6/6.12.jpg" alt=""></p>
<p>​    <strong>步骤二</strong>：用另一个RNN网络（我们将其称为Decoder）对其进行编码，方法一是将步骤一中的$c​$作为初始状态输入到Decoder，示意图如下所示：</p>
<p>  <img src="/img/ch6/6.13.jpg" alt=""></p>
<p>方法二是将$c$作为Decoder的每一步输入，示意图如下所示：</p>
<p>  <img src="/img/ch6/6.14.jpg" alt=""></p>
<h3 id="6-2-6-以上三种结构各有怎样的应用场景"><a href="#6-2-6-以上三种结构各有怎样的应用场景" class="headerlink" title="6.2.6  以上三种结构各有怎样的应用场景"></a>6.2.6  以上三种结构各有怎样的应用场景</h3><div class="table-container">
<table>
<thead>
<tr>
<th>网络结构</th>
<th style="text-align:center">结构图示</th>
<th>应用场景举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 vs N</td>
<td style="text-align:center"><img src="/img/ch6/6.9.jpg" alt=""></td>
<td>1、从图像生成文字，输入为图像的特征，输出为一段句子<br />2、根据图像生成语音或音乐，输入为图像特征，输出为一段语音或音乐</td>
</tr>
<tr>
<td>N vs 1</td>
<td style="text-align:center"><img src="/img/ch6/6.8.jpg" alt=""></td>
<td>1、输出一段文字，判断其所属类别<br />2、输入一个句子，判断其情感倾向<br />3、输入一段视频，判断其所属类别</td>
</tr>
<tr>
<td>N vs M</td>
<td style="text-align:center"><img src="/img/ch6/6.13.jpg" alt=""></td>
<td>1、机器翻译，输入一种语言文本序列，输出另外一种语言的文本序列<br />2、文本摘要，输入文本序列，输出这段文本序列摘要<br />3、阅读理解，输入文章，输出问题答案<br />4、语音识别，输入语音序列信息，输出文字序列</td>
</tr>
</tbody>
</table>
</div>
<h3 id="6-2-7-图解RNN中的Attention机制"><a href="#6-2-7-图解RNN中的Attention机制" class="headerlink" title="6.2.7 图解RNN中的Attention机制"></a>6.2.7 图解RNN中的Attention机制</h3><p>​    在上述通用的Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征$c​$再解码，因此，$c​$中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个$c​$可能存不下那么多信息，就会造成翻译精度的下降。Attention机制通过在每个时间输入不同的$c​$来解决此问题。</p>
<p>​    引入了Attention机制的Decoder中，有不同的$c$，每个$c​$会自动选择与当前输出最匹配的上下文信息，其示意图如下所示：</p>
<p><img src="/img/ch6/6.15.jpg" alt=""></p>
<p>​    <strong>举例</strong>，比如输入序列是“我爱中国”，要将此输入翻译成英文：</p>
<p>​    假如用$a_{ij}$衡量Encoder中第$j$阶段的$h_j$和解码时第$i$阶段的相关性，$a_{ij}$从模型中学习得到，和Decoder的第$i-1$阶段的隐状态、Encoder 第$j$个阶段的隐状态有关，比如$a_{3j}​$的计算示意如下所示：</p>
<p><img src="/img/ch6/6.19.jpg" alt=""></p>
<p>最终Decoder中第$i$阶段的输入的上下文信息 $c_i$来自于所有$h_j$对$a_{ij}$的加权和。</p>
<p>其示意图如下图所示：</p>
<p><img src="/img/ch6/6.16.jpg" alt=""></p>
<p>​    在Encoder中，$h_1,h_2,h_3,h_4$分别代表“我”，“爱”，“中”，“国”所代表信息。翻译的过程中，$c_1$会选择和“我”最相关的上下午信息，如上图所示，会优先选择$a_{11}$，以此类推，$c_2$会优先选择相关性较大的$a_{22}$，$c_3$会优先选择相关性较大的$a_{33}，a_{34}$，这就是attention机制。</p>
<h2 id="6-3-RNNs典型特点？"><a href="#6-3-RNNs典型特点？" class="headerlink" title="6.3 RNNs典型特点？"></a>6.3 RNNs典型特点？</h2><ol>
<li>RNNs主要用于处理序列数据。对于传统神经网络模型，从输入层到隐含层再到输出层，层与层之间一般为全连接，每层之间神经元是无连接的。但是传统神经网络无法处理数据间的前后关联问题。例如，为了预测句子的下一个单词，一般需要该词之前的语义信息。这是因为一个句子中前后单词是存在语义联系的。</li>
<li>RNNs中当前单元的输出与之前步骤输出也有关，因此称之为循环神经网络。具体的表现形式为当前单元会对之前步骤信息进行储存并应用于当前输出的计算中。隐藏层之间的节点连接起来，隐藏层当前输出由当前时刻输入向量和之前时刻隐藏层状态共同决定。</li>
<li>标准的RNNs结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。</li>
<li>在标准的RNN结构中，隐层的神经元之间也是带有权值的，且权值共享。</li>
<li>理论上，RNNs能够对任何长度序列数据进行处理。但是在实践中，为了降低复杂度往往假设当前的状态只与之前某几个时刻状态相关，<strong>下图便是一个典型的RNNs</strong>：</li>
</ol>
<p><img src="/img/ch6/figure_6.2_1.png" alt=""></p>
<p><img src="/img/ch6/figure_6.2_2.jpg" alt=""></p>
<p>输入单元(Input units)：输入集$\bigr\{x_0,x_1,…,x_t,x_{t+1},…\bigr\}$，</p>
<p>输出单元(Output units)：输出集$\bigr\{y_0,y_1,…,y_t,y_{y+1},…\bigr\}$，</p>
<p>隐藏单元(Hidden units)：输出集$\bigr\{s_0,s_1,…,s_t,s_{t+1},…\bigr\}$。</p>
<p><strong>图中信息传递特点：</strong></p>
<ol>
<li>一条单向流动的信息流是从输入单元到隐藏单元。</li>
<li>一条单向流动的信息流从隐藏单元到输出单元。</li>
<li>在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”。</li>
<li>在某些情况下，隐藏层的输入还包括上一时刻隐藏层的状态，即隐藏层内的节点可以自连也可以互连。 </li>
<li>当前单元（cell）输出是由当前时刻输入和上一时刻隐藏层状态共同决定。</li>
</ol>
<h2 id="6-4-CNN和RNN的区别-？"><a href="#6-4-CNN和RNN的区别-？" class="headerlink" title="6.4 CNN和RNN的区别 ？"></a>6.4 CNN和RNN的区别 ？</h2><div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>特点描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>相同点</td>
<td>1、传统神经网络的扩展。<br />2、前向计算产生结果，反向计算模型更新。<br />3、每层神经网络横向可以多个神经元共存,纵向可以有多层神经网络连接。</td>
</tr>
<tr>
<td>不同点</td>
<td>1、CNN空间扩展，神经元与特征卷积；RNN时间扩展，神经元与多个时间输出计算<br />2、RNN可以用于描述时间上连续状态的输出，有记忆功能，CNN用于静态输出</td>
</tr>
</tbody>
</table>
</div>
<h2 id="6-5-RNNs和FNNs有什么区别？"><a href="#6-5-RNNs和FNNs有什么区别？" class="headerlink" title="6.5 RNNs和FNNs有什么区别？"></a>6.5 RNNs和FNNs有什么区别？</h2><ol>
<li>不同于传统的前馈神经网络(FNNs)，RNNs引入了定向循环，能够处理输入之间前后关联问题。</li>
<li>RNNs可以记忆之前步骤的训练信息。<br><strong>定向循环结构如下图所示</strong>：</li>
</ol>
<p><img src="/img/ch6/figure_6.1_1.jpg" alt=""></p>
<h2 id="6-6-RNNs训练和传统ANN训练异同点？"><a href="#6-6-RNNs训练和传统ANN训练异同点？" class="headerlink" title="6.6 RNNs训练和传统ANN训练异同点？"></a>6.6 RNNs训练和传统ANN训练异同点？</h2><p><strong>相同点</strong>：</p>
<ol>
<li>RNNs与传统ANN都使用BP（Back Propagation）误差反向传播算法。</li>
</ol>
<p><strong>不同点</strong>：</p>
<ol>
<li>RNNs网络参数W,U,V是共享的(具体在本章6.2节中已介绍)，而传统神经网络各层参数间没有直接联系。</li>
<li>对于RNNs，在使用梯度下降算法中，每一步的输出不仅依赖当前步的网络，还依赖于之前若干步的网络状态。</li>
</ol>
<h2 id="6-7-为什么RNN-训练的时候Loss波动很大"><a href="#6-7-为什么RNN-训练的时候Loss波动很大" class="headerlink" title="6.7 为什么RNN 训练的时候Loss波动很大"></a>6.7 为什么RNN 训练的时候Loss波动很大</h2><p>​    由于RNN特有的memory会影响后期其他的RNN的特点，梯度时大时小，learning rate没法个性化的调整，导致RNN在train的过程中，Loss会震荡起伏，为了解决RNN的这个问题，在训练的时候，可以设置临界值，当梯度大于某个临界值，直接截断，用这个临界值作为梯度的大小，防止大幅震荡。</p>
<h2 id="6-8-标准RNN前向输出流程"><a href="#6-8-标准RNN前向输出流程" class="headerlink" title="6.8 标准RNN前向输出流程"></a>6.8 标准RNN前向输出流程</h2><p>​    以$x$表示输入，$h$是隐层单元，$o$是输出，$L$为损失函数，$y$为训练集标签。$t$表示$t$时刻的状态，$V,U,W$是权值，同一类型的连接权值相同。以下图为例进行说明标准RNN的前向传播算法：</p>
<p>​    <img src="/img/ch6/rnnbp.png" alt=""></p>
<p>对于$t$时刻：</p>
<script type="math/tex; mode=display">
h^{(t)}=\phi(Ux^{(t)}+Wh^{(t-1)}+b)</script><p>其中$\phi()$为激活函数，一般会选择tanh函数，$b$为偏置。</p>
<p>$t$时刻的输出为：</p>
<script type="math/tex; mode=display">
o^{(t)}=Vh^{(t)}+c</script><p>模型的预测输出为：</p>
<script type="math/tex; mode=display">
\widehat{y}^{(t)}=\sigma(o^{(t)})</script><p>其中$\sigma​$为激活函数，通常RNN用于分类，故这里一般用softmax函数。</p>
<h2 id="6-9-BPTT算法推导"><a href="#6-9-BPTT算法推导" class="headerlink" title="6.9 BPTT算法推导"></a>6.9 BPTT算法推导</h2><p>​    BPTT（back-propagation through time）算法是常用的训练RNN的方法，其本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。需要寻优的参数有三个，分别是U、V、W。与BP算法不同的是，其中W和U两个参数的寻优过程需要追溯之前的历史数据，参数V相对简单只需关注目前，那么我们就来先求解参数V的偏导数。</p>
<script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial V}=\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}</script><p>RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。</p>
<script type="math/tex; mode=display">
L=\sum_{t=1}^{n}L^{(t)}</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial V}=\sum_{t=1}^{n}\frac{\partial L^{(t)}}{\partial o^{(t)}}\cdot \frac{\partial o^{(t)}}{\partial V}</script><p>​    W和U的偏导的求解由于需要涉及到历史数据，其偏导求起来相对复杂。为了简化推导过程，我们假设只有三个时刻，那么在第三个时刻 L对W，L对U的偏导数分别为：</p>
<script type="math/tex; mode=display">
\frac{\partial L^{(3)}}{\partial W}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial W}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial W}</script><script type="math/tex; mode=display">
\frac{\partial L^{(3)}}{\partial U}=\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial U}+\frac{\partial L^{(3)}}{\partial o^{(3)}}\frac{\partial o^{(3)}}{\partial h^{(3)}}\frac{\partial h^{(3)}}{\partial h^{(2)}}\frac{\partial h^{(2)}}{\partial h^{(1)}}\frac{\partial h^{(1)}}{\partial U}</script><p>可以观察到，在某个时刻的对W或是U的偏导数，需要追溯这个时刻之前所有时刻的信息。根据上面两个式子得出L在t时刻对W和U偏导数的通式： </p>
<script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial W}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial W}</script><script type="math/tex; mode=display">
\frac{\partial L^{(t)}}{\partial U}=\sum_{k=0}^{t}\frac{\partial L^{(t)}}{\partial o^{(t)}}\frac{\partial o^{(t)}}{\partial h^{(t)}}(\prod_{j=k+1}^{t}\frac{\partial h^{(j)}}{\partial h^{(j-1)}})\frac{\partial h^{(k)}}{\partial U}</script><p>整体的偏导公式就是将其按时刻再一一加起来。</p>
<h2 id="6-9-RNN中为什么会出现梯度消失？"><a href="#6-9-RNN中为什么会出现梯度消失？" class="headerlink" title="6.9 RNN中为什么会出现梯度消失？"></a>6.9 RNN中为什么会出现梯度消失？</h2><p>首先来看tanh函数的函数及导数图如下所示：</p>
<p><img src="/img/ch6/tanh.jpg" alt=""></p>
<p>sigmoid函数的函数及导数图如下所示：</p>
<p><img src="/img/ch6/sigmoid.jpg" alt=""></p>
<p>从上图观察可知，sigmoid函数的导数范围是(0,0.25]，tanh函数的导数范围是(0,1]，他们的导数最大都不大于1。</p>
<p>​    基于6.8中式（9-10）中的推导，RNN的激活函数是嵌套在里面的，如果选择激活函数为$tanh$或$sigmoid$，把激活函数放进去，拿出中间累乘的那部分可得：</p>
<script type="math/tex; mode=display">
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{tanh^{'}}\cdot W_{s}</script><script type="math/tex; mode=display">
\prod_{j=k+1}^{t}{\frac{\partial{h^{j}}}{\partial{h^{j-1}}}} = \prod_{j=k+1}^{t}{sigmoid^{'}}\cdot W_{s}</script><p>​    <strong>梯度消失现象</strong>：基于上式，会发现累乘会导致激活函数导数的累乘，如果取tanh或sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。</p>
<p>​    实际使用中，会优先选择tanh函数，原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<h2 id="6-10-如何解决RNN中的梯度消失问题？"><a href="#6-10-如何解决RNN中的梯度消失问题？" class="headerlink" title="6.10 如何解决RNN中的梯度消失问题？"></a>6.10 如何解决RNN中的梯度消失问题？</h2><p>​    上节描述的梯度消失是在无限的利用历史数据而造成，但是RNN的特点本来就是能利用历史数据获取更多的可利用信息，解决RNN中的梯度消失方法主要有：</p>
<p>​    1、选取更好的激活函数，如Relu激活函数。ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。</p>
<p>​    2、加入BN层，其优点包括可加速收敛、控制过拟合，可以少用或不用Dropout和正则、降低网络对初始化权重不敏感，且能允许使用较大的学习率等。</p>
<p>​    2、改变传播结构，LSTM结构可以有效解决这个问题。下面将介绍LSTM相关内容。</p>
<h2 id="6-11-LSTM"><a href="#6-11-LSTM" class="headerlink" title="6.11 LSTM"></a>6.11 LSTM</h2><h3 id="6-11-1-LSTM的产生原因"><a href="#6-11-1-LSTM的产生原因" class="headerlink" title="6.11.1 LSTM的产生原因"></a>6.11.1 LSTM的产生原因</h3><p>​    RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象。为了解决该问题，研究人员提出了许多解决办法，例如ESN（Echo State Network），增加有漏单元（Leaky Units）等等。其中最成功应用最广泛的就是门限RNN（Gated RNN），而LSTM就是门限RNN中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许RNN累积距离较远节点间的长期联系；而门限RNN则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。</p>
<h3 id="6-11-2-图解标准RNN和LSTM的区别"><a href="#6-11-2-图解标准RNN和LSTM的区别" class="headerlink" title="6.11.2 图解标准RNN和LSTM的区别"></a>6.11.2 图解标准RNN和LSTM的区别</h3><p>​    所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层，如下图所示：</p>
<p><img src="/img/ch6/LSTM1.png" alt=""></p>
<p>​    LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。</p>
<p><img src="/img/ch6/LSTM2.png" alt=""></p>
<p>注：上图图标具体含义如下所示：</p>
<p><img src="/img/ch6/LSTM3.png" alt=""></p>
<p>​    上图中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。</p>
<h3 id="6-11-3-LSTM核心思想图解"><a href="#6-11-3-LSTM核心思想图解" class="headerlink" title="6.11.3 LSTM核心思想图解"></a>6.11.3 LSTM核心思想图解</h3><p>​    LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。示意图如下所示：</p>
<p><img src="/img/ch6/LSTM4.png" alt=""></p>
<p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。示意图如下：</p>
<p><img src="/img/ch6/LSTM5.png" alt=""></p>
<p>LSTM 拥有三个门，分别是忘记层门，输入层门和输出层门，来保护和控制细胞状态。</p>
<p><strong>忘记层门</strong></p>
<p>​    作用对象：细胞状态 。</p>
<p>​    作用：将细胞状态中的信息选择性的遗忘。</p>
<p>​    操作步骤：该门会读取$h_{t-1}$和$x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C_{t-1}​$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。示意图如下：</p>
<p><img src="/img/ch6/LSTM6.png" alt=""></p>
<p><strong>输入层门</strong></p>
<p>​    作用对象：细胞状态 </p>
<p>​    作用：将新的信息选择性的记录到细胞状态中。</p>
<p>​    操作步骤：</p>
<p>​    步骤一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。</p>
<p>​    步骤二，tanh 层创建一个新的候选值向量$\tilde{C}_t$加入到状态中。其示意图如下：</p>
<p><img src="/img/ch6/LSTM7.png" alt=""></p>
<p>​    步骤三：将$c_{t-1}$更新为$c_{t}$。将旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息。接着加上$i_t * \tilde{C}_t$得到新的候选值，根据我们决定更新每个状态的程度进行变化。其示意图如下：</p>
<p><img src="/img/ch6/LSTM8.png" alt=""></p>
<p><strong>输出层门</strong><br>    作用对象：隐层$h_t$ </p>
<p>​    作用：确定输出什么值。</p>
<p>​    操作步骤：</p>
<p>​    步骤一：通过sigmoid 层来确定细胞状态的哪个部分将输出。</p>
<p>​    步骤二：把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p>其示意图如下所示：</p>
<p><img src="/img/ch6/LSTM9.png" alt=""></p>
<h3 id="6-11-4-LSTM流行的变体"><a href="#6-11-4-LSTM流行的变体" class="headerlink" title="6.11.4 LSTM流行的变体"></a>6.11.4 LSTM流行的变体</h3><p><strong>增加peephole 连接</strong></p>
<p>​    在正常的LSTM结构中，Gers F A 等人提出增加peephole 连接，可以门层接受细胞状态的输入。示意图如下所示：</p>
<p><img src="/img/ch6/LSTM10.png" alt=""></p>
<p><strong>对忘记门和输入门进行同时确定</strong></p>
<p>​    不同于之前是分开确定什么忘记和需要添加什么新的信息，这里是一同做出决定。示意图如下所示：</p>
<p><img src="/img/ch6/LSTM11.png" alt=""></p>
<p><strong>Gated Recurrent Unit</strong></p>
<p>​     由Kyunghyun Cho等人提出的Gated Recurrent Unit (GRU)，其将忘记门和输入门合成了一个单一的更新门，同样还混合了细胞状态和隐藏状态，和其他一些改动。其示意图如下：</p>
<p><img src="/img/ch6/LSTM12.png" alt=""></p>
<p>最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<h2 id="6-12-LSTMs与GRUs的区别"><a href="#6-12-LSTMs与GRUs的区别" class="headerlink" title="6.12 LSTMs与GRUs的区别"></a>6.12 LSTMs与GRUs的区别</h2><p>LSTMs与GRUs的区别如图所示：</p>
<p><img src="/img/ch6/figure_6.6.6_2.png" alt=""></p>
<p>从上图可以看出，二者结构十分相似，<strong>不同在于</strong>：</p>
<ol>
<li>new memory都是根据之前state及input进行计算，但是GRUs中有一个reset gate控制之前state的进入量，而在LSTMs里没有类似gate；</li>
<li>产生新的state的方式不同，LSTMs有两个不同的gate，分别是forget gate (f gate)和input gate(i gate)，而GRUs只有一种update gate(z gate)；</li>
<li>LSTMs对新产生的state可以通过output gate(o gate)进行调节，而GRUs对输出无任何调节。</li>
</ol>
<h2 id="6-13-RNNs在NLP中典型应用？"><a href="#6-13-RNNs在NLP中典型应用？" class="headerlink" title="6.13 RNNs在NLP中典型应用？"></a>6.13 RNNs在NLP中典型应用？</h2><p><strong>（1）语言模型与文本生成(Language Modeling and Generating Text)</strong></p>
<p>​    给定一组单词序列，需要根据前面单词预测每个单词出现的可能性。语言模型能够评估某个语句正确的可能性，可能性越大，语句越正确。另一种应用便是使用生成模型预测下一个单词的出现概率，从而利用输出概率的采样生成新的文本。</p>
<p><strong>（2）机器翻译(Machine Translation)</strong></p>
<p>​    机器翻译是将一种源语言语句变成意思相同的另一种源语言语句，如将英语语句变成同样意思的中文语句。与语言模型关键的区别在于，需要将源语言语句序列输入后，才进行输出，即输出第一个单词时，便需要从完整的输入序列中进行获取。</p>
<p><strong>（3）语音识别(Speech Recognition)</strong></p>
<p>​    语音识别是指给定一段声波的声音信号，预测该声波对应的某种指定源语言语句以及计算该语句的概率值。 </p>
<p><strong>（4）图像描述生成 (Generating Image Descriptions)</strong></p>
<p>​    同卷积神经网络一样，RNNs已经在对无标图像描述自动生成中得到应用。CNNs与RNNs结合也被应用于图像描述自动生成。<br><img src="/img/ch6/figure_6.4_1.png" alt=""></p>
<h2 id="6-13-常见的RNNs扩展和改进模型"><a href="#6-13-常见的RNNs扩展和改进模型" class="headerlink" title="6.13 常见的RNNs扩展和改进模型"></a>6.13 常见的RNNs扩展和改进模型</h2><h3 id="6-13-1-Simple-RNNs-SRNs"><a href="#6-13-1-Simple-RNNs-SRNs" class="headerlink" title="6.13.1 Simple RNNs(SRNs)"></a>6.13.1 Simple RNNs(SRNs)</h3><ol>
<li>SRNs是一个三层网络，其在隐藏层增加了上下文单元。下图中的y是隐藏层，u是上下文单元。上下文单元节点与隐藏层中节点的连接是固定的，并且权值也是固定的。上下文节点与隐藏层节点一一对应，并且值是确定的。</li>
<li>在每一步中，使用标准的前向反馈进行传播，然后使用学习算法进行学习。上下文每一个节点保存其连接隐藏层节点上一步输出，即保存上文，并作用于当前步对应的隐藏层节点状态，即隐藏层的输入由输入层的输出与上一步的自身状态所决定。因此SRNs能够解决标准多层感知机(MLP)无法解决的对序列数据进行预测的问题。<br>SRNs网络结构如下图所示：</li>
</ol>
<p><img src="/img/ch6/figure_6.6.1_1.png" alt=""></p>
<h3 id="6-13-2-Bidirectional-RNNs"><a href="#6-13-2-Bidirectional-RNNs" class="headerlink" title="6.13.2 Bidirectional RNNs"></a>6.13.2 Bidirectional RNNs</h3><p>​    Bidirectional RNNs(双向网络)将两层RNNs叠加在一起，当前时刻输出(第t步的输出)不仅仅与之前序列有关，还与之后序列有关。例如：为了预测一个语句中的缺失词语，就需要该词汇的上下文信息。Bidirectional RNNs是一个相对较简单的RNNs，是由两个RNNs上下叠加在一起组成的。输出由前向RNNs和后向RNNs共同决定。如下图所示：</p>
<p><img src="/img/ch6/figure_6.6.2_1.png" alt=""></p>
<h3 id="6-13-3-Deep-RNNs"><a href="#6-13-3-Deep-RNNs" class="headerlink" title="6.13.3 Deep RNNs"></a>6.13.3 Deep RNNs</h3><p>​    Deep RNNs与Bidirectional RNNs相似，其也是又多层RNNs叠加，因此每一步的输入有了多层网络。该网络具有更强大的表达与学习能力，但是复杂性也随之提高，同时需要更多的训练数据。Deep RNNs的结构如下图所示：<br><img src="/img/ch6/figure_6.6.3_1.png" alt=""></p>
<h3 id="6-13-4-Echo-State-Networks（ESNs）"><a href="#6-13-4-Echo-State-Networks（ESNs）" class="headerlink" title="6.13.4 Echo State Networks（ESNs）"></a>6.13.4 Echo State Networks（ESNs）</h3><p><strong>ESNs特点</strong>：</p>
<ol>
<li>它的核心结构为一个随机生成、且保持不变的储备池(Reservoir)。储备池是大规模随机生成稀疏连接(SD通常保持1%～5%，SD表示储备池中互相连接的神经元占总神经元个数N的比例)的循环结构；</li>
<li>从储备池到输出层的权值矩阵是唯一需要调整的部分；</li>
<li>简单的线性回归便能够完成网络训练；</li>
</ol>
<p><strong>ESNs基本思想</strong>：</p>
<p>​    使用大规模随机连接的循环网络取代经典神经网络中的中间层，从而简化网络的训练过程。<br>网络中的参数包括：<br>（1）W - 储备池中节点间连接权值矩阵；<br>（2）Win - 输入层到储备池之间连接权值矩阵，表明储备池中的神经元之间是相互连接；<br>（3）Wback - 输出层到储备池之间的反馈连接权值矩阵，表明储备池会有输出层来的反馈；<br>（4）Wout - 输入层、储备池、输出层到输出层的连接权值矩阵，表明输出层不仅与储备池连接，还与输入层和自己连接。<br>（5）Woutbias - 输出层的偏置项。 </p>
<p>​    ESNs的结构如下图所示：</p>
<p><img src="/img/ch6/figure_6.6.4_2.png" alt=""></p>
<h3 id="6-13-4-Gated-Recurrent-Unit-Recurrent-Neural-Networks"><a href="#6-13-4-Gated-Recurrent-Unit-Recurrent-Neural-Networks" class="headerlink" title="6.13.4 Gated Recurrent Unit Recurrent Neural Networks"></a>6.13.4 Gated Recurrent Unit Recurrent Neural Networks</h3><p>GRUs是一般的RNNs的变型版本，其主要是从以下两个方面进行改进。</p>
<ol>
<li><p>以语句为例，序列中不同单词处的数据对当前隐藏层状态的影响不同，越前面的影响越小，即每个之前状态对当前的影响进行了距离加权，距离越远，权值越小。</p>
</li>
<li><p>在产生误差error时，其可能是由之前某一个或者几个单词共同造成，所以应当对对应的单词weight进行更新。GRUs的结构如下图所示。GRUs首先根据当前输入单词向量word vector以及前一个隐藏层状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前所有memory content，最终的memory是由之前的hidden state与new memory content一起决定。</p>
</li>
</ol>
<p><img src="/img/ch6/figure_6.6.5_1.png" alt=""></p>
<h3 id="6-13-5-Bidirectional-LSTMs"><a href="#6-13-5-Bidirectional-LSTMs" class="headerlink" title="6.13.5 Bidirectional LSTMs"></a>6.13.5 Bidirectional LSTMs</h3><ol>
<li>与bidirectional RNNs 类似，bidirectional LSTMs有两层LSTMs。一层处理过去的训练信息，另一层处理将来的训练信息。</li>
<li>在bidirectional LSTMs中，通过前向LSTMs获得前向隐藏状态，后向LSTMs获得后向隐藏状态，当前隐藏状态是前向隐藏状态与后向隐藏状态的组合。</li>
</ol>
<h3 id="6-13-6-Stacked-LSTMs"><a href="#6-13-6-Stacked-LSTMs" class="headerlink" title="6.13.6 Stacked LSTMs"></a>6.13.6 Stacked LSTMs</h3><ol>
<li>与deep rnns 类似，stacked LSTMs 通过将多层LSTMs叠加起来得到一个更加复杂的模型。</li>
<li>不同于bidirectional LSTMs，stacked LSTMs只利用之前步骤的训练信息。 </li>
</ol>
<h3 id="6-13-7-Clockwork-RNNs-CW-RNNs"><a href="#6-13-7-Clockwork-RNNs-CW-RNNs" class="headerlink" title="6.13.7 Clockwork RNNs(CW-RNNs)"></a>6.13.7 Clockwork RNNs(CW-RNNs)</h3><p>​    CW-RNNs是RNNs的改良版本，其使用时钟频率来驱动。它将隐藏层分为几个块(组，Group/Module)，每一组按照自己规定的时钟频率对输入进行处理。为了降低RNNs的复杂度，CW-RNNs减少了参数数量，并且提高了网络性能，加速网络训练。CW-RNNs通过不同隐藏层模块在不同时钟频率下工作来解决长时依赖问题。将时钟时间进行离散化，不同的隐藏层组将在不同时刻进行工作。因此，所有的隐藏层组在每一步不会全部同时工作，这样便会加快网络的训练。并且，时钟周期小组的神经元不会连接到时钟周期大组的神经元，只允许周期大的神经元连接到周期小的(组与组之间的连接以及信息传递是有向的)。周期大的速度慢，周期小的速度快，因此是速度慢的神经元连速度快的神经元，反之则不成立。</p>
<p>​    CW-RNNs与SRNs网络结构类似，也包括输入层(Input)、隐藏层(Hidden)、输出层(Output)，它们之间存在前向连接，输入层到隐藏层连接，隐藏层到输出层连接。但是与SRN不同的是，隐藏层中的神经元会被划分为若干个组，设为$g​$，每一组中的神经元个数相同，设为$k​$，并为每一个组分配一个时钟周期$T_i\epsilon\{T_1,T_2,…,T_g\}​$，每一组中的所有神经元都是全连接，但是组$j​$到组$i​$的循环连接则需要满足$T_j​$大于$T_i​$。如下图所示，将这些组按照时钟周期递增从左到右进行排序，即$T_1&lt;T_2&lt;…&lt;T_g​$，那么连接便是从右到左。例如：隐藏层共有256个节点，分为四组，周期分别是[1,2,4,8]，那么每个隐藏层组256/4=64个节点，第一组隐藏层与隐藏层的连接矩阵为64$\times​$64的矩阵，第二层的矩阵则为64$\times​$128矩阵，第三组为64$\times​$(3$\times​$64)=64$\times​$192矩阵，第四组为64$\times​$(4$\times​$64)=64$\times​$256矩阵。这就解释了上一段中速度慢的组连接到速度快的组，反之则不成立。</p>
<p><strong>CW-RNNs的网络结构如下图所示</strong>：</p>
<p><img src="/img/ch6/figure_6.6.7_1.png" alt=""></p>
<h3 id="6-13-8-CNN-LSTMs"><a href="#6-13-8-CNN-LSTMs" class="headerlink" title="6.13.8 CNN-LSTMs"></a>6.13.8 CNN-LSTMs</h3><ol>
<li>为了同时利用CNN以及LSTMs的优点，CNN-LSTMs被提出。在该模型中，CNN用于提取对象特征，LSTMs用于预测。CNN由于卷积特性，其能够快速而且准确地捕捉对象特征。LSTMs的优点在于能够捕捉数据间的长时依赖性。</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 何之源.<a href="https://zhuanlan.zhihu.com/p/28054589" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28054589</a>.</p>
<p>[2] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>[3] <a href="https://blog.csdn.net/zhaojc1995/article/details/80572098" target="_blank" rel="noopener">https://blog.csdn.net/zhaojc1995/article/details/80572098</a></p>
<p>[4] Graves A. Supervised Sequence Labelling with Recurrent Neural Networks[J]. Studies in Computational Intelligence, 2008, 385.</p>
<p>[5] Graves A. Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[6]  Greff K ,  Srivastava R K , Koutník, Jan, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp; Learning Systems, 2015, 28(10):2222-2232.</p>
<p>[7] Lanchantin J, Singh R, Wang B, et al. DEEP MOTIF DASHBOARD: VISUALIZING AND UNDERSTANDING GENOMIC SEQUENCES USING DEEP NEURAL NETWORKS.[J]. Pacific Symposium on Biocomputing Pacific Symposium on Biocomputing, 2016, 22:254.</p>
<p>[8]  Pascanu R ,  Mikolov T ,  Bengio Y . On the difficulty of training Recurrent Neural Networks[J].  2012.</p>
<p>[9]  Hochreiter S. The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions[J]. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 1998, 06(02):-.</p>
<p>[10] Dyer C, Kuncoro A, Ballesteros M, et al. Recurrent Neural Network Grammars[C]// Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.</p>
<p>[11]  Mulder W D ,  Bethard S ,  Moens M F . A survey on the application of recurrent neural networks to statistical language modeling.[M]. Academic Press Ltd.  2015.</p>
<p>[12] Graves A. Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[13] Zhang B, Xiong D, Su J. Neural Machine Translation with Deep Attention[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, PP(99):1-1.</p>
<p>[14] <a href="https://github.com/xuanyuansen/scalaLSTM" target="_blank" rel="noopener">https://github.com/xuanyuansen/scalaLSTM</a></p>
<p>[15] Deep Learning，Ian Goodfellow Yoshua Bengio and Aaron Courville，Book in preparation for MIT Press，2016；</p>
<p>[16] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>[17] Greff K, Srivastava R K, Koutník J, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp; Learning Systems, 2016, 28(10):2222-2232.</p>
<p>[18] Yao K ,  Cohn T ,  Vylomova K , et al. Depth-Gated Recurrent Neural Networks[J].  2015.</p>
<p>[19] Koutník J, Greff K, Gomez F, et al. A Clockwork RNN[J]. Computer Science, 2014:1863-1871.</p>
<p>[20]  Gers F A ,  Schmidhuber J . Recurrent nets that time and count[C]// Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on. IEEE, 2000.</p>
<p>[21] Li S, Wu C, Hai L, et al. FPGA Acceleration of Recurrent Neural Network Based Language Model[C]// IEEE International Symposium on Field-programmable Custom Computing Machines. 2015.</p>
<p>[22]  Mikolov T ,  Kombrink S ,  Burget L , et al. Extensions of recurrent neural network language model[C]// Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011.</p>
<p>[23]  Graves A . Generating Sequences With Recurrent Neural Networks[J]. Computer Science, 2013.</p>
<p>[24]  Sutskever I ,  Vinyals O ,  Le Q V . Sequence to Sequence Learning with Neural Networks[J].  2014.</p>
<p>[25] Liu B, Lane I. Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks[J].  2016.</p>
<p>[26] Graves A, Mohamed A R, Hinton G. Speech recognition with deep recurrent neural networks[C]// IEEE International Conference on Acoustics. 2013.</p>
<p>[27] <a href="https://cs.stanford.edu/people/karpathy/deepimagesent/" target="_blank" rel="noopener">https://cs.stanford.edu/people/karpathy/deepimagesent/</a></p>
<p>[28] Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/" class="post-title-link" itemprop="url">第五章 卷积神经网络(CNN)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:24:30 / 修改时间：12:24:49" itemprop="dateCreated datePublished" datetime="2020-03-03T12:24:30+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第五章-卷积神经网络（CNN）"><a href="#第五章-卷积神经网络（CNN）" class="headerlink" title="第五章 卷积神经网络（CNN）"></a>第五章 卷积神经网络（CNN）</h1><p>​    卷积神经网络是一种用来处理局部和整体相关性的计算网络结构，被应用在图像识别、自然语言处理甚至是语音识别领域，因为图像数据具有显著的局部与整体关系，其在图像识别领域的应用获得了巨大的成功。</p>
<h2 id="5-1-卷积神经网络的组成层"><a href="#5-1-卷积神经网络的组成层" class="headerlink" title="5.1 卷积神经网络的组成层"></a>5.1 卷积神经网络的组成层</h2><p>​    以图像分类任务为例，在表5.1所示卷积神经网络中，一般包含5种类型的网络层次结构：</p>
<p>​                                                                 表5.1 卷积神经网络的组成</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">CNN层次结构</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输入层</td>
<td style="text-align:center">$W_1\times H_1\times 3$</td>
<td style="text-align:left">卷积网络的原始输入，可以是原始或预处理后的像素矩阵</td>
</tr>
<tr>
<td style="text-align:center">卷积层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">参数共享、局部连接，利用平移不变性从全局特征图提取局部特征</td>
</tr>
<tr>
<td style="text-align:center">激活层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">将卷积层的输出结果进行非线性映射</td>
</tr>
<tr>
<td style="text-align:center">池化层</td>
<td style="text-align:center">$W_2\times H_2\times K$</td>
<td style="text-align:left">进一步筛选特征，可以有效减少后续网络层次所需的参数量</td>
</tr>
<tr>
<td style="text-align:center">全连接层</td>
<td style="text-align:center">$(W_2 \cdot H_2 \cdot K)\times C$</td>
<td style="text-align:left">将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值）</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>$W_1\times H_1\times 3$对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;$K$表示卷积层中卷积核（滤波器）的个数;$W_2\times H_2$ 为池化后特征图的尺度，在全局池化中尺度对应$1\times 1$;$(W_2 \cdot H_2 \cdot K)$是将多维特征压缩到1维之后的大小，$C$对应的则是图像类别个数。</p>
</blockquote>
<h3 id="5-1-1-输入层"><a href="#5-1-1-输入层" class="headerlink" title="5.1.1 输入层"></a>5.1.1 输入层</h3><p>​    输入层(Input Layer)通常是输入卷积神经网络的原始数据或经过预处理的数据，可以是图像识别领域中原始三维的多彩图像，也可以是音频识别领域中经过傅利叶变换的二维波形数据，甚至是自然语言处理中一维表示的句子向量。以图像分类任务为例，输入层输入的图像一般包含RGB三个通道，是一个由长宽分别为$H$和$W$组成的3维像素值矩阵$H\times W \times 3$，卷积网络会将输入层的数据传递到一系列卷积、池化等操作进行特征提取和转化，最终由全连接层对特征进行汇总和结果输出。根据计算能力、存储大小和模型结构的不同，卷积神经网络每次可以批量处理的图像个数不尽相同，若指定输入层接收到的图像个数为$N$，则输入层的输出数据为$N\times H\times W\times 3$。</p>
<h3 id="5-1-2-卷积层"><a href="#5-1-2-卷积层" class="headerlink" title="5.1.2 卷积层"></a>5.1.2 卷积层</h3><p>​    卷积层(Convolution Layer)通常用作对输入层输入数据进行特征提取，通过卷积核矩阵对原始数据中隐含关联性的一种抽象。卷积操作原理上其实是对两张像素矩阵进行点乘求和的数学操作，其中一个矩阵为输入的数据矩阵，另一个矩阵则为卷积核（滤波器或特征矩阵），求得的结果表示为原始图像中提取的特定局部特征。图5.1表示卷积操作过程中的不同填充策略，上半部分采用零填充，下半部分采用有效卷积（舍弃不能完整运算的边缘部分）。<br>​                                                    <img src="img/ch5/convolution.png" alt="conv-same"><br>​                                                        图5.1 卷积操作示意图</p>
<h3 id="5-1-3-激活层"><a href="#5-1-3-激活层" class="headerlink" title="5.1.3 激活层"></a>5.1.3 激活层</h3><p>​    激活层(Activation Layer)负责对卷积层抽取的特征进行激活，由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。激活层主要由激活函数组成，即在卷积层输出结果的基础上嵌套一个非线性函数，让输出的特征图具有非线性关系。卷积网络中通常采用ReLU来充当激活函数（还包括tanh和sigmoid等）ReLU的函数形式如公式（5-1）所示，能够限制小于0的值为0,同时大于等于0的值保持不变。</p>
<script type="math/tex; mode=display">
f(x)=\begin{cases}
   0 &\text{if } x<0 \\
   x &\text{if } x\ge 0
\end{cases}
\tag{5-1}</script><h3 id="5-1-4-池化层"><a href="#5-1-4-池化层" class="headerlink" title="5.1.4 池化层"></a>5.1.4 池化层</h3><p>​    池化层又称为降采样层(Downsampling Layer)，作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低输出特征尺度，进而减少模型所需要的参数量。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受域内最大、平均与总和的特征值作为输出，最常用的是最大池化。</p>
<h3 id="5-1-5-全连接层"><a href="#5-1-5-全连接层" class="headerlink" title="5.1.5 全连接层"></a>5.1.5 全连接层</h3><p>​    全连接层(Full Connected Layer)负责对卷积神经网络学习提取到的特征进行汇总，将多维的特征输入映射为二维的特征输出，高维表示样本批次，低位常常对应任务目标。</p>
<h2 id="5-2-卷积在图像中有什么直观作用"><a href="#5-2-卷积在图像中有什么直观作用" class="headerlink" title="5.2 卷积在图像中有什么直观作用"></a>5.2 卷积在图像中有什么直观作用</h2><p>​    在卷积神经网络中，卷积常用来提取图像的特征，但不同层次的卷积操作提取到的特征类型是不相同的，特征类型粗分如表5.2所示。<br>​                                                                 表5.2 卷积提取的特征类型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积层次</th>
<th style="text-align:center">特征类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">浅层卷积</td>
<td style="text-align:center">边缘特征</td>
</tr>
<tr>
<td style="text-align:center">中层卷积</td>
<td style="text-align:center">局部特征</td>
</tr>
<tr>
<td style="text-align:center">深层卷积</td>
<td style="text-align:center">全局特征</td>
</tr>
</tbody>
</table>
</div>
<p>图像与不同卷积核的卷积可以用来执行边缘检测、锐化和模糊等操作。表5.3显示了应用不同类型的卷积核（滤波器）后的各种卷积图像。<br>​                                                                 表5.3 一些常见卷积核的作用</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积作用</th>
<th style="text-align:center">卷积核</th>
<th style="text-align:center">卷积后图像</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输出原图</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; 0 &amp; 0 \ 0 &amp; 1 &amp; 0 \ 0 &amp; 0 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat.jpg" alt="origin_img"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出边缘差异）</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 0 &amp; -1 \ 0 &amp; 0 &amp; 0 \ -1 &amp; 0 &amp; 1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-edgeDetect.jpg" alt="edgeDetect-1"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出中间值）</td>
<td style="text-align:center">$\begin{bmatrix} -1 &amp; -1 &amp; -1 \ -1 &amp; 8 &amp; -1 \ -1 &amp; -1 &amp; -1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-edgeDetect-2.jpg" alt="edgeDetect-2"></td>
</tr>
<tr>
<td style="text-align:center">图像锐化</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; -1 &amp; 0 \ -1 &amp; 5 &amp; -1 \ 0 &amp; -1 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-sharpen.jpg" alt="sharpen_img"></td>
</tr>
<tr>
<td style="text-align:center">方块模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \end{bmatrix} \times \frac{1}{9}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-boxblur.jpg" alt="box_blur"></td>
</tr>
<tr>
<td style="text-align:center">高斯模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 2 &amp; 1 \ 2 &amp; 4 &amp; 2 \ 1 &amp; 2 &amp; 1 \end{bmatrix} \times \frac{1}{16}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-blur-gaussian.jpg" alt="gaussian_blur"></td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-3-卷积层有哪些基本参数？"><a href="#5-3-卷积层有哪些基本参数？" class="headerlink" title="5.3 卷积层有哪些基本参数？"></a>5.3 卷积层有哪些基本参数？</h2><p>​    卷积层中需要用到卷积核（滤波器或特征检测器）与图像特征矩阵进行点乘运算，利用卷积核与对应的特征感受域进行划窗式运算时，需要设定卷积核对应的大小、步长、个数以及填充的方式，如表5.4所示。</p>
<p>​                                                                         表5.4 卷积层的基本参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">常见设置</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积核大小 (Kernel Size)</td>
<td style="text-align:left">卷积核的大小定义了卷积的感受野</td>
<td style="text-align:left">在过去常设为5，如LeNet-5；现在多设为3，通过堆叠$3\times3$的卷积核来达到更大的感受域</td>
</tr>
<tr>
<td style="text-align:center">卷积核步长 (Stride)</td>
<td style="text-align:left">定义了卷积核在卷积过程中的步长</td>
<td style="text-align:left">常见设置为1，表示滑窗距离为1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样</td>
</tr>
<tr>
<td style="text-align:center">填充方式 (Padding)</td>
<td style="text-align:left">在卷积核尺寸不能完美匹配输入的图像矩阵时需要进行一定的填充策略</td>
<td style="text-align:left">设置为’SAME’表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；当设置为’VALID’时则对不足卷积尺寸的部分进行舍弃，输出维度就无法保证与输入维度一致</td>
</tr>
<tr>
<td style="text-align:center">输入通道数 (In Channels)</td>
<td style="text-align:left">指定卷积操作时卷积核的深度</td>
<td style="text-align:left">默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式</td>
</tr>
<tr>
<td style="text-align:center">输出通道数 (Out Channels)</td>
<td style="text-align:left">指定卷积核的个数</td>
<td style="text-align:left">若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>卷积操作维度变换公式：</p>
<p>$O_d =\begin{cases} \lceil \frac{(I_d - k_{size})+ 1)}{s}\rceil ,&amp; \text{padding=VALID}\ \lceil \frac{I_d}{s}\rceil,&amp;\text{padding=SAME} \end{cases}$</p>
<p>其中，$I_d$为输入维度，$O_d$为输出维度，$k_{size}$为卷积核大小，$s$为步长</p>
</blockquote>
<h2 id="5-4-卷积核有什么类型？"><a href="#5-4-卷积核有什么类型？" class="headerlink" title="5.4 卷积核有什么类型？"></a>5.4 卷积核有什么类型？</h2><p>​    常见的卷积主要是由连续紧密的卷积核对输入的图像特征进行滑窗式点乘求和操作，除此之外还有其他类型的卷积核在不同的任务中会用到，具体分类如表5.5所示。<br>​                                                                     表5.5 卷积核分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积类别</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">标准卷积</td>
<td style="text-align:center"><img src="/img/ch5/img7.png" alt="image"></td>
<td style="text-align:left">最常用的卷积核，连续紧密的矩阵形式可以提取图像区域中的相邻像素之间的关联关系，$3\times3$的卷积核可以获得$3\times3$像素范围的感受视野</td>
</tr>
<tr>
<td style="text-align:center">扩张卷积（带孔卷积或空洞卷积）</td>
<td style="text-align:center"><img src="/img/ch5/img8.png" alt="image"></td>
<td style="text-align:left">引入一个称作扩张率（Dilation Rate）的参数，使同样尺寸的卷积核可以获得更大的感受视野，相应的在相同感受视野的前提下比普通卷积采用更少的参数。同样是$3\times3$的卷积核尺寸，扩张卷积可以提取$5\times5$范围的区域特征，在实时图像分割领域广泛应用</td>
</tr>
<tr>
<td style="text-align:center">转置卷积</td>
<td style="text-align:center"><img src="/img/ch5/img10.png" alt="image"></td>
<td style="text-align:left">先对原始特征矩阵进行填充使其维度扩大到适配卷积目标输出维度，然后进行普通的卷积操作的一个过程，其输入到输出的维度变换关系恰好与普通卷积的变换关系相反，但这个变换并不是真正的逆变换操作，通常称为转置卷积(Transpose Convolution)而不是反卷积(Deconvolution)。转置卷积常见于目标检测领域中对小目标的检测和图像分割领域还原输入图像尺度。</td>
</tr>
<tr>
<td style="text-align:center">可分离卷积</td>
<td style="text-align:center"><img src="/img/ch5/img11.png" alt="image"></td>
<td style="text-align:left">标准的卷积操作是同时对原始图像$H\times W\times C$三个方向的卷积运算，假设有$K$个相同尺寸的卷积核，这样的卷积操作需要用到的参数为$H\times W\times C\times K$个；若将长宽与深度方向的卷积操作分离出变为$H\times W$与$C$的两步卷积操作，则同样的卷积核个数$K$，只需要$(H\times W + C)\times K$个参数，便可得到同样的输出尺度。可分离卷积(Seperable Convolution)通常应用在模型压缩或一些轻量的卷积神经网络中，如MobileNet$^{[1]}$、Xception$^{[2]}$等</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-5-二维卷积与三维卷积有什么区别？"><a href="#5-5-二维卷积与三维卷积有什么区别？" class="headerlink" title="5.5 二维卷积与三维卷积有什么区别？"></a>5.5 二维卷积与三维卷积有什么区别？</h2><ul>
<li><strong>二维卷积</strong><br>二维卷积操作如图5.3所示，为了更直观的说明，分别展示在单通道和多通道输入中，对单个通道输出的卷积操作。在单通道输入的情况下，若输入卷积核尺寸为 $(k_h, k_w, 1)​$，卷积核在输入图像的空间维度上进行滑窗操作，每次滑窗和 $(k_h, k_w)​$窗口内的值进行卷积操作，得到输出图像中的一个值。在多通道输入的情况下，假定输入图像特征通道数为3，卷积核尺寸则为$(k_h, k_w, 3)​$，每次滑窗与3个通道上的$(k_h, k_w)​$窗口内的所有值进行卷积操作，得到输出图像中的一个值。</li>
</ul>
<p><img src="/img/ch5/5.6.1.png" alt="image"></p>
<ul>
<li><strong>三维卷积</strong><br>3D卷积操作如图所示，同样分为单通道和多通道，且假定只使用1个卷积核，即输出图像仅有一个通道。对于单通道输入，与2D卷积不同之处在于，输入图像多了一个深度(depth)维度，卷积核也多了一个$k_d​$维度，因此3D卷积核的尺寸为$(k_h, k_w, k_d)​$，每次滑窗与$(k_h, k_w, k_d)​$窗口内的值进行相关操作，得到输出3D图像中的一个值。对于多通道输入，则与2D卷积的操作一样，每次滑窗与3个channels上的$(k_h, k_w, k_d)​$窗口内的所有值进行相关操作，得到输出3D图像中的一个值。</li>
</ul>
<p><img src="/img/ch5/5.6.2.png" alt="image"></p>
<h2 id="5-7-有哪些池化方法？"><a href="#5-7-有哪些池化方法？" class="headerlink" title="5.7 有哪些池化方法？"></a>5.7 有哪些池化方法？</h2><p>​    池化操作通常也叫做子采样(Subsampling)或降采样(Downsampling)，在构建卷积神经网络时，往往会用在卷积层之后，通过池化来降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象。池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。因此，为了描述一个大的图像，很直观的想法就是对不同位置的特征进行聚合统计。例如，可以计算图像在固定区域上特征的平均值 (或最大值)来代表这个区域的特征。<br>​                                                                              表5.6 池化分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">池化类型</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">一般池化(General Pooling)</td>
<td style="text-align:center"><img src="/img/ch5/general_pooling.png" alt="max_pooling"></td>
<td style="text-align:left">通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。以最大池化为例，池化范围$(2\times2)$和滑窗步长$(stride=2)$ 相同，仅提取一次相同区域的范化特征。</td>
</tr>
<tr>
<td style="text-align:center">重叠池化(Overlapping Pooling)</td>
<td style="text-align:center"><img src="/img/ch5/overlap_pooling.png" alt="overlap_pooling"></td>
<td style="text-align:left">与一般池化操作相同，但是池化范围$P_{size}$与滑窗步长$stride$关系为$P_{size}&gt;stride$，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。</td>
</tr>
<tr>
<td style="text-align:center">空间金字塔池化$^*$(Spatial Pyramid Pooling)</td>
<td style="text-align:center"><img src="/img/ch5/spatial_pooling.png" alt="spatial_pooling"></td>
<td style="text-align:left">在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>SPPNet$^{[3]}$就引入了空间池化的组合，对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同$(win_{size}=\lceil \frac{in}{out}\rceil; stride=\lfloor \frac{in}{out}\rfloor; )$，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取</p>
</blockquote>
<h2 id="5-8-1-times1-卷积作用？"><a href="#5-8-1-times1-卷积作用？" class="headerlink" title="5.8 $1\times1$卷积作用？"></a>5.8 $1\times1$卷积作用？</h2><p>​    NIN(Network in Network)$^{[4]}​$是第一篇探索$1\times1​$卷积核的论文，这篇论文通过在卷积层中使用MLP替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力，也因其网络结构中嵌套MLP子网络而得名NIN。NIN对不同通道的特征整合到MLP自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，其中的MLP子网络恰恰可以用$1\times1​$的卷积进行代替。</p>
<p>​    GoogLeNet$^{[5]}​$则采用$1\times1​$卷积核来减少模型的参数量。在原始版本的Inception模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。此时在每一个较大卷积核的卷积层前引入$1\times1​$卷积，可以通过分离通道与宽高卷积来减少模型参数量。以图5.2为例，在不考虑参数偏置项的情况下，若输入和输出的通道数为$C_1=16​$，则左半边网络模块所需的参数为$(1\times1+3\times3+5\times5+0)\times C_1\times C_1=8960​$；假定右半边网络模块采用的$1\times1​$卷积通道数为$C_2=8​$$(满足C_1&gt;C_2)​$，则右半部分的网络结构所需参数量为$(1\times1\times (3C_1+C_2)+3\times3\times C_2 +5\times5\times C_2)\times C_1=5248​$ ，可以在不改变模型表达能力的前提下大大减少所使用的参数量。</p>
<p><img src="/img/ch5/5.8-1.png" alt="image"></p>
<p>​                                图5.2 Inception模块</p>
<p>综上所述，$1\times 1​$卷积的作用主要为以下两点：</p>
<ul>
<li>实现信息的跨通道交互和整合。</li>
<li>对卷积核通道数进行降维和升维，减小参数量。</li>
</ul>
<h2 id="5-9-卷积层和池化层有什么区别？"><a href="#5-9-卷积层和池化层有什么区别？" class="headerlink" title="5.9 卷积层和池化层有什么区别？"></a>5.9 卷积层和池化层有什么区别？</h2><p>​    卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据步长设置获取到不同维度的输出，但是其内在操作是有本质区别的，如表5.7所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">卷积层</th>
<th style="text-align:center">池化层</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>结构</strong></td>
<td style="text-align:center">零填充时输出维度不变，而通道数改变</td>
<td style="text-align:center">通常特征维度会降低，通道数不变</td>
</tr>
<tr>
<td style="text-align:center"><strong>稳定性</strong></td>
<td style="text-align:center">输入特征发生细微改变时，输出结果会改变</td>
<td style="text-align:center">感受域内的细微变化不影响输出结果</td>
</tr>
<tr>
<td style="text-align:center"><strong>作用</strong></td>
<td style="text-align:center">感受域内提取局部关联特征</td>
<td style="text-align:center">感受域内提取泛化特征，降低维度</td>
</tr>
<tr>
<td style="text-align:center"><strong>参数量</strong></td>
<td style="text-align:center">与卷积核尺寸、卷积核个数相关</td>
<td style="text-align:center">不引入额外参数</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-10-卷积核是否一定越大越好？"><a href="#5-10-卷积核是否一定越大越好？" class="headerlink" title="5.10 卷积核是否一定越大越好？"></a>5.10 卷积核是否一定越大越好？</h2><p>​    在早期的卷积神经网络中（如LeNet-5、AlexNet），用到了一些较大的卷积核（$11\times11$和$5\times 5$），受限于当时的计算能力和模型结构的设计，无法将网络叠加得很深，因此卷积网络中的卷积层需要设置较大的卷积核以获取更大的感受域。但是这种大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算性能也会降低。后来的卷积神经网络（VGG、GoogLeNet等），发现通过堆叠2个$3\times 3$卷积核可以获得与$5\times 5$卷积核相同的感受视野，同时参数量会更少（$3×3×2+1$ &lt; $ 5×5×1+1$），$3\times 3$卷积核被广泛应用在许多卷积神经网络中。因此可以认为，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。</p>
<p>​    但是，这并不是表示更大的卷积核就没有作用，在某些领域应用卷积神经网络时仍然可以采用较大的卷积核。譬如在自然语言处理领域，由于文本内容不像图像数据可以对特征进行很深层的抽象，往往在该领域的特征提取只需要较浅层的神经网络即可。在将卷积神经网络应用在自然语言处理领域时，通常都是较为浅层的卷积层组成，但是文本特征有时又需要有较广的感受域让模型能够组合更多的特征（如词组和字符），此时直接采用较大的卷积核将是更好的选择。</p>
<p>​    综上所述，卷积核的大小并没有绝对的优劣，需要视具体的应用场景而定，但是极大和极小的卷积核都是不合适的，单独的$1\times 1$极小卷积核只能用作分离卷积而不能对输入的原始特征进行有效的组合，极大的卷积核通常会组合过多的无意义特征从而浪费了大量的计算资源。</p>
<h2 id="5-11-每层卷积是否只能用一种尺寸的卷积核？"><a href="#5-11-每层卷积是否只能用一种尺寸的卷积核？" class="headerlink" title="5.11 每层卷积是否只能用一种尺寸的卷积核？"></a>5.11 每层卷积是否只能用一种尺寸的卷积核？</h2><p>​    经典的神经网络一般都属于层叠式网络，每层仅用一个尺寸的卷积核，如VGG结构中使用了大量的$3×3$卷积层。事实上，同一层特征图可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好，如GoogLeNet、Inception系列的网络，均是每层使用了多个卷积核结构。如图5.3所示，输入的特征在同一层分别经过$1×1$、$3×3$和$5×5$三种不同尺寸的卷积核，再将分别得到的特征进行整合，得到的新特征可以看作不同感受域提取的特征组合，相比于单一卷积核会有更强的表达能力。</p>
<p><img src="/img/ch5/5.11-1.png" alt="image"></p>
<p>​                                图5.3 Inception模块结构</p>
<h2 id="5-12-怎样才能减少卷积层参数量？"><a href="#5-12-怎样才能减少卷积层参数量？" class="headerlink" title="5.12 怎样才能减少卷积层参数量？"></a>5.12 怎样才能减少卷积层参数量？</h2><p>减少卷积层参数量的方法可以简要地归为以下几点：</p>
<ul>
<li>使用堆叠小卷积核代替大卷积核：VGG网络中2个$3\times 3$的卷积核可以代替1个$5\times 5$的卷积核</li>
<li>使用分离卷积操作：将原本$K\times K\times C$的卷积操作分离为$K\times K\times 1$和$1\times1\times C$的两部分操作</li>
<li>添加$1\times 1$的卷积操作：与分离卷积类似，但是通道数可变，在$K\times K\times C_1$卷积前添加$1\times1\times C_2$的卷积核（满足$C_2 &lt;C_1$）</li>
<li>在卷积层前使用池化操作：池化可以降低卷积层的输入特征维度</li>
</ul>
<h2 id="5-13-在进行卷积操作时，必须同时考虑通道和区域吗？"><a href="#5-13-在进行卷积操作时，必须同时考虑通道和区域吗？" class="headerlink" title="5.13 在进行卷积操作时，必须同时考虑通道和区域吗？"></a>5.13 在进行卷积操作时，必须同时考虑通道和区域吗？</h2><p>​    标准卷积中，采用区域与通道同时处理的操作，如下图所示：</p>
<p><img src="/img/ch5/5.13-1.png" alt="image"></p>
<p>​    这样做可以简化卷积层内部的结构，每一个输出的特征像素都由所有通道的同一个区域提取而来。</p>
<p>​    但是这种方式缺乏灵活性，并且在深层的网络结构中使得运算变得相对低效，更为灵活的方式是使区域和通道的卷积分离开来，通道分离（深度分离）卷积网络由此诞生。如下图所示，Xception网络可解决上述问题。</p>
<p><img src="/img/ch5/5.13-2.png" alt="image"></p>
<p>​    我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道特征矩阵之后，再对这批新通道特征进行标准的$1×1​$跨通道卷积操作。</p>
<h2 id="5-14-采用宽卷积的好处有什么？"><a href="#5-14-采用宽卷积的好处有什么？" class="headerlink" title="5.14 采用宽卷积的好处有什么？"></a>5.14 采用宽卷积的好处有什么？</h2><p>​    宽卷积对应的是窄卷积，实际上并不是卷积操作的类型，指的是卷积过程中的填充方法，对应的是’SAME’填充和’VALID’填充。’SAME’填充通常采用零填充的方式对卷积核不满足整除条件的输入特征进行补全，以使卷积层的输出维度保持与输入特征维度一致；’VALID’填充的方式则相反，实际并不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃，因此在步长为1的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。此外，由于前一种方式通过补零来进行完整的卷积操作，可以有效地保留原始的输入特征信息。</p>
<p>​    比如下图左部分为窄卷积。注意到越在边缘的位置被卷积的次数越少。宽卷积可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，如下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一一部分0值，使得输出和输入的维度一致。</p>
<p><img src="/img/ch5/5.14.1.png" alt="image"></p>
<h2 id="5-15-理解转置卷积与棋盘效应"><a href="#5-15-理解转置卷积与棋盘效应" class="headerlink" title="5.15 理解转置卷积与棋盘效应"></a>5.15 理解转置卷积与棋盘效应</h2><h3 id="5-15-1-标准卷积"><a href="#5-15-1-标准卷积" class="headerlink" title="5.15.1 标准卷积"></a>5.15.1 标准卷积</h3><p>在理解转置卷积之前，需要先理解标准卷积的运算方式。</p>
<p>首先给出一个输入输出结果</p>
<p><img src="/img/ch5/img32.png" alt="image"></p>
<p>那是怎样计算的呢？</p>
<p>卷积的时候需要对卷积核进行180的旋转，同时卷积核中心与需计算的图像像素对齐，输出结构为中心对齐像素的一个新的像素值，计算例子如下：</p>
<p><img src="/img/ch5/5.19.1-2.png" alt="image"></p>
<p>这样计算出左上角(即第一行第一列)像素的卷积后像素值。</p>
<p>给出一个更直观的例子，从左到右看，原像素经过卷积由1变成-8。</p>
<p><img src="/img/ch5/5.19.1-3.png" alt="image"></p>
<p>通过滑动卷积核，就可以得到整张图片的卷积结果。</p>
<h3 id="5-15-2-转置卷积"><a href="#5-15-2-转置卷积" class="headerlink" title="5.15.2 转置卷积"></a>5.15.2 转置卷积</h3><p>图像的deconvolution过程如下：</p>
<p><img src="/img/ch5/5.19.2-5.png" alt="image"></p>
<p>输入：2x2， 卷积核：4x4， 滑动步长：3， 输出：7x7 </p>
<p>过程如下： </p>
<ol>
<li><p>输入图片每个像素进行一次full卷积，根据full卷积大小计算可以知道每个像素的卷积后大小为 1+4-1=4， 即4x4大小的特征图，输入有4个像素所以4个4x4的特征图 </p>
</li>
<li><p>将4个特征图进行步长为3的相加； 输出的位置和输入的位置相同。步长为3是指每隔3个像素进行相加，重叠部分进行相加，即输出的第1行第4列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。  </p>
<p>可以看出翻卷积的大小是由卷积核大小与滑动步长决定， in是输入大小， k是卷积核大小， s是滑动步长， out是输出大小 得到 out = (in - 1) <em> s + k 上图过程就是， (2 - 1) </em> 3 + 4 = 7。</p>
</li>
</ol>
<h3 id="5-15-3-棋盘效应"><a href="#5-15-3-棋盘效应" class="headerlink" title="5.15.3 棋盘效应"></a>5.15.3 棋盘效应</h3><h2 id="5-16-卷积神经网络的参数设置"><a href="#5-16-卷积神经网络的参数设置" class="headerlink" title="5.16 卷积神经网络的参数设置"></a>5.16 卷积神经网络的参数设置</h2><p>​    卷积神经网络中常见的参数在其他类型的神经网络中也是类似的，但是参数的设置还得结合具体的任务才能设置在合理的范围，具体的参数列表如表XX所示。<br>​                                                    表XX 卷积神经网络常见参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">常见设置</th>
<th style="text-align:left">参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">学习率(Learning Rate)</td>
<td style="text-align:center">$0-1$</td>
<td style="text-align:left">反向传播网络中更新权值矩阵的步长，在一些常见的网络中会在固定迭代次数或模型不再收敛后对学习率进行指数下降(如$lr=lr\times 0.1$)。当学习率越大计算误差对权值矩阵的影响越大，容易在某个局部最优解附近震荡；越小的学习率对网络权值的更新越精细，但是需要花费更多的时间去迭代</td>
</tr>
<tr>
<td style="text-align:center">批次大小(Batch Size)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">批次大小指定一次性流入模型的数据样本个数，根据任务和计算性能限制判断实际取值，在一些图像任务中往往由于计算性能和存储容量限制只能选取较小的值。在相同迭代次数的前提下，数值越大模型越稳定，泛化能力越强，损失值曲线越平滑，模型也更快地收敛，但是每次迭代需要花费更多的时间</td>
</tr>
<tr>
<td style="text-align:center">数据轮次(Epoch)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">数据轮次指定所有训练数据在模型中训练的次数，根据数据集规模和分布情况会设置为不同的值。当模型较为简单或训练数据规模较小时，通常轮次不宜过高，否则模型容易过拟合；模型较为复杂或训练数据规模足够大时，可适当提高数据的训练轮次。</td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数(Weight Decay)</td>
<td style="text-align:center">$0-0.001$</td>
<td style="text-align:left">模型训练过程中反向传播权值更新的权重衰减值</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-17-提高卷积神经网络的泛化能力"><a href="#5-17-提高卷积神经网络的泛化能力" class="headerlink" title="5.17 提高卷积神经网络的泛化能力"></a>5.17 提高卷积神经网络的泛化能力</h2><p>​    卷积神经网络与其他类型的神经网络类似，在采用反向传播进行训练的过程中比较依赖输入的数据分布，当数据分布较为极端的情况下容易导致模型欠拟合或过拟合，表XX记录了提高卷积网络泛化能力的方法。<br>​                                                                   表XX 提高卷积网络化能力的方法</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">使用更多数据</td>
<td style="text-align:left">在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">使用更大批次</td>
<td style="text-align:left">在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定</td>
</tr>
<tr>
<td style="text-align:center">调整数据分布</td>
<td style="text-align:left">大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整目标函数</td>
<td style="text-align:left">在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数$f(y,y’)=</td>
<td>y-y’</td>
<td>$在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成$f(y,y’)=(y-y’)^2$则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整网络结构</td>
<td style="text-align:left">在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用</td>
</tr>
<tr>
<td style="text-align:center">数据增强</td>
<td style="text-align:left">数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。</td>
</tr>
<tr>
<td style="text-align:center">权值正则化</td>
<td style="text-align:left">权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如$Loss=f(WX+b,y’)+\frac{\lambda}{\eta}\sum{</td>
<td>W</td>
<td>}$)。</td>
</tr>
<tr>
<td style="text-align:center">屏蔽网络节点</td>
<td style="text-align:left">该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>对大多数神经网络模型同样通用</p>
</blockquote>
<h2 id="5-18-卷积神经网络在不同领域的应用"><a href="#5-18-卷积神经网络在不同领域的应用" class="headerlink" title="5.18 卷积神经网络在不同领域的应用"></a>5.18 卷积神经网络在不同领域的应用</h2><p>​    卷积神经网络中的卷积操作是其关键组成，而卷积操作只是一种数学运算方式，实际上对不同类型的数值表示数据都是通用的，尽管这些数值可能表示的是图像像素值、文本序列中单个字符或是语音片段中单字的音频。只要使原始数据能够得到有效地数值化表示，卷积神经网络能够在不同的领域中得到应用，要关注的是如何将卷积的特性更好地在不同领域中应用，如表XX所示。<br>​                                                    表XX 卷积神经网络不同领域的应用<br>| 应用领域 | 输入数据图示 | 说明 |<br>| :——-: | :—————: | :— |<br>|   图像处理   | <img src="img/ch5/Image-process.png" alt="image_process"> | 卷积神经网络在图像处理领域有非常广泛的应用，这是因为图像数据本身具有的局部完整性非常 |<br>| 自然语言处理 | <img src="img/ch5/NLP.png" alt="NLP"> |  |<br>|   语音处理   | <img src="img/ch5/audio-recognition.png" alt="audio_process"> |  |</p>
<h3 id="5-18-1-联系"><a href="#5-18-1-联系" class="headerlink" title="5.18.1 联系"></a>5.18.1 联系</h3><p>​    自然语言处理是对一维信号（词序列）做操作。<br>​    计算机视觉是对二维（图像）或三维（视频流）信号做操作。</p>
<h3 id="5-18-2-区别"><a href="#5-18-2-区别" class="headerlink" title="5.18.2 区别"></a>5.18.2 区别</h3><p>​    自然语言处理的输入数据通常是离散取值（例如表示一个单词或字母通常表示为词典中的one hot向量），计算机视觉则是连续取值（比如归一化到0，1之间的灰度值）。CNN有两个主要特点，区域不变性(location invariance)和组合性(Compositionality)。</p>
<ol>
<li>区域不变性：滤波器在每层的输入向量(图像)上滑动，检测的是局部信息，然后通过pooling取最大值或均值。pooling这步综合了局部特征，失去了每个特征的位置信息。这很适合基于图像的任务，比如要判断一幅图里有没有猫这种生物，你可能不会去关心这只猫出现在图像的哪个区域。但是在NLP里，词语在句子或是段落里出现的位置，顺序，都是很重要的信息。</li>
<li>局部组合性：CNN中，每个滤波器都把较低层的局部特征组合生成较高层的更全局化的特征。这在CV里很好理解，像素组合成边缘，边缘生成形状，最后把各种形状组合起来得到复杂的物体表达。在语言里，当然也有类似的组合关系，但是远不如图像来的直接。而且在图像里，相邻像素必须是相关的，相邻的词语却未必相关。</li>
</ol>
<h2 id="5-19-卷积神经网络凸显共性的方法？"><a href="#5-19-卷积神经网络凸显共性的方法？" class="headerlink" title="5.19 卷积神经网络凸显共性的方法？"></a>5.19 卷积神经网络凸显共性的方法？</h2><h3 id="5-19-1-局部连接"><a href="#5-19-1-局部连接" class="headerlink" title="5.19.1 局部连接"></a>5.19.1 局部连接</h3><p>​    我们首先了解一个概念，感受野，即每个神经元仅与输入神经元相连接的一块区域。<br>在图像卷积操作中，神经元在空间维度上是局部连接，但在深度上是全连接。局部连接的思想，是受启发于生物学里的视觉系统结构，视觉皮层的神经元就是仅用局部接受信息。对于二维图像，局部像素关联性较强。这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征；<br>下图是一个很经典的图示，左边是全连接，右边是局部连接。</p>
<p><img src="/img/ch5/5.27.1.png" alt="image"></p>
<p>对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此巨大的参数量几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。</p>
<h3 id="5-19-2-权值共享"><a href="#5-19-2-权值共享" class="headerlink" title="5.19.2 权值共享"></a>5.19.2 权值共享</h3><p>​    权值共享，即计算同一深度的神经元时采用的卷积核参数是共享的。权值共享在一定程度上讲是有意义的，是由于在神经网络中，提取的底层边缘特征与其在图中的位置无关。但是在另一些场景中是无意的，如在人脸识别任务，我们期望在不同的位置学到不同的特征。<br>需要注意的是，权重只是对于同一深度切片的神经元是共享的。在卷积层中，通常采用多组卷积核提取不同的特征，即对应的是不同深度切片的特征，而不同深度切片的神经元权重是不共享。相反，偏置这一权值对于同一深度切片的所有神经元都是共享的。<br>权值共享带来的好处是大大降低了网络的训练难度。如下图，假设在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核的大小）。</p>
<p><img src="/img/ch5/5.27.2.png" alt="image"></p>
<p>这里就体现了卷积神经网络的奇妙之处，使用少量的参数，却依然能有非常出色的性能。上述仅仅是提取图像一种特征的过程。如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像不同尺度下的特征，称之为特征图（feature map）。</p>
<h3 id="5-19-3-池化操作"><a href="#5-19-3-池化操作" class="headerlink" title="5.19.3 池化操作"></a>5.19.3 池化操作</h3><p>池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：</p>
<p><img src="/img/ch5/5.27.3.png" alt="image"></p>
<h2 id="5-20-全连接、局部连接、全卷积与局部卷积"><a href="#5-20-全连接、局部连接、全卷积与局部卷积" class="headerlink" title="5.20 全连接、局部连接、全卷积与局部卷积"></a>5.20 全连接、局部连接、全卷积与局部卷积</h2><p>​    大多数神经网络中高层网络通常会采用全连接层(Global Connected Layer)，通过多对多的连接方式对特征进行全局汇总，可以有效地提取全局信息。但是全连接的方式需要大量的参数，是神经网络中最占资源的部分之一，因此就需要由局部连接(Local Connected Layer)，仅在局部区域范围内产生神经元连接，能够有效地减少参数量。根据卷积操作的作用范围可以分为全卷积(Global Convolution)和局部卷积(Local Convolution)。实际上这里所说的全卷积就是标准卷积，即在整个输入特征维度范围内采用相同的卷积核参数进行运算，全局共享参数的连接方式可以使神经元之间的连接参数大大减少;局部卷积又叫平铺卷积(Tiled Convolution)或非共享卷积(Unshared Convolution)，是局部连接与全卷积的折衷。四者的比较如表XX所示。<br>​                                                     表XX 卷积网络中连接方式的对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">连接方式</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">全连接</td>
<td style="text-align:center"><img src="img/ch5/full-connected.png" alt="full-connected"></td>
<td style="text-align:left">层间神经元完全连接，每个输出神经元可以获取到所有输入神经元的信息，有利于信息汇总，常置于网络末层；连接与连接之间独立参数，大量的连接大大增加模型的参数规模。</td>
</tr>
<tr>
<td style="text-align:center">局部连接</td>
<td style="text-align:center"><img src="img/ch5/local-connected.png" alt="local-connected"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于全连接减少了感受域外的连接，有效减少参数规模</td>
</tr>
<tr>
<td style="text-align:center">全卷积</td>
<td style="text-align:center"><img src="img/ch5/conv.png" alt="convolution"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，连接所采用的参数在不同感受域之间共享，有利于提取特定模式的特征；相比于局部连接，共用感受域之间的参数可以进一步减少参数量。</td>
</tr>
<tr>
<td style="text-align:center">局部卷积</td>
<td style="text-align:center"><img src="img/ch5/local-conv.png" alt="local-conv"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，感受域内采用全连接的方式，而感受域之间间隔采用局部连接与全卷积的连接方式；相比与全卷积成倍引入额外参数，但有更强的灵活性和表达能力；相比于局部连接，可以有效控制参数量</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-21-局部卷积的应用"><a href="#5-21-局部卷积的应用" class="headerlink" title="5.21 局部卷积的应用"></a>5.21 局部卷积的应用</h2><p>并不是所有的卷积都会进行权重共享，在某些特定任务中，会使用不权重共享的卷积。下面通过人脸这一任务来进行讲解。在读人脸方向的一些paper时，会发现很多都会在最后加入一个Local Connected Conv，也就是不进行权重共享的卷积层。总的来说，这一步的作用就是使用3D模型来将人脸对齐，从而使CNN发挥最大的效果。<br><img src="/img/ch5/img66.png" alt="image"></p>
<p>截取论文中的一部分图，经过3D对齐以后，形成的图像均是152×152，输入到上述的网络结构中。该结构的参数如下：</p>
<p>Conv：32个11×11×3的卷积核，</p>
<p>Max-pooling: 3×3，stride=2，</p>
<p>Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个7×7的卷积核，</p>
<p>Local-Conv: 16个5×5的卷积核，</p>
<p>Fully-connected: 4096维，</p>
<p>Softmax: 4030维。</p>
<p>前三层的目的在于提取低层次的特征，比如简单的边和纹理。其中Max-pooling层使得卷积的输出对微小的偏移情况更加鲁棒。但不能使用更多的Max-pooling层，因为太多的Max-pooling层会使得网络损失图像信息。全连接层将上一层的每个单元和本层的所有单元相连，用来捕捉人脸图像不同位置特征之间的相关性。最后使用softmax层用于人脸分类。<br>中间三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因：</p>
<p>（1）对齐的人脸图片中，不同的区域会有不同的统计特征，因此并不存在特征的局部稳定性，所以使用相同的卷积核会导致信息的丢失。</p>
<p>（2）不共享的卷积核并不增加inference时特征的计算量，仅会增加训练时的计算量。<br>使用不共享的卷积核，由于需要训练的参数量大大增加，因此往往需要通过其他方法增加数据量。</p>
<h2 id="5-22-NetVLAD池化-（贡献者：熊楚原-中国人民大学）"><a href="#5-22-NetVLAD池化-（贡献者：熊楚原-中国人民大学）" class="headerlink" title="5.22 NetVLAD池化    （贡献者：熊楚原-中国人民大学）"></a>5.22 NetVLAD池化    （贡献者：熊楚原-中国人民大学）</h2><p>NetVLAD是论文[15]提出的一个局部特征聚合的方法。</p>
<p>在传统的网络里面，例如VGG啊，最后一层卷积层输出的特征都是类似于Batchsize x 3 x 3 x 512的这种东西，然后会经过FC聚合，或者进行一个Global Average Pooling（NIN里的做法），或者怎么样，变成一个向量型的特征，然后进行Softmax or 其他的Loss。</p>
<p>这种方法说简单点也就是输入一个图片或者什么的结构性数据，然后经过特征提取得到一个长度固定的向量，之后可以用度量的方法去进行后续的操作，比如分类啊，检索啊，相似度对比等等。</p>
<p>那么NetVLAD考虑的主要是最后一层卷积层输出的特征这里，我们不想直接进行欠采样或者全局映射得到特征，对于最后一层输出的W x H x D，设计一个新的池化，去聚合一个“局部特征“，这即是NetVLAD的作用。</p>
<p>NetVLAD的一个输入是一个W x H x D的图像特征，例如VGG-Net最后的3 x 3 x 512这样的矩阵，在网络中还需加一个维度为Batchsize。</p>
<p>NetVLAD还需要另输入一个标量K即表示VLAD的聚类中心数量，它主要是来构成一个矩阵C，是通过原数据算出来的每一个$W \times H$特征的聚类中心，C的shape即$C: K \times D$，然后根据三个输入，VLAD是计算下式的V:</p>
<script type="math/tex; mode=display">V(j, k) = \sum_{i=1}^{N}{a_k(x_i)(x_i(j) - c_k(j))}</script><p>其中j表示维度，从1到D，可以看到V的j是和输入与c对应的，对每个类别k，都对所有的x进行了计算，如果$x_i$属于当前类别k，$a_k=1$，否则$a_k=0$，计算每一个x和它聚类中心的残差，然后把残差加起来，即是每个类别k的结果，最后分别L2正则后拉成一个长向量后再做L2正则，正则非常的重要，因为这样才能统一所有聚类算出来的值，而残差和的目的主要是消减不同聚类上的分布不均，两者共同作用才能得到最后正常的输出。</p>
<p>输入与输出如下图所示：</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad.jpeg" alt="image"></p>
<p>中间得到的K个D维向量即是对D个x都进行了与聚类中心计算残差和的过程，最终把K个D维向量合起来后进行即得到最终输出的$K \times D$长度的一维向量。</p>
<p>而VLAD本身是不可微的，因为上面的a要么是0要么是1，表示要么当前描述x是当前聚类，要么不是，是个离散的，NetVLAD为了能够在深度卷积网络里使用反向传播进行训练，对a进行了修正。</p>
<p>那么问题就是如何重构一个a，使其能够评估当前的这个x和各个聚类的关联程度？用softmax来得到：</p>
<script type="math/tex; mode=display">a_k = \frac{e^{W_k^T x_i + b_k}}{e^{W_{k'}^T x_i + b_{k'}}}</script><p>将这个把上面的a替换后，即是NetVLAD的公式，可以进行反向传播更新参数。</p>
<p>所以一共有三个可训练参数，上式a中的$W: K \times D$，上式a中的$b: K \times 1$，聚类中心$c: K \times D$，而原始VLAD只有一个参数c。</p>
<p>最终池化得到的输出是一个恒定的K x D的一维向量（经过了L2正则），如果带Batchsize，输出即为Batchsize x (K x D)的二维矩阵。</p>
<p>NetVLAD作为池化层嵌入CNN网络即如下图所示，</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad_emb.png" alt="image"></p>
<p>原论文中采用将传统图像检索方法VLAD进行改进后应用在CNN的池化部分作为一种另类的局部特征池化，在场景检索上取得了很好的效果。</p>
<p>后续相继又提出了ActionVLAD、ghostVLAD等改进。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 卷积神经网络研究综述[J]. 计算机学报, 2017, 40(6):1229-1251.</p>
<p>[2] 常亮, 邓小明, 周明全,等. 图像理解中的卷积神经网络[J]. 自动化学报, 2016, 42(9):1300-1312.</p>
<p>[3] Chua L O. CNN: A Paradigm for Complexity[M]// CNN a paradigm for complexity /.  1998.</p>
<p>[4] He K, Gkioxari G, Dollar P, et al. Mask R-CNN[J]. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, PP(99):1-1.</p>
<p>[5] Hoochang S, Roth H R, Gao M, et al. Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning[J]. IEEE Transactions on Medical Imaging, 2016, 35(5):1285-1298.</p>
<p>[6] 许可. 卷积神经网络在图像识别上的应用的研究[D]. 浙江大学, 2012.</p>
<p>[7] 陈先昌. 基于卷积神经网络的深度学习算法与应用研究[D]. 浙江工商大学, 2014.</p>
<p>[8] <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">CS231n Convolutional Neural Networks for Visual Recognition, Stanford</a></p>
<p>[9] <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.2gfx5zcw3" target="_blank" rel="noopener">Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks</a></p>
<p>[10] cs231n 动态卷积图：<a href="http://cs231n.github.io/assets/conv-demo/index.html" target="_blank" rel="noopener">http://cs231n.github.io/assets/conv-demo/index.html</a></p>
<p>[11] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</p>
<p>[12] Sun Y, Wang X, Tang X. Deep learning face representation from predicting 10,000 classes[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 1891-1898.</p>
<p>[13] 魏秀参.解析深度学习——卷积神经网络原理与视觉实践[M].电子工业出版社, 2018</p>
<p>[14]  Jianxin W U ,  Gao B B ,  Wei X S , et al. Resource-constrained deep learning: challenges and practices[J]. Scientia Sinica(Informationis), 2018.</p>
<p>[15] Arandjelovic R , Gronat P , Torii A , et al. [IEEE 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - Las Vegas, NV, USA (2016.6.27-2016.6.30)] 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - NetVLAD: CNN Architecture for Weakly Supervised Place Recognition[C]// 2016:5297-5307.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Kay</p>
  <div class="site-description" itemprop="description">千里之行，始于足下</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kay</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
