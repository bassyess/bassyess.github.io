<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bassyess.github.io","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="千里之行，始于足下">
<meta property="og:type" content="website">
<meta property="og:title" content="Home">
<meta property="og:url" content="https://bassyess.github.io/index.html">
<meta property="og:site_name" content="Home">
<meta property="og:description" content="千里之行，始于足下">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Kay">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://bassyess.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Home</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Home</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/08/23/C++%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/23/C++%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">C++基础知识</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-08-23 21:21:04 / 修改时间：22:08:08" itemprop="dateCreated datePublished" datetime="2020-08-23T21:21:04+08:00">2020-08-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="C-基础知识"><a href="#C-基础知识" class="headerlink" title="C++基础知识"></a>C++基础知识</h1><h2 id="static-静态变量"><a href="#static-静态变量" class="headerlink" title="static 静态变量"></a>static 静态变量</h2><h3 id="全局静态变量"><a href="#全局静态变量" class="headerlink" title="全局静态变量"></a>全局静态变量</h3><p>在整个程序运行期间一直存在。<br>初始化：未经初始化的全局静态变量会被自动初始化为0（自动对象的值是任意的，除非他被显式初始化）；<br>作用域：全局静态变量在声明他的文件之外是不可见的，准确地说是从定义之处开始，到文件结尾。</p>
<h3 id="局部静态变量"><a href="#局部静态变量" class="headerlink" title="局部静态变量"></a>局部静态变量</h3><p>存储在静态存储区<br>初始化：未经初始化的全局静态变量会被自动初始化为0（自动对象的值是任意的，除非他被显式初始化）；<br>作用域：作用域仍为局部作用域，当定义它的函数或者语句块结束的时候，作用域结束。但是当局部静态变量离开作用域后，并没有销毁，而是仍然驻留在内存当中，只不过我们不能再对它进行访问，直到该函数再次被调用，并且值不变；</p>
<h3 id="静态函数"><a href="#静态函数" class="headerlink" title="静态函数"></a>静态函数</h3><p>在函数返回类型前加static，函数就定义为静态函数。函数的定义和声明在默认情况下都是extern的，但静态函数只是在声明他的文件当中可见，不能被其他文件所用。<br>函数的实现使用static修饰，那么这个函数只可在本cpp内使用，不会同其他cpp中的同名函数引起冲突；<br>warning：不要再头文件中声明static的全局函数，不要在cpp内声明非static的全局函数，如果你要在多个cpp中复用该函数，就把它的声明提到头文件里去，否则cpp内部声明需加上static修饰；</p>
<h3 id="类的静态成员"><a href="#类的静态成员" class="headerlink" title="类的静态成员"></a>类的静态成员</h3><p>在类中，静态成员可以实现多个对象之间的数据共享，并且使用静态数据成员还不会破坏隐藏的原则，即保证了安全性。因此，静态成员是类的所有对象中共享的成员，而不是某个对象的成员。对多个对象来说，静态数据成员只存储一处，供所有对象共用</p>
<h3 id="类的静态函数"><a href="#类的静态函数" class="headerlink" title="类的静态函数"></a>类的静态函数</h3><p>静态成员函数和静态数据成员一样，它们都属于类的静态成员，它们都不是对象成员。因此，对静态成员的引用不需要用对象名。<br>在静态成员函数的实现中不能直接引用类中说明的非静态成员，可以引用类中说明的静态成员（这点非常重要）。如果静态成员函数中要引用非静态成员时，可通过对象来引用。从中可看出，调用静态成员函数使用如下格式：&lt;类名&gt;::&lt;静态成员函数名&gt;(&lt;参数表&gt;);</p>
<h2 id="C-和C的区别"><a href="#C-和C的区别" class="headerlink" title="C++和C的区别"></a>C++和C的区别</h2><p>设计思想上：C++是面向对象的语言，而C是面向过程的结构化编程语言。<br>语法上：（1）C++具有封装、继承和多态三种特性；（2）C++相比C，增加多许多类型安全的功能，比如强制类型转换、智能指针等；（3）C++支持范式编程，比如模板类、函数模板等。</p>
<h3 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h3><p>封装是在设计类的一个基本原理，就是将数据与对数据进行的操作进行有机的结合，形成“类”，其中数据和函数都是类的成员。</p>
<h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>如果一个类别B“继承自”另一个类别A，就把这个B称为“A的子类”，而把A称为“B的父类别”也可以称“A是B的超类”。继承可以使得子类具、有父类别的各种属性和方法，而不需要再次编写相同的代码。在令子类别继承父类别的同时，可以重新定义某些属性，并重写某些方法，即覆盖父类别的原有属性和方法，使其获得与父类别不同的功能。<br>（1）访问权限<br>public：父类对象内部、父类对象外部、子类对象内部、子类对象外部都可以访问。<br>protected：父类对象内部、子类对象内部可以访问，父类对象外部、子类对象外部都不可访问。<br>private：父类对象内部可以访问，其他都不可以访问。<br>（2）继承方式<br>三种继承方式不影响子类对父类的访问权限，子类对父类只看父类的访问控制权。继承方式是为了控制子类(也称派生类)的调用方(也叫用户)对父类(也称基类)的访问权限。<br>public、protected、private三种继承方式，相当于把父类的public访问权限在子类中变成了对应的权限。 如protected继承，把父类中的public成员在本类中变成了protected的访问控制权限；private继承，把父类的public成员和protected成员在本类中变成了private访问控制权。</p>
<h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p>多态性可以简单地概括为“一个接口，多种方法”，程序在运行时才决定调用的函数，它是面向对象编程领域的核心概念。</p>
<h4 id="静态多态"><a href="#静态多态" class="headerlink" title="静态多态"></a>静态多态</h4><p>静态多态也称为静态绑定或早绑定。编译器在编译期间完成的，编译器根据函数实参的类型(可能会进行隐式类型转换)，可推断出要调用那个函数，如果有对应的函数就调用该函数，否则出现编译错误。<br>（1）函数重载<br>编译器根据函数不同的参数表，对同名函数的名称做修饰，然后这些同名函数就成了不同的函数（至少对于编译器来说是这样的）。函数的调用，在编译器间就已经确定了，是静态的。也就是说，它们的地址在编译期就绑定了（早绑定）。<br>（2）泛型编程<br>泛型编程就是指编写独立于特定类型的代码，泛型在C++中的主要实现为模板函数和模板类。<br>泛型的特性：<br>a) 函数模板并不是真正的函数，它只是C++编译生成具体函数的一个模子。<br>b) 函数模板本身并不生成函数，实际生成的函数是替换函数模板的那个函数，比如上例中的add(sum1,sum2)，这种替换是编译期就绑定的。<br>c) 函数模板不是只编译一份满足多重需要，而是为每一种替换它的函数编译一份。<br>d) 函数模板不允许自动类型转换。<br>e) 函数模板不可以设置默认模板实参。比如template <typename T=0>不可以。</p>
<h3 id="动态多态"><a href="#动态多态" class="headerlink" title="动态多态"></a>动态多态</h3><p>c++的动态多态是基于虚函数的。对于相关的对象类型，确定它们之间的一个共同功能集，然后在基类中，把这些共同的功能声明为多个公共的虚函数接口。各个子类重写这些虚函数，以完成具体的功能。客户端的代码（操作函数）通过指向基类的引用或指针来操作这些对象，对虚函数的调用会自动绑定到实际提供的子类对象上去。</p>
<h2 id="C-的四种cast转换"><a href="#C-的四种cast转换" class="headerlink" title="C++的四种cast转换"></a>C++的四种cast转换</h2><p>C++中四种类型转换是：static_cast, dynamic_cast, const_cast, reinterpret_cast<br>(1) const_cast<br>用于将const变量转为非const<br>(2) static_cast<br>用于各种隐式转换，比如非const转const，void*转指针等, static_cast能用于多态向上转化，如果向下转能成功但是不安全，结果未知；<br>(3) dynamic_cast<br>用于动态类型转换。只能用于含有虚函数的类，用于类层次间的向上和向下转化。只能转指针或引用。向下转化时，如果是非法的对于指针返回NULL，对于引用抛异常。要深入了解内部转换的原理。<br>向上转换：指的是子类向基类的转换；<br>向下转换：指的是基类向子类的转换；<br>它通过判断在执行到该语句的时候变量的运行时类型和要转换的类型是否相同来判断是否能够进行向下转换。<br>(4) reinterpret_cast<br>几乎什么都可以转，比如将int转指针，可能会出问题，尽量少用。</p>
<p>为什么不使用C的强制转换？C的强制转换表面上看起来功能强大什么都能转，但是转化不够明确，不能进行错误检查，容易出错。</p>
<h2 id="C-的四种智能指针"><a href="#C-的四种智能指针" class="headerlink" title="C++的四种智能指针"></a>C++的四种智能指针</h2><p>C++里面的四个智能指针: auto_ptr, shared_ptr, weak_ptr, unique_ptr 其中后三个是c++11支持，并且第一个已经被11弃用。<br>为什么要使用智能指针?智能指针的作用是管理一个指针，因为存在以下这种情况：申请的空间在函数结束时忘记释放，造成内存泄漏。使用智能指针可以很大程度上的避免这个问题，因为智能指针就是一个类，当超出了类的作用域是，类会自动调用析构函数，析构函数会自动释放资源。所以智能指针的作用原理就是在函数结束时自动释放内存空间，不需要手动释放内存空间。</p>
<h3 id="auto-ptr（c-98的方案，cpp11已经抛弃）"><a href="#auto-ptr（c-98的方案，cpp11已经抛弃）" class="headerlink" title="auto_ptr（c++98的方案，cpp11已经抛弃）"></a>auto_ptr（c++98的方案，cpp11已经抛弃）</h3><p>auto_ptr的缺点是：存在潜在的内存崩溃问题！</p>
<h3 id="unique-ptr（替换auto-ptr）"><a href="#unique-ptr（替换auto-ptr）" class="headerlink" title="unique_ptr（替换auto_ptr）"></a>unique_ptr（替换auto_ptr）</h3><p>unique_ptr实现独占式拥有或严格拥有概念，保证同一时间内只有一个智能指针可以指向该对象。它对于避免资源泄露(例如“以new创建对象后因为发生异常而忘记调用delete”)特别有用。</p>
<h3 id="shared-ptr"><a href="#shared-ptr" class="headerlink" title="shared_ptr"></a>shared_ptr</h3><p>shared_ptr实现共享式拥有概念。多个智能指针可以指向相同对象，该对象和其相关资源会在“最后一个引用被销毁”时候释放。它使用计数机制来表明资源被几个指针共享。可以通过成员函数use_count()来查看资源的所有者个数。除了可以通过new来构造，还可以通过传入auto_ptr, unique_ptr,weak_ptr来构造。当我们调用release()时，当前指针会释放资源所有权，计数减一。当计数等于0时，资源会被释放。<br>shared_ptr 是为了解决 unique_ptr 在对象所有权上的局限性(unique_ptr 是独占的), 在使用引用计数的机制上提供了可以共享所有权的智能指针。<br>成员函数：<br>use_count：返回引用计数的个数;<br>unique：返回是否是独占所有权( use_count 为 1)；<br>swap：交换两个 shared_ptr 对象(即交换所拥有的对象)<br>reset 放弃内部对象的所有权或拥有对象的变更, 会引起原有对象的引用计数的减少；<br>get：返回内部对象(指针), 由于已经重载了()方法, 因此和直接使用对象是一样的.如 shared_ptr<int> sp(new int(1)); sp 与 sp.get()是等价的。</p>
<h3 id="weak-ptr"><a href="#weak-ptr" class="headerlink" title="weak_ptr"></a>weak_ptr</h3><p>weak_ptr 是一种不控制对象生命周期的智能指针, 它指向一个 shared_ptr 管理的对象.  weak_ptr只是提供了对管理对象的一个访问手段。weak_ptr 设计的目的是为配合 shared_ptr 而引入的一种智能指针来协助 shared_ptr 工作, 它只可以从一个 shared_ptr 或另一个 weak_ptr 对象构造, 它的构造和析构不会引起引用记数的增加或减少。weak_ptr可以用来解决shared_ptr相互引用时的死锁问题,如果说两个shared_ptr相互引用,那么这两个指针的引用计数永远不可能下降为0,资源永远不会释放。它是对对象的一种弱引用，不会增加对象的引用计数，和shared_ptr之间可以相互转化，shared_ptr可以直接赋值给它，它可以通过调用lock函数来获得shared_ptr。</p>
<h2 id="指针和引用的区别"><a href="#指针和引用的区别" class="headerlink" title="指针和引用的区别"></a>指针和引用的区别</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>（1）引用：引用就是某一变量的一个别名，对引用的操作与对变量直接操作完全一样。引用是C++对C语言的重要扩充。<br>（2）指针：指针存储的是变量在内存区域的地址。</p>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>（1）指针有自己的一块空间，而引用只是一个别名；<br>（2）指针可以被初始化为NULL（nullptr），而引用必须被初始化且必须是一个已有对象 的引用；<br>（3）作为参数传递时，指针需要被解引用才可以对对象进行操作，而直接对引 用的修改都会改变引用所指向的对象；<br>（4）指针在使用中可以指向其它对象，但是引用只能是一个对象的引用，不能 被改变；<br>（5）指针可以有多级指针（**p），而引用至多一级。</p>
<h2 id="malloc-和new"><a href="#malloc-和new" class="headerlink" title="malloc 和new"></a>malloc 和new</h2><p>（1）new分配内存按照数据类型进行分配，malloc分配内存按照指定的大小分配；<br>（2）new返回的是指定对象的指针，而malloc返回的是void<em>，因此malloc的返回值一般都需要进行类型转化。<br>（3）new不仅分配一段内存，而且会调用构造函数，malloc不会。<br>（4）new分配的内存要用delete销毁，malloc要用free来销毁；delete销毁的时候会调用对象的析构函数，而free则不会。<br>（5）new是一个操作符可以重载，malloc是一个库函数。<br>（6）申请数组时： new一次分配所有内存，多次调用构造函数，搭配使用delete，delete多次调用析构函数，销毁数组中的每个对象。而malloc则只能sizeof(int) </em> n。</p>
<h2 id="虚函数和多态"><a href="#虚函数和多态" class="headerlink" title="虚函数和多态"></a>虚函数和多态</h2><p>多态的实现主要分为静态多态和动态多态，静态多态主要是重载，在编译的时候就已经确定；动态多态是用虚函数机制实现的，在运行期间动态绑定。比如说，一个父类类型的指针指向一个子类对象时候，使用父类的指针去调用子类中重写了的父类中的虚函数的时候，会调用子类重写过后的函数。<br>虚函数的实现：在有虚函数的类中，类的最开始部分是一个虚函数表的指针，这个指针指向一个虚函数表，表中放了虚函数的地址。当子类继承了父类的时候也会继承其虚函数表，当子类重写父类中虚函数时候，会将其继承到的虚函数表中的地址替换为重新写的函数地址。但是由于使用了虚函数，会增加访问内存开销，降低效率。</p>
<h2 id="静态函数和虚函数"><a href="#静态函数和虚函数" class="headerlink" title="静态函数和虚函数"></a>静态函数和虚函数</h2><p>静态函数在编译的时候就已经确定运行时机；<br>虚函数在运行的时候动态绑定。虚函数因为用了虚函数表机制，调用的时候会增加一次内存开销。</p>
<h2 id="基类的析构函数"><a href="#基类的析构函数" class="headerlink" title="基类的析构函数"></a>基类的析构函数</h2><p>为什么析构函数必须是虚函数？为什么C++默认的析构函数不是虚函数？<br>将可能会被继承的父类的析构函数设置为虚函数，可以保证当我们new一个子类，然后使用基类指针指向该子类对象，释放基类指针时可以释放掉子类的空间，防止内存泄漏。<br>C++默认的析构函数不是虚函数是因为虚函数需要额外的虚函数表和虚表指针，占用额外的内存。而对于不会被继承的类来说，其析构函数如果是虚函数，就会浪费内存。因此C++默认的析构函数不是虚函数，而是只有当需要当作父类时，设置为虚函数。 </p>
<h2 id="C-中析构函数的作用"><a href="#C-中析构函数的作用" class="headerlink" title="C++中析构函数的作用"></a>C++中析构函数的作用</h2><p>析构函数与构造函数对应，当对象结束其生命周期，如对象所在的函数已调用完毕时，系统会自动执行析构函数。<br>析构函数名也应与类名相同，只是在函数名前面加一个位取反符~，以区别于构造函数。它不能带任何参数，也没有返回值（包括void类型）。一个类只能有一个析构函数，不能重载。<br>如果用户没有编写析构函数，编译系统会自动生成一个析构函数（即使自定义了析构函数，编译器也总是会为我们合成一个析构函数，并且如果自定义了析构函数，编译器在执行时会先调用自定义的析构函数再调用合成的析构函数）。<br>如果一个类中有指针成员，而且在使用的过程中动态的申请了内存，那么最好显示构造析构函数在销毁类之前，释放掉申请的内存空间，避免内存泄漏。</p>
<h2 id="map和set"><a href="#map和set" class="headerlink" title="map和set"></a>map和set</h2><p>map和set都是C++的关联容器，其底层实现都是红黑树（RB-Tree）。由于 map 和set所开放的各种操作接口，RB-tree 也都提供了，所以几乎所有的 map 和set的操作行为，都只是转调 RB-tree 的操作行为。<br>map和set区别在于：<br>（1）map中的元素是key-value（关键字—值）对：关键字起到索引的作用，值则表示与索引相关联的数据；Set与之相对就是关键字的简单集合，set中每个元素只包含一个关键字。<br>（2）set的迭代器是const的，不允许修改元素的值；map允许修改value，但不允许修改key。其原因是因为map和set是根据关键字排序来保证其有序性的，如果允许修改key的话，那么首先需要删除该键，然后调节平衡，再插入修改后的键值，调节平衡，如此一来，严重破坏了map和set的结构，导致iterator失效，不知道应该指向改变前的位置，还是指向改变后的位置。所以STL中将set的迭代器设置成const，不允许修改迭代器的值；而map的迭代器则不允许修改key值，允许修改value值。<br>（3）map支持下标操作，set不支持下标操作。map可以用key做下标，map的下标运算符[ ]将关键字作为下标去执行查找，如果关键字不存在，则插入一个具有该关键字和mapped_type类型默认值的元素至map中，因此下标运算符[ ]在map应用中需要慎用。如果find能解决需要，尽可能用find。</p>
<h2 id="Vector-和list"><a href="#Vector-和list" class="headerlink" title="Vector 和list"></a>Vector 和list</h2><h3 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h3><p>（1）连续存储的容器，动态数组，在堆上分配空间。<br>（2）底层实现：数组<br>（3）两倍容量增长：vector 增加（插入）新元素时，如果未超过当时的容量，则还有剩余空间，那么直接添加到最后（插入指定位置），然后调整迭代器。如果没有剩余空间了，则会重新配置原有元素个数的两倍空间，然后将原空间元素通过复制的方式初始化新空间，再向新空间增加元素，最后析构并释放原空间，之前的迭代器会失效。<br>（4）性能：<br>访问：O(1)<br>插入：在最后插入（空间够）速度很快；在最后插入（空间不够）则需要内存申请和释放，以及对之前数据进行拷贝。<br>在中间插入（空间够）内存拷贝，在中间插入（空间不够）需要内存申请和释放，以及对之前数据进行拷贝。<br>删除：在最后删除速度很快，在中间删除则需要内存拷贝。<br>适用场景：经常随机访问，且不经常对非尾节点进行插入删除。</p>
<h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><p>（1）动态链表，在堆上分配空间，每插入一个元数都会分配空间，每删除一个元素都会释放空间。<br>（2）底层：双向链表<br>（3）性能：<br>访问：随机访问性能很差，只能快速访问头尾节点；<br>插入：很快，一般是常数开销；<br>删除：很快，一般是常数开销；<br>适用场景：经常插入删除大量数据。<br>适用场景：经常插入删除大量数据</p>
<h3 id="vector和list区别"><a href="#vector和list区别" class="headerlink" title="vector和list区别"></a>vector和list区别</h3><p>1）vector底层实现是数组；list是双向 链表。<br>2）vector支持随机访问，list不支持。<br>3）vector是顺序内存，list不是。<br>4）vector在中间节点进行插入删除会导致内存拷贝，list不会。<br>5）vector一次性分配好内存，不够时才进行2倍扩容；list每次插入新节点都会进行内存申请。<br>6）vector随机访问性能好，插入删除性能差；list随机访问性能差，插入删除性能好。</p>
<h3 id="vector和list应用"><a href="#vector和list应用" class="headerlink" title="vector和list应用"></a>vector和list应用</h3><p>vector拥有一段连续的内存空间，因此支持随机访问，如果需要高效的随即访问，而不在乎插入和删除的效率，使用vector。<br>list拥有一段不连续的内存空间，如果需要高效的插入和删除，而不关心随机访问，则应使用list。</p>
<h2 id="STL"><a href="#STL" class="headerlink" title="STL"></a>STL</h2><p>STL主要由：以下几部分组成：<br>容器、迭代器、仿函数、算法、分配器、配接器<br>他们之间的关系：<br>1）分配器给容器分配存储空间；<br>2）算法通过迭代器获取容器中的内容；<br>3）仿函数可以协助算法完成各种操作；<br>4）配接器用来套接适配仿函数。<br>在C++标准中，STL被组织为下面的13个头文件：<algorithm>、<deque>、<functional>、<iterator>、<vector>、<list>、<map>、<memory.h>、<numeric>、<queue>、<set>、<stack>和<utility>。</p>
<h2 id="STL中迭代器的作用"><a href="#STL中迭代器的作用" class="headerlink" title="STL中迭代器的作用"></a>STL中迭代器的作用</h2><h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><p>Iterator（迭代器）模式又称Cursor（游标）模式，用于提供一种方法顺序访问一个聚合对象中各个元素, 而又不需暴露该对象的内部表示。或者这样说可能更容易理解：Iterator模式是运用于聚合对象的一种模式，通过运用该模式，使得我们可以在不知道对象内部表示的情况下，按照一定顺序（由iterator提供的方法）访问聚合对象中的各个元素。<br>由于Iterator模式的以上特性：与聚合对象耦合，在一定程度上限制了它的广泛运用，一般仅用于底层聚合支持类，如STL的list、vector、stack等容器类及ostream_iterator等扩展iterator。</p>
<h3 id="迭代器和指针的区别"><a href="#迭代器和指针的区别" class="headerlink" title="迭代器和指针的区别"></a>迭代器和指针的区别</h3><p>迭代器不是指针，是类模板，表现的像指针。他只是模拟了指针的一些功能，通过重载了指针的一些操作符，-&gt;、<em>、++、—等。迭代器封装了指针，是一个“可遍历STL（ Standard Template Library）容器内全部或部分元素”的对象， 本质是封装了原生指针，是指针概念的一种提升（lift），提供了比指针更高级的行为，相当于一种智能指针，他可以根据不同类型的数据结构来实现不同的++，—等操作。<br>迭代器返回的是对象引用而不是对象的值，所以cout只能输出迭代器使用</em>取值后的值而不能直接输出其自身。</p>
<h3 id="迭代器产生原因"><a href="#迭代器产生原因" class="headerlink" title="迭代器产生原因"></a>迭代器产生原因</h3><p>Iterator类的访问方式就是把不同集合类的访问逻辑抽象出来，使得不用暴露集合内部的结构而达到循环遍历集合的效果。</p>
<h2 id="C-源文件从文本到可执行文件经历的过程"><a href="#C-源文件从文本到可执行文件经历的过程" class="headerlink" title="C++源文件从文本到可执行文件经历的过程"></a>C++源文件从文本到可执行文件经历的过程</h2><p>对于C++源文件，从文本到可执行文件一般需要四个过程：<br>（1）预处理阶段：对源代码文件中文件的头文件、宏定义进行分析和替换，生成预编译文件；<br>（2）编译阶段：将经过预处理后的预编译文件转换成特定汇编代码，生成汇编文件；<br>（3）汇编阶段：将编译阶段生成的汇编文件转化成机器码，生成可重定位目标文件；<br>（4）链接阶段：将多个目标文件及所需要的库连接成最终的可执行目标文件。</p>
<h2 id="include头文件"><a href="#include头文件" class="headerlink" title="include头文件"></a>include头文件</h2><p>双引号和尖括号的区别：编译器预处理阶段查找头文件的路径不一样。<br>对于使用双引号包含的头文件，查找头文件路径的顺序为：<br>1）当前头文件目录；<br>2）编译器设置的头文件路径（编译器可使用-I显式指定搜索路径）；<br>3）系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径。<br>对于使用尖括号包含的头文件，查找头文件的路径顺序为：<br>1）编译器设置的头文件路径（编译器可使用-I显式指定搜索路径）；<br>2）系统变量CPLUS_INCLUDE_PATH/C_INCLUDE_PATH指定的头文件路径。</p>
<h2 id="内存溢出和内存泄漏"><a href="#内存溢出和内存泄漏" class="headerlink" title="内存溢出和内存泄漏"></a>内存溢出和内存泄漏</h2><h3 id="内存溢出"><a href="#内存溢出" class="headerlink" title="内存溢出"></a>内存溢出</h3><p>指程序申请内存时，没有足够的内存供申请者使用。内存溢出就是你要的内存空间超过了系统实际分配给你的空间，此时系统相当于没法满足你的需求，就会报内存溢出的错误。<br>内存溢出原因：<br>（1）内存中加载的数据量过于庞大，如一次从数据库取出过多数据；<br>（2）集合类中有对对象的引用，使用完后未清空，使得不能回收；<br>（3）代码中存在死循环或循环产生过多重复的对象实体<br>使用的第三方软件中的BUG；<br>（4）启动参数内存值设定的过小。</p>
<h3 id="内存泄漏"><a href="#内存泄漏" class="headerlink" title="内存泄漏"></a>内存泄漏</h3><p>在编写应用程序的时候，程序分配了一块内存，但已经不再持有引用这块内存的对象（通常是指针），虽然这些内存被分配出去，但是无法收回，将无法被其他的进程所使用，我们说这块内存泄漏了，被泄漏的内存将在整个程序声明周期内都不可使用。<br>主要原因：是在使用new或malloc动态分配堆上的内存空间，而并未使用delete或free及时释放掉内存。<br>内存泄漏情况：<br>（1）不匹配使用new[] 和 delete[]；<br>（2）delet void * 的指针，导致没有调用到对象的析构函数，析构的所有清理工作都没有去执行从而导致内存的泄露；<br>（3）没有将基类的析构函数定义为虚函数，当基类的指针指向子类时，delete该对象时，不会调用子类的析构函数。</p>
<h2 id="C-内存管理"><a href="#C-内存管理" class="headerlink" title="C++内存管理"></a>C++内存管理</h2><p>在C++中，虚拟内存分为代码段、数据段、BSS段、堆区、文件映射区以及栈区六部分。<br>（1）代码段:包括只读存储区和文本区，其中只读存储区存储字符串常量，文本区存储程序的机器代码。<br>（2）数据段：存储程序中已初始化的全局变量和静态变量<br>（3）bss 段：存储未初始化的全局变量和静态变量（局部+全局），以及所有被初始化为0的全局变量和静态变量。<br>（4）堆区：调用new/malloc函数时在堆区动态分配内存，同时需要调用delete/free来手动释放申请的内存。<br>（5）映射区:存储动态链接库以及调用mmap函数进行的文件映射。<br>（6）栈：使用栈空间存储函数的返回地址、参数、局部变量、返回值。</p>
<h2 id="C-11新特性"><a href="#C-11新特性" class="headerlink" title="C++11新特性"></a>C++11新特性</h2><p>C++11 最常用的新特性如下：<br>（1）auto关键字：编译器可以根据初始值自动推导出类型。但是不能用于函数传参以及数组类型的推导；<br>（2）nullptr关键字：nullptr是一种特殊类型的字面值，它可以被转换成任意其它的指针类型；而NULL一般被宏定义为0，在遇到重载时可能会出现问题。<br>（3）智能指针：C++11新增了std::shared_ptr、std::weak_ptr等类型的智能指针，用于解决内存管理的问题。<br>（4）初始化列表：使用初始化列表来对类进行初始化。<br>（5）右值引用：基于右值引用可以实现移动语义和完美转发，消除两个对象交互时不必要的对象拷贝，节省运算存储资源，提高效率。<br>（6）atomic原子操作用于多线程资源互斥操作。<br>（7）新增STL容器array以及tuple。</p>
<h2 id="右值引用"><a href="#右值引用" class="headerlink" title="右值引用"></a>右值引用</h2><p>右值引用是C++11中引入的新特性 , 它实现了转移语义和精确传递。它的主要目的有两个方面：<br>1）消除两个对象交互时不必要的对象拷贝，节省运算存储资源，提高效率。<br>2）能够更简洁明确地定义泛型函数。</p>
<p>左值和右值的概念：<br>左值：能对表达式取地址、或具名对象/变量。一般指表达式结束后依然存在的持久对象。<br>右值：不能对表达式取地址，或匿名对象。一般指表达式结束就不再存在的临时对象。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/08/23/%E8%85%BE%E8%AE%AF%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/23/%E8%85%BE%E8%AE%AF%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">腾讯实习项目总结</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-23 15:00:10" itemprop="dateCreated datePublished" datetime="2020-08-23T15:00:10+08:00">2020-08-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-31 15:25:53" itemprop="dateModified" datetime="2020-08-31T15:25:53+08:00">2020-08-31</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="腾讯实习工作总结"><a href="#腾讯实习工作总结" class="headerlink" title="腾讯实习工作总结"></a>腾讯实习工作总结</h1><p>AI 在视频生产流程里的应用。首先是是智能拆条，视频进来之后，把片头、片尾和广告去掉，其次是标注，就像把所有的商品打上标签一样，根据这个标签进行推荐和搜索。然后是审核，过滤盗版、情色等内容。最后是播放处理，比如自动生成封面和动态海报图，帮助对剪辑不熟悉的自媒体用户选取魅力封面图，也提高了海量视频的分发效率。<br>视频标注为视频打上时间 - 语义标签，是 AI 视频理解的关键。标签有不同类别、层次和粒度，例如识别不同人物、行为、场景、物品等等，还可以用于对内容和用户的精细化分析，例如年龄、表情、服装款式，手机品牌等，更好地进行搜索推荐，实现精准的内容分发。<br>AI 应用在文字识别，可以对字幕进行文字识别和语义理解，对审核图片、视频中的不合规文字信息进行识别和反垃圾过滤。</p>
<h2 id="视频分类"><a href="#视频分类" class="headerlink" title="视频分类"></a>视频分类</h2><p>广电场景下的海量短视频剧名分类：在算法流程中，我们首先把视频的图片进行解码，凑齐N帧，保证每个视频都有相同的采样帧，然后采用resnet网络提取视觉特征，采用NetVlad提取融合时间特征，最后融合多张图片得到视频的分类结果。<br>我们在总共有957万个短视频，只取出现频次在50-3000类别的短视频总共有552万，剧名类别有8740种，对其进行训练，分类的top1的准确率只到0.18。因此，我们考虑从关键帧和多级分类器方面提高算法的准召率。<br>抽取关键帧的思路也很简单：我们知将两帧图像进行差分，得到图像的平均像素强度可以用来衡量两帧图像的变化大小。因此，基于帧间差分的平均强度，每当视频中的某一帧与前一阵画面内容产生大的变化，我们便认为它是关键帧，并将其提取出来。<br>我们可以使用如下三种方法中的一种来提取关键帧，这都是基于帧间差分的：<br>（1）使用差分强度顺序：我们对所有帧按照平均帧间差分强度进行排序，选择平均帧间差分强度最大的若干张图片作为视频的关键帧。<br>（2）使用差分强度阈值：我们悬着平均帧间差分强度高于预设阈值的帧作为视频的关键帧。<br>（3）使用局部最大值：我们选择具有平均帧间差分强度局部最大值作为视频的关键帧。这种方法的提取结果在丰富度上表现更好，提取结果均匀分散在视频中。<br>在抽帧方面，每个视频里的关键帧数目不同，如果一个视频只有四帧，我们如何取八帧呢？有两个方法，第一个办法是再补尾帧，后面重复实现。还有一个办法是均匀插帧，经评测，均匀补帧比插尾帧效果好很多，因为插尾帧的时候，相当于把最后第四帧插了很多帧无效的信息。如果第四帧是黑帧，或者是无意义的帧，它会带来负面的影响。<br>除了在每个短视频中提取关键帧获取视频更具判别性的视觉特征外，在全类目的视频分类效果差的原因在于，每个视频的内容、风格差异非常大，从历史题材到都市生活、从科幻大片到动漫题材，模型很难去拟合出有效的高层语义信息对剧名进行分类。因此，我们考虑训练多级分类器，为每个题材的内容训练对应的分类模型，能有效提高模型的分类效果。我们首先利用每个视频的一级标签（电影、电视剧、动漫、综艺、体育、军事、纪录片、游戏）等8个一级类目训练一级分类器，然后对每个类目下的视频分别训练各自的剧名分类器模型。即网络会先判断该短视频是属于电影、电视剧类别，然后判断该短视频的剧名。在实验数据集中，一级分类器的准召率为0.82/0.71，但在看点、企鹅号和ugc数据中，模型分类的准召率分别为0.6/0.46，0.57/0.51以及0.47/0.5。而二级的剧名分类像电影、电视剧的准召率为0.86/0.79、0.75/0.54，而像军事、体育的准召率可达0.96/0.94。对实验结果进行分析，我们可以看到，每个像一些内容单一、题材近似的视频类别，如军事、体育等单独训练的分类模型具有较高的准确性，因此通过多级分类器对视频剧名进行分类方案是可行的。但是我们认为简单的将剧名分为8个一级标签，一级分类器的准召率一般，且模型的泛化性不行。我们认为是由于像电影、电视剧这种标签类别过于宽泛，视频题材丰富，模型很难去拟合有效的特征。<br>因此，我们最终考虑采用视频多标签技术，对每个视频提取语义标签，然后通过聚类和人工筛选，提取了近20个的二级标签（都市生活、娱乐综艺、历史文化、科幻战争等），然后通过这些类别标签训练多级分类器，在全类目的剧名分类模型中，算法的最终准召率为0.58/0.52，分类效果得到有效的提升。</p>
<h3 id="NetVlad模型"><a href="#NetVlad模型" class="headerlink" title="NetVlad模型"></a>NetVlad模型</h3><p>VLAD(Vector of locally aggregated descriptors，局部聚合向量)是一种类似于BOF(bag-of-feature)的描述图像的特征方法，可以理解是将局部特征表示成全局特征的编码方法。在这之前，BoF(词袋模型)通常用来描述基于Sift局部特征，在图像检索和分类模型中有着广泛的应用。相比于BoF,VLAD更加能够对于图像的准确表达，得到更具有判别力的特征，并且便于降维（PCA），同时降维对于准确率的影响也较小。<br>NetVLAD是将传统的VLAD结构嵌入到CNN网络结构中，得到一个新的VLAD层。NetVLAD可以很容易的运用到任何的CNN结构中，并且可以使用反向传播进行优化。局部聚合向量（VLAD）能够抓取图像中局部特征在图像中聚合的统计信息，视觉词袋聚合是记录每个词的数目，而VLAD是记录每个词的残差和。<br>给定$N$个D维局部图像描述子$x_i$作为输入，$K$个聚类中心作为$c_k$作为VLAD的参数，VLAD的输出是一个$K\times D$的矩阵，但是这个矩阵被转换成向量表示，然后再进行归一化，计算公式如下：</p>
<script type="math/tex; mode=display">V(j,k)=\sum_{i=1}^Na_k(X_i)(x_i(j)-c_k(j))</script><p>其中，$x_i(j)$和$c_k(j)$表示是第$i$个局部描述子和第$k$个聚类中心的第$j$个特征值。$a_k(X_i)$我们可以简单理解为第i个局部特征属于第k个聚类的权重，也就是说，如果它的值等于1就说明它属于这个聚类的簇。如果是传统的VLAD，那么这个值只能是1或0.直观上看，$V$表征着所有局部特征在每一个聚类簇上的残差$(x_i-c_k)$和。<br>在传统的VLAD中，由于$a_k(X_i)$是一个不连续的值，取值为1或0，并且满足$\sum_{i=1}^Ka_k(X_i)=1$，使其不能进行反向传播。而NetVLAD采用一种近似的方式，来对$a_k(X_i)$来做软分配(soft assignment)如下式：</p>
<script type="math/tex; mode=display">\overline{a_k}(X_i)=\frac{e^{-\alpha||x_i-c_k||^2}}{\sum_{k'}e^{-\alpha||x_i-c_k||^2}}</script><p>这个权重的分配可以把它当做一个模糊聚类的分配方式，根据每个局部特征到聚类中心的距离来产生一个概率函数权重。对于一个局部特征描述$X_i$在每个聚类簇下的权重的范围在0~1之间，权重最高的可以理解为该特征离聚类簇中心的聚类最近，权重低说明其离簇中心较远。我们将上式进行平方展开，可得到VLAD特征向量为：</p>
<script type="math/tex; mode=display">V(j,k)=\sum_{i=1}^N\frac{e^{W_k^Tx_i+b_k}}{\sum_{k'}e^{W_{k'}^TX_i+b_{k'}}}(x_i(j)-c_k(j))</script><p>NetVLAD通过这种在不同聚类簇上的软分配（soft-assignment）方式，能够有效的聚合了局部特征空间中不同部分（聚类）的一阶残差的统计量。另外我们可以注意到，NetVLAD中包含着三个参数，$W_k$，$b_k$和$c_k$，原始VLAD中只有一个参数$c_k$，这使得NetVLAD相对于传统的方法更具有灵活性，并且所有的参数在特定的任务下可以通过端到端的方式来学习。<br>由图可看出，NetVLAD层可以直接接在卷积网络中的最后一层$(H\times W\times D)$上，把最后一层的特征图看作$N$个密集的$D$维局部描述子。soft-assignment过程可分为两个步骤：1）通过K个滤波器$W_k$将$N\times D$矩阵来学到$s_k(X_i)=W_k^TX_i+b_k$；2）卷积输出然后经过一个soft-max函数来生成$\overline{a_k}(X_i)$。</p>
<h2 id="视频多标签"><a href="#视频多标签" class="headerlink" title="视频多标签"></a>视频多标签</h2><p>视频多标签是为视频标注出语义标签，是视频理解的关键，但在视频多标签领域仍存在很多问题和难点。我们先看一下视频多标签实验的数据集分布，在总共2000万个短视频中，标签出现频次大于100的标签类别有18916种。在这里我列举了部分标签及其出现的频次，我们可以看到标签的分布及其不均衡，像内地综艺出现的频次高达2194775次，而像“芒果讲”、“行车视线”出现的频次只有100次。还有一些标签，如“创业”、“聊天”、“恶搞”标签定义的过于抽象，模型很难学习这类标签的特征。当然，标签中还存在一些标签语义非常近似，如“内地综艺”和“综艺片段”，“宝宝秀”和“天真萌娃”，这类标签问题和数据标注有很大关系，我们暂且不考虑。为了更直观的看到标签的分布情况，我统计了标签出现的频次发现，在标签出现频次大于5000的类别，只占所有类别总数的10%，但这部分标签出现频次之和占所有标签出现频次之和的84%。在标签频次小于500的标签类别占所有标签类别的56%，但由于每个标签出现的频次较少，该部分标签频次之和只占所有标签频次和的3%。标签的类别严重不均衡，使得模型在训练的过程中更容易去拟合频次出现过的标签的特征，而很难去学习绝大部分的出现频次较少的标签特征。面对这个问题，我们决定重构模型的loss函数，使得标签能够去学习那些少量样本的特征。我们认为，模型无法有效地学习标签之间的特征，主要在于标签的正负样本的之间差距过大，以及每个标签之间的相对的差距过大导致的。因此，我们要调整每个标签的正负样本之间的权重，调整每个标签之间相对的权重，通过均衡每个样本在参数更新过程中的贡献来训练更好的模型。我们设负样本数为$N_2$，正样本数为$P_2$，负样本权重为$W_2^n$，正样本权重为$W_2^p$，标签整体的样本权重为$W_2$，其中$W_2^p=1$，则</p>
<script type="math/tex; mode=display">W_2^N = 
\begin{cases}
1, if \frac{N_2}{P_2}<10 \\
10\frac{P_2}{N_2},if \frac{N_2}{P_2}\ge10 
\end{cases}</script><script type="math/tex; mode=display">W_2=
\begin{cases}
1, if \frac{N_2W_2^n+P_2}{N_1W_1^n+P_1}<10 \\
\frac{N_1W_1^n+P_1}{N_2W_2^n+P_2}, if \frac{N_2W_2^n+P_2}{N_1W_1^n+P_1}\ge10
\end{cases}</script><p>对于正负样本间的权重，我们认为负样本数超过正样本数一定阈值，我们就将其倒数乘上相应阈值作为正负样本的权重；对于每个样本间的权重，为了简化计算，我们选择和最小频次的样本进行比较，当然为了防止最小频次的标签的样本数过小，拉低了标签的整体贡献，我们设置最小频次的标签样本数为1000。每个标签的调整的负样本数和正样本数之和就是该标签对在loss计算中的贡献，将其和最小的频次标签的贡献进行对比，就可以得到标签的整体分布权重$W_2$，则损失函数：</p>
<script type="math/tex; mode=display">loss=-W_2[W_2^P\cdot y\cdot log\sigma(x)+W_2^n\cdot (1-y)\cdot log(1-\sigma(x))]</script><p>我们绘制了每个标签调整后的频次占比，发现大于5000的频次占比为42%，小于500的标签占比为28%，标签分布得到有效的均衡。<br>基于梯度均衡的损失函数，是2019年AAAI的论文提出的思想，作者认为影响单阶段检测器训练的本质问题，其根本原因是由于不同难度的样本的分布不均衡导致。我们随机统计了一批不同标签的训练样本中的梯度模长，发现大部分样本是十分容易预测的，这些可被准确预测的样本所占的比重也很大，而梯度g接近于1的样本的比例也相对较大，我们认为这是一些离群样本，可能是由于数据标注本身不准确或样本比较特殊难学习造成。对于一个已收敛的模型，强行学习这些离群样本可能会导致模型参数的较大偏差，反而影响模型的准确率。我们利用论文中提出的梯度均衡的概念，即根据样本模长分布的比例对不同样本产生的梯度进行加权，使得各类型的样本对模型参数有更均衡的贡献，而这种加权在损失函数上也可以达到同样的效果。在重构的loss函数中，我们将梯度模长的取值范围划分为若干区域，统计梯度模长g落在每个区域内的样本数量，而密度就是其所在的单位区域内的样本数量除以该单位区域的长度，而梯度密度的倒数就是样本计算loss要乘的权重。不过在多分类任务中，类别标签的onehot编码中，只有属于该类别的位置值为1，其余的均为0。因此在每个batch中计算梯度密度是只需要统计label为1的那个类别的梯度。而在多标签分类中，onehot编码的每个位置都是一个类别的二分类，label为0或1对loss均有影响，因此我们需要分别取统计每个类别的梯度分布，计算每个类别的权重去调整loss函数中每个标签的贡献。<br>原始的视频多标签的mAP指标为0.2306，添加权重分布的loss函数策略后，mAP指标为0.2742；添家梯度均衡策略的mAP指标为0.2676，最终的mAP指标为0.3284。</p>
<h3 id="mAP的计算"><a href="#mAP的计算" class="headerlink" title="mAP的计算"></a>mAP的计算</h3><p>voc2010的AP计算方法是：假设N个样本中有M个正例，那么我们会得到M个recall值（1/M,2/M,…,M/M），对于每个recall值r，我们可以计算出对应（r’&gt;r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。<br>把recall当场横坐标，precision当场纵坐标，即可得到常用的precision-recall曲线，曲线下的面积也是AP值。<br>AP衡量的是学出来的模型在每个类别上的好坏，mAP衡量的是学出的模型在所有类别上的好坏，得到AP后，取所有AP的平均值就是mAP。</p>
<h2 id="片段层标题提取"><a href="#片段层标题提取" class="headerlink" title="片段层标题提取"></a>片段层标题提取</h2><h3 id="OCR检测器"><a href="#OCR检测器" class="headerlink" title="OCR检测器"></a>OCR检测器</h3><p>PSENet是一种基于语义分割的方法检测任意方向的文本，采用渐进式尺度扩展的方法区分邻近的文本块。<br><img src="https://pic4.zhimg.com/80/v2-59313cb1f5422cc242199fd80b6cfb82_720w.jpg" alt="网络模型"><br>网络结构类似于FPN的形式，先采用CNN提取四层不同level的feature map，分别是$P_2,P_3,P_4,P_5$，解决文本块尺度变换剧烈的问题，early-stage可用于预测小的文本块，late-stage可用于预测大的文本块，然后通过2倍上采样进行concate来融合特征，得到最后的特征图F。特征图F送入$3\times 3$大小的卷积中输出通道数为256的特征图，将次特征图再送入到$1\times 1$大小卷积层输出n个最终结果，这n个结果用$S_1,S_2,…,S_n$表示，最后通过渐进的尺度扩展算法PSE进一步得到最终的文字检测结果。这里的$S_1,S_2,…,S_n$, $S_i$是图像分割的文字检测框结果，不同之处在于每个结果对应的文字区域大小不一样。如$S_1$对应最小文字分割的结果，$S_n$是最大的文字分割的结果。<br><img src="https://pic2.zhimg.com/80/v2-24655da704f2a162be8da640f519e634_720w.jpg" alt="渐进式尺度扩展算法"><br>渐进尺度扩展算法（PSE）:首先看$S_1$，图中有四个不同的分割区域（$C_1,C_2,C_3,C_4$），通过CC（CC是一个寻找连接区域的函数，就是给不同的pixel设置不同的label）将不同分割区域合并得到图b（四个连通域使用不同颜色标记，不同文本行之间的margin很大，很容易区分开），然后合并$S_2$中像素，将属于$S_2$在的kernel但不属于$S_1$中的kernel的像素点分配，将b图所找到的连通域的每个pixel以BFS的方式，逐个上下左右扩展，即相当于把$S_1$中预测的文本行的区域逐渐边框（简单来说，就是讲$S_2$中的每个像素点都分别分配给$S_1$中的某个连通域）。重复上述过程，知道发现最大的核作为预测结果。</p>
<h3 id="OCR识别器"><a href="#OCR识别器" class="headerlink" title="OCR识别器"></a>OCR识别器</h3><p>现今基于深度学习的端到端OCR技术有两个主流技术：CRNN OCR和attention OCR。其实这两个方法主要区别在于最后的输出层，即怎么将网络学习到的序列特征信息转化为最终的识别结果。这两大主流技术在其特征学习阶段都采用CNN+RNN的网络结构，CRNN OCR在对齐时采取的方式是CTC算法，，而attention OCR采取的方式则是attention。<br><img src="https://img2018.cnblogs.com/blog/1093303/201901/1093303-20190129201832538-1001193226.png" alt="网络模型"><br>网络结构包含三部分，从上到下依次为：<br>（1）卷积层，使用CNN，作用是从输入图像中提取特征序列；<br>（2）循环层，使用RNN，作用是预测从卷积层获取的特征序列的标签分布；<br>（3）转录层，使用CTC，作用是从循环层获取的标签分布通过去重整合等操作转换成最终的识别结果。<br>端到端OCR的难点在哪儿呢？在于怎么处理不定长序列对齐问题！CRNN算法输入100*32归一化高度的词条图像，基于7层CNN（普遍使用VGG16）提取特征图，把特征图按列切分（Map-to-Sequence），每一列的512维特征，输入到两层各256单元的双向LSTM进行分类。在训练过程中，通过CTC损失函数的指导，实现字符位置与类标的近似软对齐。LSTM有256个隐藏节点，经过LSTM后变为长度为T × nclass的向量，再经过softmax处理，列向量每个元素代表对应的字符预测概率，最后再将这个T的预测结果去冗余合并成一个完整识别结果即可。<br>CRNN算法最大的贡献，是把CNN做图像特征工程的潜力与LSTM做序列化识别的潜力，进行结合。它既提取了鲁棒特征，又通过序列识别避免了传统算法中难度极高的单字符切分与单字符识别，同时序列化识别也嵌入时序依赖（隐含利用语料）。<br>我们现在要将 RNN 输出的序列翻译成最终的识别结果，RNN进行时序分类时，不可避免地会出现很多冗余信息，比如一个字母被连续识别两次，这就需要一套去冗余机制。<br><img src="https://pic1.zhimg.com/80/v2-3f14dcfe7b671a77856d11da8013aad4_720w.jpg" alt=""><br>比如我们要识别上面这个文本，其中 RNN 中有 5 个时间步，理想情况下 t0, t1, t2 时刻都应映射为“a”，t3, t4 时刻都应映射为“b”，然后将这些字符序列连接起来得到“aaabb”，我们再将连续重复的字符合并成一个，那么最终结果为“ab”。<br>这似乎是个比较好的方法，但是存在一个问题，如果是book，hello之类的词，合并连续字符后就会得到 bok 和 helo，这显然不行，所以 CTC 有一个blank机制来解决这个问题。<br>我们以“-”符号代表blank，RNN 输出序列时，在文本标签中的重复的字符之间插入一个“-”，比如输出序列为“bbooo-ookk”，则最后将被映射为“book”，即有blank字符隔开的话，连续相同字符就不进行合并。即对字符序列先删除连续重复字符，然后从路径中删除所有“-”字符，这个称为解码过程，而编码则是由神经网络来实现。引入blank机制，我们就可以很好地解决重复字符的问题。</p>
<h3 id="标题提取"><a href="#标题提取" class="headerlink" title="标题提取"></a>标题提取</h3><p>对于一段新闻视频，由于每个标题出现的位置固定，因此我们1.0版本的算法中采用规则筛选来提取标题。我们首先采用OCR检测器去获取每一帧图片中文本的位置，通过一些位置规则去除一些文本框；然后利用OCR识别器获取剩下的文本框中的文本信息。在1.0版本的算法中，我们会在前5帧中去搜索视频的四个角的文本信息，通过正则匹配查找电视的台标。然后根据台标在配置文件中找到每个电视的配置信息。基于规则的文本筛选主要有，（1）标题出现在视频帧的区域位置，如在横轴方向的0.1-0.9区间，在纵轴方向的0.67-0.9区间；（2）在可能的区间中先搜索文本框最大的文本作为可能的标题，然后通过该文本块搜索临近的文本块，拼接成段落。我们还会通过一些内容匹配，如“新闻联播”、“记者”、“编辑”等文字的正则匹配过滤无效的文本。在配置文件中，我们对每个电视台的阈值进行调整，通过这些规则过滤后剩下的文本内容就是新闻标题。提测反馈结果，基于规则筛选的标题提取对于符合规则的视频可能有很好的识别效果，如“央视新闻”，但在卫视新闻中，对于配置文件中没有的电视台，默认的配置需求可能满足，导致无法识别正确的标题。同时，由于该版本算法没有对文本的内容进行过滤，经常讲记者姓名、插播广告等识别为标题。面对这种情况，我们在2.0版本的算法中提出基于内容过滤的新闻标题提取算法。虽然基于规则的算法在某些电视台能很好的识别出标题，但是这些规则也严格限制了算法性能。我们考虑能否利用少量的规则甚至不利用规则就能很好的识别出新闻标题，因此，我们利用NLP中的bert模型+FC层进行finetune，训练文本二分类模型，并通过sigmoid函数的阈值划分，过滤那些非标题的文本。我们将每个OCR识别文本送入到二分类模型进行过滤，这样的做法的有点在于可以过滤掉很大部分的受访者信息和记者姓名等文本，而且算法没有严格的规则限制，鲁棒性好。当然，文本分类器也可能会将一部分正确的标题文本过滤，甚至对于段落文本，由于逐行的过滤可能导致标题不完整或没有标题。因此我们考虑结合部分宽松条件的规则和文本分类器进行更好的标题提取。先阶段的算法的在自己的100段测试视频中的准召率为0.95和0.91，相对于1.0版本的算法有很大的提升。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/08/22/%E5%BF%AB%E6%89%8B%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE%E6%95%B4%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/22/%E5%BF%AB%E6%89%8B%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE%E6%95%B4%E7%90%86/" class="post-title-link" itemprop="url">快手实习项目整理</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-08-22 15:13:04 / 修改时间：23:12:53" itemprop="dateCreated datePublished" datetime="2020-08-22T15:13:04+08:00">2020-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="快手实习项目整理"><a href="#快手实习项目整理" class="headerlink" title="快手实习项目整理"></a>快手实习项目整理</h1><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><p>首先，是图像修复领域的调研和复现，主要是《Image Inpainting with Learnable Bidirectional Attention Maps》、《EdgeConnect: Structure Guided Image Inpainting using Edge Prediction》以及《Free-Form Image Inpainting with Gated Convolution》。<br>《Free-Form Image Inpainting with Gated Convolution》</p>
<h2 id="图像修复"><a href="#图像修复" class="headerlink" title="图像修复"></a>图像修复</h2><p>1、使用低阶图像特征的块（patch）匹配：生成看似合理的平稳纹理，但在处理复杂的场景，例如人脸和物体的时候，常出现严重的错误。<br>2、使用卷积神经网络推理生成模型： 从大规模数据集中学习到的语义，以端到端的方式合成非平稳图像中的内容。但是普通卷积的深度生成模型，在图像填充上存在严重的问题，因为普通卷积识所有输入的像素和特征，均为有效像素。对于图像填充来说，每一层的输入时由确实外的有效像素/特征和缺失区域（掩码区域）的无效像素组成。普通卷积使用相同的卷积核，适用于所有有效、无效和混合（例如那些空洞边界）的像素/特征，在自由形状上的掩码做测试时，导致视觉上的伪影（如颜色差异，模糊和孔周围明显的边缘响应）。</p>
<h3 id="部分卷积"><a href="#部分卷积" class="headerlink" title="部分卷积"></a>部分卷积</h3><p>为了解决这一局限性，部分卷积（PartialConv），其中卷积被遮掩（masked）和归一化，仅以有效像素为条件。基于规则的掩码更新策略，用于更新下一层的有效位置。部分卷积将所有位置视为无效或有效，并用0或1掩码乘以所有层的输入，该掩码可以看做是一个单一的不可学习的特征门（gate）通道。<br>然而，这种假设是有几个局限性：<br>（1）考虑跨网络不同层的输入空间位置，他们可能包括：a)输入图像中有有效像素；b)输入图像中有掩蔽mask像素；c)感受野的神经元没有覆盖到输入图像的有效像素；d)感受野的神经元覆盖了不同数量的输入图像的有效像素，这些有效地图像像素也可能有不同的相对位置；e)深层合成的像素；f)启发式的将所有的位置归类为无效或有效，会忽略以上这些重要信息。<br>（2）如果我们扩展到用户知道的图像修复，用户在掩码内提供的稀疏草图（sparse sketch），这些像素位置应该被视为有效地还是无效的？如何正确地更新下一层的掩码？<br>（3）对于部分卷积，无效的像素将逐层逐渐消失，基于规则的掩码将在深层全部消失。然而，为了合成孔内的像素，这些深层可能还需要知道当前位置是在孔内还是孔外？全1掩码的部分卷积不能提供这样的信息。</p>
<h3 id="门控卷积"><a href="#门控卷积" class="headerlink" title="门控卷积"></a>门控卷积</h3><p>因此提出了门控卷积的概念，门控卷积掩码更新过程，输入特征首先用于计算门控值$g=\theta(w_gx)$，$\theta$是激活函数，$w_g$是可学习参数。最终的输出是学习到的特征和门控值$y=\fi(wx)\bigdot g$的乘积。<br>由于普通卷积是将每一个像素都当成有效值去计算的，这个特性适用于分类和检测任务，但是不适用于inpainting任务，因为inpainting任务中hole里面的像素是无效值，因此对空洞hole里面的内容和外面的内容要加以区分，部分卷积虽然将里面和外面的内容加以区分，但是它将含有1个有效值像素的区域与含有9个有效值像素的区域同等对待，这明显不合理，gated conv则是使用卷积核sigmoid函数使得网络去学习这种区分。</p>
<h3 id="光谱归一化马尔科夫判别器（SNPatchGAN）"><a href="#光谱归一化马尔科夫判别器（SNPatchGAN）" class="headerlink" title="光谱归一化马尔科夫判别器（SNPatchGAN）"></a>光谱归一化马尔科夫判别器（SNPatchGAN）</h3><p>之前的修复网络，为了修复带有矩形确实部分的图片，提出 local GAN 来提升实验结果；然而，我们要研究的是对任意形状缺失的形状，借鉴 global and GANs、MarkovianGANs、perpetual loss、spectral-normalized GANs，作者提出一个有效地GAN loss，即为 SN-PatchGAN。<br>SN-PatchGAN的组成，是由卷积网络构成，输入为image、mask、guidance channel，输出是一个形状为h×w×c的3维特征，h、w、c分别代表高、宽和通道数。SN-PatchGAN由6个卷积层（卷积核大小为2，步幅为2）堆叠来获得Markovian Patches特征的统计信息。然后直接将SN-PatchGAN应用到特征图的每一个特征元素，以输入图像的不同位置和不同语义（在不同的通道中表示）的形式表示GAN的$h\times w\times c$个。<br>值得注意的是，在训练的环境中，输出图中每个神经元的感受野可以覆盖整个输入图像，因此不需要全局判别器。<br>作者也采用了最近提出的Spectral normalization来进一步稳定GANs的训练。我们采用SN-GANs中描述的默认Spectral normalization的fast approximation算法。<br>为了判别输入的真假，作者也采用了hings loss来作为目标函数，未采用Perceptual loss的原因是相似的patch-level information已经被编码在SN-PatchGAN中。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>作者定制了一个带有Gated convolution layer和SN-PatchGan loss的generative inpainting network。网络结构由粗修复和细修复两个网络构成，采用了encoder-decoder network（PartialConv采用的是类似U-net的结构）。网络为全卷积神经网络，支持不同分辨率图片的输入。训练是一个端到端的过程。</p>
<h2 id="“蝴蝶消失”特效算法"><a href="#“蝴蝶消失”特效算法" class="headerlink" title="“蝴蝶消失”特效算法"></a>“蝴蝶消失”特效算法</h2><p>快手的“蝴蝶消失”特效算法，主要是在用户上传的图片中，在蝴蝶飞舞的特效下，图片中的人像逐渐消失，还原完整的背景图像信息。算法的实现，是通过人像分割算法，检测图片中人体的位置并去除人体图像信息，然后采用图像修复算法还原人像区域可能存在的背景内容，实现蝴蝶飞舞下人像消失的特效。在这项工作中，我主要负责的是图像修复算法的研究以及后期的模型压缩等工作。<br>对于图像修复算法，我们使用卷积神经网络推理生成模型，从大规模数据集中学习到的语义，以端到端的方式合成非平稳图像中的内容。但是普通卷积的深度生成模型，在图像填充上存在严重的问题，因为普通卷积识所有输入的像素和特征，均为有效像素。但是对于图像填充来说，每一层的输入是由图像的有效像素和缺失区域的无效像素组成。普通卷积使用相同的卷积核，在人像这种自由形状上进行修复时，会导致视觉上的伪影（如颜色差异，模糊和孔周围明显的边缘响应）。<br>因此，我们采用门控卷积用于不规则的图像修复，它会为每个通道和每个空间位置（例如内部和外部掩码，RGB通道和用户知道通道）学习动态特征门控机制，使得最终的输出是学习到的特征与门控值g的乘积，门控卷积可以学会在单独的通道中突出掩码区域的信息，提高了不规则掩码输入的图像修复质量。我们通过堆叠门控卷积形成编码器和解码器网络，并将上下文注意模块集成到网络中，来更好的捕获远距离的依赖关系。<br><img src="https://upload-images.jianshu.io/upload_images/13278405-ec200149adb2f468.png?imageMogr2/auto-orient/strip|imageView2/2/w/852/format/webp" alt="网络结构"><br>由于网络采用的是生成对抗网络，处理像素级的l1重建损失函数和GAN loss(生成器和判别器都使用hinge loss)外，我们还尝试添加其他的损失函数优化图像修复效果。 我们发现不同的损失函数对图像的修复结果有很大的影响，添加log-loss以及ls-loss生成的纹理信息相对来说比较差，但可以避免斑块状伪影的问题，而relgan-loss生成的纹理信息比较好，但会出现斑块状的纹理。添加感知损失perceptual loss，通过VGG16计算全图区域的影响能有效平滑修复区域的斑块信息，减少伪影现象，而添加风格损失style loss计算图像的风格变化，并没有带来明显的效果提升，因此模型最终选择添加感知损失来优化模型修复效果。（感知损失将真实图片卷积得到的feature与生产图片卷积得到的feature进行对比，是高层的内容和全局信息接近，也就是感知的含义）。<br>由于图像修复是基于人像分割算法得到的人像区域上进行修复的，在真实场景中，可能由于人像手持物体或者头发等导致人像分割的区域不准确，有残留的边缘区域会导致修复的结果出现斑块状的伪影现象。针对这种现象，我们在模型训练的时候添加了攻击噪声，使得模型更加的稳定和鲁棒。考虑到人像分割的不准确性多是发生于边缘区域，因此我们训练的时候对人像的mask轮廓上随机添加块状的缺口，模拟分割不准确问题。在验证集的添加30%的边界攻击数据集上进行验证，模型修复的SSIM相较于原始模型降低0.04，但相较于原始模型在攻击数据集的表降低0.1现，添加边界噪声的模型具有更加鲁棒的结果，模型最终的修复指标SSIM为0.737。</p>
<h3 id="图像修复的评价指标"><a href="#图像修复的评价指标" class="headerlink" title="图像修复的评价指标"></a>图像修复的评价指标</h3><p>计算图像修复的质量，最直接的思路即比较修复后的图片与真实图像之间的可视误差，通过visibility of errors评价图像的质量。</p>
<h4 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h4><p>PSNR(Peak Signal to nise Ratio)，峰值信噪比，即峰值信号的能量与噪声的平均能量之比，通常表示时取log变成db计算，由于MSE为真实图片与含噪图像之差的能量均值，而两者的差即为噪声，因此PSNR即峰值信号能量与MSE之比。<br>优点：算法简单，计算的速度快。<br>缺点：基于对应像素点间的误差，呈现的差异值与人的主观感受不成比例，不符合人类视觉系统（HVS）的评价结果。</p>
<h4 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h4><p>SSIM(Structural Similarity)，结构相似性，也就是一种全参考的图像质量评价指标，它分别从亮度、对比度、结构三方面度量图像相似性。SSIM取值范围[0, 1]，值越大，表示图像失真越小，SSIM在图像去噪、图像相似度评价上是由于PSNR的。</p>
<script type="math/tex; mode=display">SSIM = \frac{(2\mu_x\mu_y+c_1)(\sigma_{xy}+c_2}{(\mu_x^2+\mu_y^2+c_1)(\sigma_x^2+\sigma_y^2+c_2)}</script><p>我们每次计算都是从图片上取一个NxN的窗口，然后不断滑动窗口进行计算，最后取平均值作为全局的SSIM。<br>优点：改进了PSNR的缺点，比较符合人体视觉系统的评估。<br>缺点：结构相似性指标对于图像出现位移、缩放、旋转等非结构性的失真无法有效的运作。</p>
<h2 id="模型压缩算法"><a href="#模型压缩算法" class="headerlink" title="模型压缩算法"></a>模型压缩算法</h2><h3 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h3><p>分组卷积（Group convolution）：将多个卷积核拆分为分组，每个分组单独执行一系列运算之后，最终在全连接层再拼接在一起。分组卷积的重点不在于卷积，而在于分组：在执行卷积之后，将输出的feature map 执行分组。然后在每个组的数据会在各个GPU 上单独训练。<br>分组卷积在网络的全连接层才进行融合，这使得每个GPU 中只能看到部分通道的数据，这降低了模型的泛化能力。但是分组卷积降低了模型的参数数量以及计算量。<br>假设输入feature map具有$C_I$的输入通道，宽/高分别为$W_I, H_I$，假设卷积核的宽/高分别为$W_K, H_K$，有$C_O$个卷积核，则卷积的参数量为:$W_K\times H_K\times C_I\times C_O$，计算量为：$W_K\times H_K\times C_I\times W_O\times H_O\times C_O$。假设采用分组卷积，将输入通道分成G组，则分组后：参数量为$W_K\times H_K\times \frac{C_I}{G}\times \frac{C_O}{G}$，分组卷积的参数量、计算量均为标准卷积计算的$\frac{1}{G}$。</p>
<h3 id="可分离卷积DepthWise"><a href="#可分离卷积DepthWise" class="headerlink" title="可分离卷积DepthWise"></a>可分离卷积DepthWise</h3><p>标准的卷积会考虑所有的输入通道，而DepthWise 卷积会针对每一个输入通道进行卷积操作，然后接一个1x1 的跨通道卷积操作。<br>DepthWise 卷积与分组卷积的区别在于：<br>1）分组卷积是一种通道分组的方式，它改变的是对输入的feature map 处理的方式。Depthwise 卷积是一种卷积的方式，它改变的是卷积的形式。<br>2）Depthwise 分组卷积结合了两者：首先沿着通道进行分组，然后每个分组执行DepthWise 卷积。<br>假设使用标准卷积，输入通道的数量为$C_I$，输出通道的数量为$C_O$，卷积核的尺寸为$W_K\times H_K$。则需要的参数数量为$C_I\times W_K\times H_K\times C_O$。使用Depthwise卷积时，图像的每个通道先通过一个$W_K\times H_K$的depthwise卷积层，再经过一个1x1、输出为$C_O$的卷积层。参数量为：</p>
<script type="math/tex; mode=display">C_I\times W_K\times H_K+C_I\times 1\times 1\times C_O=W_K\times H_K\times C_I+C_IC_O</script><p>其参数数量是标准卷积的$\frac{1}{C_O}+\frac{1}{W_KH_K}$。</p>
<h3 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h3><p>知识蒸馏的本质是让大的teacher model来协助线上的student model进行训练。因为模型训练时，我们通常采用复杂模型或Ensemble方式来获取最好的结果，从而导致参数冗余验证，在前向推理的时候，需要对模型进行复杂的计算。而知识蒸馏是把复杂模型或多个模型Ensemble（Teacher）学到的知识迁移到另一个轻量级模型上（Student）叫知识蒸馏，使得模型变轻量的同时（方便部署），尽量不损失性能。<br>知识蒸馏主要分成三个大类：<br>（1）输出迁移（output transfer），将网络的输出（soft-target）作为知识；<br>（2）特征迁移（feature transfer），将网络学习的特征作为知识；<br>（3）关系迁移（relation transfer），将网络或样本的关系作为知识。<br>在输出迁移中，我们对一些术语进行定义：</p>
<ul>
<li>Teacher：原始较大的模型或模型Ensemble，用于获取知识；</li>
<li>Student：新的较小模型，接收teacher的知识，训练后用于前向预测；</li>
<li>Hard target：样本原本的标签，Onehot</li>
<li>Soft target：Teacher输出的预测结果（一般是softmax之后的概率）</li>
</ul>
<p>这里的软目标的优势在于：<br>（1）弥补了简单分类中监督信号不足（信息熵比较少）的问题，增加了信息量；<br>（2）提供了训练数据中类别的关系（数据增强）；<br>（3）可能增强了模型泛化的能力。<br>而特征迁移Feature Transfer——将网络学习的特征作为知识，对卷积网络隐藏层输出的特征图——feature map（特征 &amp; 知识）进行迁移（Attention transfer），让学生网络的feature map与教师网络的feature map尽可能相似。<br>关系迁移Relation Transfer——将网络或者样本的关系作为知识，让学生网络学习教师网络层与层之间的关系（特征关系）。</p>
<h3 id="模型压缩过程"><a href="#模型压缩过程" class="headerlink" title="模型压缩过程"></a>模型压缩过程</h3><p>在原始的图像修复模型中，输入的图像大小为256x256，卷积的第一层通道数为48，模型的参数量接近25G，模型修复指标SSIM为0.7273。首先，们将输出图像的大小resize到128x128大小，模型的参数量降低为6G，但是模型的修复指标SSIM降低为0.6918，然后我们采用mobile_v2中的可分离卷积的思想，替换模型中的卷积方式，模型的参数量减少为1G，此时模型的SSIM指标为0.6486；接着我们进一步将卷积层的通道数减少为24，模型的参数量为300M，修复指标为0.6276。我们采用压缩的方式，很好的降低了模型的参数量，但由于模型压缩过程导致有用的参数的丢失，使得模型修复指标出现严重的下降，图像修复结果也出现严重的斑块状的伪影。为此，我们采用知识蒸馏的思想，将未压缩的模型作为教师网络，利用教师网络中生成的粗糙和中间特征图和输出的精细修复图，对学生网络的粗糙特征图和输出进行是修复图进行监督，在损失函数中计算二者的重建损失l1-loss，使得学生网络在训练过程中保留更多有用的信息。采用知识蒸馏的做法后，在参数量为1G的压缩模型中，SSIM指标提升为0.6661，在300M的蒸馏版本中模型的SSIM指标提升为0.6500，相较于未蒸馏前的效果有了明显的改进，而且修复的图片的斑块状伪影减少，修复区域更加的平滑。但在一些颜色变化复杂的区域，修复效果和原始相比还是会呈现明显的亮斑，因此我们添加感知损失计算teacher模型输出图像和学生网络输出图像的差值来平滑图像学生网络图像修复的结果。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/06/01/%E5%BF%AB%E6%89%8B%E5%AE%9E%E4%B9%A0%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/06/01/%E5%BF%AB%E6%89%8B%E5%AE%9E%E4%B9%A0%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">快手实习记录</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-06-01 20:19:11" itemprop="dateCreated datePublished" datetime="2020-06-01T20:19:11+08:00">2020-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-02 17:04:24" itemprop="dateModified" datetime="2020-06-02T17:04:24+08:00">2020-06-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          xxx
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/06/01/%E5%BF%AB%E6%89%8B%E5%AE%9E%E4%B9%A0%E8%AE%B0%E5%BD%95/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/04/20/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/20/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">生成对抗网络</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-20 14:28:47" itemprop="dateCreated datePublished" datetime="2020-04-20T14:28:47+08:00">2020-04-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-08 14:57:34" itemprop="dateModified" datetime="2020-08-08T14:57:34+08:00">2020-08-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="什么是GAN"><a href="#什么是GAN" class="headerlink" title="什么是GAN"></a>什么是GAN</h1><p>GAN是一种由生成网络和判别网络组成的深度神经网络架构。通过在生成和判别之间多次循环，两个网络相互对抗，试图胜过对方，从而训练了彼此。</p>
<h2 id="生成网络"><a href="#生成网络" class="headerlink" title="生成网络"></a>生成网络</h2><p>生成网络使用现有数据生成新数据，比如使用现有图像来生成新图像。生成网络的核心任务是从随机生成的由数字构成的向量（称为“潜在空间”， latent space）中生成数据（比如图像、视频、音频或文本）。在构建生成网络时需要明确该网络的目标，例如生成图像、文本、音频、视频，等等。</p>
<h2 id="判别网络"><a href="#判别网络" class="headerlink" title="判别网络"></a>判别网络</h2><p>判别网络试图区分真实数据和由生成网络生成的数据。对于输入的数据，判别网络需要基于事先定义的类别对其分类。这可能是多分类或二分类。通常，GAN 中进行的是二分类。</p>
<h2 id="GAN中重要的概念"><a href="#GAN中重要的概念" class="headerlink" title="GAN中重要的概念"></a>GAN中重要的概念</h2><h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h3><p>KL 散度，也称相对熵，用于判定两个概率分布之间的相似度。它可以测量一个概率分布 p相对于另一个概率分布 q 的偏离。</p>
<script type="math/tex; mode=display">D_{KL}(p||q)=\int_xp(x)log\frac{p(x)}{q(x)}</script><p>如果 p(x)和 q(x)处处相等，则此时 KL 散度为 0，达到最小值。<br>由于 KL 散度具有不对称性，因此不用于测量两个概率分布之间的距离，因此也不用作距离的度量（metric）。</p>
<h3 id="JS散度"><a href="#JS散度" class="headerlink" title="JS散度"></a>JS散度</h3><p>JS 散度，也称信息半径（information radius， IRaD）或者平均值总偏离（total divergence to the average），是测量两个概率分布之间相似度的另一种方法。它基于 KL 散度，但具有对称性，可用于测量两个概率分布之间的距离。对 JS 散度开平方即可得到 JS 距离， 所以它是一种距离度量。<br>计算两个概率分布p和q之间JS散度的公式如下。</p>
<script type="math/tex; mode=display">D_{JS}(p||q)=frac{1}{2}D_{KL}(p||frac{p+q}{2})+frac{1}{2}D_{KL}(q||frac{p+q}{2})</script><p>其中，$frac{p+q}{2}$是p和q的中点测度，$D_{KL}$是KL散度。</p>
<h3 id="纳什均衡"><a href="#纳什均衡" class="headerlink" title="纳什均衡"></a>纳什均衡</h3><p>博弈论中的纳什均衡描述了一种在非合作博弈中可以达到的特殊状态。其中每个参与者都试图基于对其他参与者行为的预判，选择使自己获益最多的最佳策略。最终形成的局面是，所有参与者都基于其他参与者的选择，采取了对自己来说最佳的策略，此时已经无法通过改变策略获益了。这种状态就称为纳什均衡。</p>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>为了使生成网络生成的图像能以假乱真，应尽量提高生成网络所生成数据和真实数据之间的相似度。可使用目标函数测量这种相似度。生成网络和判别网络各有目标函数，训练过程中也分别试图最小化各自的目标函数。 GAN 最终的目标函数如下所示。<br><img src="/images/gan_loss.png" alt=""><br>其中， D(x)是判别网络模型， G(z)是生成网络模型， p(x)是真实数据分布， p(z)是生成网络生成的数据分布， E 是期望输出。<br>在训练过程中， D（判别网络， discriminator）试图最大化公式的最终取值，而 G（生成网络，generator）试图最小化该值。如此训练出来的 GAN 中，生成网络和判别网络之间会达到一种平衡，此时模型即“收敛”了。这种平衡状态就是纳什均衡。训练完成之后，就得到了一个可以生成逼真图像的生成网络。</p>
<h3 id="评分算法"><a href="#评分算法" class="headerlink" title="评分算法"></a>评分算法</h3><p>GAN 的目标函数不是均方误差（mean-square error）或者交叉熵（cross entropy）这样确定的函数，而是在训练过程中习得的。研究者们提出了多种可以测量模型准确度的评分算法，下面介绍其中几个。</p>
<h4 id="Inception分数"><a href="#Inception分数" class="headerlink" title="Inception分数"></a>Inception分数</h4><p>Inception 分数（IS）是应用最广泛的 GAN 评分算法。它使用一个在 Imagenet 上预训练过的Inception V3 网络分别提取真实图像和生成图像的特征。IS 测量生成图片的质量和多样性。计算 IS 的公式如下。</p>
<script type="math/tex; mode=display">IS(G)=exp(E_{\chi \sim p_g}D_{KL}(p(y|\chi)||p(y)))</script><p>其中，$p_g$表示一个概率分布，$\chi \sim p_g$表示$\chi$是该概率分布中的一个抽样。$p(y|\chi)$是条件类别分布，$p(y)$是边缘类别分布。<br>计算Inception分数的步骤如下：<br>1）首先从模型生成的图像中抽取N个样本，记为（$\chi^i$）。<br>2）然后使用如下公式构建边缘类别分布。</p>
<script type="math/tex; mode=display">p(y)=\int_{\chi}p(y|\chi)p_g(\chi)</script><p>3）接着使用如下公式计算KL散度以及期望值。<br>$IS(G)=exp(E_{\chi \sim p_g}D_{KL}(p(y|\chi)||p(y)))$<br>4）最后计算上述结果的指数，即可得到IS。<br>IS 越高，说明模型质量越好。 IS 虽然是重要的测度（measure），却也存在一些问题。比如模型对于每个类别只生成一张图像，其 IS 仍然可以很高，但这样的模型缺乏多样性。</p>
<h2 id="GAN的优势"><a href="#GAN的优势" class="headerlink" title="GAN的优势"></a>GAN的优势</h2><p>1）GAN 是无监督学习方法。带标注数据需要人工制作，非常耗时。 GAN 不需要带标注数据，而可以通过无标注数据进行训练，学习数据的内在表现形式。<br>2）GAN 可以生成数据。 GAN 可以生成能跟真实数据媲美的数据，应用潜力巨大。 GAN 可以生成图像、文本、音频和视频等，并且和真实数据相差无几。用 GAN 生成图像可应用于市场营销、电子商务、游戏、广告等很多行业。<br>3）GAN 可以学习数据的概率密度分布。 GAN 可以学习数据的内在表现形式。前面提到了GAN 可以学习混乱而复杂的数据概率分布，有助于解决机器学习领域的很多问题。<br>4）训练后的判别网络是分类器。 GAN 训练完成之后会得到一个判别网络和一个生成网络，而判别网络可用作分类器。</p>
<h2 id="训练GAN的问题"><a href="#训练GAN的问题" class="headerlink" title="训练GAN的问题"></a>训练GAN的问题</h2><p>GAN 也存在一些问题。这些问题通常与训练过程有关，包括模式塌陷、内部协变量转移以及梯度消失等。</p>
<h3 id="模式塌陷"><a href="#模式塌陷" class="headerlink" title="模式塌陷"></a>模式塌陷</h3><p>模式塌陷问题指的是生成网络所生成的样本之间差异不大，有时甚至始终只生成同样的图像。有一些概率分布是多峰的（multimodal），构造十分复杂。数据可能是通过不同类型的观测得来的，因此样本中可能会暗含一些细类，每个细类下的样本之间比较相似。这样会导致数据的概率分布出现多个“峰”，每个峰对应一个细类。如果数据的概率分布是多峰的， GAN 有时就会出现模式塌陷问题，无法成功构建模型。如果生成的所有样本几乎都相同，这种情况就被称为“完全塌陷”。<br>解决模式坍塌问题有多种方法，例如：<br>1）针对不同的峰训练不同的GAN模型；<br>2）使用多样化的数据训练GAN。</p>
<h3 id="内部协变量转移"><a href="#内部协变量转移" class="headerlink" title="内部协变量转移"></a>内部协变量转移</h3><p>内部协变量转移问题之所以产生，是因为神经网络输入数据的概率分布发生了变化。输入数据的概率分布改变之后，隐藏层会试图适应新的概率分布，训练速度因此放缓，需要很长时间才会收敛到全局最小值。神经网络输入数据的概率分布和该网络之前接触的数据概率分布之间差异过大是问题根源。解决方法包括批归一化以及其他归一化技术。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/09/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/09/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/" class="post-title-link" itemprop="url">面试记录</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-09 15:43:54" itemprop="dateCreated datePublished" datetime="2020-03-09T15:43:54+08:00">2020-03-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-20 09:34:35" itemprop="dateModified" datetime="2020-04-20T09:34:35+08:00">2020-04-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Aibee-2020-03-09"><a href="#Aibee-2020-03-09" class="headerlink" title="Aibee(2020.03.09)"></a>Aibee(2020.03.09)</h1><h2 id="一面（电话面）："><a href="#一面（电话面）：" class="headerlink" title="一面（电话面）："></a>一面（电话面）：</h2><p>项目介绍</p>
<h2 id="二面（视频面）："><a href="#二面（视频面）：" class="headerlink" title="二面（视频面）："></a>二面（视频面）：</h2><p>1）介绍论文<br>论文研究的方向是基于骨骼关键点的人体行为识别，现在的算法采用分层图卷积网络聚集关节更宽范围内邻域的特征，这会减弱局部邻域的信息，针对这种现象首先提出一个残差图卷积操作增强节点的局部领域信息，接着利用密集连接重用不同模块间的上下文时空特征来增加节点的全局和局部特征。在重用特征的过程中会带来冗余信息的干扰，利用通道注意模块计算不同通道间的特征相关性，增强有用特征，抑制无关信息。同时在行为中，并不是所有的帧和关节都对行为识别有用，冗余的帧信息会带来识别的干扰，因此进一步采用注意模块来增强关键帧和关键节点的特征。<br>2）什么是图卷积网络<br>卷积是通过计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征提取。<br>3）反向传播了解吗，代入L2正则化<br>4）算法题<br>找到数组中第k大的数<br>两个有序数组找中位数</p>
<h1 id="快手-2020-03-11"><a href="#快手-2020-03-11" class="headerlink" title="快手(2020.03.11)"></a>快手(2020.03.11)</h1><h2 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h2><p>1) 最熟悉的项目，重点介绍算法模块<br>为什么采用openpose？动作分类的评价指标是什么？动作的相似度是怎么判断的？<br>2) 论文介绍<br>残差图操作是什么？注意模块怎么实现？邻接矩阵是什么？<br>3) 基础题<br>防止网络过拟合的方法有哪些？<br>BN层原理？具体是怎么实现的？优缺点？<br>网络压缩中Depthwise卷积核正常卷积差别，减少多少计算量。<br>SVD和PCA原理<br>4) 算法题<br>最大连续子序列和</p>
<h2 id="二面"><a href="#二面" class="headerlink" title="二面"></a>二面</h2><p>1) 比较深入和有创新的项目？<br>动态时间规划在项目中怎么实现<br>关键点的相似度是怎么计算的？<br>2) 论文<br>3) 基础题<br>分类网络能用mse吗？为什么<br>4) 算法题<br>股票问题，只能买卖一次</p>
<h1 id="头条"><a href="#头条" class="headerlink" title="头条"></a>头条</h1><h2 id="一面-1"><a href="#一面-1" class="headerlink" title="一面"></a>一面</h2><p>算法题<br>1）找到字符串的最长无重复字符子串<br>2）整数对查找<br>请设计一个高效算法，找出数组中两数之和为指定值的所有整数对。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FindPair</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">countPairs</span><span class="params">(self, A, n, tsum)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> A <span class="keyword">or</span> len(A)==<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        dic = defaultdict(int)</span><br><span class="line">        ans = <span class="number">0</span></span><br><span class="line">        setA = list(set(A))</span><br><span class="line">        setA.sort()</span><br><span class="line">        left, right = <span class="number">0</span>, len(setA)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> A:</span><br><span class="line">            dic[num] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left&lt;right:</span><br><span class="line">            <span class="keyword">if</span> setA[left]+setA[right]&lt;tsum:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> setA[left]+setA[right]&gt;tsum:</span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ans += dic[setA[left]]*dic[setA[right]]</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> setA[left]*<span class="number">2</span>==tsum:</span><br><span class="line">            ans += dic[setA[left]]*(dic[setA[left]]<span class="number">-1</span>)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><br>基础题<br>知道set和字典的低层实现吗？<br>在Python中，字典是通过散列表（哈希表）实现的。字典也叫哈希数组或关联数组，所以其本质是数组（如下图），每个 bucket 有两部分：一个是键对象的引用，一个是值对象的引用。<br>了解哈希表吗？哈希冲突是怎么解决的<br>哈希表（Hash table，也叫散列表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。<br>解决哈希冲突的方法一般有：开放定址法、链地址法（拉链法）、再哈希法、建立公共溢出区等方法。<br>开放定址法：从发生冲突的那个单元起，按照一定的次序，从哈希表中找到一个空闲的单元。然后把发生冲突的元素存入到该单元的一种方法。开放定址法需要的表长度要大于等于所需要存放的元素。<br>在开放定址法中解决冲突的方法有：线行探查法、平方探查法、双散列函数探查法。<br>链接地址法的思路是将哈希值相同的元素构成一个同义词的单链表，并将单链表的头指针存放在哈希表的第i个单元中，查找、插入和删除主要在同义词链表中进行。链表法适用于经常进行插入和删除的情况。<br>再哈希法，同时构造多个不同的哈希函数：Hi = RHi(key) i= 1,2,3 … k;当H1 = RH1(key) 发生冲突时，再用H2 = RH2(key) 进行计算，直到冲突不再产生，这种方法不易产生聚集，但是增加了计算时间。<br>建立公共溢出区：将哈希表分为公共表和溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。<br>项目介绍<br>你能介绍一下注意机制吗？<br>你能介绍一下ResNet机制吗？<br>梯度消失是什么原因？如何解决梯度消失？<br>激活函数relu为什么可以解决梯度消失？</p>
<h1 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h1><h2 id="一面-20200408"><a href="#一面-20200408" class="headerlink" title="一面 20200408"></a>一面 20200408</h2><p>项目介绍<br>算法的评估模块是如何实现的？<br>每一帧的相似度是如何计算的？采用的欧式距离是计算哪些对象的空间距离？<br>不同的姿态和不同的角度是如何处理的？<br>不同远近，不同角度、大人和小孩等进行测试的时候出现什么具体的问题，如何解决的，问题的指标是哪些？<br>用户的动作时间不匹配是如何解决的？<br>通信机制的设计和解决方案的目的是什么，为什么这么设计？<br>消息中间件还有哪些？其功能是什么？<br>服务器的系统架构是怎样的？分布式系统要如何部署？多服务器多人同时在线该如何解决？<br>hashset的原理及内部实现的细节？<br>HashSet实现自set接口，set集合中元素无序且不能重复。<br>因为HashSet底层是基于HashMap实现的，当你new一个HashSet时候，实际上是new了一个map，执行add方法时，实际上调用map的put方法，value始终是PRESENT，所以根据HashMap的一个特性: 将一个key-value对放入HashMap中时，首先根据key的hashCode()返回值决定该Entry的存储位置，如果两个key的hash值相同，那么它们的存储位置相同。如果这个两个key的equalus比较返回true。那么新添加的Entry的value会覆盖原来的Entry的value，key不会覆盖。因此,如果向HashSet中添加一个已经存在的元素，新添加的集合元素不会覆盖原来已有的集合元素。</p>
<h2 id="二面-1"><a href="#二面-1" class="headerlink" title="二面"></a>二面</h2><p>一个1亿行的文本文件（文件内容为人民日报真实语料），每行有大约100个汉字。计算如下：<br>1）计算每个汉字出现次数<br>2）计算出现频率最高的连续10个字符（包含标点符号）<br>限制：<br>1）使用多线程实现<br>2）系统资源：4core，8g内存<br>3）指标1需要完整代码，指标2计算为开放性问题，伪代码描述思路即可。<br>4）使用Java实现</p>
<h1 id="腾讯优图"><a href="#腾讯优图" class="headerlink" title="腾讯优图"></a>腾讯优图</h1><h2 id="一面-2"><a href="#一面-2" class="headerlink" title="一面"></a>一面</h2><p>算法题：快排、最长公共子序列、c++创建和删除二维数组<br>常见的激活函数？<br>ResNet和DenseNet的区别？<br>过拟合解决的方法？<br>Dropout的原理？<br>R-CNN的实现原理？<br>Softmax反向传播的推导？<br>项目介绍。</p>
<h1 id="微软"><a href="#微软" class="headerlink" title="微软"></a>微软</h1><h2 id="一面-3"><a href="#一面-3" class="headerlink" title="一面"></a>一面</h2><p>项目介绍<br>你知道哪些距离计算方法？为什么采用欧式距离来评估？<br>比赛的评价指标是什么？是比赛的mAP?mAP是如何计算的？其原理是什么？<br>什么是NMS?什么是softer NMS？<br>算法题：<br>第K大的数？时间复杂度是多少？<br>两个很大的文件，每一行是url地址，找到一个url地址既在A文件又在B文件中？<br>在操作系统中，进程和线程的区别？<br>在python语言中，什么时候会用多进程，什么时候会用多线程？</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E4%B8%89%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E4%B8%89%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">第三章 深度学习基础</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-03 21:13:44" itemprop="dateCreated datePublished" datetime="2020-03-03T21:13:44+08:00">2020-03-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-22 11:12:24" itemprop="dateModified" datetime="2020-03-22T11:12:24+08:00">2020-03-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第三章-深度学习基础"><a href="#第三章-深度学习基础" class="headerlink" title="第三章 深度学习基础"></a>第三章 深度学习基础</h1><h2 id="3-1-基本概念"><a href="#3-1-基本概念" class="headerlink" title="3.1 基本概念"></a>3.1 基本概念</h2><h3 id="3-1-1-神经网络组成？"><a href="#3-1-1-神经网络组成？" class="headerlink" title="3.1.1 神经网络组成？"></a>3.1.1 神经网络组成？</h3><p>神经网络类型众多，其中最为重要的是多层感知机。为了详细地描述神经网络，我们先从最简单的神经网络说起。</p>
<p><strong>感知机</strong></p>
<p>多层感知机中的特征神经元模型称为感知机，由<em>Frank Rosenblatt</em>于1957年发明。</p>
<p>简单的感知机如下图所示：</p>
<p><img src="/img/ch3/3-1.png" alt=""></p>
<p>其中$x_1$，$x_2$，$x_3$为感知机的输入，其输出为：</p>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \sum_i w_i x_i \leqslant threshold \\
1, \quad if \ \ \sum_i w_i x_i > threshold
\end{aligned}
\right.</script><p>假如把感知机想象成一个加权投票机制，比如 3 位评委给一个歌手打分，打分分别为4分、1分、-3分，这3位评分的权重分别是1、2、3，则该歌手最终得分为 $4 \times 1 + 1 \times 3 + (-3) \times 2 = 1$ 。按照比赛规则，选取的 $threshold$ 为 $3$，说明只有歌手的综合评分大于$ 3$ 时，才可顺利晋级。对照感知机，该选手被淘汰，因为：</p>
<script type="math/tex; mode=display">
\sum_i w_i x_i < threshold=3, output = 0</script><p>用 $-b$  代替 $threshold$，输出变为：</p>
<script type="math/tex; mode=display">
output = \left\{
\begin{aligned}
0, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b \leqslant 0 \\
1, \quad if \ \ \boldsymbol{w} \cdot \boldsymbol{x} + b > 0
\end{aligned}
\right.</script><p>设置合适的  $\boldsymbol{x}$  和  $b$ ，一个简单的感知机单元的与非门表示如下：</p>
<p><img src="/img/ch3/3-2.png" alt=""></p>
<p>当输入为 $0$，$1$ 时，感知机输出为 $ 0 \times (-2) + 1 \times (-2) + 3 = 1$。</p>
<p>复杂一些的感知机由简单的感知机单元组合而成：</p>
<p><img src="/img/ch3/3-3.png" alt=""></p>
<p><strong>多层感知机</strong></p>
<p>多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络。相比于单独的感知机，多层感知机的第 $ i $ 层的每个神经元和第 $ i-1 $ 层的每个神经元都有连接。</p>
<p><img src="/img/ch3/3.1.1.5.png" alt=""></p>
<p>输出层可以不止有$ 1$ 个神经元。隐藏层可以只有$ 1$ 层，也可以有多层。输出层为多个神经元的神经网络例如下图所示：</p>
<p><img src="/img/ch3/3.1.1.6.png" alt=""></p>
<h3 id="3-1-2-神经网络有哪些常用模型结构？"><a href="#3-1-2-神经网络有哪些常用模型结构？" class="headerlink" title="3.1.2 神经网络有哪些常用模型结构？"></a>3.1.2 神经网络有哪些常用模型结构？</h3><p>下图包含了大部分常用的模型：</p>
<p><img src="/img/ch3/3-7.jpg" alt=""></p>
<h3 id="3-1-3-如何选择深度学习开发平台？"><a href="#3-1-3-如何选择深度学习开发平台？" class="headerlink" title="3.1.3 如何选择深度学习开发平台？"></a>3.1.3 如何选择深度学习开发平台？</h3><p>​    现有的深度学习开源平台主要有 Caffe, PyTorch, MXNet, CNTK, Theano, TensorFlow, Keras, fastai等。那如何选择一个适合自己的平台呢，下面列出一些衡量做参考。</p>
<p><strong>参考1：与现有编程平台、技能整合的难易程度</strong></p>
<p>​    主要是前期积累的开发经验和资源，比如编程语言，前期数据集存储格式等。</p>
<p><strong>参考2: 与相关机器学习、数据处理生态整合的紧密程度</strong></p>
<p>​    深度学习研究离不开各种数据处理、可视化、统计推断等软件包。考虑建模之前，是否具有方便的数据预处理工具？建模之后，是否具有方便的工具进行可视化、统计推断、数据分析。  </p>
<p><strong>参考3：对数据量及硬件的要求和支持</strong></p>
<p>​    深度学习在不同应用场景的数据量是不一样的，这也就导致我们可能需要考虑分布式计算、多GPU计算的问题。例如，对计算机图像处理研究的人员往往需要将图像文件和计算任务分部到多台计算机节点上进行执行。当下每个深度学习平台都在快速发展，每个平台对分布式计算等场景的支持也在不断演进。</p>
<p><strong>参考4：深度学习平台的成熟程度</strong></p>
<p>​    成熟程度的考量是一个比较主观的考量因素，这些因素可包括：社区的活跃程度；是否容易和开发人员进行交流；当前应用的势头。</p>
<p><strong>参考5：平台利用是否多样性？</strong></p>
<p>​    有些平台是专门为深度学习研究和应用进行开发的，有些平台对分布式计算、GPU 等构架都有强大的优化，能否用这些平台/软件做其他事情？比如有些深度学习软件是可以用来求解二次型优化；有些深度学习平台很容易被扩展，被运用在强化学习的应用中。</p>
<h3 id="3-1-4-为什么使用深层表示"><a href="#3-1-4-为什么使用深层表示" class="headerlink" title="3.1.4 为什么使用深层表示?"></a>3.1.4 为什么使用深层表示?</h3><ol>
<li>深度神经网络是一种特征递进式的学习算法，浅层的神经元直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而深层的特征则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。</li>
<li>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</li>
</ol>
<h3 id="3-1-5-为什么深层神经网络难以训练？"><a href="#3-1-5-为什么深层神经网络难以训练？" class="headerlink" title="3.1.5 为什么深层神经网络难以训练？"></a>3.1.5 为什么深层神经网络难以训练？</h3><ol>
<li><p>梯度消失</p>
<pre><code> 梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。
</code></pre><p> ​    梯度消失的原因受到多种因素影响，例如学习率的大小，网络参数的初始化，激活函数的边缘效应等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：</p>
</li>
</ol>
<p><img src="/img/ch3/3-8.png" alt=""></p>
<ol>
<li><p>梯度爆炸</p>
<pre><code> 在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为$NaN$值，再也无法更新。
</code></pre></li>
<li><p>权重矩阵的退化导致模型的有效自由度减少。</p>
<p> ​    参数空间中学习的退化速度减慢，导致减少了模型的有效维数，网络的可用自由度对学习中梯度范数的贡献不均衡，随着相乘矩阵的数量（即网络深度）的增加，矩阵的乘积变得越来越退化。在有硬饱和边界的非线性网络中（例如 ReLU 网络），随着深度增加，退化过程会变得越来越快。Duvenaud等人2014年的论文里展示了关于该退化过程的可视化：</p>
</li>
</ol>
<p><img src="/img/ch3/3-9.jpg" alt=""></p>
<p>随着深度的增加，输入空间（左上角所示）会在输入空间中的每个点处被扭曲成越来越细的单丝，只有一个与细丝正交的方向影响网络的响应。沿着这个方向，网络实际上对变化变得非常敏感。</p>
<h3 id="3-1-6-深度学习和机器学习有什么不同？"><a href="#3-1-6-深度学习和机器学习有什么不同？" class="headerlink" title="3.1.6 深度学习和机器学习有什么不同？"></a>3.1.6 深度学习和机器学习有什么不同？</h3><p>​    <strong>机器学习</strong>：利用计算机、概率论、统计学等知识，输入数据，让计算机学会新知识。机器学习的过程，就是训练数据去优化目标函数。</p>
<p>​    <strong>深度学习</strong>：是一种特殊的机器学习，具有强大的能力和灵活性。它通过学习将世界表示为嵌套的层次结构，每个表示都与更简单的特征相关，而抽象的表示则用于计算更抽象的表示。</p>
<p>​    传统的机器学习需要定义一些手工特征，从而有目的的去提取目标信息， 非常依赖任务的特异性以及设计特征的专家经验。而深度学习可以从大数据中先学习简单的特征，并从其逐渐学习到更为复杂抽象的深层特征，不依赖人工的特征工程，这也是深度学习在大数据时代受欢迎的一大原因。</p>
<p><img src="/img/ch3/3.1.6.1.png" alt=""></p>
<p><img src="/img/ch3/3-11.jpg" alt=""></p>
<h2 id="3-2-网络操作与计算"><a href="#3-2-网络操作与计算" class="headerlink" title="3.2 网络操作与计算"></a>3.2 网络操作与计算</h2><h3 id="3-2-1-前向传播与反向传播？"><a href="#3-2-1-前向传播与反向传播？" class="headerlink" title="3.2.1 前向传播与反向传播？"></a>3.2.1 前向传播与反向传播？</h3><p>神经网络的计算主要有两种：前向传播（foward propagation, FP）作用于每一层的输入，通过逐层计算得到输出结果；反向传播（backward propagation, BP）作用于网络的输出，通过计算梯度由深到浅更新网络参数。</p>
<p><strong>前向传播</strong></p>
<p><img src="/img/ch3/3.2.1.1.png" alt=""></p>
<p>假设上一层结点 $ i,j,k,… $ 等一些结点与本层的结点 $ w $ 有连接，那么结点 $ w $ 的值怎么算呢？就是通过上一层的 $ i,j,k,… $ 等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如 $ReLu$，$sigmoid$ 等函数，最后得到的结果就是本层结点 $ w $ 的输出。 </p>
<p>最终不断的通过这种方法一层层的运算，得到输出层结果。</p>
<p><strong>反向传播</strong></p>
<p><img src="/img/ch3/3.2.1.2.png" alt=""></p>
<p>由于我们前向传播最终得到的结果，以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下：</p>
<p>设最终误差为 $ E $且输出层的激活函数为线性激活函数，对于输出那么 $ E $ 对于输出节点 $ y_l $ 的偏导数是 $ y_l - t_l $，其中 $ t_l $ 是真实值，$ \frac{\partial y_l}{\partial z_l} $ 是指上面提到的激活函数，$ z_l $ 是上面提到的加权和，那么这一层的 $ E $ 对于 $ z_l $ 的偏导数为 $ \frac{\partial E}{\partial z_l} = \frac{\partial E}{\partial y_l} \frac{\partial y_l}{\partial z_l} $。同理，下一层也是这么计算，只不过 $ \frac{\partial E}{\partial y_k} $ 计算方法变了，一直反向传播到输入层，最后有 $ \frac{\partial E}{\partial x_i} = \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j} $，且 $ \frac{\partial z_j}{\partial x_i} = w_i j $。然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。</p>
<h3 id="3-2-2-如何计算神经网络的输出？"><a href="#3-2-2-如何计算神经网络的输出？" class="headerlink" title="3.2.2 如何计算神经网络的输出？"></a>3.2.2 如何计算神经网络的输出？</h3><p><img src="/img/ch3/3.2.2.1.png" alt=""></p>
<p>如上图，输入层有三个节点，我们将其依次编号为 1、2、3；隐藏层的 4 个节点，编号依次为 4、5、6、7；最后输出层的两个节点编号为 8、9。比如，隐藏层的节点 4，它和输入层的三个节点 1、2、3 之间都有连接，其连接上的权重分别为是 $ w_{41}, w_{42}, w_{43} $。</p>
<p>为了计算节点 4 的输出值，我们必须先得到其所有上游节点（也就是节点 1、2、3）的输出值。节点 1、2、3 是输入层的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点 1、2、3 的输出值分别是 $ x_1, x_2, x_3 $。</p>
<script type="math/tex; mode=display">
a_4 = \sigma(w^T \cdot a) = \sigma(w_{41}x_4 + w_{42}x_2 + w_{43}a_3 + w_{4b})</script><p>其中 $ w_{4b} $ 是节点 4 的偏置项。</p>
<p>同样，我们可以继续计算出节点 5、6、7 的输出值 $ a_5, a_6, a_7 $。</p>
<p>计算输出层的节点 8 的输出值 $ y_1 $：</p>
<script type="math/tex; mode=display">
y_1 = \sigma(w^T \cdot a) = \sigma(w_{84}a_4 + w_{85}a_5 + w_{86}a_6 + w_{87}a_7 + w_{8b})</script><p>其中 $ w_{8b} $ 是节点 8 的偏置项。</p>
<p>同理，我们还可以计算出 $ y_2 $。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 $ x_1, x_2, x_3, x_4 $ 时，神经网络的输出向量 $ y_1, y_2 $ 。这里我们也看到，输出向量的维度和输出层神经元个数相同。</p>
<h3 id="3-2-3-如何计算卷积神经网络输出值？"><a href="#3-2-3-如何计算卷积神经网络输出值？" class="headerlink" title="3.2.3 如何计算卷积神经网络输出值？"></a>3.2.3 如何计算卷积神经网络输出值？</h3><p>假设有一个 5*5 的图像，使用一个 3*3 的 filter 进行卷积，想得到一个 3*3 的 Feature Map，如下所示：</p>
<p><img src="/img/ch3/3.2.3.1.png" alt=""></p>
<p>$ x_{i,j} $ 表示图像第  $ i $ 行第 $ j $ 列元素。$ w_{m,n} $ 表示 filter​ 第 $ m $ 行第 $ n $ 列权重。 $ w_b $ 表示 $filter$ 的偏置项。 表$a_i,_j$示 feature map 第 $ i$ 行第 $ j $ 列元素。 $f$ 表示激活函数，这里以$ ReLU$ 函数为例。</p>
<p>卷积计算公式如下：</p>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{i+m, j+n} + w_b )</script><p>当步长为 $1$ 时，计算 feature map 元素 $ a_{0,0} $ 如下：</p>
<script type="math/tex; mode=display">
a_{0,0} = f(\sum_{m=0}^2 \sum_{n=0}^2 w_{m,n} x_{0+m, 0+n} + w_b )

= relu(w_{0,0} x_{0,0} + w_{0,1} x_{0,1} + w_{0,2} x_{0,2} + w_{1,0} x_{1,0} + \\w_{1,1} x_{1,1} + w_{1,2} x_{1,2} + w_{2,0} x_{2,0} + w_{2,1} x_{2,1} + w_{2,2} x_{2,2}) \\

= 1 + 0 + 1 + 0 + 1 + 0 + 0 + 0 + 1 \\

= 4</script><p>其计算过程图示如下：</p>
<p><img src="/img/ch3/3.2.3.2.png" alt=""></p>
<p>以此类推，计算出全部的Feature Map。</p>
<p><img src="/img/ch3/3.2.3.4.png" alt=""></p>
<p>当步幅为 2 时，Feature Map计算如下</p>
<p><img src="/img/ch3/3.2.3.5.png" alt=""></p>
<p>注：图像大小、步幅和卷积后的Feature Map大小是有关系的。它们满足下面的关系：</p>
<script type="math/tex; mode=display">
W_2 = (W_1 - F + 2P)/S + 1\\
H_2 = (H_1 - F + 2P)/S + 1</script><p>​    其中 $ W_2 $， 是卷积后 Feature Map 的宽度；$ W_1 $ 是卷积前图像的宽度；$ F $ 是 filter 的宽度；$ P $ 是 Zero Padding 数量，Zero Padding 是指在原始图像周围补几圈 $0$，如果 $P$ 的值是 $1$，那么就补 $1$ 圈 $0$；$S$ 是步幅；$ H_2 $ 卷积后 Feature Map 的高度；$ H_1 $ 是卷积前图像的宽度。</p>
<p>​    举例：假设图像宽度 $ W_1 = 5 $，filter 宽度 $ F=3 $，Zero Padding $ P=0 $，步幅 $ S=2 $，$ Z $ 则</p>
<script type="math/tex; mode=display">
W_2 = (W_1 - F + 2P)/S + 1

= (5-3+0)/2 + 1

= 2</script><p>​    说明 Feature Map 宽度是2。同样，我们也可以计算出 Feature Map 高度也是 2。</p>
<p>如果卷积前的图像深度为 $ D $，那么相应的 filter 的深度也必须为 $ D $。深度大于 1 的卷积计算公式：</p>
<script type="math/tex; mode=display">
a_{i,j} = f(\sum_{d=0}^{D-1} \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} w_{d,m,n} x_{d,i+m,j+n} + w_b)</script><p>​    其中，$ D $ 是深度；$ F $ 是 filter 的大小；$ w_{d,m,n} $ 表示 filter 的第 $ d $ 层第 $ m $ 行第 $ n $ 列权重；$ a_{d,i,j} $ 表示 feature map 的第 $ d $ 层第 $ i $ 行第 $ j $ 列像素；其它的符号含义前面相同，不再赘述。</p>
<p>​    每个卷积层可以有多个 filter。每个 filter 和原始图像进行卷积后，都可以得到一个 Feature Map。卷积后 Feature Map 的深度(个数)和卷积层的 filter 个数相同。下面的图示显示了包含两个 filter 的卷积层的计算。$7<em>7</em>3$ 输入，经过两个 $3<em>3</em>3$ filter 的卷积(步幅为 $2$)，得到了 $3<em>3</em>2$ 的输出。图中的 Zero padding 是 $1$，也就是在输入元素的周围补了一圈 $0$。</p>
<p><img src="/img/ch3/3.2.3.6.png" alt=""></p>
<p>​    以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 $ 3 <em> 3 </em> 3 $ 的 fitler 的卷积层来说，其参数数量仅有 $ (3 <em> 3 </em> 3+1) * 2 = 56 $ 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<h3 id="3-2-4-如何计算-Pooling-层输出值输出值？"><a href="#3-2-4-如何计算-Pooling-层输出值输出值？" class="headerlink" title="3.2.4 如何计算 Pooling 层输出值输出值？"></a>3.2.4 如何计算 Pooling 层输出值输出值？</h3><p>​    Pooling 层主要的作用是下采样，通过去掉 Feature Map 中不重要的样本，进一步减少参数数量。Pooling 的方法很多，最常用的是 Max Pooling。Max Pooling 实际上就是在 n*n 的样本中取最大值，作为采样后的样本值。下图是 2*2 max pooling：</p>
<p><img src="/img/ch3/3.2.4.1.png" alt=""></p>
<p>​    除了 Max Pooing 之外，常用的还有 Average Pooling ——取各样本的平均值。<br>​    对于深度为 $ D $ 的 Feature Map，各层独立做 Pooling，因此 Pooling 后的深度仍然为 $ D $。</p>
<h3 id="3-2-5-实例理解反向传播"><a href="#3-2-5-实例理解反向传播" class="headerlink" title="3.2.5 实例理解反向传播"></a>3.2.5 实例理解反向传播</h3><p>​    一个典型的三层神经网络如下所示：</p>
<p><img src="/img/ch3/3.2.5.1.png" alt=""></p>
<p>​    其中 Layer $ L_1 $ 是输入层，Layer $ L_2 $ 是隐含层，Layer $ L_3 $ 是输出层。</p>
<p>​    假设输入数据集为 $ D={x_1, x_2, …, x_n} $，输出数据集为 $ y_1, y_2, …, y_n $。</p>
<p>​    如果输入和输出是一样，即为自编码模型。如果原始数据经过映射，会得到不同于输入的输出。</p>
<p>假设有如下的网络层：</p>
<p><img src="/img/ch3/3.2.5.2.png" alt=""></p>
<p>​    输入层包含神经元 $ i_1, i_2 $，偏置 $ b_1 $；隐含层包含神经元 $ h_1, h_2 $，偏置 $ b_2 $，输出层为  $ o_1, o_2 $，$ w_i $ 为层与层之间连接的权重，激活函数为 $sigmoid$ 函数。对以上参数取初始值，如下图所示：</p>
<p><img src="/img/ch3/3.2.5.3.png" alt=""></p>
<p>其中：</p>
<ul>
<li>输入数据 $ i1=0.05, i2 = 0.10 $</li>
<li>输出数据 $ o1=0.01, o2=0.99 $;</li>
<li>初始权重 $ w1=0.15, w2=0.20, w3=0.25,w4=0.30, w5=0.40, w6=0.45, w7=0.50, w8=0.55 $</li>
<li>目标：给出输入数据 $ i1,i2 $ ( $0.05$和$0.10$ )，使输出尽可能与原始输出 $ o1,o2 $，( $0.01$和$0.99$)接近。</li>
</ul>
<p><strong>前向传播</strong></p>
<ol>
<li>输入层 —&gt; 输出层</li>
</ol>
<p>计算神经元 $ h1 $ 的输入加权和：</p>
<script type="math/tex; mode=display">
net_{h1} = w_1 * i_1 + w_2 * i_2 + b_1 * 1\\

net_{h1} = 0.15 * 0.05 + 0.2 * 0.1 + 0.35 * 1 = 0.3775</script><p>神经元 $ h1 $ 的输出 $ o1 $ ：（此处用到激活函数为 sigmoid 函数）：</p>
<script type="math/tex; mode=display">
out_{h1} = \frac{1}{1 + e^{-net_{h1}}} = \frac{1}{1 + e^{-0.3775}} = 0.593269992</script><p>同理，可计算出神经元 $ h2 $ 的输出 $ o1 $：</p>
<script type="math/tex; mode=display">
out_{h2} = 0.596884378</script><ol>
<li>隐含层—&gt;输出层：  　　</li>
</ol>
<p>计算输出层神经元 $ o1 $ 和 $ o2 $ 的值：</p>
<script type="math/tex; mode=display">
net_{o1} = w_5 * out_{h1} + w_6 * out_{h2} + b_2 * 1</script><script type="math/tex; mode=display">
net_{o1} = 0.4 * 0.593269992 + 0.45 * 0.596884378 + 0.6 * 1 = 1.105905967</script><script type="math/tex; mode=display">
out_{o1} = \frac{1}{1 + e^{-net_{o1}}} = \frac{1}{1 + e^{1.105905967}} = 0.75136079</script><p>这样前向传播的过程就结束了，我们得到输出值为 $ [0.75136079 ,  0.772928465] $，与实际值 $ [0.01 , 0.99] $ 相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
<p><strong>反向传播 </strong></p>
<p>​    1.计算总误差</p>
<p>总误差：(这里使用Square Error)</p>
<script type="math/tex; mode=display">
E_{total} = \sum \frac{1}{2}(target - output)^2</script><p>但是有两个输出，所以分别计算 $ o1 $ 和 $ o2 $ 的误差，总误差为两者之和：</p>
<p>$E_{o1} = \frac{1}{2}(target_{o1} - out_{o1})^2<br>= \frac{1}{2}(0.01 - 0.75136507)^2 = 0.274811083$.</p>
<p>$E_{o2} = 0.023560026$.</p>
<p>$E_{total} = E_{o1} + E_{o2} = 0.274811083 + 0.023560026 = 0.298371109$.</p>
<p>​    2.隐含层 —&gt; 输出层的权值更新：</p>
<p>以权重参数 $ w5 $ 为例，如果我们想知道 $ w5 $ 对整体误差产生了多少影响，可以用整体误差对 $ w5 $ 求偏导求出：（链式法则）</p>
<script type="math/tex; mode=display">
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{o1}} * \frac{\partial out_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial w5}</script><p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
<p><img src="/img/ch3/3.2.5.4.png" alt=""></p>
<h3 id="3-2-6-神经网络更“深”有什么意义？"><a href="#3-2-6-神经网络更“深”有什么意义？" class="headerlink" title="3.2.6 神经网络更“深”有什么意义？"></a>3.2.6 神经网络更“深”有什么意义？</h3><p>前提：在一定范围内。</p>
<ul>
<li>在神经元数量相同的情况下，深层网络结构具有更大容量，分层组合带来的是指数级的表达空间，能够组合成更多不同类型的子结构，这样可以更容易地学习和表示各种特征。</li>
<li>隐藏层增加则意味着由激活函数带来的非线性变换的嵌套层数更多，就能构造更复杂的映射关系。</li>
</ul>
<h2 id="3-3-超参数"><a href="#3-3-超参数" class="headerlink" title="3.3 超参数"></a>3.3 超参数</h2><h3 id="3-3-1-什么是超参数？"><a href="#3-3-1-什么是超参数？" class="headerlink" title="3.3.1 什么是超参数？"></a>3.3.1 什么是超参数？</h3><p>​    <strong>超参数</strong> : 在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。</p>
<p>​    超参数通常存在于：</p>
<pre><code>1.  定义关于模型的更高层次的概念，如复杂性或学习能力。
2.  不能直接从标准模型培训过程中的数据中学习，需要预先定义。
3.  可以通过设置不同的值，训练不同的模型和选择更好的测试值来决定
</code></pre><p>​    超参数具体来讲比如算法中的学习率（learning rate）、梯度下降法迭代的数量（iterations）、隐藏层数目（hidden layers）、隐藏层单元数目、激活函数（ activation function）都需要根据实际情况来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。</p>
<h3 id="3-3-2-如何寻找超参数的最优值？"><a href="#3-3-2-如何寻找超参数的最优值？" class="headerlink" title="3.3.2 如何寻找超参数的最优值？"></a>3.3.2 如何寻找超参数的最优值？</h3><p>​    在使用机器学习算法时，总有一些难调的超参数。例如权重衰减大小，高斯核宽度等等。这些参数需要人为设置，设置的值对结果产生较大影响。常见设置超参数的方法有：</p>
<ol>
<li><p>猜测和检查：根据经验或直觉，选择参数，一直迭代。</p>
</li>
<li><p>网格搜索：让计算机尝试在一定范围内均匀分布的一组值。</p>
</li>
<li><p>随机搜索：让计算机随机挑选一组值。</p>
</li>
<li><p>贝叶斯优化：使用贝叶斯优化超参数，会遇到贝叶斯优化算法本身就需要很多的参数的困难。</p>
</li>
<li><p>MITIE方法，好初始猜测的前提下进行局部优化。它使用BOBYQA算法，并有一个精心选择的起始点。由于BOBYQA只寻找最近的局部最优解，所以这个方法是否成功很大程度上取决于是否有一个好的起点。在MITIE的情况下，我们知道一个好的起点，但这不是一个普遍的解决方案，因为通常你不会知道好的起点在哪里。从好的方面来说，这种方法非常适合寻找局部最优解。稍后我会再讨论这一点。</p>
</li>
<li><p>最新提出的LIPO的全局优化方法。这个方法没有参数，而且经验证比随机搜索方法好。</p>
</li>
</ol>
<h3 id="3-3-3-超参数搜索一般过程？"><a href="#3-3-3-超参数搜索一般过程？" class="headerlink" title="3.3.3 超参数搜索一般过程？"></a>3.3.3 超参数搜索一般过程？</h3><p>超参数搜索一般过程：</p>
<ol>
<li>将数据集划分成训练集、验证集及测试集。</li>
<li>在训练集上根据模型的性能指标对模型参数进行优化。</li>
<li>在验证集上根据模型的性能指标对模型的超参数进行搜索。</li>
<li>步骤 2 和步骤 3 交替迭代，最终确定模型的参数和超参数，在测试集中验证评价模型的优劣。</li>
</ol>
<p>其中，搜索过程需要搜索算法，一般有：网格搜索、随机搜过、启发式智能搜索、贝叶斯搜索。</p>
<h2 id="3-4-激活函数"><a href="#3-4-激活函数" class="headerlink" title="3.4 激活函数"></a>3.4 激活函数</h2><h3 id="3-4-1-为什么需要非线性激活函数？"><a href="#3-4-1-为什么需要非线性激活函数？" class="headerlink" title="3.4.1 为什么需要非线性激活函数？"></a>3.4.1 为什么需要非线性激活函数？</h3><p><strong>为什么需要激活函数？</strong></p>
<ol>
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
</ol>
<p><strong>为什么激活函数需要非线性函数？</strong></p>
<ol>
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
<li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>
<h3 id="3-4-2-常见的激活函数及图像"><a href="#3-4-2-常见的激活函数及图像" class="headerlink" title="3.4.2 常见的激活函数及图像"></a>3.4.2 常见的激活函数及图像</h3><ol>
<li><p>sigmoid 激活函数</p>
<p>函数的定义为：$ f(x) = \frac{1}{1 + e^{-x}} $，其值域为 $ (0,1) $。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="/img/ch3/3-26.png" alt=""></p>
<ol>
<li><p>tanh激活函数</p>
<p>函数的定义为：$ f(x) = tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $，值域为 $ (-1,1) $。</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="/img/ch3/3-27.png" alt=""></p>
<ol>
<li><p>Relu激活函数</p>
<p>函数的定义为：$ f(x) = max(0, x) $  ，值域为 $ [0,+∞) $；</p>
<p>函数图像如下：</p>
</li>
</ol>
<p><img src="/img/ch3/3-28.png" alt=""></p>
<ol>
<li><p>Leak Relu 激活函数 </p>
<p>函数定义为： $ f(x) =  \left\{<br>\begin{aligned}<br>ax, \quad x<0 \\
x, \quad x>0<br>\end{aligned}<br>\right. $，值域为 $ (-∞,+∞) $。 </p>
<p>图像如下（$ a = 0.5 $）：</p>
</li>
</ol>
<p><img src="/img/ch3/3-29.png" alt=""></p>
<ol>
<li><p>SoftPlus 激活函数</p>
<p>函数的定义为：$ f(x) = ln( 1 + e^x) $，值域为 $ (0,+∞) $。</p>
<p>函数图像如下:</p>
</li>
</ol>
<p><img src="/img/ch3/3-30.png" alt=""></p>
<ol>
<li><p>softmax 函数</p>
<p>函数定义为： $ \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} $。</p>
<p>Softmax 多用于多分类神经网络输出。</p>
</li>
</ol>
<h3 id="3-4-3-常见激活函数的导数计算？"><a href="#3-4-3-常见激活函数的导数计算？" class="headerlink" title="3.4.3 常见激活函数的导数计算？"></a>3.4.3 常见激活函数的导数计算？</h3><p>对常见激活函数，导数计算如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原函数</th>
<th>函数表达式</th>
<th>导数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sigmoid激活函数</td>
<td>$f(x)=\frac{1}{1+e^{-x}}$</td>
<td>$f^{‘}(x)=\frac{1}{1+e^{-x}}\left( 1- \frac{1}{1+e^{-x}} \right)=f(x)(1-f(x))$</td>
<td>当$x=10$,或$x=-10​$，$f^{‘}(x) \approx0​$,当$x=0​$$f^{‘}(x) =0.25​$</td>
</tr>
<tr>
<td>Tanh激活函数</td>
<td>$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td>
<td>$f^{‘}(x)=-(tanh(x))^2$</td>
<td>当$x=10$,或$x=-10$，$f^{‘}(x) \approx0$,当$x=0$$f^{`}(x) =1$</td>
</tr>
<tr>
<td>Relu激活函数</td>
<td>$f(x)=max(0,x)$</td>
<td>$c(u)=\begin{cases} 0,x<0 \\ 1,x>0 \ undefined,x=0\end{cases}$</td>
<td>通常$x=0$时，给定其导数为1和0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-4-4-激活函数有哪些性质？"><a href="#3-4-4-激活函数有哪些性质？" class="headerlink" title="3.4.4 激活函数有哪些性质？"></a>3.4.4 激活函数有哪些性质？</h3><ol>
<li>非线性： 当激活函数是线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 $ f(x)=x $，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性： 当优化方法是基于梯度的时候，就体现了该性质；</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数；</li>
<li>$ f(x)≈x $： 当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</li>
<li>输出值的范围： 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的 Learning Rate。</li>
</ol>
<h3 id="3-4-5-如何选择激活函数？"><a href="#3-4-5-如何选择激活函数？" class="headerlink" title="3.4.5 如何选择激活函数？"></a>3.4.5 如何选择激活函数？</h3><p>​    选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者测试集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>以下是常见的选择情况：</p>
<ol>
<li>如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li>
<li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li>
<li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li>
<li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li>
<li>如果遇到了一些死的神经元，我们可以使用 Leaky ReLU 函数。</li>
</ol>
<h3 id="3-4-6-使用-ReLu-激活函数的优点？"><a href="#3-4-6-使用-ReLu-激活函数的优点？" class="headerlink" title="3.4.6 使用 ReLu 激活函数的优点？"></a>3.4.6 使用 ReLu 激活函数的优点？</h3><ol>
<li>在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
<li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。</li>
<li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。</li>
</ol>
<h3 id="3-4-7-什么时候可以用线性激活函数？"><a href="#3-4-7-什么时候可以用线性激活函数？" class="headerlink" title="3.4.7 什么时候可以用线性激活函数？"></a>3.4.7 什么时候可以用线性激活函数？</h3><ol>
<li>输出层，大多使用线性激活函数。</li>
<li>在隐含层可能会使用一些线性激活函数。</li>
<li>一般用到的线性激活函数很少。</li>
</ol>
<h3 id="3-4-8-怎样理解-Relu（-lt-0-时）是非线性激活函数？"><a href="#3-4-8-怎样理解-Relu（-lt-0-时）是非线性激活函数？" class="headerlink" title="3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？"></a>3.4.8 怎样理解 Relu（&lt; 0 时）是非线性激活函数？</h3><p>Relu 激活函数图像如下：</p>
<p><img src="/img/ch3/3-32.png" alt=""></p>
<p>根据图像可看出具有如下特点：</p>
<ol>
<li><p>单侧抑制；</p>
</li>
<li><p>相对宽阔的兴奋边界；</p>
</li>
<li><p>稀疏激活性；</p>
<p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p>
<p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<p><strong>稀疏激活性</strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 $ x<0 $ 时，ReLU 硬饱和，而当 $ x>0 $ 时，则不存在饱和问题。ReLU 能够在 $ x&gt;0 $ 时保持梯度不衰减，从而缓解梯度消失问题。</p>
</li>
</ol>
<h3 id="3-4-9-Softmax-定义及作用"><a href="#3-4-9-Softmax-定义及作用" class="headerlink" title="3.4.9 Softmax 定义及作用"></a>3.4.9 Softmax 定义及作用</h3><p>Softmax 是一种形如下式的函数：</p>
<script type="math/tex; mode=display">
P(i) = \frac{exp(\theta_i^T x)}{\sum_{k=1}^{K} exp(\theta_i^T x)}</script><p>​    其中，$ \theta_i $ 和 $ x $ 是列向量，$ \theta_i^T x $ 可能被换成函数关于 $ x $ 的函数 $ f_i(x) $</p>
<p>​    通过 softmax 函数，可以使得 $ P(i) $ 的范围在 $ [0,1] $ 之间。在回归和分类问题中，通常 $ \theta $ 是待求参数，通过寻找使得 $ P(i) $ 最大的 $ \theta_i $ 作为最佳参数。</p>
<p>​    但是，使得范围在 $ [0,1] $  之间的方法有很多，为啥要在前面加上以 $ e $ 的幂函数的形式呢？参考 logistic 函数：</p>
<script type="math/tex; mode=display">
P(i) = \frac{1}{1+exp(-\theta_i^T x)}</script><p>​    这个函数的作用就是使得 $ P(i) $ 在负无穷到 0 的区间趋向于 0， 在 0 到正无穷的区间趋向 1,。同样 softmax 函数加入了 $ e $ 的幂函数正是为了两极化：正样本的结果将趋近于 1，而负样本的结果趋近于 0。这样为多类别提供了方便（可以把 $ P(i) $ 看做是样本属于类别的概率）。可以说，Softmax 函数是 logistic 函数的一种泛化。</p>
<p>​    softmax 函数可以把它的输入，通常被称为 logits 或者 logit scores，处理成 0 到 1 之间，并且能够把输出归一化到和为 1。这意味着 softmax 函数与分类的概率分布等价。它是一个网络预测多酚类问题的最佳输出激活函数。</p>
<h3 id="3-4-10-Softmax-函数如何应用于多分类？"><a href="#3-4-10-Softmax-函数如何应用于多分类？" class="headerlink" title="3.4.10 Softmax 函数如何应用于多分类？"></a>3.4.10 Softmax 函数如何应用于多分类？</h3><p>​    softmax 用于多分类过程中，它将多个神经元的输出，映射到 $ (0,1) $ 区间内，可以看成概率来理解，从而来进行多分类！</p>
<p>​    假设我们有一个数组，$ V_i $ 表示 $ V $  中的第 $ i $ 个元素，那么这个元素的 softmax 值就是</p>
<script type="math/tex; mode=display">
S_i = \frac{e^{V_i}}{\sum_j e^{V_j}}</script><p>​    从下图看，神经网络中包含了输入层，然后通过两个特征层处理，最后通过 softmax 分析器就能得到不同条件下的概率，这里需要分成三个类别，最终会得到 $ y=0, y=1, y=2 $ 的概率值。</p>
<p><img src="/img/ch3/3.4.9.1.png" alt=""></p>
<p>继续看下面的图，三个输入通过 softmax 后得到一个数组 $ [0.05 , 0.10 , 0.85] $，这就是 soft 的功能。</p>
<p><img src="/img/ch3/3.4.9.2.png" alt=""></p>
<p>更形象的映射过程如下图所示：</p>
<p><img src="/img/ch3/3.4.9.3.png" alt="****"></p>
<p>​    softmax 直白来说就是将原来输出是 $ 3,1,-3 $ 通过 softmax 函数一作用，就映射成为 $ (0,1) $ 的值，而这些值的累和为 $ 1 $（满足概率的性质），那么我们就可以将它理解成概率，在最后选取输出结点的时候，我们就可以选取概率最大（也就是值对应最大的）结点，作为我们的预测目标！</p>
<h3 id="3-4-11-交叉熵代价函数定义及其求导推导"><a href="#3-4-11-交叉熵代价函数定义及其求导推导" class="headerlink" title="3.4.11 交叉熵代价函数定义及其求导推导"></a>3.4.11 交叉熵代价函数定义及其求导推导</h3><p>(<strong>贡献者：黄钦建－华南理工大学</strong>)</p>
<p>​    神经元的输出就是 a = σ(z)，其中$z=\sum w_{j}i_{j}+b$是输⼊的带权和。</p>
<p>$C=-\frac{1}{n}\sum[ylna+(1-y)ln(1-a)]$</p>
<p>​    其中 n 是训练数据的总数，求和是在所有的训练输⼊ x 上进⾏的， y 是对应的⽬标输出。</p>
<p>​    表达式是否解决学习缓慢的问题并不明显。实际上，甚⾄将这个定义看做是代价函数也不是显⽽易⻅的！在解决学习缓慢前，我们来看看交叉熵为何能够解释成⼀个代价函数。</p>
<p>​    将交叉熵看做是代价函数有两点原因。</p>
<p>​    第⼀，它是⾮负的， C &gt; 0。可以看出：式子中的求和中的所有独⽴的项都是负数的，因为对数函数的定义域是 (0，1)，并且求和前⾯有⼀个负号，所以结果是非负。</p>
<p>​    第⼆，如果对于所有的训练输⼊ x，神经元实际的输出接近⽬标值，那么交叉熵将接近 0。</p>
<p>​    假设在这个例⼦中， y = 0 ⽽ a ≈ 0。这是我们想到得到的结果。我们看到公式中第⼀个项就消去了，因为 y = 0，⽽第⼆项实际上就是 − ln(1 − a) ≈ 0。反之， y = 1 ⽽ a ≈ 1。所以在实际输出和⽬标输出之间的差距越⼩，最终的交叉熵的值就越低了。（这里假设输出结果不是0，就是1，实际分类也是这样的）</p>
<p>​    综上所述，交叉熵是⾮负的，在神经元达到很好的正确率的时候会接近 0。这些其实就是我们想要的代价函数的特性。其实这些特性也是⼆次代价函数具备的。所以，交叉熵就是很好的选择了。但是交叉熵代价函数有⼀个⽐⼆次代价函数更好的特性就是它避免了学习速度下降的问题。为了弄清楚这个情况，我们来算算交叉熵函数关于权重的偏导数。我们将$a={\varsigma}(z)$代⼊到 公式中应⽤两次链式法则，得到：</p>
<p>$\begin{eqnarray}\frac{\partial C}{\partial w_{j}}&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial w_{j}}[ylna+(1-y)ln(1-a)]\\&amp;=&amp;-\frac{1}{n}\sum \frac{\partial }{\partial a}[ylna+(1-y)ln(1-a)]<em>\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{a}-\frac{1-y}{1-a})</em>\frac{\partial a}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)})\frac{\partial \varsigma(z)}{\partial w_{j}}\\&amp;=&amp;-\frac{1}{n}\sum (\frac{y}{\varsigma(z)}-\frac{1-y}{1-\varsigma(z)}){\varsigma}’(z)x_{j}\end{eqnarray}$</p>
<p>​    根据$\varsigma(z)=\frac{1}{1+e^{-z}}$ 的定义，和⼀些运算，我们可以得到 ${\varsigma}’(z)=\varsigma(z)(1-\varsigma(z))$。化简后可得：</p>
<p>$\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum x_{j}({\varsigma}(z)-y)$</p>
<p>​    这是⼀个优美的公式。它告诉我们权重学习的速度受到$\varsigma(z)-y$，也就是输出中的误差的控制。更⼤的误差，更快的学习速度。这是我们直觉上期待的结果。特别地，这个代价函数还避免了像在⼆次代价函数中类似⽅程中${\varsigma}’(z)$导致的学习缓慢。当我们使⽤交叉熵的时候，${\varsigma}’(z)$被约掉了，所以我们不再需要关⼼它是不是变得很⼩。这种约除就是交叉熵带来的特效。实际上，这也并不是⾮常奇迹的事情。我们在后⾯可以看到，交叉熵其实只是满⾜这种特性的⼀种选择罢了。</p>
<p>​    根据类似的⽅法，我们可以计算出关于偏置的偏导数。我这⾥不再给出详细的过程，你可以轻易验证得到：</p>
<p>$\frac{\partial C}{\partial b}=\frac{1}{n}\sum ({\varsigma}(z)-y)$</p>
<p>​    再⼀次, 这避免了⼆次代价函数中类似${\varsigma}’(z)$项导致的学习缓慢。</p>
<h3 id="3-4-12-为什么Tanh收敛速度比Sigmoid快？"><a href="#3-4-12-为什么Tanh收敛速度比Sigmoid快？" class="headerlink" title="3.4.12 为什么Tanh收敛速度比Sigmoid快？"></a>3.4.12 为什么Tanh收敛速度比Sigmoid快？</h3><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>首先看如下两个函数的求导：</p>
<p>$tanh^{,}(x)=1-tanh(x)^{2}\in (0,1)$</p>
<p>$s^{,}(x)=s(x)*(1-s(x))\in (0,\frac{1}{4}]$</p>
<p>由上面两个公式可知tanh(x)梯度消失的问题比sigmoid轻，所以Tanh收敛速度比Sigmoid快。</p>
<p>3.4.13</p>
<h3 id="3-4-12-内聚外斥-Center-Loss"><a href="#3-4-12-内聚外斥-Center-Loss" class="headerlink" title="3.4.12 内聚外斥 - Center Loss"></a>3.4.12 内聚外斥 - Center Loss</h3><p><strong>（贡献者：李世轩－加州大学伯克利分校）</strong></p>
<p>在计算机视觉任务中, 由于其简易性, 良好的表现, 与对分类任务的概率性理解, Cross Entropy Loss (交叉熵代价) + Softmax 组合被广泛应用于以分类任务为代表的任务中. 在此应用下, 我们可将其学习过程进一步理解为: 更相似(同类/同物体)的图像在特征域中拥有“更近的距离”, 相反则”距离更远“. 换而言之, 我们可以进一步理解为其学习了一种低类内距离(Intra-class Distance)与高类间距离(Inter-class Distance)的特征判别模型. 在此Center Loss则可以高效的计算出这种具判别性的特征. 不同于传统的Softmax Loss, Center Loss通过学习“特征中心”从而最小化其类内距离. 其表达形式如下:</p>
<p>$L_{C} = \frac{1}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}$</p>
<p>其中$x_{i}$表示FCN(全连接层)之前的特征, $c_{y_{i}}$表示第$y_{i} $个类别的特征中心, $m$表示mini-batch的大小. 我们很清楚的看到$L_{C}$的终极目标为最小化每个特征与其特征中心的方差, 即最小化类内距离. 其迭代公式为:</p>
<p>$\frac{\partial L_{C}}{\partial x_{i}}=x_{i}-c_{y_{i}}$</p>
<p>$\Delta{c_{j}} = \frac{\sum^{m}_{i=1}\delta(y_{i}=j)\cdot(c_{j}-x_{i})}{1+\sum^{m}_{i=1}\delta(y_{i}=j)}$</p>
<p>其中$ \delta(condition)=\left\{<br>\begin{array}{rcl}<br>1       &amp;      &amp; {condition is True}\\<br>0     &amp;      &amp; {otherwise}\ \end{array} \right.$</p>
<p>结合Softmax, 我们可以搭配二者使用, 适当平衡这两种监督信号. 在Softmax拉开类间距离的同时, 利用Center Loss最小化类内距离. 例如:</p>
<p>$\begin{eqnarray}L &amp; = &amp; L_{S} + \lambda L_{C} \ &amp;=&amp; -\sum^{m}_{i=1}log\frac{e^{W_{y}^{T}x_{i}+b_{y_{i}}}}{\sum^{m}_{i=1}e^{W^{T}_{j}x_{i}+b_{j}}} + \frac{\lambda}{2}\sum^{m}_{i=1}||x_{i}-c_{y_{i}}||^{2}_{2}\ \end{eqnarray}$</p>
<p>即便如此, Center Loss仍有它的不足之处: 其特征中心为存储在网络模型之外的额外参数, 不能与模型参数一同优化. 这些额外参数将与记录每一步特征变化的自动回归均值估计(autoregressive mean estimator)进行更迭. 当需要学习的类别数量较大时, mini-batch可能无力提供足够的样本进行均值估计. 若此Center Loss将需要平衡两种监督损失来以确定更迭, 其过程需要一个对平衡超参数的搜索过程, 使得其择值消耗昂贵.</p>
<h2 id="3-5-Batch-Size"><a href="#3-5-Batch-Size" class="headerlink" title="3.5 Batch_Size"></a>3.5 Batch_Size</h2><h3 id="3-5-1-为什么需要-Batch-Size？"><a href="#3-5-1-为什么需要-Batch-Size？" class="headerlink" title="3.5.1 为什么需要 Batch_Size？"></a>3.5.1 为什么需要 Batch_Size？</h3><p>Batch的选择，首先决定的是下降的方向。</p>
<p>如果数据集比较小，可采用全数据集的形式，好处是：</p>
<ol>
<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。</li>
<li>由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。</li>
</ol>
<p>对于更大的数据集，假如采用全数据集的形式，坏处是：</p>
<ol>
<li>随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。</li>
<li>以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。 </li>
</ol>
<h3 id="3-5-2-Batch-Size-值的选择"><a href="#3-5-2-Batch-Size-值的选择" class="headerlink" title="3.5.2 Batch_Size 值的选择"></a>3.5.2 Batch_Size 值的选择</h3><p>​    假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。</p>
<p>​    既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？</p>
<p>​    此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。</p>
<h3 id="3-5-3-在合理范围内，增大Batch-Size有何好处？"><a href="#3-5-3-在合理范围内，增大Batch-Size有何好处？" class="headerlink" title="3.5.3 在合理范围内，增大Batch_Size有何好处？"></a>3.5.3 在合理范围内，增大Batch_Size有何好处？</h3><ol>
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。</li>
</ol>
<h3 id="3-5-4-盲目增大-Batch-Size-有何坏处？"><a href="#3-5-4-盲目增大-Batch-Size-有何坏处？" class="headerlink" title="3.5.4 盲目增大 Batch_Size 有何坏处？"></a>3.5.4 盲目增大 Batch_Size 有何坏处？</h3><ol>
<li>内存利用率提高了，但是内存容量可能撑不住了。</li>
<li>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ol>
<h3 id="3-5-5-调节-Batch-Size-对训练效果影响到底如何？"><a href="#3-5-5-调节-Batch-Size-对训练效果影响到底如何？" class="headerlink" title="3.5.5 调节 Batch_Size 对训练效果影响到底如何？"></a>3.5.5 调节 Batch_Size 对训练效果影响到底如何？</h3><ol>
<li>Batch_Size 太小，模型表现效果极其糟糕(error飙升)。</li>
<li>随着 Batch_Size 增大，处理相同数据量的速度越快。</li>
<li>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</li>
<li>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。</li>
<li>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。 </li>
</ol>
<h2 id="3-6-归一化"><a href="#3-6-归一化" class="headerlink" title="3.6 归一化"></a>3.6 归一化</h2><h3 id="3-6-1-归一化含义？"><a href="#3-6-1-归一化含义？" class="headerlink" title="3.6.1 归一化含义？"></a>3.6.1 归一化含义？</h3><ol>
<li><p>归纳统一样本的统计分布性。归一化在 $ 0-1$ 之间是统计的概率分布，归一化在$ -1—+1$ 之间是统计的坐标分布。</p>
</li>
<li><p>无论是为了建模还是为了计算，首先基本度量单位要同一，神经网络是以样本在事件中的统计分别几率来进行训练（概率计算）和预测，且 sigmoid 函数的取值是 0 到 1 之间的，网络最后一个节点的输出也是如此，所以经常要对样本的输出归一化处理。</p>
</li>
<li><p>归一化是统一在 $ 0-1 $ 之间的统计概率分布，当所有样本的输入信号都为正值时，与第一隐含层神经元相连的权值只能同时增加或减小，从而导致学习速度很慢。</p>
</li>
<li><p>另外在数据中常存在奇异样本数据，奇异样本数据存在所引起的网络训练时间增加，并可能引起网络无法收敛。为了避免出现这种情况及后面数据处理的方便，加快网络学习速度，可以对输入信号进行归一化，使得所有样本的输入信号其均值接近于 0 或与其均方差相比很小。</p>
</li>
</ol>
<h3 id="3-6-2-为什么要归一化？"><a href="#3-6-2-为什么要归一化？" class="headerlink" title="3.6.2 为什么要归一化？"></a>3.6.2 为什么要归一化？</h3><ol>
<li>为了后面数据处理的方便，归一化的确可以避免一些不必要的数值问题。</li>
<li>为了程序运行时收敛加快。 </li>
<li>同一量纲。样本数据的评价标准不一样，需要对其量纲化，统一评价标准。这算是应用层面的需求。</li>
<li>避免神经元饱和。啥意思？就是当神经元的激活在接近 0 或者 1 时会饱和，在这些区域，梯度几乎为 0，这样，在反向传播过程中，局部梯度就会接近 0，这会有效地“杀死”梯度。</li>
<li>保证输出数据中数值小的不被吞食。 </li>
</ol>
<h3 id="3-6-3-为什么归一化能提高求解最优解速度？"><a href="#3-6-3-为什么归一化能提高求解最优解速度？" class="headerlink" title="3.6.3 为什么归一化能提高求解最优解速度？"></a>3.6.3 为什么归一化能提高求解最优解速度？</h3><p><img src="/img/ch3/3.6.3.1.png" alt=""></p>
<p>​    上图是代表数据是否均一化的最优解寻解过程（圆圈可以理解为等高线）。左图表示未经归一化操作的寻解过程，右图表示经过归一化后的寻解过程。</p>
<p>​    当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛；而右图对两个原始特征进行了归一化，其对应的等高线显得很圆，在梯度下降进行求解时能较快的收敛。</p>
<p>​    因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。</p>
<h3 id="3-6-4-3D-图解未归一化"><a href="#3-6-4-3D-图解未归一化" class="headerlink" title="3.6.4 3D 图解未归一化"></a>3.6.4 3D 图解未归一化</h3><p>例子：</p>
<p>​    假设 $ w1 $ 的范围在 $ [-10, 10] $，而 $ w2 $ 的范围在 $ [-100, 100] $，梯度每次都前进 1 单位，那么在 $ w1 $ 方向上每次相当于前进了 $ 1/20 $，而在 $ w2 $ 上只相当于 $ 1/200 $！某种意义上来说，在 $ w2 $ 上前进的步长更小一些,而 $ w1 $ 在搜索过程中会比 $ w2 $ “走”得更快。</p>
<p>​    这样会导致，在搜索过程中更偏向于 $ w1 $ 的方向。走出了“L”形状，或者成为“之”字形。</p>
<p><img src="/img/ch3/3-37.png" alt=""></p>
<h3 id="3-6-5-归一化有哪些类型？"><a href="#3-6-5-归一化有哪些类型？" class="headerlink" title="3.6.5 归一化有哪些类型？"></a>3.6.5 归一化有哪些类型？</h3><ol>
<li>线性归一化</li>
</ol>
<script type="math/tex; mode=display">
x^{\prime} = \frac{x-min(x)}{max(x) - min(x)}</script><p>​    适用范围：比较适用在数值比较集中的情况。</p>
<p>​    缺点：如果 max 和 min 不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。</p>
<ol>
<li>标准差标准化</li>
</ol>
<script type="math/tex; mode=display">
x^{\prime} = \frac{x-\mu}{\sigma}</script><p>​    含义：经过处理的数据符合标准正态分布，即均值为 0，标准差为 1 其中 $ \mu $ 为所有样本数据的均值，$ \sigma $ 为所有样本数据的标准差。</p>
<ol>
<li><p>非线性归一化</p>
<p>适用范围：经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。该方法包括 $ log $、指数，正切等。</p>
</li>
</ol>
<h3 id="3-6-6-局部响应归一化作用"><a href="#3-6-6-局部响应归一化作用" class="headerlink" title="3.6.6 局部响应归一化作用"></a>3.6.6 局部响应归一化作用</h3><p>​    LRN 是一种提高深度学习准确度的技术方法。LRN 一般是在激活、池化函数后的一种方法。</p>
<p>​    在 ALexNet 中，提出了 LRN 层，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。</p>
<h3 id="3-6-7-理解局部响应归一化"><a href="#3-6-7-理解局部响应归一化" class="headerlink" title="3.6.7 理解局部响应归一化"></a>3.6.7 理解局部响应归一化</h3><p>​    局部响应归一化原理是仿造生物学上活跃的神经元对相邻神经元的抑制现象（侧抑制），其公式如下：</p>
<script type="math/tex; mode=display">
b_{x,y}^i = a_{x,y}^i / (k + \alpha \sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^j)^2 )^\beta</script><p>其中，<br>1) $ a $：表示卷积层（包括卷积操作和池化操作）后的输出结果，是一个四维数组[batch,height,width,channel]。</p>
<ul>
<li>batch：批次数(每一批为一张图片)。</li>
<li>height：图片高度。</li>
<li>width：图片宽度。</li>
<li>channel：通道数。可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数，或理解为处理后的图片深度。</li>
</ul>
<p>2) $ a_{x,y}^i $ 表示在这个输出结构中的一个位置 $ [a,b,c,d] $，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第 $ a $ 张图的第 $ d $ 个通道下的高度为b宽度为c的点。</p>
<p>3) $ N $：论文公式中的 $ N $ 表示通道数 (channel)。</p>
<p>4) $ a $，$ n/2 $， $ k $ 分别表示函数中的 input,depth_radius,bias。参数 $ k, n, \alpha, \beta $ 都是超参数，一般设置 $ k=2, n=5, \alpha=1*e-4, \beta=0.75 $</p>
<p>5) $ \sum $：$ \sum $ 叠加的方向是沿着通道方向的，即每个点值的平方和是沿着 $ a $ 中的第 3 维 channel 方向的，也就是一个点同方向的前面 $ n/2 $ 个通道（最小为第 $ 0 $ 个通道）和后 $ n/2 $ 个通道（最大为第 $ d-1 $ 个通道）的点的平方和(共 $ n+1 $ 个点)。而函数的英文注解中也说明了把 input 当成是 $ d $ 个 3 维的矩阵，说白了就是把 input 的通道数当作 3 维矩阵的个数，叠加的方向也是在通道方向。 </p>
<p>简单的示意图如下：</p>
<p><img src="/img/ch3/3.6.7.1.png" alt=""></p>
<h3 id="3-6-8-什么是批归一化（Batch-Normalization）"><a href="#3-6-8-什么是批归一化（Batch-Normalization）" class="headerlink" title="3.6.8 什么是批归一化（Batch Normalization）"></a>3.6.8 什么是批归一化（Batch Normalization）</h3><p>​    以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ \sigma(WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。 </p>
<p>​    这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。</p>
<h3 id="3-6-9-批归一化（BN）算法的优点"><a href="#3-6-9-批归一化（BN）算法的优点" class="headerlink" title="3.6.9 批归一化（BN）算法的优点"></a>3.6.9 批归一化（BN）算法的优点</h3><p>下面我们来说一下BN算法的优点： </p>
<ol>
<li>减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数； </li>
<li>减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛； </li>
<li>可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在) </li>
<li>破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。 </li>
<li>减少梯度消失，加快收敛速度，提高训练精度。</li>
</ol>
<h3 id="3-6-10-批归一化（BN）算法流程"><a href="#3-6-10-批归一化（BN）算法流程" class="headerlink" title="3.6.10 批归一化（BN）算法流程"></a>3.6.10 批归一化（BN）算法流程</h3><p>下面给出 BN 算法在训练时的过程</p>
<p>输入：上一层输出结果 $ X = {x_1, x_2, …, x_m} $，学习参数 $ \gamma, \beta $</p>
<p>算法流程：</p>
<ol>
<li>计算上一层输出数据的均值</li>
</ol>
<script type="math/tex; mode=display">
\mu_{\beta} = \frac{1}{m} \sum_{i=1}^m(x_i)</script><p>其中，$ m $ 是此次训练样本 batch 的大小。</p>
<ol>
<li>计算上一层输出数据的标准差</li>
</ol>
<script type="math/tex; mode=display">
\sigma_{\beta}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\beta})^2</script><ol>
<li>归一化处理，得到</li>
</ol>
<script type="math/tex; mode=display">
\hat x_i = \frac{x_i + \mu_{\beta}}{\sqrt{\sigma_{\beta}^2} + \epsilon}</script><p>其中 $ \epsilon $ 是为了避免分母为 0 而加进去的接近于 0 的很小值</p>
<ol>
<li>重构，对经过上面归一化处理得到的数据进行重构，得到</li>
</ol>
<script type="math/tex; mode=display">
y_i = \gamma \hat x_i + \beta</script><p>其中，$ \gamma, \beta $ 为可学习参数。</p>
<p>注：上述是 BN 训练时的过程，但是当在投入使用时，往往只是输入一个样本，没有所谓的均值 $ \mu_{\beta} $ 和标准差 $ \sigma_{\beta}^2 $。此时，均值 $ \mu_{\beta} $ 是计算所有 batch $ \mu_{\beta} $ 值的平均值得到，标准差 $ \sigma_{\beta}^2 $ 采用每个batch $ \sigma_{\beta}^2 $  的无偏估计得到。</p>
<h3 id="3-6-11-批归一化和群组归一化比较"><a href="#3-6-11-批归一化和群组归一化比较" class="headerlink" title="3.6.11 批归一化和群组归一化比较"></a>3.6.11 批归一化和群组归一化比较</h3><div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>批量归一化（Batch Normalization，以下简称 BN）</td>
<td style="text-align:left">可让各种网络并行训练。但是，批量维度进行归一化会带来一些问题——批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的 BN。</td>
</tr>
<tr>
<td>群组归一化 Group Normalization (简称 GN)</td>
<td style="text-align:left">GN 将通道分成组，并在每组内计算归一化的均值和方差。GN 的计算与批量大小无关，并且其准确度在各种批量大小下都很稳定。</td>
</tr>
<tr>
<td>比较</td>
<td style="text-align:left">在 ImageNet 上训练的 ResNet-50上，GN 使用批量大小为 2 时的错误率比 BN 的错误率低 10.6％ ;当使用典型的批量时，GN 与 BN 相当，并且优于其他标归一化变体。而且，GN 可以自然地从预训练迁移到微调。在进行 COCO 中的目标检测和分割以及 Kinetics 中的视频分类比赛中，GN 可以胜过其竞争对手，表明 GN 可以在各种任务中有效地取代强大的 BN。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-6-12-Weight-Normalization和Batch-Normalization比较"><a href="#3-6-12-Weight-Normalization和Batch-Normalization比较" class="headerlink" title="3.6.12 Weight Normalization和Batch Normalization比较"></a>3.6.12 Weight Normalization和Batch Normalization比较</h3><p>​    Weight Normalization 和 Batch Normalization 都属于参数重写（Reparameterization）的方法，只是采用的方式不同。</p>
<p>​    Weight Normalization 是对网络权值$  W $ 进行 normalization，因此也称为 Weight Normalization；</p>
<p>​    Batch Normalization 是对网络某一层输入数据进行 normalization。</p>
<p>​    Weight Normalization相比Batch Normalization有以下三点优势：</p>
<ol>
<li><p>Weight Normalization 通过重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入 minbatch 的依赖，适用于 RNN（LSTM）网络（Batch Normalization 不能直接用于RNN，进行 normalization 操作，原因在于：1) RNN 处理的 Sequence 是变长的；2) RNN 是基于 time step 计算，如果直接使用 Batch Normalization 处理，需要保存每个 time step 下，mini btach 的均值和方差，效率低且占内存）。</p>
</li>
<li><p>Batch Normalization 基于一个 mini batch 的数据计算均值和方差，而不是基于整个 Training set 来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization 不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。相反，Weight Normalization 对通过标量 $ g $ 和向量 $ v $ 对权重 $ W $ 进行重写，重写向量 $ v $ 是固定的，因此，基于 Weight Normalization 的 Normalization 可以看做比 Batch Normalization 引入更少的噪声。    </p>
</li>
<li><p>不需要额外的存储空间来保存 mini batch 的均值和方差，同时实现 Weight Normalization 时，对深度学习网络进行正向信号传播和反向梯度计算带来的额外计算开销也很小。因此，要比采用 Batch Normalization 进行 normalization 操作时，速度快。  但是 Weight Normalization 不具备 Batch Normalization 把网络每一层的输出 Y 固定在一个变化范围的作用。因此，采用 Weight Normalization 进行 Normalization 时需要特别注意参数初始值的选择。</p>
</li>
</ol>
<h3 id="3-6-13-Batch-Normalization在什么时候用比较合适？"><a href="#3-6-13-Batch-Normalization在什么时候用比较合适？" class="headerlink" title="3.6.13 Batch Normalization在什么时候用比较合适？"></a>3.6.13 Batch Normalization在什么时候用比较合适？</h3><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​    在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>
<p>​    BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。</p>
<h2 id="3-7-预训练与微调-fine-tuning"><a href="#3-7-预训练与微调-fine-tuning" class="headerlink" title="3.7 预训练与微调(fine tuning)"></a>3.7 预训练与微调(fine tuning)</h2><h3 id="3-7-1-为什么无监督预训练可以帮助深度学习？"><a href="#3-7-1-为什么无监督预训练可以帮助深度学习？" class="headerlink" title="3.7.1 为什么无监督预训练可以帮助深度学习？"></a>3.7.1 为什么无监督预训练可以帮助深度学习？</h3><p>深度网络存在问题:</p>
<ol>
<li><p>网络越深，需要的训练样本数越多。若用监督则需大量标注样本，不然小规模样本容易造成过拟合。深层网络特征比较多，会出现的多特征问题主要有多样本问题、规则化问题、特征选择问题。</p>
</li>
<li><p>多层神经网络参数优化是个高阶非凸优化问题，经常得到收敛较差的局部解；</p>
</li>
<li><p>梯度扩散问题，BP算法计算出的梯度随着深度向前而显著下降，导致前面网络参数贡献很小，更新速度慢。</p>
</li>
</ol>
<p><strong>解决方法：</strong></p>
<p>​    逐层贪婪训练，无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。</p>
<p>经过预训练最终能得到比较好的局部最优解。</p>
<h3 id="3-7-2-什么是模型微调fine-tuning"><a href="#3-7-2-什么是模型微调fine-tuning" class="headerlink" title="3.7.2 什么是模型微调fine tuning"></a>3.7.2 什么是模型微调fine tuning</h3><p>​    用别人的参数、修改后的网络和自己的数据进行训练，使得参数适应自己的数据，这样一个过程，通常称之为微调（fine tuning). </p>
<p><strong>模型的微调举例说明：</strong></p>
<p>​    我们知道，CNN 在图像识别这一领域取得了巨大的进步。如果想将 CNN 应用到我们自己的数据集上，这时通常就会面临一个问题：通常我们的 dataset 都不会特别大，一般不会超过 1 万张，甚至更少，每一类图片只有几十或者十几张。这时候，直接应用这些数据训练一个网络的想法就不可行了，因为深度学习成功的一个关键性因素就是大量带标签数据组成的训练集。如果只利用手头上这点数据，即使我们利用非常好的网络结构，也达不到很高的 performance。这时候，fine-tuning 的思想就可以很好解决我们的问题：我们通过对 ImageNet 上训练出来的模型（如CaffeNet,VGGNet,ResNet) 进行微调，然后应用到我们自己的数据集上。</p>
<h3 id="3-7-3-微调时候网络参数是否更新？"><a href="#3-7-3-微调时候网络参数是否更新？" class="headerlink" title="3.7.3 微调时候网络参数是否更新？"></a>3.7.3 微调时候网络参数是否更新？</h3><p>答案：会更新。</p>
<ol>
<li>finetune 的过程相当于继续训练，跟直接训练的区别是初始化的时候。 </li>
<li>直接训练是按照网络定义指定的方式初始化。</li>
<li>finetune是用你已经有的参数文件来初始化。</li>
</ol>
<h3 id="3-7-4-fine-tuning-模型的三种状态"><a href="#3-7-4-fine-tuning-模型的三种状态" class="headerlink" title="3.7.4 fine-tuning 模型的三种状态"></a>3.7.4 fine-tuning 模型的三种状态</h3><ol>
<li><p>状态一：只预测，不训练。<br>特点：相对快、简单，针对那些已经训练好，现在要实际对未知数据进行标注的项目，非常高效；</p>
</li>
<li><p>状态二：训练，但只训练最后分类层。<br>特点：fine-tuning的模型最终的分类以及符合要求，现在只是在他们的基础上进行类别降维。</p>
</li>
<li><p>状态三：完全训练，分类层+之前卷积层都训练<br>特点：跟状态二的差异很小，当然状态三比较耗时和需要训练GPU资源，不过非常适合fine-tuning到自己想要的模型里面，预测精度相比状态二也提高不少。</p>
</li>
</ol>
<h2 id="3-8-权重偏差初始化"><a href="#3-8-权重偏差初始化" class="headerlink" title="3.8 权重偏差初始化"></a>3.8 权重偏差初始化</h2><h3 id="3-8-1-全都初始化为-0"><a href="#3-8-1-全都初始化为-0" class="headerlink" title="3.8.1 全都初始化为 0"></a>3.8.1 全都初始化为 0</h3><p><strong>偏差初始化陷阱</strong>： 都初始化为 0。</p>
<p><strong>产生陷阱原因</strong>：因为并不知道在训练神经网络中每一个权重最后的值，但是如果进行了恰当的数据归一化后，我们可以有理由认为有一半的权重是正的，另一半是负的。令所有权重都初始化为 0，如果神经网络计算出来的输出值是一样的，神经网络在进行反向传播算法计算出来的梯度值也一样，并且参数更新值也一样。更一般地说，如果权重初始化为同一个值，网络就是对称的。</p>
<p><strong>形象化理解</strong>：在神经网络中考虑梯度下降的时候，设想你在爬山，但身处直线形的山谷中，两边是对称的山峰。由于对称性，你所在之处的梯度只能沿着山谷的方向，不会指向山峰；你走了一步之后，情况依然不变。结果就是你只能收敛到山谷中的一个极大值，而走不到山峰上去。</p>
<h3 id="3-8-2-全都初始化为同样的值"><a href="#3-8-2-全都初始化为同样的值" class="headerlink" title="3.8.2 全都初始化为同样的值"></a>3.8.2 全都初始化为同样的值</h3><p>​    偏差初始化陷阱： 都初始化为一样的值。<br>​    以一个三层网络为例：<br>首先看下结构</p>
<p><img src="/img/ch3/3.8.2.1.png" alt=""></p>
<p>它的表达式为： </p>
<script type="math/tex; mode=display">
a_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})</script><script type="math/tex; mode=display">
a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})</script><script type="math/tex; mode=display">
a_3^{(2)} = f(W_{31}^{(1)} x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})</script><script type="math/tex; mode=display">
h_{W,b}(x) = a_1^{(3)} = f(W_{11}^{(2)} a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)})</script><script type="math/tex; mode=display">
xa_1^{(2)} = f(W_{11}^{(1)} x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})a_2^{(2)} = f(W_{21}^{(1)} x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 +</script><p>如果每个权重都一样，那么在多层网络中，从第二层开始，每一层的输入值都是相同的了也就是$ a1=a2=a3=…. $，既然都一样，就相当于一个输入了，为啥呢？？</p>
<p>如果是反向传递算法（如果这里不明白请看上面的连接），其中的偏置项和权重项的迭代的偏导数计算公式如下</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b;x,y) = a_j^{(l)} \delta_i^{(l+1)}

\frac{\partial}{\partial b_{i}^{(l)}} J(W,b;x,y) = \delta_i^{(l+1)}</script><p>$ \delta $ 的计算公式</p>
<script type="math/tex; mode=display">
\delta_i^{(l)} = (\sum_{j=1}^{s_{t+1}} W_{ji}^{(l)} \delta_j^{(l+1)} ) f^{\prime}(z_i^{(l)})</script><p>如果用的是 sigmoid 函数</p>
<script type="math/tex; mode=display">
f^{\prime}(z_i^{(l)}) = a_i^{(l)}(1-a_i^{(l)})</script><p>把后两个公式代入，可以看出所得到的梯度下降法的偏导相同，不停的迭代，不停的相同，不停的迭代，不停的相同……，最后就得到了相同的值（权重和截距）。</p>
<h3 id="3-8-3-初始化为小的随机数"><a href="#3-8-3-初始化为小的随机数" class="headerlink" title="3.8.3 初始化为小的随机数"></a>3.8.3 初始化为小的随机数</h3><p>​    将权重初始化为很小的数字是一个普遍的打破网络对称性的解决办法。这个想法是，神经元在一开始都是随机的、独一无二的，所以它们会计算出不同的更新，并将自己整合到整个网络的各个部分。一个权重矩阵的实现可能看起来像 $ W=0.01∗np.random.randn(D,H) $，其中 randn 是从均值为 0 的单位标准高斯分布进行取样。通过这个公式(函数)，每个神经元的权重向量初始化为一个从多维高斯分布取样的随机向量，所以神经元在输入空间中指向随机的方向(so the neurons point in random direction in the input space). 应该是指输入空间对于随机方向有影响)。其实也可以从均匀分布中来随机选取小数，但是在实际操作中看起来似乎对最后的表现并没有太大的影响。</p>
<p>​    备注：并不是数字越小就会表现的越好。比如，如果一个神经网络层的权重非常小，那么在反向传播算法就会计算出很小的梯度(因为梯度 gradient 是与权重成正比的)。在网络不断的反向传播过程中将极大地减少“梯度信号”，并可能成为深层网络的一个需要注意的问题。</p>
<h3 id="3-8-4-用-1-sqrt-n-校准方差"><a href="#3-8-4-用-1-sqrt-n-校准方差" class="headerlink" title="3.8.4 用 $ 1/\sqrt n $ 校准方差"></a>3.8.4 用 $ 1/\sqrt n $ 校准方差</h3><p>​    上述建议的一个问题是，随机初始化神经元的输出的分布有一个随输入量增加而变化的方差。结果证明，我们可以通过将其权重向量按其输入的平方根(即输入的数量)进行缩放，从而将每个神经元的输出的方差标准化到 1。也就是说推荐的启发式方法 (heuristic) 是将每个神经元的权重向量按下面的方法进行初始化: $ w=np.random.randn(n)/\sqrt n $，其中 n 表示输入的数量。这保证了网络中所有的神经元最初的输出分布大致相同，并在经验上提高了收敛速度。</p>
<h3 id="3-8-5-稀疏初始化-Sparse-Initialazation"><a href="#3-8-5-稀疏初始化-Sparse-Initialazation" class="headerlink" title="3.8.5 稀疏初始化(Sparse Initialazation)"></a>3.8.5 稀疏初始化(Sparse Initialazation)</h3><p>​    另一种解决未校准方差问题的方法是把所有的权重矩阵都设为零，但是为了打破对称性，每个神经元都是随机连接地(从如上面所介绍的一个小的高斯分布中抽取权重)到它下面的一个固定数量的神经元。一个典型的神经元连接的数目可能是小到 10 个。</p>
<h3 id="3-8-6-初始化偏差"><a href="#3-8-6-初始化偏差" class="headerlink" title="3.8.6 初始化偏差"></a>3.8.6 初始化偏差</h3><p>​    将偏差初始化为零是可能的，也是很常见的，因为非对称性破坏是由权重的小随机数导致的。因为 ReLU 具有非线性特点，所以有些人喜欢使用将所有的偏差设定为小的常数值如 0.01，因为这样可以确保所有的 ReLU 单元在最开始就激活触发(fire)并因此能够获得和传播一些梯度值。然而，这是否能够提供持续的改善还不太清楚(实际上一些结果表明这样做反而使得性能更加糟糕)，所以更通常的做法是简单地将偏差初始化为 0.</p>
<h2 id="3-9-学习率"><a href="#3-9-学习率" class="headerlink" title="3.9 学习率"></a>3.9 学习率</h2><h3 id="3-9-1-学习率的作用"><a href="#3-9-1-学习率的作用" class="headerlink" title="3.9.1 学习率的作用"></a>3.9.1 学习率的作用</h3><p>​    在机器学习中，监督式学习通过定义一个模型，并根据训练集上的数据估计最优参数。梯度下降法是一个广泛被用来最小化模型误差的参数优化算法。梯度下降法通过多次迭代，并在每一步中最小化成本函数（cost 来估计模型的参数。学习率 (learning rate)，在迭代过程中会控制模型的学习进度。</p>
<p>​    在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。故而如何对学习率的更新成为了研究者的关注点。<br>​    在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减</p>
<h3 id="3-9-2-学习率衰减常用参数有哪些"><a href="#3-9-2-学习率衰减常用参数有哪些" class="headerlink" title="3.9.2 学习率衰减常用参数有哪些"></a>3.9.2 学习率衰减常用参数有哪些</h3><div class="table-container">
<table>
<thead>
<tr>
<th>参数名称</th>
<th>参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>learning_rate</td>
<td>初始学习率</td>
</tr>
<tr>
<td>global_step</td>
<td>用于衰减计算的全局步数，非负，用于逐步计算衰减指数</td>
</tr>
<tr>
<td>decay_steps</td>
<td>衰减步数，必须是正值，决定衰减周期</td>
</tr>
<tr>
<td>decay_rate</td>
<td>衰减率</td>
</tr>
<tr>
<td>end_learning_rate</td>
<td>最低的最终学习率</td>
</tr>
<tr>
<td>cycle</td>
<td>学习率下降后是否重新上升</td>
</tr>
<tr>
<td>alpha</td>
<td>最小学习率</td>
</tr>
<tr>
<td>num_periods</td>
<td>衰减余弦部分的周期数</td>
</tr>
<tr>
<td>initial_variance</td>
<td>噪声的初始方差</td>
</tr>
<tr>
<td>variance_decay</td>
<td>衰减噪声的方差</td>
</tr>
</tbody>
</table>
</div>
<h3 id="3-9-3-分段常数衰减"><a href="#3-9-3-分段常数衰减" class="headerlink" title="3.9.3 分段常数衰减"></a>3.9.3 分段常数衰减</h3><p>​    分段常数衰减需要事先定义好的训练次数区间，在对应区间置不同的学习率的常数值，一般情况刚开始的学习率要大一些，之后要越来越小，要根据样本量的大小设置区间的间隔大小，样本量越大，区间间隔要小一点。下图即为分段常数衰减的学习率变化图，横坐标代表训练次数，纵坐标代表学习率。</p>
<p><img src="/img/ch3/learnrate1.png" alt=""></p>
<h3 id="3-9-4-指数衰减"><a href="#3-9-4-指数衰减" class="headerlink" title="3.9.4 指数衰减"></a>3.9.4 指数衰减</h3><p>​    以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关，其更新规则为：</p>
<script type="math/tex; mode=display">
decayed{\_}learning{\_}rate =learning{\_}rate*decay{\_}rate^{\frac{global{\_step}}{decay{\_}steps}}</script><p>​    这种衰减方式简单直接，收敛速度快，是最常用的学习率衰减方式，如下图所示，绿色的为学习率随<br>训练次数的指数衰减方式，红色的即为分段常数衰减，它在一定的训练区间内保持学习率不变。</p>
<p><img src="/img/ch3/learnrate2.png" alt=""></p>
<h3 id="3-9-5-自然指数衰减"><a href="#3-9-5-自然指数衰减" class="headerlink" title="3.9.5 自然指数衰减"></a>3.9.5 自然指数衰减</h3><p>​    它与指数衰减方式相似，不同的在于它的衰减底数是$e$，故而其收敛的速度更快，一般用于相对比较<br>容易训练的网络，便于较快的收敛，其更新规则如下</p>
<script type="math/tex; mode=display">
decayed{\_}learning{\_}rate =learning{\_}rate*e^{\frac{-decay{\_rate}}{global{\_}step}}</script><p>​    下图为为分段常数衰减、指数衰减、自然指数衰减三种方式的对比图，红色的即为分段常数衰减图，阶梯型曲线。蓝色线为指数衰减图，绿色即为自然指数衰减图，很明可以看到自然指数衰减方式下的学习率衰减程度要大于一般指数衰减方式，有助于更快的收敛。</p>
<p><img src="/img/ch3/learnrate3.png" alt=""></p>
<h3 id="3-9-6-多项式衰减"><a href="#3-9-6-多项式衰减" class="headerlink" title="3.9.6 多项式衰减"></a>3.9.6 多项式衰减</h3><p>​    应用多项式衰减的方式进行更新学习率，这里会给定初始学习率和最低学习率取值，然后将会按照<br>给定的衰减方式将学习率从初始值衰减到最低值,其更新规则如下式所示。</p>
<script type="math/tex; mode=display">
global{\_}step=min(global{\_}step,decay{\_}steps)</script><script type="math/tex; mode=display">
decayed{\_}learning{\_}rate =(learning{\_}rate-end{\_}learning{\_}rate)* \left( 1-\frac{global{\_step}}{decay{\_}steps}\right)^{power} \\
 +end{\_}learning{\_}rate</script><p>​    需要注意的是，有两个机制，降到最低学习率后，到训练结束可以一直使用最低学习率进行更新，另一个是再次将学习率调高，使用 decay_steps 的倍数，取第一个大于 global_steps 的结果，如下式所示.它是用来防止神经网络在训练的后期由于学习率过小而导致的网络一直在某个局部最小值附近震荡，这样可以通过在后期增大学习率跳出局部极小值。</p>
<script type="math/tex; mode=display">
decay{\_}steps = decay{\_}steps*ceil \left( \frac{global{\_}step}{decay{\_}steps}\right)</script><p>​    如下图所示，红色线代表学习率降低至最低后，一直保持学习率不变进行更新，绿色线代表学习率衰减到最低后，又会再次循环往复的升高降低。</p>
<p><img src="/img/ch3/learnrate4.png" alt=""></p>
<h3 id="3-9-7-余弦衰减"><a href="#3-9-7-余弦衰减" class="headerlink" title="3.9.7 余弦衰减"></a>3.9.7 余弦衰减</h3><p>​    余弦衰减就是采用余弦的相关方式进行学习率的衰减，衰减图和余弦函数相似。其更新机制如下式所示：</p>
<script type="math/tex; mode=display">
global{\_}step=min(global{\_}step,decay{\_}steps)</script><script type="math/tex; mode=display">
cosine{\_}decay=0.5*\left( 1+cos\left( \pi* \frac{global{\_}step}{decay{\_}steps}\right)\right)</script><script type="math/tex; mode=display">
decayed=(1-\alpha)*cosine{\_}decay+\alpha</script><script type="math/tex; mode=display">
decayed{\_}learning{\_}rate=learning{\_}rate*decayed</script><p>​    如下图所示，红色即为标准的余弦衰减曲线，学习率从初始值下降到最低学习率后保持不变。蓝色的线是线性余弦衰减方式曲线，它是学习率从初始学习率以线性的方式下降到最低学习率值。绿色噪声线性余弦衰减方式。</p>
<p><img src="/img/ch3/learnrate5.png" alt=""></p>
<h2 id="3-12-Dropout-系列问题"><a href="#3-12-Dropout-系列问题" class="headerlink" title="3.12 Dropout 系列问题"></a>3.12 Dropout 系列问题</h2><h3 id="3-12-1-为什么要正则化？"><a href="#3-12-1-为什么要正则化？" class="headerlink" title="3.12.1 为什么要正则化？"></a>3.12.1 为什么要正则化？</h3><ol>
<li>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。  </li>
<li>如果你怀疑神经网络过度拟合了数据，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据，这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者，获取更多数据的成本很高，但正则化有助于避免过度拟合，或者减少网络误差。</li>
</ol>
<h3 id="3-12-2-为什么正则化有利于预防过拟合？"><a href="#3-12-2-为什么正则化有利于预防过拟合？" class="headerlink" title="3.12.2 为什么正则化有利于预防过拟合？"></a>3.12.2 为什么正则化有利于预防过拟合？</h3><p><img src="/img/ch3/3.12.2.1.png" alt=""><br><img src="/img/ch3/3.12.2.2.png" alt=""> </p>
<p>左图是高偏差，右图是高方差，中间是Just Right，这几张图我们在前面课程中看到过。  </p>
<h3 id="3-12-3-理解dropout正则化"><a href="#3-12-3-理解dropout正则化" class="headerlink" title="3.12.3 理解dropout正则化"></a>3.12.3 理解dropout正则化</h3><p>​    Dropout可以随机删除网络中的神经单元，它为什么可以通过正则化发挥如此大的作用呢？  </p>
<p>​    直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。  </p>
<h3 id="3-12-4-dropout率的选择"><a href="#3-12-4-dropout率的选择" class="headerlink" title="3.12.4 dropout率的选择"></a>3.12.4 dropout率的选择</h3><ol>
<li>经过交叉验证，隐含节点 dropout 率等于 0.5 的时候效果最好，原因是 0.5 的时候 dropout 随机生成的网络结构最多。</li>
<li>dropout 也可以被用作一种添加噪声的方法，直接对 input 进行操作。输入层设为更接近 1 的数。使得输入变化不会太大（0.8） </li>
<li>对参数 $ w $ 的训练进行球形限制 (max-normalization)，对 dropout 的训练非常有用。</li>
<li>球形半径 $ c $ 是一个需要调整的参数，可以使用验证集进行参数调优。</li>
<li>dropout 自己虽然也很牛，但是 dropout、max-normalization、large decaying learning rates and high momentum 组合起来效果更好，比如 max-norm regularization 就可以防止大的learning rate 导致的参数 blow up。</li>
<li>使用 pretraining 方法也可以帮助 dropout 训练参数，在使用 dropout 时，要将所有参数都乘以 $ 1/p $。</li>
</ol>
<h3 id="3-12-5-dropout有什么缺点？"><a href="#3-12-5-dropout有什么缺点？" class="headerlink" title="3.12.5 dropout有什么缺点？"></a>3.12.5 dropout有什么缺点？</h3><p>​    dropout一大缺点就是代价函数J不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭dropout函数，将keep-prob的值设为1，运行代码，确保J函数单调递减。然后打开dropout函数，希望在dropout过程中，代码并未引入bug。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与dropout方法一起使用。  </p>
<h2 id="3-13-深度学习中常用的数据增强方法？"><a href="#3-13-深度学习中常用的数据增强方法？" class="headerlink" title="3.13 深度学习中常用的数据增强方法？"></a>3.13 深度学习中常用的数据增强方法？</h2><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<ul>
<li><p>Color Jittering：对颜色的数据增强：图像亮度、饱和度、对比度变化（此处对色彩抖动的理解不知是否得当）；</p>
</li>
<li><p>PCA  Jittering：首先按照RGB三个颜色通道计算均值和标准差，再在整个训练集上计算协方差矩阵，进行特征分解，得到特征向量和特征值，用来做PCA Jittering；</p>
</li>
<li><p>Random Scale：尺度变换；</p>
</li>
<li><p>Random Crop：采用随机图像差值方式，对图像进行裁剪、缩放；包括Scale Jittering方法（VGG及ResNet模型使用）或者尺度和长宽比增强变换；</p>
</li>
<li><p>Horizontal/Vertical Flip：水平/垂直翻转；</p>
</li>
<li><p>Shift：平移变换；</p>
</li>
<li><p>Rotation/Reflection：旋转/仿射变换；</p>
</li>
<li><p>Noise：高斯噪声、模糊处理；</p>
</li>
<li><p>Label Shuffle：类别不平衡数据的增广；</p>
</li>
</ul>
<h2 id="3-14-如何理解-Internal-Covariate-Shift？"><a href="#3-14-如何理解-Internal-Covariate-Shift？" class="headerlink" title="3.14 如何理解 Internal Covariate Shift？"></a>3.14 如何理解 Internal Covariate Shift？</h2><p><strong>（贡献者：黄钦建－华南理工大学）</strong></p>
<p>​    深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p>
<p>​    Google 将这一现象总结为 Internal Covariate Shift，简称 ICS。 什么是 ICS 呢？</p>
<p>​    大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同。</p>
<p>​    大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p>
<p><strong>那么ICS会导致什么问题？</strong></p>
<p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
<p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</p>
<p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</p>
<p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Rosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain.[J]. Psychological Review, 1958, 65(6):386-408.</p>
<p>[2] Duvenaud D , Rippel O , Adams R P , et al. Avoiding pathologies in very deep networks[J]. Eprint Arxiv, 2014:202-210.</p>
<p>[3] Rumelhart D E, Hinton G E, Williams R J. Learning representations by back-propagating errors[J]. Cognitive modeling, 1988, 5(3): 1.</p>
<p>[4] Hecht-Nielsen R. Theory of the backpropagation neural network[M]//Neural networks for perception. Academic Press, 1992: 65-93.</p>
<p>[5] Felice M. Which deep learning network is best for you?| CIO[J]. 2017.</p>
<p>[6] Conneau A, Schwenk H, Barrault L, et al. Very deep convolutional networks for natural language processing[J]. arXiv preprint arXiv:1606.01781, 2016, 2.</p>
<p>[7] Ba J, Caruana R. Do deep nets really need to be deep?[C]//Advances in neural information processing systems. 2014: 2654-2662.</p>
<p>[8] Nielsen M A. Neural networks and deep learning[M]. USA: Determination press, 2015.</p>
<p>[9] Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.</p>
<p>[10] 周志华. 机器学习[M].清华大学出版社, 2016.</p>
<p>[11] Kim J, Kwon Lee J, Mu Lee K. Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 1646-1654.</p>
<p>[12] Chen Y, Lin Z, Zhao X, et al. Deep learning-based classification of hyperspectral data[J]. IEEE Journal of Selected topics in applied earth observations and remote sensing, 2014, 7(6): 2094-2107.</p>
<p>[13] Domhan T, Springenberg J T, Hutter F. Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves[C]//Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.</p>
<p>[14] Maclaurin D, Duvenaud D, Adams R. Gradient-based hyperparameter optimization through reversible learning[C]//International Conference on Machine Learning. 2015: 2113-2122.</p>
<p>[15] Srivastava R K, Greff K, Schmidhuber J. Training very deep networks[C]//Advances in neural information processing systems. 2015: 2377-2385.</p>
<p>[16] Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.</p>
<p>[17] Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28th international conference on machine learning (ICML-11). 2011: 689-696.</p>
<p>[18] Deng L, Yu D. Deep learning: methods and applications[J]. Foundations and Trends® in Signal Processing, 2014, 7(3–4): 197-387.</p>
<p>[19] Erhan D, Bengio Y, Courville A, et al. Why does unsupervised pre-training help deep learning?[J]. Journal of Machine Learning Research, 2010, 11(Feb): 625-660.</p>
<p>[20] Dong C, Loy C C, He K, et al. Learning a deep convolutional network for image super resolution[C]//European conference on computer vision. Springer, Cham, 2014: 184-199.</p>
<p>[21] 郑泽宇，梁博文，顾思宇.TensorFlow：实战Google深度学习框架（第2版）[M].电子工业出版社,2018.</p>
<p>[22] 焦李成. 深度学习优化与识别[M].清华大学出版社,2017.</p>
<p>[23] 吴岸城. 神经网络与深度学习[M].电子工业出版社,2016.</p>
<p>[24] Wei, W.G.H., Liu, T., Song, A., et al. (2018) An Adaptive Natural Gradient Method with Adaptive Step Size in Multilayer Perceptrons. Chinese Automation Congress, 1593-1597.</p>
<p>[25] Y Feng, Y Li.An Overview of Deep Learning Optimization Methods and Learning Rate Attenuation Methods[J].Hans Journal of Data Mining,2018,8(4),186-200.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" class="post-title-link" itemprop="url">第十七章 模型压缩及移动端部署</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-03 21:13:44" itemprop="dateCreated datePublished" datetime="2020-03-03T21:13:44+08:00">2020-03-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-30 13:40:27" itemprop="dateModified" datetime="2020-03-30T13:40:27+08:00">2020-03-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第十七章-模型压缩及移动端部署"><a href="#第十七章-模型压缩及移动端部署" class="headerlink" title="第十七章 模型压缩及移动端部署"></a>第十七章 模型压缩及移动端部署</h1><p>​    深度神经网络在人工智能的应用中，包括语音识别、计算机视觉、自然语言处理等各方面，在取得巨大成功的同时，这些深度神经网络需要巨大的计算开销和内存开销，严重阻碍了资源受限下的使用。本章总结了模型压缩、加速一般原理和方法，以及在移动端如何部署。</p>
<h2 id="17-1-模型压缩理解"><a href="#17-1-模型压缩理解" class="headerlink" title="17.1 模型压缩理解"></a>17.1 模型压缩理解</h2><p>​    模型压缩是指利用数据集对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署再受限的硬件环境中。</p>
<h2 id="17-2-为什么需要模型压缩和加速？"><a href="#17-2-为什么需要模型压缩和加速？" class="headerlink" title="17.2 为什么需要模型压缩和加速？"></a>17.2 为什么需要模型压缩和加速？</h2><p>（1）随着AI技术的飞速发展，越来越多的公司希望在自己的移动端产品中注入AI能力。</p>
<p>（2）对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要。  </p>
<p>（3）模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。</p>
<p>（4）复杂的模型固然具有更好的性能，但是高额的存储空间、计算资源消耗是使其难以有效的应用在各硬件平台上的重要原因。</p>
<p>（5）智能设备的流行提供了内存、CPU、能耗和宽带等资源，使得深度学习模型部署在智能移动设备上变得可行。<br>（6）高效的深度学习方法可以有效的帮助嵌入式设备、分布式系统完成复杂工作，在移动端部署深度学习有很重要的意义。   </p>
<h2 id="17-3-模型压缩的必要性及可行性"><a href="#17-3-模型压缩的必要性及可行性" class="headerlink" title="17.3 模型压缩的必要性及可行性"></a>17.3 模型压缩的必要性及可行性</h2><div class="table-container">
<table>
<thead>
<tr>
<th>必要性</th>
<th>首先是资源受限，其次在许多网络结构中，如VGG-16网络，参数数量1亿3千多万，占用500MB空间，需要进行309亿次浮点运算才能完成一次图像识别任务。</th>
</tr>
</thead>
<tbody>
<tr>
<td>可行性</td>
<td>模型的参数在一定程度上能够表达其复杂性,相关研究表明,并不是所有的参数都在模型中发挥作用,部分参数作用有限、表达冗余,甚至会降低模型的性能。论文<Predicting parameters in deep learning>提出，很多的深度神经网络仅仅使用很少一部分（5%）权值就足以预测剩余的权值。该论文还提出这些剩下的权值甚至可以直接不用被学习。也就是说，仅仅训练一小部分原来的权值参数就有可能达到和原来网络相近甚至超过原来网络的性能（可以看作一种正则化）。</td>
</tr>
<tr>
<td>最终目的</td>
<td>最大程度的减小模型复杂度，减少模型存储需要的空间，也致力于加速模型的训练和推测</td>
</tr>
</tbody>
</table>
</div>
<h2 id="17-4-目前有哪些深度学习模型压缩方法？"><a href="#17-4-目前有哪些深度学习模型压缩方法？" class="headerlink" title="17.4 目前有哪些深度学习模型压缩方法？"></a>17.4 目前有哪些深度学习模型压缩方法？</h2><p>​    目前深度学习模型压缩方法主要分为更精细化模型设计、模型裁剪、核的稀疏化、量化、低秩分解、迁移学习等方法，而这些方法又可分为前端压缩和后端压缩。</p>
<h3 id="17-4-1-前端压缩和后端压缩对比"><a href="#17-4-1-前端压缩和后端压缩对比" class="headerlink" title="17.4.1 前端压缩和后端压缩对比"></a>17.4.1 前端压缩和后端压缩对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">对比项目</th>
<th style="text-align:center">前端压缩</th>
<th style="text-align:center">后端压缩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">含义</td>
<td style="text-align:center">不会改变原始网络结构的压缩技术</td>
<td style="text-align:center">会大程度上改变原始网络结构的压缩技术</td>
</tr>
<tr>
<td style="text-align:center">主要方法</td>
<td style="text-align:center">知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝</td>
<td style="text-align:center">低秩近似、未加限制的剪枝、参数量化、二值网络</td>
</tr>
<tr>
<td style="text-align:center">实现难度</td>
<td style="text-align:center">较简单</td>
<td style="text-align:center">较难</td>
</tr>
<tr>
<td style="text-align:center">是否可逆</td>
<td style="text-align:center">可逆</td>
<td style="text-align:center">不可逆</td>
</tr>
<tr>
<td style="text-align:center">成熟应用</td>
<td style="text-align:center">剪枝</td>
<td style="text-align:center">低秩近似、参数量化</td>
</tr>
<tr>
<td style="text-align:center">待发展应用</td>
<td style="text-align:center">知识蒸馏</td>
<td style="text-align:center">二值网络</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-2-网络剪枝"><a href="#17-4-2-网络剪枝" class="headerlink" title="17.4.2 网络剪枝"></a>17.4.2 网络剪枝</h3><p>深度学习模型因其<strong>稀疏性</strong>，可以被裁剪为结构精简的网络模型，具体包括结构性剪枝与非结构性剪枝。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
<th>举例</th>
</tr>
</thead>
<tbody>
<tr>
<td>非结构化剪枝</td>
<td>通常是连接级、细粒度的剪枝方法，精度相对较高，但依赖于特定算法库或硬件平台的支持</td>
<td>Deep Compression [5], Sparse-Winograd [6] 算法等；</td>
</tr>
<tr>
<td>结构化剪枝</td>
<td>是filter级或layer级、粗粒度的剪枝方法，精度相对较低，但剪枝策略更为有效，不需要特定算法库或硬件平台的支持，能够直接在成熟深度学习框架上运行。</td>
<td>如局部方式的、通过layer by layer方式的、最小化输出FM重建误差的Channel Pruning [7], ThiNet [8], Discrimination-aware Channel Pruning [9]；全局方式的、通过训练期间对BN层Gamma系数施加L1正则约束的Network Slimming [10]；全局方式的、按Taylor准则对Filter作重要性排序的Neuron Pruning [11]；全局方式的、可动态重新更新pruned filters参数的剪枝方法 [12];<br /><a href="https://blog.csdn.net/baidu_31437863/article/details/84474847" target="_blank" rel="noopener">https://blog.csdn.net/baidu_31437863/article/details/84474847</a></td>
</tr>
</tbody>
</table>
</div>
<p>如果按剪枝粒度分，从粗到细，可分为中间隐含层剪枝、通道剪枝、卷积核剪枝、核内剪枝、单个权重剪枝。下面按照剪枝粒度的分类从粗（左）到细（右）。</p>
<p><img src="/img/ch17/剪枝粒度分类.png" alt=""></p>
<p>（a）层间剪枝   （b）特征图剪枝    （c）k*k核剪枝   （d）核内剪枝</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>单个权重粒度</td>
<td>早期 Le Cun[16]提出的 OBD(optimal brain damage)将网络中的任意权重参数都看作单个参数,能够有效地提高预测准确率,却不能减小运行时间;同时,剪枝代价过高,只适用于小网络</td>
</tr>
<tr>
<td>核内权重粒度</td>
<td>网络中的任意权重被看作是单个参数并进行随机非结构化剪枝,该粒度的剪枝导致网络连接不规整,需要通过稀疏表达来减少内存占用,进而导致在前向传播预测时,需要大量的条件判断和额外空间来标明零或非零参数的位置,因此不适用于并行计算</td>
</tr>
<tr>
<td>卷积核粒度与通道粒度</td>
<td>卷积核粒度与通道粒度属于粗粒度剪枝,不依赖任何稀疏卷积计算库及专用硬件;同时,能够在获得高压缩率的同时大量减小测试阶段的计算时间.由</td>
</tr>
</tbody>
</table>
</div>
<p>从剪枝目标上分类，可分为减少参数/网络复杂度、减少过拟合/增加泛化能力/提高准确率、减小部署运行时间/提高网络效率及减小训练时间等。</p>
<h3 id="17-4-3-典型剪枝方法对比"><a href="#17-4-3-典型剪枝方法对比" class="headerlink" title="17.4.3 典型剪枝方法对比"></a>17.4.3 典型剪枝方法对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">剪枝方法</th>
<th style="text-align:center">修剪对象</th>
<th style="text-align:center">修剪方式</th>
<th style="text-align:center">效果</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Deep Compression</td>
<td style="text-align:center">权重</td>
<td style="text-align:center">随机修剪</td>
<td style="text-align:center">50倍压缩</td>
</tr>
<tr>
<td style="text-align:center">Structured Pruning</td>
<td style="text-align:center">权重</td>
<td style="text-align:center">组稀疏+排他性稀疏</td>
<td style="text-align:center">性能提升</td>
</tr>
<tr>
<td style="text-align:center">Network Slimming</td>
<td style="text-align:center">特征图通道</td>
<td style="text-align:center">根据尺度因子修剪</td>
<td style="text-align:center">节省计算资源</td>
</tr>
<tr>
<td style="text-align:center">mProp</td>
<td style="text-align:center">梯度</td>
<td style="text-align:center">修剪幅值小的梯度</td>
<td style="text-align:center">加速</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-4-网络蒸馏"><a href="#17-4-4-网络蒸馏" class="headerlink" title="17.4.4 网络蒸馏"></a>17.4.4 网络蒸馏</h3><p>​    网络精馏是指利用大量未标记的迁移数据(transfer data),让小模型去拟合大模型,从而让小模型学到与大模型相似的函数映射.网络精馏可以看成在同一个域上迁移学习[34]的一种特例,目的是获得一个比原模型更为精简的网络,整体的框架图如图 4所示. </p>
<p><img src="/img/ch17/网络蒸馏.png" alt=""></p>
<h3 id="17-4-5-前端压缩"><a href="#17-4-5-前端压缩" class="headerlink" title="17.4.5 前端压缩"></a>17.4.5 前端压缩</h3><p>（1）知识蒸馏</p>
<p>​    一个复杂模型可由多个简单模型或者强约束条件训练得到。复杂模型特点是性能好，但其参数量大，计算效率低。小模型特点是计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中,使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。<br>（2）紧凑的模型结构设计<br>​    紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个3x3的卷积替换一个5x5的卷积、使用深度可分离卷积等等方式降低计算参数量。  目前很多网络基于模块化设计思想，在深度和宽度两个维度上都很大，导致参数冗余。因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。<br>（3）滤波器层面的剪枝<br>​    滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。  具体操作是在训练时使用稀疏约束（加入权重的稀疏正则项，引导模型的大部分权重趋向于0）。完成训练后，剪去滤波器上的这些 0 。</p>
<p>​    优点是简单，缺点是剪得不干净，非结构化剪枝会增加内存访问成本。</p>
<h3 id="17-4-6-后端压缩"><a href="#17-4-6-后端压缩" class="headerlink" title="17.4.6 后端压缩"></a>17.4.6 后端压缩</h3><p>（1）低秩近似<br>​    在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">事项</th>
<th style="text-align:left">特点</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">优点</td>
<td style="text-align:left">可以降低存储和计算消耗；<br />一般可以压缩2-3倍；精度几乎没有损失；</td>
</tr>
<tr>
<td style="text-align:left">缺点</td>
<td style="text-align:left">模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能 ；   <br />超参数的数量随着网络层数的增加呈线性变化趋势，例如中间层的特征通道数等等。 <br />随着模型复杂度的提升，搜索空间急剧增大。</td>
</tr>
</tbody>
</table>
</div>
<p>（2）未加限制的剪枝    </p>
<p>​    完成训练后，不加限制地剪去那些冗余参数。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>保持模型性能不损失的情况下，减少参数量9-11倍； <br />剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力；</td>
</tr>
<tr>
<td>缺点</td>
<td>极度依赖专门的运行库和特殊的运行平台，不具有通用性；<br /> 压缩率过大时，破坏性能；</td>
</tr>
</tbody>
</table>
</div>
<p>（3）参数量化    </p>
<p>​    神经网络的参数类型一般是32位浮点型，使用较小的精度代替32位所表示的精度。或者是将多个权重映射到同一数值，权重共享。<strong>量化其实是一种权值共享的策略</strong>。量化后的权值张量是一个高度稀疏的有很多共享权值的矩阵，对非零参数，我们还可以进行定点压缩，以获得更高的压缩率。 </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>模型性能损失很小，大小减少8-16倍；</td>
</tr>
<tr>
<td>缺点</td>
<td>压缩率大时，性能显著下降； <br />依赖专门的运行库，通用性较差；</td>
</tr>
<tr>
<td>举例</td>
<td>二值化网络：XNORnet [13], ABCnet with Multiple Binary Bases [14], <br />Bin-net with High-Order Residual Quantization [15], Bi-Real Net [16]；<br/>三值化网络：Ternary weight networks [17], Trained Ternary Quantization [18]；</td>
</tr>
</tbody>
</table>
</div>
<p>W1-A8 或 W2-A8量化： Learning Symmetric Quantization [19]；<br>INT8量化：TensorFlow-lite [20], TensorRT [21]；<br>其他（非线性）：Intel INQ [22], log-net, CNNPack [23] 等；<br>原文：<a href="https://blog.csdn.net/baidu_31437863/article/details/84474847" target="_blank" rel="noopener">https://blog.csdn.net/baidu_31437863/article/details/84474847</a> |<br>| 总结 | 最为典型就是二值网络、XNOR网络等。其主要原理就是采用1bit对网络的输入、权重、响应进行编码。减少模型大小的同时，原始网络的卷积操作可以被bit-wise运算代替，极大提升了模型的速度。但是，如果原始网络结果不够复杂（模型描述能力），由于二值网络会较大程度降低模型的表达能力。因此现阶段有相关的论文开始研究n-bit编码方式成为n值网络或者多值网络或者变bit、组合bit量化来克服二值网络表达能力不足的缺点。 |</p>
<p>（4）二值网络</p>
<p>​    相对量化更为极致，对于32bit浮点型数用1bit二进制数-1或者1表示，可大大减小模型尺寸。  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>网络体积小，运算速度快，有时可避免部分网络的overfitting</td>
</tr>
<tr>
<td>缺点</td>
<td>二值神经网络损失的信息相对于浮点精度是非常大；<br />粗糙的二值化近似导致训练时模型收敛速度非常慢</td>
</tr>
</tbody>
</table>
</div>
<p>（5）三值网络</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事项</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>相对于二值神经网络，三值神经网络(Ternary Weight Networks)在同样的模型结构下可以达到成百上千倍的表达能力提升;并且，在计算时间复杂度上，三元网络和二元网络的计算复杂度是一样的。<br />例如，对于ResNet-18层网络中最常出现的卷积核(3x3大小)，二值神经网络模型最多可以表达2的3x3次方(=512)种结构，而三元神经网络则可以表达3的3x3次方(=19683)种卷积核结构。在表达能力上，三元神经网络相对要高19683/512 = 38倍。因此，三元神经网络模型能够在保证计算复杂度很低的情况下大幅的提高网络的表达能力，进而可以在精度上相对于二值神经网络有质的飞跃。另外，由于对中间信息的保存更多，三元神经网络可以极大的加快网络训练时的收敛速度，从而更快、更稳定的达到最优的结果。</td>
</tr>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-6-低秩分解"><a href="#17-4-6-低秩分解" class="headerlink" title="17.4.6 低秩分解"></a>17.4.6 低秩分解</h3><p>基于低秩分解的深度神经网络压缩与加速的核心思想是利用矩阵或张量分解技术估计并分解深度模型中的原始卷积核．卷积计算是整个卷积神经网络中计算复杂 度 最 高 的 计 算 操 作，通 过 分 解４Ｄ 卷积核张量，可以有效地减少模型内部的冗余性．此外对于２Ｄ的全 连 接 层 矩 阵 参 数，同样可以利用低秩分解技术进行处理．但由于卷积层与全连接层的分解方式不同，本文分别从卷积层和全连接层２个不同角度回顾与分析低秩分解技术在深度神经网络中的应用.</p>
<p>在２０１３年，Ｄｅｎｉｌ等人［５７］从理论上利用低秩分解的技术并分析了深度神经网络存在大量的冗余信<br>息，开创了基于低秩分解的深度网络模型压缩与加速的新思路．如图７所示，展示了主流的张量分解后卷积 计 算．</p>
<p><img src=".\img\ch17\低秩分解模型压缩加速.jpg" alt=""></p>
<p>(出自《深度神经网络压缩与加速综述》)</p>
<h3 id="17-4-7-总体压缩效果评价指标有哪些？"><a href="#17-4-7-总体压缩效果评价指标有哪些？" class="headerlink" title="17.4.7 总体压缩效果评价指标有哪些？"></a>17.4.7 总体压缩效果评价指标有哪些？</h3><p>​    网络压缩评价指标包括运行效率、参数压缩率、准确率.与基准模型比较衡量性能提升时,可以使用提升倍数(speedup)或提升比例(ratio)。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评价指标</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>准确率</td>
<td>目前,大部分研究工作均会测量 Top-1 准确率,只有在 ImageNet 这类大型数据集上才会只用 Top-5 准确率.为方便比较</td>
</tr>
<tr>
<td>参数压缩率</td>
<td>统计网络中所有可训练的参数,根据机器浮点精度转换为字节(byte)量纲,通常保留两位有效数字以作近似估计.</td>
</tr>
<tr>
<td>运行效率</td>
<td>可以从网络所含浮点运算次数(FLOP)、网络所含乘法运算次数(MULTS)或随机实验测得的网络平均前向传播所需时间这 3 个角度来评价</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-8-几种轻量化网络结构对比"><a href="#17-4-8-几种轻量化网络结构对比" class="headerlink" title="17.4.8 几种轻量化网络结构对比"></a>17.4.8 几种轻量化网络结构对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">网络结构</th>
<th style="text-align:center">TOP1 准确率/%</th>
<th style="text-align:center">参数量/M</th>
<th style="text-align:center">CPU运行时间/ms</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MobileNet V1</td>
<td style="text-align:center">70.6</td>
<td style="text-align:center">4.2</td>
<td style="text-align:center">123</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNet(1.5)</td>
<td style="text-align:center">69.0</td>
<td style="text-align:center">2.9</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">ShuffleNet(x2)</td>
<td style="text-align:center">70.9</td>
<td style="text-align:center">4.4</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">MobileNet V2</td>
<td style="text-align:center">71.7</td>
<td style="text-align:center">3.4</td>
<td style="text-align:center">80</td>
</tr>
<tr>
<td style="text-align:center">MobileNet V2(1.4)</td>
<td style="text-align:center">74.7</td>
<td style="text-align:center">6.9</td>
<td style="text-align:center">149</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-4-9-网络压缩未来研究方向有哪些？"><a href="#17-4-9-网络压缩未来研究方向有哪些？" class="headerlink" title="17.4.9 网络压缩未来研究方向有哪些？"></a>17.4.9 网络压缩未来研究方向有哪些？</h3><p>网络剪枝、网络精馏和网络分解都能在一定程度上实现网络压缩的目的.回归到深度网络压缩的本质目的上,即提取网络中的有用信息,以下是一些值得研究和探寻的方向.<br>(1) 权重参数对结果的影响度量.深度网络的最终结果是由全部的权重参数共同作用形成的,目前,关于单个卷积核/卷积核权重的重要性的度量仍然是比较简单的方式,尽管文献[14]中给出了更为细节的分析,但是由于计算难度大,并不实用.因此,如何通过更有效的方式来近似度量单个参数对模型的影响,具有重要意义.<br>(2) 学生网络结构的构造.学生网络的结构构造目前仍然是由人工指定的,然而,不同的学生网络结构的训练难度不同,最终能够达到的效果也有差异.因此,如何根据教师网络结构设计合理的网络结构在精简模型的条件下获取较高的模型性能,是未来的一个研究重点.<br>(3) 参数重建的硬件架构支持.通过分解网络可以无损地获取压缩模型,在一些对性能要求高的场景中是非常重要的.然而,参数的重建步骤会拖累预测阶段的时间开销,如何通过硬件的支持加速这一重建过程,将是未来的一个研究方向.<br>(4) 任务或使用场景层面的压缩.大型网络通常是在量级较大的数据集上训练完成的,比如,在 ImageNet上训练的模型具备对 1 000 类物体的分类,但在一些具体场景的应用中,可能仅需要一个能识别其中几类的小型模型.因此,如何从一个全功能的网络压缩得到部分功能的子网络,能够适应很多实际应用场景的需求.<br>(5) 网络压缩效用的评价.目前,对各类深度网络压缩算法的评价是比较零碎的,侧重于和被压缩的大型网络在参数量和运行时间上的比较.未来的研究可以从提出更加泛化的压缩评价标准出发,一方面平衡运行速度和模型大小在不同应用场景下的影响;另一方面,可以从模型本身的结构性出发,对压缩后的模型进行评价. </p>
<p>（出自《深度网络模型压缩综述》）</p>
<h2 id="17-5-目前有哪些深度学习模型优化加速方法？"><a href="#17-5-目前有哪些深度学习模型优化加速方法？" class="headerlink" title="17.5 目前有哪些深度学习模型优化加速方法？"></a>17.5 目前有哪些深度学习模型优化加速方法？</h2><p><a href="https://blog.csdn.net/nature553863/article/details/81083955" target="_blank" rel="noopener">https://blog.csdn.net/nature553863/article/details/81083955</a></p>
<h3 id="17-5-1-模型优化加速方法"><a href="#17-5-1-模型优化加速方法" class="headerlink" title="17.5.1 模型优化加速方法"></a>17.5.1 模型优化加速方法</h3><p>模型优化加速能够提升网络的计算效率，具体包括：<br>（1）Op-level的快速算法：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；<br>（2）Layer-level的快速算法：Sparse-block net [1] 等；<br>（3）优化工具与库：TensorRT (Nvidia), Tensor Comprehension (Facebook) 和 Distiller (Intel) 等；   </p>
<p>原文：<a href="https://blog.csdn.net/nature553863/article/details/81083955" target="_blank" rel="noopener">https://blog.csdn.net/nature553863/article/details/81083955</a>   </p>
<h3 id="17-5-2-TensorRT加速原理"><a href="#17-5-2-TensorRT加速原理" class="headerlink" title="17.5.2 TensorRT加速原理"></a>17.5.2 TensorRT加速原理</h3><p><a href="https://blog.csdn.net/xh_hit/article/details/79769599" target="_blank" rel="noopener">https://blog.csdn.net/xh_hit/article/details/79769599</a></p>
<p>​    在计算资源并不丰富的嵌入式设备上，TensorRT之所以能加速神经网络的的推断主要得益于两点：</p>
<ul>
<li><p>首先是TensorRT支持int8和fp16的计算，通过在减少计算量和保持精度之间达到一个理想的trade-off，达到加速推断的目的。</p>
</li>
<li><p>更为重要的是TensorRT对于网络结构进行了重构和优化，主要体现在一下几个方面。</p>
<p>(1) TensorRT通过解析网络模型将网络中无用的输出层消除以减小计算。</p>
<p>(2) 对于网络结构的垂直整合，即将目前主流神经网络的Conv、BN、Relu三个层融合为了一个层，例如将图1所示的常见的Inception结构重构为图2所示的网络结构。</p>
<p>(3) 对于网络结构的水平组合，水平组合是指将输入为相同张量和执行相同操作的层融合一起，例如图2向图3的转化。</p>
</li>
</ul>
<p><img src="/img/ch17/tensorRT1.png" alt=""></p>
<p><img src="/img/ch17/tensorRT2.png" alt=""></p>
<p><img src="/img/ch17/tensorRT3.png" alt=""></p>
<p>​    以上3步即是TensorRT对于所部署的深度学习网络的优化和重构，根据其优化和重构策略，第一和第二步适用于所有的网络架构，但是第三步则对于含有Inception结构的神经网络加速效果最为明显。</p>
<p>​    Tips: 想更好地利用TensorRT加速网络推断，可在基础网络中多采用Inception模型结构，充分发挥TensorRT的优势。</p>
<h3 id="17-5-3-TensorRT如何优化重构模型？"><a href="#17-5-3-TensorRT如何优化重构模型？" class="headerlink" title="17.5.3 TensorRT如何优化重构模型？"></a>17.5.3 TensorRT如何优化重构模型？</h3><div class="table-container">
<table>
<thead>
<tr>
<th>条件</th>
<th>方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>若训练的网络模型包含TensorRT支持的操作</td>
<td>1、对于Caffe与TensorFlow训练的模型，若包含的操作都是TensorRT支持的，则可以直接由TensorRT优化重构</td>
</tr>
<tr>
<td></td>
<td>2、对于MXnet, PyTorch或其他框架训练的模型，若包含的操作都是TensorRT支持的，可以采用TensorRT API重建网络结构，并间接优化重构；</td>
</tr>
<tr>
<td>若训练的网络模型包含TensorRT不支持的操作</td>
<td>1、TensorFlow模型可通过tf.contrib.tensorrt转换，其中不支持的操作会保留为TensorFlow计算节点；</td>
</tr>
<tr>
<td></td>
<td>2、不支持的操作可通过Plugin API实现自定义并添加进TensorRT计算图；</td>
</tr>
<tr>
<td></td>
<td>3、将深度网络划分为两个部分，一部分包含的操作都是TensorRT支持的，可以转换为TensorRT计算图。另一部则采用其他框架实现，如MXnet或PyTorch；</td>
</tr>
</tbody>
</table>
</div>
<h3 id="17-5-4-TensorRT加速效果如何？"><a href="#17-5-4-TensorRT加速效果如何？" class="headerlink" title="17.5.4 TensorRT加速效果如何？"></a>17.5.4 TensorRT加速效果如何？</h3><p>以下是在TitanX (Pascal)平台上，TensorRT对大型分类网络的优化加速效果：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Network</th>
<th style="text-align:center">Precision</th>
<th style="text-align:center">Framework/GPU:TitanXP</th>
<th style="text-align:center">Avg.Time(Batch=8,unit:ms)</th>
<th style="text-align:center">Top1 Val.Acc.(ImageNet-1k)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TensorFlow</td>
<td style="text-align:center">24.1</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">MXnet</td>
<td style="text-align:center">15.7</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">12.1</td>
<td style="text-align:center">0.7374</td>
</tr>
<tr>
<td style="text-align:center">Resnet50</td>
<td style="text-align:center">int8</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">0.7226</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TensorFlow</td>
<td style="text-align:center">36.7</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">MXnet</td>
<td style="text-align:center">25.8</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">fp32</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">19.3</td>
<td style="text-align:center">0.7612</td>
</tr>
<tr>
<td style="text-align:center">Resnet101</td>
<td style="text-align:center">int8</td>
<td style="text-align:center">TRT4.0.1</td>
<td style="text-align:center">9</td>
<td style="text-align:center">0.7574</td>
</tr>
</tbody>
</table>
</div>
<h2 id="17-6-影响神经网络速度的4个因素（再稍微详细一点）"><a href="#17-6-影响神经网络速度的4个因素（再稍微详细一点）" class="headerlink" title="17.6 影响神经网络速度的4个因素（再稍微详细一点）"></a>17.6 影响神经网络速度的4个因素（再稍微详细一点）</h2><ol>
<li><p>FLOPs(FLOPs就是网络执行了多少multiply-adds操作)；  </p>
</li>
<li><p>MAC(内存访问成本)；   </p>
</li>
<li><p>并行度(如果网络并行度高，速度明显提升)；   </p>
</li>
<li><p>计算平台(GPU，ARM)   </p>
</li>
</ol>
<h2 id="17-7-压缩和加速方法如何选择？"><a href="#17-7-压缩和加速方法如何选择？" class="headerlink" title="17.7 压缩和加速方法如何选择？"></a>17.7 压缩和加速方法如何选择？</h2><p>​    １）对于在线计算内存存储有限的应用场景或设备，可以选择参数共享和参数剪枝方法，特别是二值量化权值和激活、结构化剪枝．其他方法虽然能够有效的压缩模型中的权值参数，但无法减小计算中隐藏的内存大小（如特征图）．<br>​    ２）如果在应用中用到的紧性模型需要利用预训练模型，那么参数剪枝、参数共享以及低秩分解将成为首要考虑的方法．相反地，若不需要借助预训练模型，则可以考虑紧性滤波设计及知识蒸馏方法．<br>​    ３）若需要一次性端对端训练得到压缩与加速后模型，可以利用基于紧性滤波设计的深度神经网络压缩与加速方法．<br>​    ４）一般情况下，参数剪枝，特别是非结构化剪枝，能大大压缩模型大小，且不容易丢失分类精度．对于需要稳定的模型分类的应用，非结构化剪枝成为首要选择．<br>​    ５）若采用的数据集较小时，可以考虑知识蒸馏方法．对于小样本的数据集，学生网络能够很好地迁移教师模型的知识，提高学生网络的判别性．<br>​    ６）主流的５个深度神经网络压缩与加速算法相互之间是正交的，可以结合不同技术进行进一步的压缩与加速．如：韩 松 等 人［３０］结合了参数剪枝和参数共享；温伟等人［６４］以及 Ａｌｖａｒｅｚ等人［８５］结合了参数剪枝和低秩分解．此外对于特定的应用场景，如目标检测，可以对卷积层和全连接层使用不同的压缩与加速技术分别处理．</p>
<p>参考《深度神经网络压缩与加速综述》</p>
<h2 id="17-8-改变网络结构设计为什么会实现模型压缩、加速？"><a href="#17-8-改变网络结构设计为什么会实现模型压缩、加速？" class="headerlink" title="17.8 改变网络结构设计为什么会实现模型压缩、加速？"></a>17.8 改变网络结构设计为什么会实现模型压缩、加速？</h2><h3 id="17-8-1-Group-convolution"><a href="#17-8-1-Group-convolution" class="headerlink" title="17.8.1 Group convolution"></a>17.8.1 Group convolution</h3><p>​    Group convolution最早出现在AlexNet中，是为了解决单卡显存不够，将网络部署到多卡上进行训练而提出。Group convolution可以减少单个卷积1/g的参数量。如何计算的呢？  </p>
<p>​    假设</p>
<ul>
<li>输入特征的的维度为$H\times W\times C_1$;</li>
<li>卷积核的维度为$H_1\times W_1\times C_1$，共$C_2$个；</li>
<li>输出特征的维度为$H_1\times W_1\times C_2$ 。  </li>
</ul>
<p>传统卷积计算方式如下：<br><img src="/img/ch17/1.png" alt="image"><br>传统卷积运算量为：  </p>
<script type="math/tex; mode=display">
A = H * W * h1 * w1 * c1 * c2</script><p>Group convolution是将输入特征的维度c1分成g份，每个group对应的channel数为c1/g，特征维度H * W * c1/g；，每个group对应的卷积核的维度也相应发生改变为h1 * w1 * c1/g，共c2/g个；每个group相互独立运算，最后将结果叠加在一起。<br>Group convolution计算方式如下：<br><img src="/img/ch17/2.png" alt="image"><br>Group convolution运算量为：  </p>
<script type="math/tex; mode=display">
B = H * W * h1 * w1 * c1/g * c2/g * g</script><p>Group卷积相对于传统卷积的运算量：  </p>
<script type="math/tex; mode=display">
\dfrac{B}{A} = \dfrac{ H * W * h1 * w1 * c1/g * c2/g * g}{H * W * h1 * w1 * c1 * c2} = \dfrac{1}{g}</script><p>由此可知：group卷积相对于传统卷积减少了1/g的参数量。</p>
<h3 id="17-8-2-Depthwise-separable-convolution"><a href="#17-8-2-Depthwise-separable-convolution" class="headerlink" title="17.8.2. Depthwise separable convolution"></a>17.8.2. Depthwise separable convolution</h3><p>Depthwise separable convolution是由depthwise conv和pointwise conv构成。<br>depthwise conv(DW)有效减少参数数量并提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。<br>pointwise conv(PW)实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。<br>假设输入特征的的维度为H * W * c1；卷积核的维度为h1 * w1 * c1，共c2个；输出特征的维度为 H1 * W1 * c2。<br>传统卷积计算方式如下：<br><img src="/img/ch17/3.png" alt="image"><br>传统卷积运算量为：  </p>
<script type="math/tex; mode=display">
A = H * W * h1 * w1 * c1 * c2</script><p>DW卷积的计算方式如下：<br><img src="/img/ch17/4.png" alt="image"><br>DW卷积运算量为： </p>
<script type="math/tex; mode=display">
B_DW = H * W * h1 * w1 * 1 * c1</script><p>PW卷积的计算方式如下：<br><img src="/img/ch17/5.png" alt="image"></p>
<script type="math/tex; mode=display">
B_PW = H_m * W_m * 1 * 1 * c_1 * c_2</script><p>Depthwise separable convolution运算量为：</p>
<script type="math/tex; mode=display">
B = B_DW + B_PW</script><p>Depthwise separable convolution相对于传统卷积的运算量：</p>
<script type="math/tex; mode=display">
\dfrac{B}{A} = \dfrac{ H * W * h_1 * w_1 * 1 * c_1 + H_m * W_m * 1 * 1 * c_1 * c_2}{H * W * h1 * w1 * c_1 * c_2}  

= \dfrac{1}{c_2} + \dfrac{1}{h_1 * w_1}</script><p>由此可知，随着卷积通道数的增加，Depthwise separable convolution的运算量相对于传统卷积更少。</p>
<h3 id="17-8-3-输入输出的channel相同时，MAC最小"><a href="#17-8-3-输入输出的channel相同时，MAC最小" class="headerlink" title="17.8.3 输入输出的channel相同时，MAC最小"></a>17.8.3 输入输出的channel相同时，MAC最小</h3><p><strong>卷积层的输入和输出特征通道数相等时MAC最小，此时模型速度最快。</strong><br>假设feature map的大小为h*w，输入通道$c_1$，输出通道$c_2$。<br>已知：</p>
<script type="math/tex; mode=display">
FLOPs = B = h * w * c1 * c2   
=> c1 * c2 = \dfrac{B}{h * w}</script><script type="math/tex; mode=display">
MAC = h * w * (c1 + c2) + c1 * c2</script><script type="math/tex; mode=display">
=> MAC \geq 2 * h * w \sqrt{\dfrac{B}{h * w}} + \dfrac{B}{h * w}</script><p>根据均值不等式得到(c1-c2)^2&gt;=0，等式成立的条件是c1=c2，也就是输入特征通道数和输出特征通道数相等时，在给定FLOPs前提下，MAC达到取值的下界。</p>
<h3 id="17-8-4-减少组卷积的数量"><a href="#17-8-4-减少组卷积的数量" class="headerlink" title="17.8.4 减少组卷积的数量"></a>17.8.4 减少组卷积的数量</h3><p><strong>过多的group操作会增大MAC，从而使模型速度变慢</strong><br>由以上公式可知，group卷积想比与传统的卷积可以降低计算量，提高模型的效率；如果在相同的FLOPs时，group卷积为了满足FLOPs会是使用更多channels，可以提高模型的精度。但是随着channel数量的增加，也会增加MAC。<br>FLOPs：</p>
<script type="math/tex; mode=display">
B = \dfrac{h * w * c1 * c2}{g}</script><p>MAC：</p>
<script type="math/tex; mode=display">
MAC = h * w * (c1 + c2) + \dfrac{c1 * c2}{g}</script><p>由MAC，FLOPs可知：</p>
<script type="math/tex; mode=display">
MAC = h * w * c1 + \dfrac{B*g}{c1} + \dfrac{B}{h * w}</script><p>当FLOPs固定(B不变)时，g越大，MAC越大。</p>
<h3 id="17-8-5-减少网络碎片化程度-分支数量"><a href="#17-8-5-减少网络碎片化程度-分支数量" class="headerlink" title="17.8.5 减少网络碎片化程度(分支数量)"></a>17.8.5 减少网络碎片化程度(分支数量)</h3><p><strong>模型中分支数量越少，模型速度越快</strong><br>此结论主要是由实验结果所得。<br>以下为网络分支数和各分支包含的卷积数目对神经网络速度的影响。<br><img src="/img/ch17/6.png" alt="image"><br>实验中使用的基本网络结构，分别将它们重复10次，然后进行实验。实验结果如下：<br><img src="/img/ch17/7.png" alt="image"><br>由实验结果可知，随着网络分支数量的增加，神经网络的速度在降低。网络碎片化程度对GPU的影响效果明显，对CPU不明显，但是网络速度同样在降低。</p>
<h3 id="17-8-7-减少元素级操作"><a href="#17-8-7-减少元素级操作" class="headerlink" title="17.8.7 减少元素级操作"></a>17.8.7 减少元素级操作</h3><p><strong>元素级操作所带来的时间消耗也不能忽视</strong><br>ReLU ，Tensor 相加，Bias相加的操作，分离卷积（depthwise convolution）都定义为元素级操作。<br>FLOPs大多数是对于卷积计算而言的，因为元素级操作的FLOPs相对要低很多。但是过的元素级操作也会带来时间成本。ShuffleNet作者对ShuffleNet v1和MobileNet v2的几种层操作的时间消耗做了分析，发现元素级操作对于网络速度的影响也很大。<br><img src="/img/ch17/8.png" alt="image"></p>
<h2 id="17-9-常用的轻量级网络有哪些？"><a href="#17-9-常用的轻量级网络有哪些？" class="headerlink" title="17.9 常用的轻量级网络有哪些？"></a>17.9 常用的轻量级网络有哪些？</h2><h3 id="17-9-1-SequeezeNet"><a href="#17-9-1-SequeezeNet" class="headerlink" title="17.9.1 SequeezeNet"></a>17.9.1 SequeezeNet</h3><p>SqueenzeNet出自F. N. Iandola, S.Han等人发表的论文《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt; 0.5MB model size》，作者在保证精度不损失的同时，将原始AlexNet压缩至原来的510倍。  </p>
<h4 id="1-1-设计思想"><a href="#1-1-设计思想" class="headerlink" title="1.1 设计思想"></a>1.1 设计思想</h4><p>在网络结构设计方面主要采取以下三种方式：</p>
<ul>
<li>用1*1卷积核替换3*3卷积<ul>
<li>理论上一个1*1卷积核的参数是一个3*3卷积核的1/9，可以将模型尺寸压缩9倍。</li>
</ul>
</li>
<li>减小3*3卷积的输入通道数<ul>
<li>根据上述公式，减少输入通道数不仅可以减少卷积的运算量，而且输入通道数与输出通道数相同时还可以减少MAC。</li>
</ul>
</li>
<li>延迟降采样<ul>
<li>分辨率越大的输入能够提供更多特征的信息，有利于网络的训练判断，延迟降采样可以提高网络精度。<h4 id="1-2-网络架构"><a href="#1-2-网络架构" class="headerlink" title="1.2 网络架构"></a>1.2 网络架构</h4>SqueezeNet提出一种多分支结构——fire model，其中是由Squeeze层和expand层构成。Squeeze层是由s1个1*1卷积组成，主要是通过1*1的卷积降低expand层的输入维度；expand层利用e1个1*1和e3个3*3卷积构成多分支结构提取输入特征，以此提高网络的精度(其中e1=e3=4*s1)。<br><img src="/img/ch17/9.png" alt="image"><br>SqueezeNet整体网络结构如下图所示：<br><img src="/img/ch17/10.png" alt="image"></li>
</ul>
</li>
</ul>
<h4 id="1-3实验结果"><a href="#1-3实验结果" class="headerlink" title="1.3实验结果"></a>1.3实验结果</h4><p>不同压缩方法在ImageNet上的对比实验结果<br><img src="/img/ch17/11.png" alt="image"><br>由实验结果可知，SqueezeNet不仅保证了精度，而且将原始AlexNet从240M压缩至4.8M，压缩50倍，说明此轻量级网络设计是可行。</p>
<h3 id="17-9-2-MobileNet"><a href="#17-9-2-MobileNet" class="headerlink" title="17.9.2 MobileNet"></a>17.9.2 MobileNet</h3><p>MobileNet 是Google团队于CVPR-2017的论文《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》中针对手机等嵌入式设备提出的一种轻量级的深层神经网络，该网络结构在VGG的基础上使用DW+PW的组合，在保证不损失太大精度的同时，降低模型参数量。</p>
<h4 id="2-1-设计思想"><a href="#2-1-设计思想" class="headerlink" title="2.1 设计思想"></a>2.1 设计思想</h4><ul>
<li>采用深度可分离卷积代替传统卷积<ul>
<li>采用DW卷积在减少参数数量的同时提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。</li>
<li>采用PW卷积实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。</li>
</ul>
</li>
<li>使用stride=2的卷积替换pooling<ul>
<li>直接在卷积时利用stride=2完成了下采样，从而节省了需要再去用pooling再去进行一次下采样的时间，可以提升运算速度。同时，因为pooling之前需要一个stride=1的 conv，而与stride=2 conv的计算量想比要高近4倍(<strong>个人理解</strong>)。<h4 id="2-2-网络架构"><a href="#2-2-网络架构" class="headerlink" title="2.2 网络架构"></a>2.2 网络架构</h4></li>
</ul>
</li>
<li><p>DW conv和PW conv<br>MobileNet的网络架构主要是由DW conv和PW conv组成，相比于传统卷积可以降低$\dfrac{1}{N} + \dfrac{1}{Dk}​$倍的计算量。<br>标准卷积与DW conv和PW conv如图所示:<br><img src="/img/ch17/12.png" alt="image"><br>深度可分离卷积与传统卷积运算量对比：<br><img src="/img/ch17/13.png" alt="image"><br>网络结构：<br><img src="/img/ch17/14.png" alt="image"></p>
</li>
<li><p>MobileNets的架构<br><img src="/img/ch17/15.png" alt="image"></p>
</li>
</ul>
<h4 id="2-3-实验结果"><a href="#2-3-实验结果" class="headerlink" title="2.3 实验结果"></a>2.3 实验结果</h4><p><img src="/img/ch17/16.png" alt="image"><br>由上表可知，使用相同的结构，深度可分离卷积虽然准确率降低1%，但是参数量减少了6/7。</p>
<h3 id="17-9-3-MobileNet-v2"><a href="#17-9-3-MobileNet-v2" class="headerlink" title="17.9.3 MobileNet-v2"></a>17.9.3 MobileNet-v2</h3><p>MobileNet-V2是2018年1月公开在arXiv上论文《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》，是对MobileNet-V1的改进，同样是一个轻量化卷积神经网络。</p>
<h4 id="3-1-设计思想"><a href="#3-1-设计思想" class="headerlink" title="3.1 设计思想"></a>3.1 设计思想</h4><ul>
<li>采用Inverted residuals<ul>
<li>为了保证网络可以提取更多的特征，在residual block中第一个1*1 Conv和3*3 DW Conv之前进行通道扩充</li>
</ul>
</li>
<li>Linear bottlenecks<ul>
<li>为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1*1 Conv 不再采用Relu</li>
</ul>
</li>
<li>stride=2的conv不使用short-cut，stride=1的conv使用short-cut</li>
</ul>
<h4 id="3-2-网络架构"><a href="#3-2-网络架构" class="headerlink" title="3.2 网络架构"></a>3.2 网络架构</h4><ul>
<li>Inverted residuals<br>ResNet中Residuals block先经过1*1的Conv layer，把feature map的通道数降下来，再经过3*3 Conv layer，最后经过一个1*1 的Conv layer，将feature map 通道数再“扩张”回去。即采用先压缩，后扩张的方式。而 inverted residuals采用先扩张，后压缩的方式。<br>MobileNet采用DW conv提取特征，由于DW conv本身提取的特征数就少，再经过传统residuals block进行“压缩”，此时提取的特征数会更少，因此inverted residuals对其进行“扩张”，保证网络可以提取更多的特征。<br><img src="/img/ch17/17.png" alt="image"></li>
<li>Linear bottlenecks<br>ReLu激活函数会破坏特征。ReLu对于负的输入，输出全为0，而本来DW conv特征通道已经被“压缩”，再经过ReLu的话，又会损失一部分特征。采用Linear，目的是防止Relu破坏特征。<br><img src="/img/ch17/18.png" alt="image"></li>
<li>shortcut<br>stride=2的conv不使用short-cut，stride=1的conv使用short-cut<br><img src="/img/ch17/19.png" alt="image"></li>
<li>网络架构<br><img src="/img/ch17/20.png" alt="image"></li>
</ul>
<h3 id="17-9-4-Xception"><a href="#17-9-4-Xception" class="headerlink" title="17.9.4 Xception"></a>17.9.4 Xception</h3><p>Xception是Google提出的，arXiv 的V1 于2016年10月公开《Xception: Deep Learning with Depthwise Separable Convolutions 》，Xception是对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。</p>
<h4 id="4-1设计思想"><a href="#4-1设计思想" class="headerlink" title="4.1设计思想"></a>4.1设计思想</h4><ul>
<li>采用depthwise separable convolution来替换原来Inception v3中的卷积操作<br>  与原版的Depth-wise convolution有两个不同之处：<ul>
<li>第一个：原版Depth-wise convolution，先逐通道卷积，再1<em>1卷积; 而Xception是反过来，先1\</em>1卷积，再逐通道卷积；</li>
<li>第二个：原版Depth-wise convolution的两个卷积之间是不带激活函数的，而Xception在经过1*1卷积之后会带上一个Relu的非线性激活函数；</li>
</ul>
</li>
</ul>
<h4 id="4-2网络架构"><a href="#4-2网络架构" class="headerlink" title="4.2网络架构"></a>4.2网络架构</h4><p>feature map在空间和通道上具有一定的相关性，通过Inception模块和非线性激活函数实现通道之间的解耦。增多3*3的卷积的分支的数量，使它与1*1的卷积的输出通道数相等，此时每个3*3的卷积只作用与一个通道的特征图上，作者称之为“极致的Inception（Extream Inception）”模块，这就是Xception的基本模块。<br><img src="/img/ch17/21.png" alt="image"></p>
<h3 id="17-9-5-ShuffleNet-v1"><a href="#17-9-5-ShuffleNet-v1" class="headerlink" title="17.9.5 ShuffleNet-v1"></a>17.9.5 ShuffleNet-v1</h3><p>ShuffleNet 是Face++团队提出的，晚于MobileNet两个月在arXiv上公开《ShuffleNet： An Extremely Efficient Convolutional Neural Network for Mobile Devices 》用于移动端前向部署的网络架构。ShuffleNet基于MobileNet的group思想，将卷积操作限制到特定的输入通道。而与之不同的是，ShuffleNet将输入的group进行打散，从而保证每个卷积核的感受野能够分散到不同group的输入中，增加了模型的学习能力。</p>
<h4 id="5-1-设计思想"><a href="#5-1-设计思想" class="headerlink" title="5.1 设计思想"></a>5.1 设计思想</h4><ul>
<li>采用group conv减少大量参数<ul>
<li>roup conv与DW conv存在相同的“信息流通不畅”问题 </li>
</ul>
</li>
<li>采用channel shuffle解决上述问题<ul>
<li>MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle</li>
</ul>
</li>
<li>采用concat替换add操作<ul>
<li>avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失</li>
</ul>
</li>
</ul>
<h4 id="5-2-网络架构"><a href="#5-2-网络架构" class="headerlink" title="5.2 网络架构"></a>5.2 网络架构</h4><p>MobileNet中1*1卷积的操作占据了约95%的计算量，所以作者将1*1也更改为group卷积，使得相比MobileNet的计算量大大减少。<br><img src="/img/ch17/22.png" alt="image"><br>group卷积与DW存在同样使“通道信息交流不畅”的问题，MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle。<br>ShuffleNet的shuffle操作如图所示<br><img src="/img/ch17/24.png" alt="image"><br>avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失；实验表明：多多使用通道(提升通道的使用率)，有助于提高小模型的准确率。<br><img src="/img/ch17/23.png" alt="image"><br>网络结构：<br><img src="/img/ch17/25.png" alt="image"></p>
<h3 id="17-9-6-ShuffleNet-v2"><a href="#17-9-6-ShuffleNet-v2" class="headerlink" title="17.9.6 ShuffleNet-v2"></a>17.9.6 ShuffleNet-v2</h3><p>huffleNet-v2 是Face++团队提出的《ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design》，旨在设计一个轻量级但是保证精度、速度的深度网络。</p>
<h4 id="6-1-设计思想"><a href="#6-1-设计思想" class="headerlink" title="6.1 设计思想"></a>6.1 设计思想</h4><ul>
<li>文中提出影响神经网络速度的4个因素：<ul>
<li>a. FLOPs(FLOPs就是网络执行了多少multiply-adds操作)</li>
<li>b. MAC(内存访问成本)</li>
<li>c. 并行度(如果网络并行度高，速度明显提升)</li>
<li>d. 计算平台(GPU，ARM)</li>
</ul>
</li>
<li>ShuffleNet-v2 提出了4点网络结构设计策略：<ul>
<li>G1.输入输出的channel相同时，MAC最小</li>
<li>G2.过度的组卷积会增加MAC</li>
<li>G3.网络碎片化会降低并行度</li>
<li>G4.元素级运算不可忽视  </li>
</ul>
</li>
</ul>
<h4 id="6-2-网络结构"><a href="#6-2-网络结构" class="headerlink" title="6.2 网络结构"></a>6.2 网络结构</h4><p>depthwise convolution 和 瓶颈结构增加了 MAC，用了太多的 group，跨层连接中的 element-wise Add 操作也是可以优化的点。所以在 shuffleNet V2 中增加了几种新特性。<br>所谓的 channel split 其实就是将通道数一分为2，化成两分支来代替原先的分组卷积结构（G2），并且每个分支中的卷积层都是保持输入输出通道数相同（G1），其中一个分支不采取任何操作减少基本单元数（G3），最后使用了 concat 代替原来的 elementy-wise add，并且后面不加 ReLU 直接（G4），再加入channle shuffle 来增加通道之间的信息交流。 对于下采样层，在这一层中对通道数进行翻倍。 在网络结构的最后，即平均值池化层前加入一层 1x1 的卷积层来进一步的混合特征。<br><img src="/img/ch17/26.png" alt="image"><br>网络结构<br><img src="/img/ch17/27.png" alt="image"></p>
<h4 id="6-4-ShuffleNet-v2具有高精度的原因"><a href="#6-4-ShuffleNet-v2具有高精度的原因" class="headerlink" title="6.4  ShuffleNet-v2具有高精度的原因"></a>6.4  ShuffleNet-v2具有高精度的原因</h4><ul>
<li>由于高效，可以增加更多的channel，增加网络容量</li>
<li>采用split使得一部分特征直接与下面的block相连，特征复用(DenseNet)</li>
</ul>
<h2 id="17-10-现有移动端开源框架及其特点"><a href="#17-10-现有移动端开源框架及其特点" class="headerlink" title="17.10 现有移动端开源框架及其特点"></a>17.10 现有移动端开源框架及其特点</h2><h3 id="17-10-1-NCNN"><a href="#17-10-1-NCNN" class="headerlink" title="17.10.1 NCNN"></a>17.10.1 NCNN</h3><p>１、开源时间：2017年7月　　　</p>
<p>２、开源用户：腾讯优图　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/ncnn" target="_blank" rel="noopener">https://github.com/Tencent/ncnn</a> 　　</p>
<p>4、特点：</p>
<ul>
<li>1）NCNN考虑了手机端的硬件和系统差异以及调用方式，架构设计以手机端运行为主要原则。</li>
<li>2）无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架（以开源时间为参照对象）。</li>
<li>3）基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP。   </li>
</ul>
<p>5、功能：    </p>
<ul>
<li>1、NCNN支持卷积神经网络、多分支多输入的复杂网络结构，如vgg、googlenet、resnet、squeezenet 等。</li>
<li>2、NCNN无需依赖任何第三方库。    </li>
<li>3、NCNN全部使用C/C++实现，以及跨平台的cmake编译系统，可轻松移植到其他系统和设备上。    </li>
<li>4、汇编级优化，计算速度极快。使用ARM NEON指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。 </li>
<li>5、精细的数据结构设计，没有采用需消耗大量内存的通常框架——im2col + 矩阵乘法，使得内存占用极低。   </li>
<li>6、支持多核并行计算，优化CPU调度。   </li>
<li>7、整体库体积小于500K，可精简到小于300K。   </li>
<li>8、可扩展的模型设计，支持8bit 量化和半精度浮点存储。   </li>
<li>9、支持直接内存引用加载网络模型。   </li>
<li>10、可注册自定义层实现并扩展。   </li>
</ul>
<p>6、NCNN在Android端部署示例</p>
<ul>
<li>1）选择合适的Android Studio版本并安装。</li>
<li>2）根据需求选择NDK版本并安装。</li>
<li>3）在Android Studio上配置NDK的环境变量。</li>
<li>4）根据自己需要编译NCNN sdk</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir build-android cd build-android cmake -DCMAKE_TOOLCHAIN_FILE&#x3D;$ANDROID_NDK&#x2F;build&#x2F;cmake&#x2F;android.toolchain.cmake \ -DANDROID_ABI&#x3D;&quot;armeabi-v7a&quot; -DANDROID_ARM_NEON&#x3D;ON \ -DANDROID_PLATFORM&#x3D;android-14 .. make make install</span><br></pre></td></tr></table></figure>
<p>​    安装完成之后，install下有include和lib两个文件夹。</p>
<p>​    备注：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ANDROID_ABI 是架构名字，&quot;armeabi-v7a&quot; 支持绝大部分手机硬件 </span><br><span class="line">ANDROID_ARM_NEON 是否使用 NEON 指令集，设为 ON 支持绝大部分手机硬件 </span><br><span class="line">ANDROID_PLATFORM 指定最低系统版本，&quot;android-14&quot; 就是 android-4.0</span><br></pre></td></tr></table></figure>
<ul>
<li>5）进行NDK开发。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1）assets文件夹下放置你的bin和param文件。</span><br><span class="line">2）jni文件夹下放置你的cpp和mk文件。</span><br><span class="line">3）修改你的app gradle文件。</span><br><span class="line">4）配置Android.mk和Application.mk文件。</span><br><span class="line">5）进行java接口的编写。</span><br><span class="line">6）读取拷贝bin和param文件（有些则是pb文件，根据实际情况）。</span><br><span class="line">7）进行模型的初始化和执行预测等操作。</span><br><span class="line">8）build。</span><br><span class="line">9）cd到src&#x2F;main&#x2F;jni目录下，执行ndk-build，生成.so文件。</span><br><span class="line">10）接着就可写自己的操作处理需求。</span><br></pre></td></tr></table></figure>
<h3 id="17-10-2-QNNPACK"><a href="#17-10-2-QNNPACK" class="headerlink" title="17.10.2 QNNPACK"></a>17.10.2 QNNPACK</h3><p>全称：Quantized Neural Network PACKage（量化神经网络包）　　　</p>
<p>１、开源时间：2018年10月　　　</p>
<p>２、开源用户：Facebook　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/pytorch/QNNPACK" target="_blank" rel="noopener">https://github.com/pytorch/QNNPACK</a>　　　　</p>
<p>４、特点：　　　</p>
<p>​    １）低密度卷积优化函数库；　　　</p>
<p>　    ２）可在手机上实时运行Mask R-CNN 和 DensePose;</p>
<p>​    ３） 能在性能受限的移动设备中用 100ms 以内的时间实施图像分类；　　　</p>
<p>5、QNNPACK 如何提高效率？</p>
<p>1)<strong>QNNPACK 使用与安卓神经网络 API 兼容的线性量化方案</strong></p>
<p>QNNPACK 的输入矩阵来自低精度、移动专用的计算机视觉模型。其它库在计算A和B矩阵相乘时，重新打包 A 和 B 矩阵以更好地利用缓存层次结构，希望在大量计算中分摊打包开销，QNNPACK 删除所有计算非必需的内存转换，针对 A和B矩阵相乘适用于一级缓存的情况进行了优化。</p>
<p><img src="/img/ch17/QNNPACK1.jpeg" alt=""></p>
<p>​    1）优化了L1缓存计算，不需要输出中间结果，直接输出最终结果，节省内存带宽和缓存占用。</p>
<p>具体分析：</p>
<ul>
<li>常规实现：在量化矩阵-矩阵乘法中，8位整数的乘积通常会被累加至 32 位的中间结果中，随后重新量化以产生 8 位的输出。遇到大矩阵尺寸时，比如有时K太大，A和B的面板无法直接转入缓存，此时，需利用缓存层次结构，借助GEMM将A和B的面板沿着K维分割成固定大小的子面板，以便于每个子面板都能适应L1缓存，随后为每个子面板调用微内核。这一缓存优化需要 PDOT 为内核输出 32  位中间结果，最终将它们相加并重新量化为 8 位整数。</li>
<li>优化实现：由于  ONNPACK 对于面板 A 和 B 总是适应 L1 缓存的移动神经网络进行了优化，因此它在调用微内核时处理整个 A 和 B  的面板。而由于无需在微内核之外积累 32 位的中间结果，QNNPACK 会将 32 位的中间结果整合进微内核中并写出 8  位值，这节省了内存带宽和缓存占用。</li>
</ul>
<p><img src="/img/ch17/QNNPACK2.jpeg" alt=""></p>
<p>​    2）取消了矩阵 A 的重新打包。</p>
<ul>
<li><p>常规实现：</p>
<pre><code>  矩阵 B 包含静态权重，可以一次性转换成任何内存布局，但矩阵  A 包含卷积输入，每次推理运行都会改变。因此，重新打包矩阵 A 在每次运行时都会产生开销。尽管存在开销，传统的 GEMM实现还是出于以下两个原因对矩阵 A 进行重新打包：

  a 缓存关联性及微内核效率受限。如果不重新打包，微内核将不得不读取被潜在的大跨距隔开的几行A。如果这个跨距恰好是 2 的许多次幂的倍数，面板中不同行 A  的元素可能会落入同一缓存集中。如果冲突的行数超过了缓存关联性，它们就会相互驱逐，性能也会大幅下降。

  b 打包对微内核效率的影响与当前所有移动处理器支持的  SIMD  向量指令的使用密切相关。这些指令加载、存储或者计算小型的固定大小元素向量，而不是单个标量（scalar）。在矩阵相乘中，充分利用向量指令达到高性能很重要。在传统的  GEMM 实现中，微内核把 MR 元素重新打包到向量暂存器里的 MR 线路中。
</code></pre></li>
<li><p>优化实现：</p>
<pre><code>  a 当面板适配一级缓存时，不会存在缓存关联性及微内核效率受限的问题。

  b 在 QNNPACK 实现中，MR  元素在存储中不是连续的，微内核需要把它们加载到不同的向量暂存器中。越来越大的暂存器压力迫使 QNNPACK 使用较小的 MRxNR  拼贴，但实际上这种差异很小，而且可以通过消除打包开销来补偿。例如，在 32 位 ARM 架构上，QNNPACK 使用 4×8 微内核，其中  57% 的向量指令是乘-加；另一方面，gemmlowp 库使用效率稍高的 4×12 微内核，其中 60% 的向量指令是乘-加。微内核加载 A  的多个行，乘以 B 的满列，结果相加，然后完成再量化并记下量化和。A 和 B 的元素被量化为 8 位整数，但乘积结果相加到 32 位。大部分  ARM 和 ARM64 处理器没有直接完成这一运算的指令，所以它必须分解为多个支持运算。QNNPACK  提供微内核的两个版本，其不同之处在于用于乘以 8 位值并将它们累加到 32 位的指令序列。
</code></pre></li>
</ul>
<p>2)<strong>从矩阵相乘到卷积</strong></p>
<p><img src="/img/ch17/QNNPACK3.jpeg" alt=""></p>
<p>​    传统实现：</p>
<p>​    简单的 1×1  卷积可直接映射到矩阵相乘</p>
<p>​    但对于具备较大卷积核、padding 或子采样（步幅）的卷积而言则并非如此。但是，这些较复杂的卷积能够通过记忆变换  im2col 映射到矩阵相乘。对于每个输出像素，im2col 复制输入图像的图像块并将其计算为 2D 矩阵。由于每个输出像素都受 KHxKWxC  输入像素值的影响（KH 和 KW 分别指卷积核的高度和宽度，C 指输入图像中的通道数），因此该矩阵的大小是输入图像的 KHxKW  倍，im2col 给内存占用和性能都带来了一定的开销。和 Caffe 一样，大部分深度学习框架转而使用基于 im2col  的实现，利用现有的高度优化矩阵相乘库来执行卷积操作。</p>
<p>​    优化实现：</p>
<p>​    Facebook  研究者在 QNNPACK 中实现了一种更高效的算法。</p>
<ul>
<li>他们没有变换卷积输入使其适应矩阵相乘的实现，而是调整 PDOT 微内核的实现，在运行中执行  im2col 变换。这样就无需将输入张量的实际输入复制到 im2col 缓存，而是使用输入像素行的指针设置 indirection  buffer，输入像素与每个输出像素的计算有关。</li>
<li>研究者还修改了矩阵相乘微内核，以便从 indirection buffer  加载虚构矩阵（imaginary matrix）A 的行指针，indirection buffer 通常比 im2col buffer  小得多。</li>
<li>此外，如果两次推断运行的输入张量存储位置不变，则 indirection buffer  还可使用输入张量行的指针进行初始化，然后在多次推断运行中重新使用。研究者观察到具备 indirection buffer 的微内核不仅消除了  im2col 变换的开销，其性能也比矩阵相乘微内核略好（可能由于输入行在计算不同输出像素时被重用）。</li>
</ul>
<p>3)<strong>深度卷积</strong></p>
<p><img src="/img/ch17/QNNPACK4.jpeg" alt=""></p>
<p>分组卷积（grouped   convolution）将输入和输出通道分割成多组，然后对每个组进行分别处理。在有限条件下，当组数等于通道数时，该卷积就是深度卷积，常用于当前的神经网络架构中。深度卷积对每个通道分别执行空间滤波，展示了与正常卷积非常不同的计算模式。因此，通常要向深度卷积提供单独实现，QNNPACK  包括一个高度优化版本 3×3 深度卷积。</p>
<p>深度卷积的传统实现是每次都在卷积核元素上迭代，然后将一个卷积核行和一个输入行的结果累加到输出行。对于一个  3×3 的深度卷积，此类实现将把每个输出行更新 9 次。在 QNNPACK 中，研究者计算所有 3×3 卷积核行和 3×3  输入行的结果，一次性累加到输出行，然后再处理下个输出行。</p>
<p>QNNPACK  实现高性能的关键因素在于完美利用通用暂存器（GPR）来展开卷积核元素上的循环，同时避免在 hot loop 中重新加载地址寄存器。32-bit  ARM 架构将实现限制在 14 个 GPR。在 3×3 深度卷积中，需要读取 9 个输入行和 9 个卷积核行。这意味着如果想完全展开循环必须存储  18 个地址。然而，实践中推断时卷积核不会发生变化。因此 Facebook 研究者使用之前在 CxKHxKW 中的滤波器，将它们封装进  [C/8]xKWxKHx8，这样就可以仅使用具备地址增量（address increment）的一个 GPR 访问所有滤波器。（研究者使用数字 8  的原因在于，在一个命令中加载 8 个元素然后减去零，在 128-bit NEON 暂存器中生成 8 个 16-bit 值。）然后使用 9  个输入行指针，指针将滤波器重新装进 10 个 GPR，完全展开滤波器元素上的循环。64-bit ARM 架构相比 32-bit 架构，GPR  的数量翻了一倍。QNNPACK 利用额外的 ARM64 GPR，一次性存储 3×5 输入行的指针，并计算 3 个输出行。</p>
<p>7、性能优势：</p>
<p>​    测试结果显示出 QNNPACK 在端到端基准上的性能优势。在量化当前最优 MobileNetV2 架构上，基于QNNPACK 的 Caffe2 算子的速度大约是 TensorFlow Lite 速度的 2 倍，在多种手机上都是如此。除了 QNNPACK 之外，Facebook 还开源了 Caffe2 quantized MobileNet v2 模型，其 top-1 准确率比相应的 TensorFlow 模型高出 1.3%。    </p>
<p><strong>MobileNetV1</strong></p>
<p>MobileNetV1  架构在使用深度卷积（depthwise convolution）使模型更适合移动设备方面具备开创性。MobileNetV1 包括几乎整个  1×1 卷积和 3×3 卷积。Facebook 研究者将量化 MobileNetV1 模型从 TensorFlow Lite 转换而来，并在  TensorFlow Lite 和 QNNPACK 的 32-bit ARM 设备上对 MobileNetV1 进行基准测试。二者运行时均使用 4  线程，研究者观察到 QNNPACK 的运行速度几何平均值是 TensorFlow Lite 的 1.8 倍。</p>
<p><img src="/img/ch17/mv1.jpg" alt=""></p>
<p><strong>MobileNetV2</strong></p>
<p>作为移动视觉任务的当前最优架构之一，MobileNetV2  引入了瓶颈构造块和瓶颈之间的捷径连接。研究者在 MobileNetV2 分类模型的量化版上对比基于 QNNPACK 的 Caffe2 算子和  TensorFlow Lite 实现。使用的量化 Caffe2 MobileNetV2 模型已开源，量化 TensorFlow Lite  模型来自官方库：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md。下表展示了二者在常用测试集上的" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md。下表展示了二者在常用测试集上的</a>  top1 准确率：</p>
<p><img src="/img/ch17/mv2.jpg" alt=""></p>
<p>​    Facebook 研究者利用这些模型建立了 Facebook AI 性能评估平台（<a href="https://github.com/facebook/FAI-PEP）的基准，该基准基于" target="_blank" rel="noopener">https://github.com/facebook/FAI-PEP）的基准，该基准基于</a> 32-bit ARM 环境的大量手机设备。对于 TensorFlow Lite 线程设置，研究者尝试了一到四个线程，并报告了最快速的结果。结果显示 TensorFlow Lite 使用四线程的性能最优，因此后续研究中使用四线程来对比 TensorFlow Lite 和 QNNPACK。下表展示了结果，以及在典型智能手机和高端机上，基于 QNNPACK 的算子速度比 TensorFlow Lite 快得多。</p>
<p><img src="/img/ch17/mv3.jpg" alt=""></p>
<p>Facebook开源高性能内核库QNNPACK<br><a href="https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1615725346726413945&amp;wfr=spider&amp;for=pc</a><br><a href="http://www.sohu.com/a/272158070_610300" target="_blank" rel="noopener">http://www.sohu.com/a/272158070_610300</a></p>
<p>支持移动端深度学习的几种开源框架<br><a href="https://blog.csdn.net/zchang81/article/details/74280019" target="_blank" rel="noopener">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="17-10-3-Prestissimo"><a href="#17-10-3-Prestissimo" class="headerlink" title="17.10.3 Prestissimo"></a>17.10.3 Prestissimo</h3><p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：九言科技　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/in66-dev/In-Prestissimo" target="_blank" rel="noopener">https://github.com/in66-dev/In-Prestissimo</a>　　</p>
<p>４、功能特点：　</p>
<p><strong>基础功能</strong></p>
<ul>
<li>支持卷积神经网络，支持多输入和多分支结构</li>
<li>精炼简洁的API设计，使用方便</li>
<li>提供调试接口，支持打印各个层的数据以及耗时</li>
<li>不依赖任何第三方计算框架，整体库体积 500K 左右（32位 约400k，64位 约600k）</li>
<li>纯 C++ 实现，跨平台，支持 android 和 ios</li>
<li>模型为纯二进制文件，不暴露开发者设计的网络结构</li>
</ul>
<p><strong>极快的速度</strong></p>
<ul>
<li>大到框架设计，小到汇编书写上全方位的优化，iphone7 上跑 SqueezeNet 仅需 26ms（单线程）</li>
<li>支持浮点(float)和整型(int)两种运算模式，float模式精度与caffe相同，int模式运算速度快，大部分网络用int的精度便已经足够</li>
<li>以巧妙的内存布局提升cpu的cache命中率，在中低端机型上性能依然强劲</li>
<li>针对 float-arm32, float-arm64, int-arm32, int-arm64 四个分支均做了细致的优化，保证arm32位和arm64位版本都有非常好的性能</li>
</ul>
<p><strong>SqueezeNet-v1.1 测试结果</strong></p>
<p><strong>Note</strong>: 手机测试性能存在一定的抖动，连续多次运算取平均时间</p>
<p><strong>Note</strong>: 像华为mate8, mate9，Google nexus 6 虽然是64位的CPU，但测试用的是 32位的库，因此cpu架构依然写 arm-v7a</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">CPU架构</th>
<th style="text-align:center">机型</th>
<th style="text-align:center">CPU</th>
<th style="text-align:center">ncnn（4线程）</th>
<th style="text-align:center">mdl</th>
<th style="text-align:center">Prestissimo_float(单线程)</th>
<th style="text-align:center">Prestissimo_int(单线程)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">小米2</td>
<td style="text-align:center">高通APQ8064 1.5GHz</td>
<td style="text-align:center">185 ms</td>
<td style="text-align:center">370 ms</td>
<td style="text-align:center">184 ms</td>
<td style="text-align:center">115 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">小米2s</td>
<td style="text-align:center">四核 骁龙APQ8064 Pro 1.7GHz</td>
<td style="text-align:center">166 ms</td>
<td style="text-align:center">-</td>
<td style="text-align:center">136 ms</td>
<td style="text-align:center">96 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">红米Note 4x</td>
<td style="text-align:center">骁龙625 四核2.0GHz</td>
<td style="text-align:center">124 ms</td>
<td style="text-align:center">306 ms</td>
<td style="text-align:center">202 ms</td>
<td style="text-align:center">110 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">Google Nexus 6</td>
<td style="text-align:center">骁龙805 四核 2.7GHz</td>
<td style="text-align:center">84 ms</td>
<td style="text-align:center">245 ms</td>
<td style="text-align:center">103 ms</td>
<td style="text-align:center">63 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">Vivo x6d</td>
<td style="text-align:center">联发科 MT6752 1.7GHz</td>
<td style="text-align:center">245 ms</td>
<td style="text-align:center">502 ms</td>
<td style="text-align:center">370 ms</td>
<td style="text-align:center">186 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">华为 Mate 8</td>
<td style="text-align:center">海思麒麟950 4大4小 2.3GHz 1.8GHz</td>
<td style="text-align:center">75 ms</td>
<td style="text-align:center">180 ms</td>
<td style="text-align:center">95 ms</td>
<td style="text-align:center">57 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v7a</td>
<td style="text-align:center">华为 Mate 9</td>
<td style="text-align:center">海思麒麟960 4大4小 2.4GHz 1.8GHz</td>
<td style="text-align:center">61 ms</td>
<td style="text-align:center">170 ms</td>
<td style="text-align:center">94 ms</td>
<td style="text-align:center">48 ms</td>
</tr>
<tr>
<td style="text-align:center">arm-v8</td>
<td style="text-align:center">iphone7</td>
<td style="text-align:center">Apple A10 Fusion 2.34GHz</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">27 ms</td>
<td style="text-align:center">26 ms</td>
</tr>
</tbody>
</table>
</div>
<p><strong>未开放特性</strong></p>
<ul>
<li>多核并行加速（多核机器可以再提升30%-100% 的速度）</li>
<li>depthwise卷积运算（支持mobilenet）</li>
<li>模型压缩功能，压缩后的模型体积可缩小到20%以下</li>
<li>GPU 运算模式（Android 基于opengl es 3.1，ios 基于metal）</li>
</ul>
<p><strong>同类框架对比</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">框架</th>
<th style="text-align:center">caffe</th>
<th style="text-align:center">tensorflow</th>
<th style="text-align:center">mdl-android</th>
<th style="text-align:center">mdl-ios</th>
<th style="text-align:center">ncnn</th>
<th style="text-align:center">CoreML</th>
<th style="text-align:center">Prestissimo</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">计算硬件</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">gpu</td>
<td style="text-align:center">cpu</td>
<td style="text-align:center">gpu</td>
<td style="text-align:center">cpu （gpu版本未开放）</td>
</tr>
<tr>
<td style="text-align:center">计算速度</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">很快</td>
<td style="text-align:center">很快</td>
<td style="text-align:center">极快</td>
<td style="text-align:center">极快</td>
</tr>
<tr>
<td style="text-align:center">库大小</td>
<td style="text-align:center">大</td>
<td style="text-align:center">较大</td>
<td style="text-align:center">中等</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
<td style="text-align:center">小</td>
</tr>
<tr>
<td style="text-align:center">兼容性</td>
<td style="text-align:center">好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">限ios8以上</td>
<td style="text-align:center">很好</td>
<td style="text-align:center">仅支持 ios11</td>
<td style="text-align:center">很好</td>
</tr>
<tr>
<td style="text-align:center">模型支持度</td>
<td style="text-align:center">很好</td>
<td style="text-align:center">好</td>
<td style="text-align:center">-</td>
<td style="text-align:center">差（仅限指定模型）</td>
<td style="text-align:center">较好</td>
<td style="text-align:center">-</td>
<td style="text-align:center">中等（当前版本不支持mobilenet）</td>
</tr>
</tbody>
</table>
</div>
<p><strong>使用方法-模型转换</strong></p>
<p>绝影支持的是私有的模型文件格式，需要把 caffe 训练出来的模型转换为 .prestissimo 格式，模型转换工具为 caffe2Prestissimo.out。caffe2Prestissimo.out 依赖 protobuf 3.30。将 XXX.prototxt 和 YYY.caffemodel 转化为 Prestissimo 模型 ZZZ.prestissimo：（得到）./caffe2Prestissimo.out XXX.prototxt YYY.caffemodel ZZZ.prestissimo</p>
<h3 id="17-10-4-MDL（mobile-deep-learning）"><a href="#17-10-4-MDL（mobile-deep-learning）" class="headerlink" title="17.10.4 MDL（mobile-deep-learning）"></a>17.10.4 MDL（mobile-deep-learning）</h3><p>１、开源时间：2017年9月（已暂停更新）　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/allonli/mobile-deep-learning" target="_blank" rel="noopener">https://github.com/allonli/mobile-deep-learning</a></p>
<p>４、功能特点：</p>
<ul>
<li>一键部署，脚本参数就可以切换ios或者android</li>
<li>支持iOS  gpu运行MobileNet、squeezenet模型</li>
<li>已经测试过可以稳定运行MobileNet、GoogLeNet v1、squeezenet、ResNet-50模型</li>
<li>体积极小，无任何第三方依赖。纯手工打造。</li>
<li>提供量化函数，对32位float转8位uint直接支持，模型体积量化后4M上下</li>
<li>与ARM相关算法团队线上线下多次沟通，针对ARM平台会持续优化</li>
<li>NEON使用涵盖了卷积、归一化、池化所有方面的操作</li>
<li>汇编优化，针对寄存器汇编操作具体优化</li>
<li>loop unrolling 循环展开，为提升性能减少不必要的CPU消耗，全部展开判断操作</li>
<li>将大量繁重的计算任务前置到overhead过程</li>
</ul>
<p>5、框架结构</p>
<p><img src="/img/ch17/MDL1.png" alt=""></p>
<p>MDL 框架主要包括：<strong>模型转换模块（MDL Converter）、模型加载模块（Loader）、网络管理模块（Net）、矩阵运算模块（Gemmers）及供 Android 端调用的 JNI 接口层（JNI Interfaces）。</strong></p>
<p>​    其中，模型转换模块主要负责将Caffe 模型转为 MDL 模型，同时支持将 32bit 浮点型参数量化为 8bit 参数，从而极大地压缩模型体积；模型加载模块主要完成模型的反量化及加载校验、网络注册等过程，网络管理模块主要负责网络中各层 Layer 的初始化及管理工作；MDL 提供了供 Android 端调用的 JNI 接口层，开发者可以通过调用 JNI 接口轻松完成加载及预测过程。</p>
<p>6、MDL 的性能及兼容性</p>
<ul>
<li>体积 armv7 300k+</li>
<li>速度 iOS GPU mobilenet 可以达到 40ms、squeezenet 可以达到 30ms</li>
</ul>
<p>​        MDL  从立项到开源，已经迭代了一年多。移动端比较关注的多个指标都表现良好，如体积、功耗、速度。百度内部产品线在应用前也进行过多次对比，和已开源的相关项目对比，MDL  能够在保证速度和能耗的同时支持多种深度学习模型，如 mobilenet、googlenet v1、squeezenet 等，且具有 iOS  GPU 版本，squeezenet 一次运行最快可以达到 3-40ms。</p>
<p><strong>同类框架对比</strong></p>
<p>​     框架Caffe2TensorFlowncnnMDL(CPU)MDL(GPU)硬件CPUCPUCPUCPUGPU速度慢慢快快极快体积大大小小小兼容Android&amp;iOSAndroid&amp;iOSAndroid&amp;iOSAndroid&amp;iOSiOS</p>
<p>​     与支持 CNN 的移动端框架对比，MDL 速度快、性能稳定、兼容性好、demo 完备。</p>
<p><strong>兼容性</strong></p>
<p>​     MDL 在 iOS 和 Android 平台均可以稳定运行，其中 iOS10 及以上平台有基于 GPU 运算的 API，性能表现非常出色，在 Android 平台则是纯 CPU 运行。高中低端机型运行状态和手机百度及其他 App 上的覆盖都有绝对优势。</p>
<p>​     MDL 同时也支持 Caffe 模型直接转换为 MDL 模型。</p>
<h3 id="17-10-5-Paddle-Mobile"><a href="#17-10-5-Paddle-Mobile" class="headerlink" title="17.10.5 Paddle-Mobile"></a>17.10.5 Paddle-Mobile</h3><p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：百度　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/PaddlePaddle/paddle-mobile" target="_blank" rel="noopener">https://github.com/PaddlePaddle/paddle-mobile</a>　</p>
<p>４、功能特点：</p>
<p><strong>功能特点</strong></p>
<ul>
<li><p>高性能支持ARM CPU </p>
</li>
<li><p>支持Mali GPU</p>
</li>
<li><p>支持Andreno GPU</p>
</li>
<li><p>支持苹果设备的GPU Metal实现</p>
</li>
<li><p>支持ZU5、ZU9等FPGA开发板</p>
</li>
<li><p>支持树莓派等arm-linux开发板</p>
</li>
</ul>
<h3 id="17-10-6-MACE（-Mobile-AI-Compute-Engine）"><a href="#17-10-6-MACE（-Mobile-AI-Compute-Engine）" class="headerlink" title="17.10.6 MACE（ Mobile AI Compute Engine）"></a>17.10.6 MACE（ Mobile AI Compute Engine）</h3><p>１、开源时间：2018年4月(持续更新，v0.9.0 (2018-07-20))　　　</p>
<p>２、开源用户：小米　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/XiaoMi/mace" target="_blank" rel="noopener">https://github.com/XiaoMi/mace</a>    </p>
<p>４、简介：Mobile AI Compute Engine (MACE) 是一个专为移动端异构计算设备优化的深度学习前向预测框架。<br>MACE覆盖了常见的移动端计算设备（CPU，GPU和DSP），并且提供了完整的工具链和文档，用户借助MACE能够很方便地在移动端部署深度学习模型。MACE已经在小米内部广泛使用并且被充分验证具有业界领先的性能和稳定性。</p>
<p>5、MACE的基本框架：</p>
<p><img src="/img/ch17/mace-arch.png" alt=""></p>
<p><strong>MACE Model</strong></p>
<p>MACE定义了自有的模型格式（类似于Caffe2），通过MACE提供的工具可以将Caffe和TensorFlow的模型 转为MACE模型。</p>
<p><strong>MACE Interpreter</strong></p>
<p>MACE Interpreter主要负责解析运行神经网络图（DAG）并管理网络中的Tensors。</p>
<p><strong>Runtime</strong></p>
<p>CPU/GPU/DSP Runtime对应于各个计算设备的算子实现。</p>
<p>6、MACE使用的基本流程</p>
<p><img src="/img/ch17/mace-work-flow-zh.png" alt=""></p>
<p><strong>1. 配置模型部署文件(.yml)</strong></p>
<p>模型部署文件详细描述了需要部署的模型以及生成库的信息，MACE根据该文件最终生成对应的库文件。</p>
<p><strong>2.编译MACE库</strong></p>
<p>编译MACE的静态库或者动态库。</p>
<p><strong>3.转换模型</strong></p>
<p>将TensorFlow 或者 Caffe的模型转为MACE的模型。</p>
<p><strong>4.1. 部署</strong></p>
<p>根据不同使用目的集成Build阶段生成的库文件，然后调用MACE相应的接口执行模型。</p>
<p><strong>4.2. 命令行运行</strong></p>
<p>MACE提供了命令行工具，可以在命令行运行模型，可以用来测试模型运行时间，内存占用和正确性。</p>
<p><strong>4.3. Benchmark</strong></p>
<p>MACE提供了命令行benchmark工具，可以细粒度的查看模型中所涉及的所有算子的运行时间。</p>
<p>7、MACE在哪些角度������行了优化?</p>
<p><strong>MACE</strong> 专为移动端异构计算平台优化的神经网络计算框架。主要从以下的角度做了专门的优化：</p>
<ul>
<li>性能<ul>
<li>代码经过NEON指令，OpenCL以及Hexagon HVX专门优化，并且采用<br><a href="https://arxiv.org/abs/1509.09308" target="_blank" rel="noopener">Winograd算法</a>来进行卷积操作的加速。<br>此外，还对启动速度进行了专门的优化。</li>
</ul>
</li>
<li><p>功耗</p>
<ul>
<li>支持芯片的功耗管理，例如ARM的big.LITTLE调度，以及高通Adreno GPU功耗选项。</li>
</ul>
</li>
<li>系统响应<ul>
<li>支持自动拆解长时间的OpenCL计算任务，来保证UI渲染任务能够做到较好的抢占调度，<br>从而保证系统UI的相应和用户体验。</li>
</ul>
</li>
<li>内存占用<ul>
<li>通过运用内存依赖分析技术，以及内存复用，减少内存的占用。另外，保持尽量少的外部<br>依赖，保证代码尺寸精简。</li>
</ul>
</li>
<li><p>模型加密与保护</p>
<ul>
<li>模型保护是重要设计目标之一。支持将模型转换成C++代码，以及关键常量字符混淆，增加逆向的难度。</li>
</ul>
</li>
<li>硬件支持范围<ul>
<li>支持高通，联发科，以及松果等系列芯片的CPU，GPU与DSP(目前仅支持Hexagon)计算加速。</li>
<li>同时支持在具有POSIX接口的系统的CPU上运行。</li>
</ul>
</li>
</ul>
<p>8、性能对比：</p>
<p>MACE 支持 TensorFlow 和 Caffe 模型，提供转换工具，可以将训练好的模型转换成专有的模型数据文件，同时还可以选择将模型转换成C++代码，支持生成动态库或者静态库，提高模型保密性。</p>
<p><img src="/img/ch17/maca_com.jpg" alt=""></p>
<h3 id="17-10-7-FeatherCNN"><a href="#17-10-7-FeatherCNN" class="headerlink" title="17.10.7 FeatherCNN"></a>17.10.7 FeatherCNN</h3><p>１、开源时间：持续更新，已到3.0版本　　　</p>
<p>２、开源用户：腾讯AI　　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/FeatherCNN" target="_blank" rel="noopener">https://github.com/Tencent/FeatherCNN</a></p>
<p>４、功能特点：</p>
<p><strong>FeatherCNN 是由腾讯 AI 平台部研发的基于 ARM 架构的高效 CNN 推理库，该项目支持 Caffe 模型，且具有高性能、易部署、轻量级三大特性。</strong></p>
<p><strong>该项目具体特性如下：</strong></p>
<ul>
<li><p>高性能：无论是在移动设备（iOS / Android），嵌入式设备（Linux）还是基于 ARM 的服务器（Linux）上，FeatherCNN 均能发挥最先进的推理计算性能；</p>
</li>
<li><p>易部署：FeatherCNN 的所有内容都包含在一个代码库中，以消除第三方依赖关系。因此，它便于在移动平台上部署。FeatherCNN 自身的模型格式与 Caffe 模型完全兼容。</p>
</li>
<li><p>轻量级：编译后的 FeatherCNN 库的体积仅为数百 KB。</p>
</li>
</ul>
<h3 id="17-10-8-TensorFlow-Lite"><a href="#17-10-8-TensorFlow-Lite" class="headerlink" title="17.10.8 TensorFlow Lite"></a>17.10.8 TensorFlow Lite</h3><p>１、开源时间：2017年11月　　　</p>
<p>２、开源用户：谷歌　　　</p>
<p>３、GitHub地址：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite</a></p>
<p>４、简介：</p>
<p>Google 表示 Lite 版本 TensorFlow 是 TensorFlow Mobile 的一个延伸版本。此前，通过TensorFlow Mobile API，TensorFlow已经支持手机上的模型嵌入式部署。TensorFlow Lite应该被视为TensorFlow Mobile的升级版。</p>
<p>TensorFlow Lite可以与Android 8.1中发布的神经网络API完美配合，即便在没有硬件加速时也能调用CPU处理，确保模型在不同设备上的运行。 而Android端版本演进的控制权是掌握在谷歌手中的，从长期看，TensorFlow Lite会得到Android系统层面上的支持。</p>
<p>5、架构：</p>
<p><img src="/img/ch17/tflite_artc.JPEG" alt=""></p>
<p>其组件包括：</p>
<ul>
<li>TensorFlow 模型（TensorFlow Model）：保存在磁盘中的训练模型。</li>
<li>TensorFlow Lite 转化器（TensorFlow Lite Converter）：将模型转换成 TensorFlow Lite 文件格式的项目。</li>
<li>TensorFlow Lite 模型文件（TensorFlow Lite Model File）：基于 FlatBuffers，适配最大速度和最小规模的模型。</li>
</ul>
<p>6、移动端开发步骤：</p>
<p>Android Studio 3.0, SDK Version API26, NDK Version 14</p>
<p>步骤：</p>
<ol>
<li><p>将此项目导入到Android Studio：<br> <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/java/demo</a></p>
</li>
<li><p>下载移动端的模型（model）和标签数据（lables）：<br> <a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip" target="_blank" rel="noopener">https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip</a></p>
</li>
<li><p>下载完成解压mobilenet_v1_224_android_quant_2017_11_08.zip文件得到一个xxx.tflite和labes.txt文件，分别是模型和标签文件，并且把这两个文件复制到assets文件夹下。</p>
</li>
<li><p>构建app，run……</p>
</li>
</ol>
<p>17.7.9 TensorFlow Lite和TensorFlow Mobile的区别？</p>
<ul>
<li>TensorFlow Lite是TensorFlow Mobile的进化版。</li>
<li>在大多数情况下，TensorFlow Lite拥有跟小的二进制大小，更少的依赖以及更好的性能。</li>
<li>相比TensorFlow Mobile是对完整TensorFlow的裁减，TensorFlow Lite基本就是重新实现了。从内部实现来说，在TensorFlow内核最基本的OP，Context等数据结构，都是新的。从外在表现来说，模型文件从PB格式改成了FlatBuffers格式，TensorFlow的size有大幅度优化，降至300K，然后提供一个converter将普通TensorFlow模型转化成TensorFlow Lite需要的格式。因此，无论从哪方面看，TensorFlow Lite都是一个新的实现方案。</li>
</ul>
<h3 id="17-10-9-PocketFlow"><a href="#17-10-9-PocketFlow" class="headerlink" title="17.10.9 PocketFlow"></a>17.10.9 PocketFlow</h3><p>１、开源时间：2018年9月　　　</p>
<p>２、开源用户：腾讯　　　</p>
<p>３、GitHub地址：<a href="https://github.com/Tencent/PocketFlow" target="_blank" rel="noopener">https://github.com/Tencent/PocketFlow</a></p>
<p>４、简介：</p>
<p>全球首个自动模型压缩框架</p>
<p>一款面向移动端AI开发者的自动模型压缩框架，集成了当前主流的模型压缩与训练算法，结合自研超参数优化组件实现了全程自动化托管式的模型压缩与加速。开发者无需了解具体算法细节，即可快速地将AI技术部署到移动端产品上，实现了自动托管式模型压缩与加速，实现用户数据的本地高效处理。</p>
<p>5、框架介绍</p>
<p>PocketFlow 框架主要由两部分组件构成，分别是模型压缩/加速算法组件和超参数优化组件，具体结构如下图所示。</p>
<p><img src="/img/ch17/framework_design.png" alt=""></p>
<p>​    开发者将未压缩的原始模型作为 PocketFlow 框架的输入，同时指定期望的性能指标，例如模型的压缩和/或加速倍数；在每一轮迭代过程中，超参数优化组件选取一组超参数取值组合，之后模型压缩/加速算法组件基于该超参数取值组合，对原始模型进行压缩，得到一个压缩后的候选模型；基于对候选模型进行性能评估的结果，超参数优化组件调整自身的模型参数，并选取一组新的超参数取值组合，以开始下一轮迭代过程；当迭代终止时，PocketFlow 选取最优的超参数取值组合以及对应的候选模型，作为最终输出，返回给开发者用作移动端的模型部署。</p>
<p>6、PocketFlow如何实现模型压缩与加速？</p>
<p>​    具体地，PocketFlow 通过下列各个算法组件的有效结合，实现了精度损失更小、自动化程度更高的深度学习模型的压缩与加速：</p>
<ul>
<li><p>a) 通道剪枝（channel pruning）组件：在CNN网络中，通过对特征图中的通道维度进行剪枝，可以同时降低模型大小和计算复杂度，并且压缩后的模型可以直接基于现有的深度学习框架进行部署。在CIFAR-10图像分类任务中，通过对  ResNet-56 模型进行通道剪枝，可以实现2.5倍加速下分类精度损失0.4%，3.3倍加速下精度损失0.7%。</p>
</li>
<li><p>b) 权重稀疏化（weight sparsification）组件：通过对网络权重引入稀疏性约束，可以大幅度降低网络权重中的非零元素个数；压缩后模型的网络权重可以以稀疏矩阵的形式进行存储和传输，从而实现模型压缩。对于  MobileNet 图像分类模型，在删去50%网络权重后，在 ImageNet 数据集上的 Top-1 分类精度损失仅为0.6%。</p>
</li>
<li><p>c) 权重量化（weight quantization）组件：通过对网络权重引入量化约束，可以降低用于表示每个网络权重所需的比特数；团队同时提供了对于均匀和非均匀两大类量化算法的支持，可以充分利用  ARM 和 FPGA 等设备的硬件优化，以提升移动端的计算效率，并为未来的神经网络芯片设计提供软件支持。以用于 ImageNet  图像分类任务的 ResNet-18 模型为例，在8比特定点量化下可以实现精度无损的4倍压缩。</p>
</li>
<li><p>d)网络蒸馏（network distillation）组件：对于上述各种模型压缩组件，通过将未压缩的原始模型的输出作为额外的监督信息，指导压缩后模型的训练，在压缩/加速倍数不变的前提下均可以获得0.5%-2.0%不等的精度提升。</p>
</li>
<li><p>e) 多GPU训练（multi-GPU training）组件：深度学习模型训练过程对计算资源要求较高，单个GPU难以在短时间内完成模型训练，因此团队提供了对于多机多卡分布式训练的全面支持，以加快使用者的开发流程。无论是基于  ImageNet 数据的Resnet-50图像分类模型还是基于 WMT14 数据的 Transformer  机器翻译模型，均可以在一个小时内训练完毕。[1] </p>
</li>
<li><p>f) 超参数优化（hyper-parameter optimization）组件：多数开发者对模型压缩算法往往不甚了解，但超参数取值对最终结果往往有着巨大的影响，因此团队引入了超参数优化组件，采用了包括强化学习等算法以及  AI Lab 自研的 AutoML  自动超参数优化框架来根据具体性能需求，确定最优超参数取值组合。例如，对于通道剪枝算法，超参数优化组件可以自动地根据原始模型中各层的冗余程度，对各层采用不同的剪枝比例，在保证满足模型整体压缩倍数的前提下，实现压缩后模型识别精度的最大化。</p>
<p><img src="/img/ch17/packflow1.jpg" alt=""></p>
</li>
</ul>
<p>7、PocketFlow 性能</p>
<p>​    通过引入超参数优化组件，不仅避免了高门槛、繁琐的人工调参工作，同时也使得  PocketFlow 在各个压缩算法上全面超过了人工调参的效果。以图像分类任务为例，在 CIFAR-10 和 ImageNet  等数据集上，PocketFlow 对 ResNet 和 MobileNet 等多种 CNN 网络结构进行有效的模型压缩与加速。</p>
<p>​    在  CIFAR-10 数据集上，PocketFlow 以 ResNet-56  作为基准模型进行通道剪枝，并加入了超参数优化和网络蒸馏等训练策略，实现了 2.5 倍加速下分类精度损失 0.4%，3.3 倍加速下精度损失  0.7%，且显著优于未压缩的 ResNet-44 模型； 在 ImageNet 数据集上，PocketFlow 可以对原本已经十分精简的  MobileNet 模型继续进行权重稀疏化，以更小的模型尺寸取得相似的分类精度；与 Inception-V1、ResNet-18  等模型相比，模型大小仅为后者的约 20~40%，但分类精度基本一致（甚至更高）。</p>
<p><img src="/img/ch17/packflow2.jpg" alt=""></p>
<p><img src="/img/ch17/packflow3.jpg" alt=""></p>
<p>相比于费时费力的人工调参，PocketFlow 框架中的 AutoML 自动超参数优化组件仅需 10<br>余次迭代就能达到与人工调参类似的性能，在经过 100 次迭代后搜索得到的超参数组合可以降低约 0.6%<br>的精度损失；通过使用超参数优化组件自动地确定网络中各层权重的量化比特数，PocketFlow 在对用于 ImageNet 图像分类任务的<br>ResNet-18 模型进行压缩时，取得了一致性的性能提升；当平均量化比特数为 4 比特时，超参数优化组件的引入可以将分类精度从 63.6%<br>提升至 68.1%（原始模型的分类精度为 70.3%）。</p>
<p><img src="/img/ch17/packflow4.jpg" alt=""></p>
<p><img src="/img/ch17/packflow5.jpg" alt=""></p>
<p><strong>参考文献</strong></p>
<p>[1]  Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao,  Qingyao Wu, Junzhou Huang, Jinhui Zhu,「Discrimination-aware Channel  Pruning for Deep Neural Networks”, In Proc. of the 32nd Annual  Conference on Neural Information Processing Systems, NIPS ‘18, Montreal,  Canada, December 2018.</p>
<p>[2] Jiaxiang  Wu, Weidong Huang, Junzhou Huang, Tong Zhang,「Error Compensated  Quantized SGD and its Applications to Large-scale Distributed  Optimization」, In Proc. of the 35th International Conference on Machine  Learning, ICML’18, Stockholm, Sweden, July 2018.</p>
<h3 id="17-10-10-其他几款支持移动端深度学习的开源框架"><a href="#17-10-10-其他几款支持移动端深度学习的开源框架" class="headerlink" title="17.10.10 其他几款支持移动端深度学习的开源框架"></a>17.10.10 其他几款支持移动端深度学习的开源框架</h3><p><a href="https://blog.csdn.net/zchang81/article/details/74280019" target="_blank" rel="noopener">https://blog.csdn.net/zchang81/article/details/74280019</a></p>
<h3 id="17-10-11-MDL、NCNN和-TFLite比较"><a href="#17-10-11-MDL、NCNN和-TFLite比较" class="headerlink" title="17.10.11 MDL、NCNN和 TFLite比较"></a>17.10.11 MDL、NCNN和 TFLite比较</h3><p>百度-MDL框架、腾讯-NCNN框架和谷歌TFLite框架比较。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">MDL</th>
<th style="text-align:center">NCNN</th>
<th style="text-align:center">TFLite</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">代码质量</td>
<td style="text-align:center">中</td>
<td style="text-align:center">高</td>
<td style="text-align:center">很高</td>
</tr>
<tr>
<td style="text-align:center">跨平台</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">支持caffe模型</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">×</td>
</tr>
<tr>
<td style="text-align:center">支持TensorFlow模型</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">CPU NEON指令优化</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">GPU加速</td>
<td style="text-align:center">√</td>
<td style="text-align:center">×</td>
<td style="text-align:center">×</td>
</tr>
</tbody>
</table>
</div>
<p>相同点：</p>
<ul>
<li>只含推理（inference）功能，使用的模型文件需要通过离线的方式训练得到。</li>
<li>最终生成的库尺寸较小，均小于500kB。</li>
<li>为了提升执行速度，都使用了ARM NEON指令进行加速。</li>
<li>跨平台，iOS和Android系统都支持。</li>
</ul>
<p>不同点：</p>
<ul>
<li>MDL和NCNN均是只支持Caffe框架生成的模型文件，而TfLite则毫无意外的只支持自家大哥TensorFlow框架生成的模型文件。</li>
<li>MDL支持利用iOS系统的Matal框架进行GPU加速，能够显著提升在iPhone上的运行速度，达到准实时的效果。而NCNN和TFLite还没有这个功能。</li>
</ul>
<h2 id="17-11-移动端开源框架部署"><a href="#17-11-移动端开源框架部署" class="headerlink" title="17.11 移动端开源框架部署"></a>17.11 移动端开源框架部署</h2><h3 id="17-8-1-以NCNN为例"><a href="#17-8-1-以NCNN为例" class="headerlink" title="17.8.1 以NCNN为例"></a>17.8.1 以NCNN为例</h3><p>部署步骤   </p>
<h3 id="17-8-2-以QNNPACK为例"><a href="#17-8-2-以QNNPACK为例" class="headerlink" title="17.8.2 以QNNPACK为例"></a>17.8.2 以QNNPACK为例</h3><p>部署步骤     </p>
<h3 id="17-8-4-在Android手机上使用MACE实现图像分类"><a href="#17-8-4-在Android手机上使用MACE实现图像分类" class="headerlink" title="17.8.4 在Android手机上使用MACE实现图像分类"></a>17.8.4 在Android手机上使用MACE实现图像分类</h3><h3 id="17-8-3-在Android手机上使用PaddleMobile实现图像分类"><a href="#17-8-3-在Android手机上使用PaddleMobile实现图像分类" class="headerlink" title="17.8.3 在Android手机上使用PaddleMobile实现图像分类"></a>17.8.3 在Android手机上使用PaddleMobile实现图像分类</h3><p><strong>编译paddle-mobile库</strong></p>
<p>1）编译Android能够使用的CPP库：编译Android的paddle-mobile库，可选择使用Docker编译和Ubuntu交叉编译，这里介绍使用Ubuntu交叉编译paddle-mobile库。</p>
<p><em>注</em>：在Android项目，Java代码调用CPP代码，CPP的函数需要遵循一定的命名规范，比如Java_包名_类名_对应的Java的方法名。</p>
<p>​    目前官方提供了5个可以给Java调用的函数，该代码在：paddle-mobile/src/jni/paddle_mobile_jni.cpp，如果想要让这些函数能够在自己的包名下的类调用，就要修改CPP的函数名称修改如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">JNIEXPORT jboolean JNICALL <span class="title">Java_com_baidu_paddle_PML_load</span><span class="params">(JNIEnv *env, </span></span></span><br><span class="line"><span class="function"><span class="params">	jclass thiz,</span></span></span><br><span class="line"><span class="function"><span class="params">	jstring modelPath)</span> </span>&#123; </span><br><span class="line">		ANDROIDLOGI(<span class="string">"load invoked"</span>); </span><br><span class="line">		bool optimize = <span class="keyword">true</span>; </span><br><span class="line">		<span class="keyword">return</span> getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath), optimize); &#125;</span><br></pre></td></tr></table></figure>
<p>​    笔者项目的包名为<code>com.example.paddlemobile1</code>，在这个包下有一个<code>ImageRecognition.java</code>的程序来对应这个CPP程序，那么修改<code>load</code>函数如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">JNIEXPORT jboolean JNICALL <span class="title">Java_com_example_paddlemobile1_ImageRecognition_load</span><span class="params">(JNIEnv *env,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                          jclass thiz,</span></span></span><br><span class="line"><span class="function"><span class="params">                                                          jstring modelPath)</span> </span>&#123;</span><br><span class="line">  ANDROIDLOGI(<span class="string">"load invoked"</span>);</span><br><span class="line">  bool optimize = <span class="keyword">true</span>;</span><br><span class="line">  <span class="keyword">return</span> getPaddleMobileInstance()-&gt;Load(jstring2cppstring(env, modelPath),</span><br><span class="line">                                         optimize);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>使用Ubuntu交叉编译paddle-mobile库</strong></p>
<p>1、下载和解压NDK。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;dl.google.com&#x2F;android&#x2F;repository&#x2F;android-ndk-r17b-linux-x86_64.zip</span><br><span class="line">unzip android-ndk-r17b-linux-x86_64.zip</span><br></pre></td></tr></table></figure>
<p>2、设置NDK环境变量，目录是NDK的解压目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export NDK_ROOT&#x3D;&quot;&#x2F;home&#x2F;test&#x2F;paddlepaddle&#x2F;android-ndk-r17b&quot;</span><br></pre></td></tr></table></figure>
<p>设置好之后，可以使用以下的命令查看配置情况。</p>
<pre><code>root@test:/home/test/paddlepaddle# echo $NDK_ROOT
/home/test/paddlepaddle/android-ndk-r17b
</code></pre><p>3、安装cmake，需要安装较高版本的，笔者的cmake版本是3.11.2。</p>
<p>下载cmake源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;cmake.org&#x2F;files&#x2F;v3.11&#x2F;cmake-3.11.2.tar.gz</span><br></pre></td></tr></table></figure>
<p>解压cmake源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cmake-3.11.2.tar.gz</span><br></pre></td></tr></table></figure>
<p>进入到cmake源码根目录，并执行bootstrap。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd cmake-3.11.2</span><br><span class="line">.&#x2F;bootstrap</span><br></pre></td></tr></table></figure>
<p>最后执行以下两条命令开始安装cmake。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>安装完成之后，可以使用cmake —version是否安装成功.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@test:&#x2F;home&#x2F;test&#x2F;paddlepaddle# cmake --version</span><br><span class="line">cmake version 3.11.2</span><br><span class="line"></span><br><span class="line">CMake suite maintained and supported by Kitware (kitware.com&#x2F;cmake).</span><br></pre></td></tr></table></figure>
<p>4、克隆paddle-mobile源码。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;PaddlePaddle&#x2F;paddle-mobile.git</span><br></pre></td></tr></table></figure>
<p>5、进入到paddle-mobile的tools目录下，执行编译。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd paddle-mobile&#x2F;tools&#x2F;</span><br><span class="line">sh build.sh android</span><br></pre></td></tr></table></figure>
<p>（可选）如果想编译针对某一个网络编译更小的库时，可以在命令后面加上相应的参数，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh build.sh android googlenet</span><br></pre></td></tr></table></figure>
<p>6、最后会在paddle-mobile/build/release/arm-v7a/build目录下生产paddle-mobile库。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@test:/home/test/paddlepaddle/paddle-mobile/build/release/arm-v7a/build# ls</span><br><span class="line">libpaddle-mobile.so</span><br></pre></td></tr></table></figure>
<p>libpaddle-mobile.so就是我们在开发Android项目的时候使用到的paddle-mobile库。</p>
<p><strong>创建Android项目</strong></p>
<p>1、首先使用Android Studio创建一个普通的Android项目，包名为<code>com.example.paddlemobile1</code></p>
<p>2、在main目录下创建l两个assets/paddle_models文件夹，这个文件夹存放PaddleFluid训练好的预测模型。PaddleMobile支持量化模型，使用模型量化可以把模型缩小至原来的四分之一，如果使用量化模型，那加载模型的接口也有修改一下，使用以下的接口加载模型：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadQualified</span><span class="params">(String modelDir)</span></span>;</span><br></pre></td></tr></table></figure>
<p>3、在<code>main</code>目录下创建一个<code>jniLibs</code>文件夹，这个文件夹是存放CPP编译库的，在本项目中就存放上一部分编译的<code>libpaddle-mobile.so</code></p>
<p>4、在Android项目的配置文件夹中加上权限声明，因为我们要使用到读取相册和使用相机，所以加上以下的权限声明：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.CAMERA"</span> /&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.WRITE_EXTERNAL_STORAGE"</span> /&gt;</span><br><span class="line">&lt;uses-permission android:name=<span class="string">"android.permission.READ_EXTERNAL_STORAGE"</span> /&gt;</span><br></pre></td></tr></table></figure>
<p>5、修改<code>activity_main.xml</code>界面，修改成如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"utf-8"</span>?&gt;</span><br><span class="line">&lt;RelativeLayout xmlns:android=<span class="string">"http://schemas.android.com/apk/res/android"</span></span><br><span class="line">    xmlns:app=<span class="string">"http://schemas.android.com/apk/res-auto"</span></span><br><span class="line">    xmlns:tools=<span class="string">"http://schemas.android.com/tools"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"match_parent"</span></span><br><span class="line">    tools:context=<span class="string">".MainActivity"</span>&gt;</span><br><span class="line">&lt;LinearLayout</span><br><span class="line">    android:id=<span class="string">"@+id/btn_ll"</span></span><br><span class="line">    android:layout_alignParentBottom=<span class="string">"true"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">    android:orientation=<span class="string">"horizontal"</span>&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Button</span><br><span class="line">        android:id=<span class="string">"@+id/use_photo"</span></span><br><span class="line">        android:layout_weight=<span class="string">"1"</span></span><br><span class="line">        android:layout_width=<span class="string">"0dp"</span></span><br><span class="line">        android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">        android:text=<span class="string">"相册"</span> /&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Button</span><br><span class="line">        android:id=<span class="string">"@+id/start_camera"</span></span><br><span class="line">        android:layout_weight=<span class="string">"1"</span></span><br><span class="line">        android:layout_width=<span class="string">"0dp"</span></span><br><span class="line">        android:layout_height=<span class="string">"wrap_content"</span></span><br><span class="line">        android:text=<span class="string">"拍照"</span> /&gt;</span><br><span class="line">&lt;/LinearLayout&gt;</span><br><span class="line"></span><br><span class="line">&lt;TextView</span><br><span class="line">    android:layout_above=<span class="string">"@id/btn_ll"</span></span><br><span class="line">    android:id=<span class="string">"@+id/result_text"</span></span><br><span class="line">    android:textSize=<span class="string">"16sp"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:hint=<span class="string">"预测结果会在这里显示"</span></span><br><span class="line">    android:layout_height=<span class="string">"100dp"</span> /&gt;</span><br><span class="line"></span><br><span class="line">&lt;ImageView</span><br><span class="line">    android:layout_alignParentTop=<span class="string">"true"</span></span><br><span class="line">    android:layout_above=<span class="string">"@id/result_text"</span></span><br><span class="line">    android:id=<span class="string">"@+id/show_image"</span></span><br><span class="line">    android:layout_width=<span class="string">"match_parent"</span></span><br><span class="line">    android:layout_height=<span class="string">"match_parent"</span> /&gt;</span><br><span class="line">&lt;/RelativeLayout&gt;</span><br></pre></td></tr></table></figure>
<p>6、创建一个<code>ImageRecognition.java</code>的Java程序，这个程序的作用就是调用<code>paddle-mobile/src/jni/paddle_mobile_jni.cpp</code>的函数，对应的是里面的函数。目前支持一下几个接口。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ImageRecognition</span> </span>&#123;</span><br><span class="line">    <span class="comment">// set thread num</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">setThread</span><span class="params">(<span class="keyword">int</span> threadCount)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Load seperated parameters</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">load</span><span class="params">(String modelDir)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load qualified model</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadQualified</span><span class="params">(String modelDir)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Load combined parameters</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadCombined</span><span class="params">(String modelPath, String paramPath)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load qualified model</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">boolean</span> <span class="title">loadCombinedQualified</span><span class="params">(String modelPath, String paramPath)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// object detection</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">float</span>[] predictImage(<span class="keyword">float</span>[] buf, <span class="keyword">int</span>[]ddims);</span><br><span class="line"></span><br><span class="line"><span class="comment">// predict yuv image</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">float</span>[] predictYuv(<span class="keyword">byte</span>[] buf, <span class="keyword">int</span> imgWidth, <span class="keyword">int</span> imgHeight, <span class="keyword">int</span>[] ddims, <span class="keyword">float</span>[]meanValues);</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear model</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>7、然后编写一个<code>PhotoUtil.java</code>的工具类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.app.Activity;</span><br><span class="line"><span class="keyword">import</span> android.content.Context;</span><br><span class="line"><span class="keyword">import</span> android.content.Intent;</span><br><span class="line"><span class="keyword">import</span> android.database.Cursor;</span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap;</span><br><span class="line"><span class="keyword">import</span> android.graphics.BitmapFactory;</span><br><span class="line"><span class="keyword">import</span> android.net.Uri;</span><br><span class="line"><span class="keyword">import</span> android.os.Build;</span><br><span class="line"><span class="keyword">import</span> android.provider.MediaStore;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.content.FileProvider;</span><br><span class="line"><span class="keyword">import</span> android.util.Log;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhotoUtil</span> </span>&#123;</span><br><span class="line"><span class="comment">// start camera</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Uri <span class="title">start_camera</span><span class="params">(Activity activity, <span class="keyword">int</span> requestCode)</span> </span>&#123;</span><br><span class="line">    Uri imageUri;</span><br><span class="line">    <span class="comment">// save image in cache path</span></span><br><span class="line">    File outputImage = <span class="keyword">new</span> File(activity.getExternalCacheDir(), <span class="string">"out_image.jpg"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (outputImage.exists()) &#123;</span><br><span class="line">            outputImage.delete();</span><br><span class="line">        &#125;</span><br><span class="line">        outputImage.createNewFile();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (Build.VERSION.SDK_INT &gt;= <span class="number">24</span>) &#123;</span><br><span class="line">        <span class="comment">// compatible with Android 7.0 or over</span></span><br><span class="line">        imageUri = FileProvider.getUriForFile(activity,</span><br><span class="line">                <span class="string">"com.example.paddlemobile1"</span>, outputImage);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        imageUri = Uri.fromFile(outputImage);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// set system camera Action</span></span><br><span class="line">    Intent intent = <span class="keyword">new</span> Intent(MediaStore.ACTION_IMAGE_CAPTURE);</span><br><span class="line">    <span class="comment">// set save photo path</span></span><br><span class="line">    intent.putExtra(MediaStore.EXTRA_OUTPUT, imageUri);</span><br><span class="line">    <span class="comment">// set photo quality, min is 0, max is 1</span></span><br><span class="line">    intent.putExtra(MediaStore.EXTRA_VIDEO_QUALITY, <span class="number">0</span>);</span><br><span class="line">    activity.startActivityForResult(intent, requestCode);</span><br><span class="line">    <span class="keyword">return</span> imageUri;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// get picture in photo</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">use_photo</span><span class="params">(Activity activity, <span class="keyword">int</span> requestCode)</span></span>&#123;</span><br><span class="line">    Intent intent = <span class="keyword">new</span> Intent(Intent.ACTION_PICK);</span><br><span class="line">    intent.setType(<span class="string">"image/*"</span>);</span><br><span class="line">    activity.startActivityForResult(intent, requestCode);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// get photo from Uri</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">get_path_from_URI</span><span class="params">(Context context, Uri uri)</span> </span>&#123;</span><br><span class="line">    String result;</span><br><span class="line">    Cursor cursor = context.getContentResolver().query(uri, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (cursor == <span class="keyword">null</span>) &#123;</span><br><span class="line">        result = uri.getPath();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        cursor.moveToFirst();</span><br><span class="line">        <span class="keyword">int</span> idx = cursor.getColumnIndex(MediaStore.Images.ImageColumns.DATA);</span><br><span class="line">        result = cursor.getString(idx);</span><br><span class="line">        cursor.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Compress the image to the size of the training image，and change RGB</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">float</span>[] getScaledMatrix(Bitmap bitmap, <span class="keyword">int</span> desWidth,</span><br><span class="line">                               <span class="keyword">int</span> desHeight) &#123;</span><br><span class="line">    <span class="keyword">float</span>[] dataBuf = <span class="keyword">new</span> <span class="keyword">float</span>[<span class="number">3</span> * desWidth * desHeight];</span><br><span class="line">    <span class="keyword">int</span> rIndex;</span><br><span class="line">    <span class="keyword">int</span> gIndex;</span><br><span class="line">    <span class="keyword">int</span> bIndex;</span><br><span class="line">    <span class="keyword">int</span>[] pixels = <span class="keyword">new</span> <span class="keyword">int</span>[desWidth * desHeight];</span><br><span class="line">    Bitmap bm = Bitmap.createScaledBitmap(bitmap, desWidth, desHeight, <span class="keyword">false</span>);</span><br><span class="line">    bm.getPixels(pixels, <span class="number">0</span>, desWidth, <span class="number">0</span>, <span class="number">0</span>, desWidth, desHeight);</span><br><span class="line">    <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; pixels.length; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> clr = pixels[i];</span><br><span class="line">        j = i / desHeight;</span><br><span class="line">        k = i % desWidth;</span><br><span class="line">        rIndex = j * desWidth + k;</span><br><span class="line">        gIndex = rIndex + desHeight * desWidth;</span><br><span class="line">        bIndex = gIndex + desHeight * desWidth;</span><br><span class="line">        dataBuf[rIndex] = (<span class="keyword">float</span>) ((clr &amp; <span class="number">0x00ff0000</span>) &gt;&gt; <span class="number">16</span>) - <span class="number">148</span>;</span><br><span class="line">        dataBuf[gIndex] = (<span class="keyword">float</span>) ((clr &amp; <span class="number">0x0000ff00</span>) &gt;&gt; <span class="number">8</span>) - <span class="number">148</span>;</span><br><span class="line">        dataBuf[bIndex] = (<span class="keyword">float</span>) ((clr &amp; <span class="number">0x000000ff</span>)) - <span class="number">148</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (bm.isRecycled()) &#123;</span><br><span class="line">        bm.recycle();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dataBuf;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// compress picture</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Bitmap <span class="title">getScaleBitmap</span><span class="params">(String filePath)</span> </span>&#123;</span><br><span class="line">    BitmapFactory.Options opt = <span class="keyword">new</span> BitmapFactory.Options();</span><br><span class="line">    opt.inJustDecodeBounds = <span class="keyword">true</span>;</span><br><span class="line">    BitmapFactory.decodeFile(filePath, opt);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> bmpWidth = opt.outWidth;</span><br><span class="line">    <span class="keyword">int</span> bmpHeight = opt.outHeight;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> maxSize = <span class="number">500</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// compress picture with inSampleSize</span></span><br><span class="line">    opt.inSampleSize = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (bmpWidth / opt.inSampleSize &lt; maxSize || bmpHeight / opt.inSampleSize &lt; maxSize) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        opt.inSampleSize *= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    opt.inJustDecodeBounds = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">return</span> BitmapFactory.decodeFile(filePath, opt);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>start_camera()方法是启动相机并返回图片的URI。</li>
<li>use_photo()方法是打开相册，获取到的图片URI在回到函数中获取。</li>
<li>get_path_from_URI()方法是把图片的URI转换成绝对路径。</li>
<li>getScaledMatrix()方法是把图片压缩成跟训练时的大小，并转换成预测需要用的数据格式浮点数组。</li>
<li>getScaleBitmap()方法是对图片进行等比例压缩，减少内存的支出。</li>
</ul>
<p>8、最后修改<code>MainActivity.java</code>，修改如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.paddlemobile1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> android.Manifest;</span><br><span class="line"><span class="keyword">import</span> android.annotation.SuppressLint;</span><br><span class="line"><span class="keyword">import</span> android.app.Activity;</span><br><span class="line"><span class="keyword">import</span> android.content.Context;</span><br><span class="line"><span class="keyword">import</span> android.content.Intent;</span><br><span class="line"><span class="keyword">import</span> android.content.pm.PackageManager;</span><br><span class="line"><span class="keyword">import</span> android.graphics.Bitmap;</span><br><span class="line"><span class="keyword">import</span> android.net.Uri;</span><br><span class="line"><span class="keyword">import</span> android.os.Bundle;</span><br><span class="line"><span class="keyword">import</span> android.os.Environment;</span><br><span class="line"><span class="keyword">import</span> android.support.annotation.NonNull;</span><br><span class="line"><span class="keyword">import</span> android.support.annotation.Nullable;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.app.ActivityCompat;</span><br><span class="line"><span class="keyword">import</span> android.support.v4.content.ContextCompat;</span><br><span class="line"><span class="keyword">import</span> android.support.v7.app.AppCompatActivity;</span><br><span class="line"><span class="keyword">import</span> android.util.Log;</span><br><span class="line"><span class="keyword">import</span> android.view.View;</span><br><span class="line"><span class="keyword">import</span> android.widget.Button;</span><br><span class="line"><span class="keyword">import</span> android.widget.ImageView;</span><br><span class="line"><span class="keyword">import</span> android.widget.TextView;</span><br><span class="line"><span class="keyword">import</span> android.widget.Toast;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.Glide;</span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.load.engine.DiskCacheStrategy;</span><br><span class="line"><span class="keyword">import</span> com.bumptech.glide.request.RequestOptions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MainActivity</span> <span class="keyword">extends</span> <span class="title">AppCompatActivity</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TAG = MainActivity<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>()</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> USE_PHOTO = <span class="number">1001</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> START_CAMERA = <span class="number">1002</span>;</span><br><span class="line">    <span class="keyword">private</span> Uri image_uri;</span><br><span class="line">    <span class="keyword">private</span> ImageView show_image;</span><br><span class="line">    <span class="keyword">private</span> TextView result_text;</span><br><span class="line">    <span class="keyword">private</span> String assets_path = <span class="string">"paddle_models"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> load_result = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>[] ddims = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String[] PADDLE_MODEL = &#123;</span><br><span class="line">        <span class="string">"lenet"</span>,</span><br><span class="line">        <span class="string">"alexnet"</span>,</span><br><span class="line">        <span class="string">"vgg16"</span>,</span><br><span class="line">        <span class="string">"resnet"</span>,</span><br><span class="line">        <span class="string">"googlenet"</span>,</span><br><span class="line">        <span class="string">"mobilenet_v1"</span>,</span><br><span class="line">        <span class="string">"mobilenet_v2"</span>,</span><br><span class="line">        <span class="string">"inception_v1"</span>,</span><br><span class="line">        <span class="string">"inception_v2"</span>,</span><br><span class="line">        <span class="string">"squeezenet"</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load paddle-mobile api</span></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        System.loadLibrary(<span class="string">"paddle-mobile"</span>);</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (SecurityException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (UnsatisfiedLinkError e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> (NullPointerException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.onCreate(savedInstanceState);</span><br><span class="line">    setContentView(R.layout.activity_main);</span><br><span class="line"></span><br><span class="line">    init();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// initialize view</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    request_permissions();</span><br><span class="line">    show_image = (ImageView) findViewById(R.id.show_image);</span><br><span class="line">    result_text = (TextView) findViewById(R.id.result_text);</span><br><span class="line">    Button use_photo = (Button) findViewById(R.id.use_photo);</span><br><span class="line">    Button start_photo = (Button) findViewById(R.id.start_camera);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// use photo click</span></span><br><span class="line">    use_photo.setOnClickListener(<span class="keyword">new</span> View.OnClickListener() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onClick</span><span class="params">(View view)</span> </span>&#123;</span><br><span class="line">            PhotoUtil.use_photo(MainActivity.<span class="keyword">this</span>, USE_PHOTO);</span><br><span class="line">            <span class="comment">//                load_model();</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">// start camera click</span></span><br><span class="line">    start_photo.setOnClickListener(<span class="keyword">new</span> View.OnClickListener() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onClick</span><span class="params">(View view)</span> </span>&#123;</span><br><span class="line">            image_uri = PhotoUtil.start_camera(MainActivity.<span class="keyword">this</span>, START_CAMERA);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy file from assets to sdcard</span></span><br><span class="line">    String sdcard_path = Environment.getExternalStorageDirectory()</span><br><span class="line">            + File.separator + assets_path;</span><br><span class="line">    copy_file_from_asset(<span class="keyword">this</span>, assets_path, sdcard_path);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// load model</span></span><br><span class="line">    load_model();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// load infer model</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">load_model</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String model_path = Environment.getExternalStorageDirectory()</span><br><span class="line">            + File.separator + assets_path + File.separator + PADDLE_MODEL[<span class="number">4</span>];</span><br><span class="line">    Log.d(TAG, model_path);</span><br><span class="line">    load_result = ImageRecognition.load(model_path);</span><br><span class="line">    <span class="keyword">if</span> (load_result) &#123;</span><br><span class="line">        Log.d(TAG, <span class="string">"model load success"</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        Log.d(TAG, <span class="string">"model load fail"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear infer model</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">clear_model</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ImageRecognition.clear();</span><br><span class="line">    Log.d(TAG, <span class="string">"model is clear"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy file from asset to sdcard</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copy_file_from_asset</span><span class="params">(Context context, String oldPath, String newPath)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        String[] fileNames = context.getAssets().list(oldPath);</span><br><span class="line">        <span class="keyword">if</span> (fileNames.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// directory</span></span><br><span class="line">            File file = <span class="keyword">new</span> File(newPath);</span><br><span class="line">            <span class="keyword">if</span> (!file.exists()) &#123;</span><br><span class="line">                file.mkdirs();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// copy recursivelyC</span></span><br><span class="line">            <span class="keyword">for</span> (String fileName : fileNames) &#123;</span><br><span class="line">                copy_file_from_asset(context, oldPath + <span class="string">"/"</span> + fileName, newPath + <span class="string">"/"</span> + fileName);</span><br><span class="line">            &#125;</span><br><span class="line">            Log.d(TAG, <span class="string">"copy files finish"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// file</span></span><br><span class="line">            File file = <span class="keyword">new</span> File(newPath);</span><br><span class="line">            <span class="comment">// if file exists will never copy</span></span><br><span class="line">            <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// copy file to new path</span></span><br><span class="line">            InputStream is = context.getAssets().open(oldPath);</span><br><span class="line">            FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(file);</span><br><span class="line">            <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</span><br><span class="line">            <span class="keyword">int</span> byteCount;</span><br><span class="line">            <span class="keyword">while</span> ((byteCount = is.read(buffer)) != -<span class="number">1</span>) &#123;</span><br><span class="line">                fos.write(buffer, <span class="number">0</span>, byteCount);</span><br><span class="line">            &#125;</span><br><span class="line">            fos.flush();</span><br><span class="line">            is.close();</span><br><span class="line">            fos.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onActivityResult</span><span class="params">(<span class="keyword">int</span> requestCode, <span class="keyword">int</span> resultCode, @Nullable Intent data)</span> </span>&#123;</span><br><span class="line">    String image_path;</span><br><span class="line">    RequestOptions options = <span class="keyword">new</span> RequestOptions().skipMemoryCache(<span class="keyword">true</span>).diskCacheStrategy(DiskCacheStrategy.NONE);</span><br><span class="line">    <span class="keyword">if</span> (resultCode == Activity.RESULT_OK) &#123;</span><br><span class="line">        <span class="keyword">switch</span> (requestCode) &#123;</span><br><span class="line">            <span class="keyword">case</span> USE_PHOTO:</span><br><span class="line">                <span class="keyword">if</span> (data == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    Log.w(TAG, <span class="string">"user photo data is null"</span>);</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                image_uri = data.getData();</span><br><span class="line">                Glide.with(MainActivity.<span class="keyword">this</span>).load(image_uri).apply(options).into(show_image);</span><br><span class="line">                <span class="comment">// get image path from uri</span></span><br><span class="line">                image_path = PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri);</span><br><span class="line">                <span class="comment">// show result</span></span><br><span class="line">                result_text.setText(image_path);</span><br><span class="line">                <span class="comment">// predict image</span></span><br><span class="line">                predict_image(PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri));</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> START_CAMERA:</span><br><span class="line">                <span class="comment">// show photo</span></span><br><span class="line">                Glide.with(MainActivity.<span class="keyword">this</span>).load(image_uri).apply(options).into(show_image);</span><br><span class="line">                <span class="comment">// get image path from uri</span></span><br><span class="line">                image_path = PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri);</span><br><span class="line">                <span class="comment">// show result</span></span><br><span class="line">                result_text.setText(image_path);</span><br><span class="line">                <span class="comment">// predict image</span></span><br><span class="line">                predict_image(PhotoUtil.get_path_from_URI(MainActivity.<span class="keyword">this</span>, image_uri));</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@SuppressLint</span>(<span class="string">"SetTextI18n"</span>)</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">predict_image</span><span class="params">(String image_path)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// picture to float array</span></span><br><span class="line">    Bitmap bmp = PhotoUtil.getScaleBitmap(image_path);</span><br><span class="line">    <span class="keyword">float</span>[] inputData = PhotoUtil.getScaledMatrix(bmp, ddims[<span class="number">2</span>], ddims[<span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">        <span class="comment">// get predict result</span></span><br><span class="line">        <span class="keyword">float</span>[] result = ImageRecognition.predictImage(inputData, ddims);</span><br><span class="line">        Log.d(TAG, <span class="string">"origin predict result:"</span> + Arrays.toString(result));</span><br><span class="line">        <span class="keyword">long</span> end = System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">long</span> time = end - start;</span><br><span class="line">        Log.d(<span class="string">"result length"</span>, String.valueOf(result.length));</span><br><span class="line">        <span class="comment">// show predict result and time</span></span><br><span class="line">        <span class="keyword">int</span> r = get_max_result(result);</span><br><span class="line">        String show_text = <span class="string">"result："</span> + r + <span class="string">"\nprobability："</span> + result[r] + <span class="string">"\ntime："</span> + time + <span class="string">"ms"</span>;</span><br><span class="line">        result_text.setText(show_text);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">get_max_result</span><span class="params">(<span class="keyword">float</span>[] result)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">float</span> probability = result[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> r = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; result.length; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (probability &lt; result[i]) &#123;</span><br><span class="line">            probability = result[i];</span><br><span class="line">            r = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> r;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// request permissions</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">request_permissions</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; permissionList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="keyword">this</span>, Manifest.permission.CAMERA) != PackageManager.PERMISSION_GRANTED) &#123;</span><br><span class="line">        permissionList.add(Manifest.permission.CAMERA);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="keyword">this</span>, Manifest.permission.WRITE_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) &#123;</span><br><span class="line">        permissionList.add(Manifest.permission.WRITE_EXTERNAL_STORAGE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ContextCompat.checkSelfPermission(<span class="keyword">this</span>, Manifest.permission.READ_EXTERNAL_STORAGE) != PackageManager.PERMISSION_GRANTED) &#123;</span><br><span class="line">        permissionList.add(Manifest.permission.READ_EXTERNAL_STORAGE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if list is not empty will request permissions</span></span><br><span class="line">    <span class="keyword">if</span> (!permissionList.isEmpty()) &#123;</span><br><span class="line">        ActivityCompat.requestPermissions(<span class="keyword">this</span>, permissionList.toArray(<span class="keyword">new</span> String[permissionList.size()]), <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onRequestPermissionsResult</span><span class="params">(<span class="keyword">int</span> requestCode, @NonNull String[] permissions, @NonNull <span class="keyword">int</span>[] grantResults)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>.onRequestPermissionsResult(requestCode, permissions, grantResults);</span><br><span class="line">    <span class="keyword">switch</span> (requestCode) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> (grantResults.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; grantResults.length; i++) &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">int</span> grantResult = grantResults[i];</span><br><span class="line">                    <span class="keyword">if</span> (grantResult == PackageManager.PERMISSION_DENIED) &#123;</span><br><span class="line">                        String s = permissions[i];</span><br><span class="line">                        Toast.makeText(<span class="keyword">this</span>, s + <span class="string">" permission was denied"</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onDestroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// clear model before destroy app</span></span><br><span class="line">    clear_model();</span><br><span class="line">    <span class="keyword">super</span>.onDestroy();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>load_model()方法是加载预测模型的。</li>
<li>clear_model()方法是清空预测模型的。</li>
<li>copy_file_from_asset()方法是把预测模型复制到内存卡上。</li>
<li>predict_image()方法是预测图片的。</li>
<li>get_max_result()方法是获取概率最大的预测结果。</li>
<li>request_permissions()方法是动态请求权限的。</li>
</ul>
<p>因为使用到图像加载框架Glide，所以要在<code>build.gradle</code>加入以下的引用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">implementation &#39;com.github.bumptech.glide:glide:4.3.1&#39;</span><br></pre></td></tr></table></figure>
<p>8、最后运行项目，选择图片预测就会得到结果。</p>
<h2 id="17-9-移动端开源框架部署疑难"><a href="#17-9-移动端开源框架部署疑难" class="headerlink" title="17.9 移动端开源框架部署疑难"></a>17.9 移动端开源框架部署疑难</h2><p>增加常见的几个问题</p>
<p>知识蒸馏（Distillation）相关论文阅读（1）——Distilling the Knowledge in a Neural Network（以及代码复现）</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/" class="post-title-link" itemprop="url">卷积神经网络（CNN）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-03 16:02:14" itemprop="dateCreated datePublished" datetime="2020-03-03T16:02:14+08:00">2020-03-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-20 14:06:02" itemprop="dateModified" datetime="2020-03-20T14:06:02+08:00">2020-03-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第五章-卷积神经网络（CNN）"><a href="#第五章-卷积神经网络（CNN）" class="headerlink" title="第五章 卷积神经网络（CNN）"></a>第五章 卷积神经网络（CNN）</h1><p>​    卷积神经网络是一种用来处理局部和整体相关性的计算网络结构，被应用在图像识别、自然语言处理甚至是语音识别领域，因为图像数据具有显著的局部与整体关系，其在图像识别领域的应用获得了巨大的成功。</p>
<h2 id="5-1-卷积神经网络的组成层"><a href="#5-1-卷积神经网络的组成层" class="headerlink" title="5.1 卷积神经网络的组成层"></a>5.1 卷积神经网络的组成层</h2><p>​    以图像分类任务为例，在表5.1所示卷积神经网络中，一般包含5种类型的网络层次结构：</p>
<p>​                                                                 表5.1 卷积神经网络的组成</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">CNN层次结构</th>
<th style="text-align:center">输出尺寸</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输入层</td>
<td style="text-align:center">$W_1\times H_1\times 3$</td>
<td style="text-align:left">卷积网络的原始输入，可以是原始或预处理后的像素矩阵</td>
</tr>
<tr>
<td style="text-align:center">卷积层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">参数共享、局部连接，利用平移不变性从全局特征图提取局部特征</td>
</tr>
<tr>
<td style="text-align:center">激活层</td>
<td style="text-align:center">$W_1\times H_1\times K$</td>
<td style="text-align:left">将卷积层的输出结果进行非线性映射</td>
</tr>
<tr>
<td style="text-align:center">池化层</td>
<td style="text-align:center">$W_2\times H_2\times K$</td>
<td style="text-align:left">进一步筛选特征，可以有效减少后续网络层次所需的参数量</td>
</tr>
<tr>
<td style="text-align:center">全连接层</td>
<td style="text-align:center">$(W_2 \cdot H_2 \cdot K)\times C$</td>
<td style="text-align:left">将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值）</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>$W_1\times H_1\times 3$对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;$K$表示卷积层中卷积核（滤波器）的个数;$W_2\times H_2$ 为池化后特征图的尺度，在全局池化中尺度对应$1\times 1$;$(W_2 \cdot H_2 \cdot K)$是将多维特征压缩到1维之后的大小，$C$对应的则是图像类别个数。</p>
</blockquote>
<h3 id="5-1-1-输入层"><a href="#5-1-1-输入层" class="headerlink" title="5.1.1 输入层"></a>5.1.1 输入层</h3><p>​    输入层(Input Layer)通常是输入卷积神经网络的原始数据或经过预处理的数据，可以是图像识别领域中原始三维的多彩图像，也可以是音频识别领域中经过傅利叶变换的二维波形数据，甚至是自然语言处理中一维表示的句子向量。以图像分类任务为例，输入层输入的图像一般包含RGB三个通道，是一个由长宽分别为$H$和$W$组成的3维像素值矩阵$H\times W \times 3$，卷积网络会将输入层的数据传递到一系列卷积、池化等操作进行特征提取和转化，最终由全连接层对特征进行汇总和结果输出。根据计算能力、存储大小和模型结构的不同，卷积神经网络每次可以批量处理的图像个数不尽相同，若指定输入层接收到的图像个数为$N$，则输入层的输出数据为$N\times H\times W\times 3$。</p>
<h3 id="5-1-2-卷积层"><a href="#5-1-2-卷积层" class="headerlink" title="5.1.2 卷积层"></a>5.1.2 卷积层</h3><p>​    卷积层(Convolution Layer)通常用作对输入层输入数据进行特征提取，通过卷积核矩阵对原始数据中隐含关联性的一种抽象。卷积操作原理上其实是对两张像素矩阵进行点乘求和的数学操作，其中一个矩阵为输入的数据矩阵，另一个矩阵则为卷积核（滤波器或特征矩阵），求得的结果表示为原始图像中提取的特定局部特征。图5.1表示卷积操作过程中的不同填充策略，上半部分采用零填充，下半部分采用有效卷积（舍弃不能完整运算的边缘部分）。<br>​                                                    <img src="img/ch5/convolution.png" alt="conv-same"><br>​                                                        图5.1 卷积操作示意图</p>
<h3 id="5-1-3-激活层"><a href="#5-1-3-激活层" class="headerlink" title="5.1.3 激活层"></a>5.1.3 激活层</h3><p>​    激活层(Activation Layer)负责对卷积层抽取的特征进行激活，由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。激活层主要由激活函数组成，即在卷积层输出结果的基础上嵌套一个非线性函数，让输出的特征图具有非线性关系。卷积网络中通常采用ReLU来充当激活函数（还包括tanh和sigmoid等）ReLU的函数形式如公式（5-1）所示，能够限制小于0的值为0,同时大于等于0的值保持不变。</p>
<script type="math/tex; mode=display">
f(x)=\begin{cases}
   0 &\text{if } x<0 \\
   x &\text{if } x\ge 0
\end{cases}
\tag{5-1}</script><h3 id="5-1-4-池化层"><a href="#5-1-4-池化层" class="headerlink" title="5.1.4 池化层"></a>5.1.4 池化层</h3><p>​    池化层又称为降采样层(Downsampling Layer)，作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低输出特征尺度，进而减少模型所需要的参数量。按操作类型通常分为最大池化(Max Pooling)、平均池化(Average Pooling)和求和池化(Sum Pooling)，它们分别提取感受域内最大、平均与总和的特征值作为输出，最常用的是最大池化。</p>
<h3 id="5-1-5-全连接层"><a href="#5-1-5-全连接层" class="headerlink" title="5.1.5 全连接层"></a>5.1.5 全连接层</h3><p>​    全连接层(Full Connected Layer)负责对卷积神经网络学习提取到的特征进行汇总，将多维的特征输入映射为二维的特征输出，高维表示样本批次，低位常常对应任务目标。</p>
<h2 id="5-2-卷积在图像中有什么直观作用"><a href="#5-2-卷积在图像中有什么直观作用" class="headerlink" title="5.2 卷积在图像中有什么直观作用"></a>5.2 卷积在图像中有什么直观作用</h2><p>​    在卷积神经网络中，卷积常用来提取图像的特征，但不同层次的卷积操作提取到的特征类型是不相同的，特征类型粗分如表5.2所示。<br>​                                                                 表5.2 卷积提取的特征类型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积层次</th>
<th style="text-align:center">特征类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">浅层卷积</td>
<td style="text-align:center">边缘特征</td>
</tr>
<tr>
<td style="text-align:center">中层卷积</td>
<td style="text-align:center">局部特征</td>
</tr>
<tr>
<td style="text-align:center">深层卷积</td>
<td style="text-align:center">全局特征</td>
</tr>
</tbody>
</table>
</div>
<p>图像与不同卷积核的卷积可以用来执行边缘检测、锐化和模糊等操作。表5.3显示了应用不同类型的卷积核（滤波器）后的各种卷积图像。<br>​                                                                 表5.3 一些常见卷积核的作用</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积作用</th>
<th style="text-align:center">卷积核</th>
<th style="text-align:center">卷积后图像</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">输出原图</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; 0 &amp; 0 \ 0 &amp; 1 &amp; 0 \ 0 &amp; 0 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat.jpg" alt="origin_img"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出边缘差异）</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 0 &amp; -1 \ 0 &amp; 0 &amp; 0 \ -1 &amp; 0 &amp; 1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-edgeDetect.jpg" alt="edgeDetect-1"></td>
</tr>
<tr>
<td style="text-align:center">边缘检测（突出中间值）</td>
<td style="text-align:center">$\begin{bmatrix} -1 &amp; -1 &amp; -1 \ -1 &amp; 8 &amp; -1 \ -1 &amp; -1 &amp; -1 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-edgeDetect-2.jpg" alt="edgeDetect-2"></td>
</tr>
<tr>
<td style="text-align:center">图像锐化</td>
<td style="text-align:center">$\begin{bmatrix} 0 &amp; -1 &amp; 0 \ -1 &amp; 5 &amp; -1 \ 0 &amp; -1 &amp; 0 \end{bmatrix}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-sharpen.jpg" alt="sharpen_img"></td>
</tr>
<tr>
<td style="text-align:center">方块模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 \end{bmatrix} \times \frac{1}{9}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-boxblur.jpg" alt="box_blur"></td>
</tr>
<tr>
<td style="text-align:center">高斯模糊</td>
<td style="text-align:center">$\begin{bmatrix} 1 &amp; 2 &amp; 1 \ 2 &amp; 4 &amp; 2 \ 1 &amp; 2 &amp; 1 \end{bmatrix} \times \frac{1}{16}$</td>
<td style="text-align:center"><img src="/img/ch5/cat-blur-gaussian.jpg" alt="gaussian_blur"></td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-3-卷积层有哪些基本参数？"><a href="#5-3-卷积层有哪些基本参数？" class="headerlink" title="5.3 卷积层有哪些基本参数？"></a>5.3 卷积层有哪些基本参数？</h2><p>​    卷积层中需要用到卷积核（滤波器或特征检测器）与图像特征矩阵进行点乘运算，利用卷积核与对应的特征感受域进行划窗式运算时，需要设定卷积核对应的大小、步长、个数以及填充的方式，如表5.4所示。</p>
<p>​                                                                         表5.4 卷积层的基本参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:left">作用</th>
<th style="text-align:left">常见设置</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">卷积核大小 (Kernel Size)</td>
<td style="text-align:left">卷积核的大小定义了卷积的感受野</td>
<td style="text-align:left">在过去常设为5，如LeNet-5；现在多设为3，通过堆叠$3\times3$的卷积核来达到更大的感受域</td>
</tr>
<tr>
<td style="text-align:center">卷积核步长 (Stride)</td>
<td style="text-align:left">定义了卷积核在卷积过程中的步长</td>
<td style="text-align:left">常见设置为1，表示滑窗距离为1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样</td>
</tr>
<tr>
<td style="text-align:center">填充方式 (Padding)</td>
<td style="text-align:left">在卷积核尺寸不能完美匹配输入的图像矩阵时需要进行一定的填充策略</td>
<td style="text-align:left">设置为’SAME’表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；当设置为’VALID’时则对不足卷积尺寸的部分进行舍弃，输出维度就无法保证与输入维度一致</td>
</tr>
<tr>
<td style="text-align:center">输入通道数 (In Channels)</td>
<td style="text-align:left">指定卷积操作时卷积核的深度</td>
<td style="text-align:left">默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式</td>
</tr>
<tr>
<td style="text-align:center">输出通道数 (Out Channels)</td>
<td style="text-align:left">指定卷积核的个数</td>
<td style="text-align:left">若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>卷积操作维度变换公式：</p>
<p>$O_d =\begin{cases} \lceil \frac{(I_d - k_{size})+ 1)}{s}\rceil ,&amp; \text{padding=VALID}\ \lceil \frac{I_d}{s}\rceil,&amp;\text{padding=SAME} \end{cases}$</p>
<p>其中，$I_d$为输入维度，$O_d$为输出维度，$k_{size}$为卷积核大小，$s$为步长</p>
</blockquote>
<h2 id="5-4-卷积核有什么类型？"><a href="#5-4-卷积核有什么类型？" class="headerlink" title="5.4 卷积核有什么类型？"></a>5.4 卷积核有什么类型？</h2><p>​    常见的卷积主要是由连续紧密的卷积核对输入的图像特征进行滑窗式点乘求和操作，除此之外还有其他类型的卷积核在不同的任务中会用到，具体分类如表5.5所示。<br>​                                                                     表5.5 卷积核分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">卷积类别</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">标准卷积</td>
<td style="text-align:center"><img src="/img/ch5/img7.png" alt="image"></td>
<td style="text-align:left">最常用的卷积核，连续紧密的矩阵形式可以提取图像区域中的相邻像素之间的关联关系，$3\times3$的卷积核可以获得$3\times3$像素范围的感受视野</td>
</tr>
<tr>
<td style="text-align:center">扩张卷积（带孔卷积或空洞卷积）</td>
<td style="text-align:center"><img src="/img/ch5/img8.png" alt="image"></td>
<td style="text-align:left">引入一个称作扩张率（Dilation Rate）的参数，使同样尺寸的卷积核可以获得更大的感受视野，相应的在相同感受视野的前提下比普通卷积采用更少的参数。同样是$3\times3$的卷积核尺寸，扩张卷积可以提取$5\times5$范围的区域特征，在实时图像分割领域广泛应用</td>
</tr>
<tr>
<td style="text-align:center">转置卷积</td>
<td style="text-align:center"><img src="/img/ch5/img10.png" alt="image"></td>
<td style="text-align:left">先对原始特征矩阵进行填充使其维度扩大到适配卷积目标输出维度，然后进行普通的卷积操作的一个过程，其输入到输出的维度变换关系恰好与普通卷积的变换关系相反，但这个变换并不是真正的逆变换操作，通常称为转置卷积(Transpose Convolution)而不是反卷积(Deconvolution)。转置卷积常见于目标检测领域中对小目标的检测和图像分割领域还原输入图像尺度。</td>
</tr>
<tr>
<td style="text-align:center">可分离卷积</td>
<td style="text-align:center"><img src="/img/ch5/img11.png" alt="image"></td>
<td style="text-align:left">标准的卷积操作是同时对原始图像$H\times W\times C$三个方向的卷积运算，假设有$K$个相同尺寸的卷积核，这样的卷积操作需要用到的参数为$H\times W\times C\times K$个；若将长宽与深度方向的卷积操作分离出变为$H\times W$与$C$的两步卷积操作，则同样的卷积核个数$K$，只需要$(H\times W + C)\times K$个参数，便可得到同样的输出尺度。可分离卷积(Seperable Convolution)通常应用在模型压缩或一些轻量的卷积神经网络中，如MobileNet$^{[1]}$、Xception$^{[2]}$等</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-5-二维卷积与三维卷积有什么区别？"><a href="#5-5-二维卷积与三维卷积有什么区别？" class="headerlink" title="5.5 二维卷积与三维卷积有什么区别？"></a>5.5 二维卷积与三维卷积有什么区别？</h2><ul>
<li><strong>二维卷积</strong><br>二维卷积操作如图5.3所示，为了更直观的说明，分别展示在单通道和多通道输入中，对单个通道输出的卷积操作。在单通道输入的情况下，若输入卷积核尺寸为 $(k_h, k_w, 1)​$，卷积核在输入图像的空间维度上进行滑窗操作，每次滑窗和 $(k_h, k_w)​$窗口内的值进行卷积操作，得到输出图像中的一个值。在多通道输入的情况下，假定输入图像特征通道数为3，卷积核尺寸则为$(k_h, k_w, 3)​$，每次滑窗与3个通道上的$(k_h, k_w)​$窗口内的所有值进行卷积操作，得到输出图像中的一个值。</li>
</ul>
<p><img src="/img/ch5/5.6.1.png" alt="image"></p>
<ul>
<li><strong>三维卷积</strong><br>3D卷积操作如图所示，同样分为单通道和多通道，且假定只使用1个卷积核，即输出图像仅有一个通道。对于单通道输入，与2D卷积不同之处在于，输入图像多了一个深度(depth)维度，卷积核也多了一个$k_d​$维度，因此3D卷积核的尺寸为$(k_h, k_w, k_d)​$，每次滑窗与$(k_h, k_w, k_d)​$窗口内的值进行相关操作，得到输出3D图像中的一个值。对于多通道输入，则与2D卷积的操作一样，每次滑窗与3个channels上的$(k_h, k_w, k_d)​$窗口内的所有值进行相关操作，得到输出3D图像中的一个值。</li>
</ul>
<p><img src="/img/ch5/5.6.2.png" alt="image"></p>
<h2 id="5-7-有哪些池化方法？"><a href="#5-7-有哪些池化方法？" class="headerlink" title="5.7 有哪些池化方法？"></a>5.7 有哪些池化方法？</h2><p>​    池化操作通常也叫做子采样(Subsampling)或降采样(Downsampling)，在构建卷积神经网络时，往往会用在卷积层之后，通过池化来降低卷积层输出的特征维度，有效减少网络参数的同时还可以防止过拟合现象。池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。因此，为了描述一个大的图像，很直观的想法就是对不同位置的特征进行聚合统计。例如，可以计算图像在固定区域上特征的平均值 (或最大值)来代表这个区域的特征。<br>​                                                                              表5.6 池化分类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">池化类型</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">一般池化(General Pooling)</td>
<td style="text-align:center"><img src="/img/ch5/general_pooling.png" alt="max_pooling"></td>
<td style="text-align:left">通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。以最大池化为例，池化范围$(2\times2)$和滑窗步长$(stride=2)$ 相同，仅提取一次相同区域的范化特征。</td>
</tr>
<tr>
<td style="text-align:center">重叠池化(Overlapping Pooling)</td>
<td style="text-align:center"><img src="/img/ch5/overlap_pooling.png" alt="overlap_pooling"></td>
<td style="text-align:left">与一般池化操作相同，但是池化范围$P_{size}$与滑窗步长$stride$关系为$P_{size}&gt;stride$，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。</td>
</tr>
<tr>
<td style="text-align:center">空间金字塔池化$^*$(Spatial Pyramid Pooling)</td>
<td style="text-align:center"><img src="/img/ch5/spatial_pooling.png" alt="spatial_pooling"></td>
<td style="text-align:left">在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>SPPNet$^{[3]}$就引入了空间池化的组合，对不同输出尺度采用不同的滑窗大小和步长以确保输出尺度相同$(win_{size}=\lceil \frac{in}{out}\rceil; stride=\lfloor \frac{in}{out}\rfloor; )$，同时用如金字塔式叠加的多种池化尺度组合，以提取更加丰富的图像特征。常用于多尺度训练和目标检测中的区域提议网络(Region Proposal Network)的兴趣区域(Region of Interest)提取</p>
</blockquote>
<h2 id="5-8-1-times1-卷积作用？"><a href="#5-8-1-times1-卷积作用？" class="headerlink" title="5.8 $1\times1$卷积作用？"></a>5.8 $1\times1$卷积作用？</h2><p>​    NIN(Network in Network)$^{[4]}​$是第一篇探索$1\times1​$卷积核的论文，这篇论文通过在卷积层中使用MLP替代传统线性的卷积核，使单层卷积层内具有非线性映射的能力，也因其网络结构中嵌套MLP子网络而得名NIN。NIN对不同通道的特征整合到MLP自网络中，让不同通道的特征能够交互整合，使通道之间的信息得以流通，其中的MLP子网络恰恰可以用$1\times1​$的卷积进行代替。</p>
<p>​    GoogLeNet$^{[5]}​$则采用$1\times1​$卷积核来减少模型的参数量。在原始版本的Inception模块中，由于每一层网络采用了更多的卷积核，大大增加了模型的参数量。此时在每一个较大卷积核的卷积层前引入$1\times1​$卷积，可以通过分离通道与宽高卷积来减少模型参数量。以图5.2为例，在不考虑参数偏置项的情况下，若输入和输出的通道数为$C_1=16​$，则左半边网络模块所需的参数为$(1\times1+3\times3+5\times5+0)\times C_1\times C_1=8960​$；假定右半边网络模块采用的$1\times1​$卷积通道数为$C_2=8​$$(满足C_1&gt;C_2)​$，则右半部分的网络结构所需参数量为$(1\times1\times (3C_1+C_2)+3\times3\times C_2 +5\times5\times C_2)\times C_1=5248​$ ，可以在不改变模型表达能力的前提下大大减少所使用的参数量。</p>
<p><img src="/img/ch5/5.8-1.png" alt="image"></p>
<p>​                                图5.2 Inception模块</p>
<p>综上所述，$1\times 1​$卷积的作用主要为以下两点：</p>
<ul>
<li>实现信息的跨通道交互和整合。</li>
<li>对卷积核通道数进行降维和升维，减小参数量。</li>
</ul>
<h2 id="5-9-卷积层和池化层有什么区别？"><a href="#5-9-卷积层和池化层有什么区别？" class="headerlink" title="5.9 卷积层和池化层有什么区别？"></a>5.9 卷积层和池化层有什么区别？</h2><p>​    卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据步长设置获取到不同维度的输出，但是其内在操作是有本质区别的，如表5.7所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">卷积层</th>
<th style="text-align:center">池化层</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>结构</strong></td>
<td style="text-align:center">零填充时输出维度不变，而通道数改变</td>
<td style="text-align:center">通常特征维度会降低，通道数不变</td>
</tr>
<tr>
<td style="text-align:center"><strong>稳定性</strong></td>
<td style="text-align:center">输入特征发生细微改变时，输出结果会改变</td>
<td style="text-align:center">感受域内的细微变化不影响输出结果</td>
</tr>
<tr>
<td style="text-align:center"><strong>作用</strong></td>
<td style="text-align:center">感受域内提取局部关联特征</td>
<td style="text-align:center">感受域内提取泛化特征，降低维度</td>
</tr>
<tr>
<td style="text-align:center"><strong>参数量</strong></td>
<td style="text-align:center">与卷积核尺寸、卷积核个数相关</td>
<td style="text-align:center">不引入额外参数</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-10-卷积核是否一定越大越好？"><a href="#5-10-卷积核是否一定越大越好？" class="headerlink" title="5.10 卷积核是否一定越大越好？"></a>5.10 卷积核是否一定越大越好？</h2><p>​    在早期的卷积神经网络中（如LeNet-5、AlexNet），用到了一些较大的卷积核（$11\times11$和$5\times 5$），受限于当时的计算能力和模型结构的设计，无法将网络叠加得很深，因此卷积网络中的卷积层需要设置较大的卷积核以获取更大的感受域。但是这种大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算性能也会降低。后来的卷积神经网络（VGG、GoogLeNet等），发现通过堆叠2个$3\times 3$卷积核可以获得与$5\times 5$卷积核相同的感受视野，同时参数量会更少（$3×3×2+1$ &lt; $ 5×5×1+1$），$3\times 3$卷积核被广泛应用在许多卷积神经网络中。因此可以认为，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。</p>
<p>​    但是，这并不是表示更大的卷积核就没有作用，在某些领域应用卷积神经网络时仍然可以采用较大的卷积核。譬如在自然语言处理领域，由于文本内容不像图像数据可以对特征进行很深层的抽象，往往在该领域的特征提取只需要较浅层的神经网络即可。在将卷积神经网络应用在自然语言处理领域时，通常都是较为浅层的卷积层组成，但是文本特征有时又需要有较广的感受域让模型能够组合更多的特征（如词组和字符），此时直接采用较大的卷积核将是更好的选择。</p>
<p>​    综上所述，卷积核的大小并没有绝对的优劣，需要视具体的应用场景而定，但是极大和极小的卷积核都是不合适的，单独的$1\times 1$极小卷积核只能用作分离卷积而不能对输入的原始特征进行有效的组合，极大的卷积核通常会组合过多的无意义特征从而浪费了大量的计算资源。</p>
<h2 id="5-11-每层卷积是否只能用一种尺寸的卷积核？"><a href="#5-11-每层卷积是否只能用一种尺寸的卷积核？" class="headerlink" title="5.11 每层卷积是否只能用一种尺寸的卷积核？"></a>5.11 每层卷积是否只能用一种尺寸的卷积核？</h2><p>​    经典的神经网络一般都属于层叠式网络，每层仅用一个尺寸的卷积核，如VGG结构中使用了大量的$3×3$卷积层。事实上，同一层特征图可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好，如GoogLeNet、Inception系列的网络，均是每层使用了多个卷积核结构。如图5.3所示，输入的特征在同一层分别经过$1×1$、$3×3$和$5×5$三种不同尺寸的卷积核，再将分别得到的特征进行整合，得到的新特征可以看作不同感受域提取的特征组合，相比于单一卷积核会有更强的表达能力。</p>
<p><img src="/img/ch5/5.11-1.png" alt="image"></p>
<p>​                                图5.3 Inception模块结构</p>
<h2 id="5-12-怎样才能减少卷积层参数量？"><a href="#5-12-怎样才能减少卷积层参数量？" class="headerlink" title="5.12 怎样才能减少卷积层参数量？"></a>5.12 怎样才能减少卷积层参数量？</h2><p>减少卷积层参数量的方法可以简要地归为以下几点：</p>
<ul>
<li>使用堆叠小卷积核代替大卷积核：VGG网络中2个$3\times 3$的卷积核可以代替1个$5\times 5$的卷积核</li>
<li>使用分离卷积操作：将原本$K\times K\times C$的卷积操作分离为$K\times K\times 1$和$1\times1\times C$的两部分操作</li>
<li>添加$1\times 1$的卷积操作：与分离卷积类似，但是通道数可变，在$K\times K\times C_1$卷积前添加$1\times1\times C_2$的卷积核（满足$C_2 &lt;C_1$）</li>
<li>在卷积层前使用池化操作：池化可以降低卷积层的输入特征维度</li>
</ul>
<h2 id="5-13-在进行卷积操作时，必须同时考虑通道和区域吗？"><a href="#5-13-在进行卷积操作时，必须同时考虑通道和区域吗？" class="headerlink" title="5.13 在进行卷积操作时，必须同时考虑通道和区域吗？"></a>5.13 在进行卷积操作时，必须同时考虑通道和区域吗？</h2><p>​    标准卷积中，采用区域与通道同时处理的操作，如下图所示：</p>
<p><img src="/img/ch5/5.13-1.png" alt="image"></p>
<p>​    这样做可以简化卷积层内部的结构，每一个输出的特征像素都由所有通道的同一个区域提取而来。</p>
<p>​    但是这种方式缺乏灵活性，并且在深层的网络结构中使得运算变得相对低效，更为灵活的方式是使区域和通道的卷积分离开来，通道分离（深度分离）卷积网络由此诞生。如下图所示，Xception网络可解决上述问题。</p>
<p><img src="/img/ch5/5.13-2.png" alt="image"></p>
<p>​    我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道特征矩阵之后，再对这批新通道特征进行标准的$1×1​$跨通道卷积操作。</p>
<h2 id="5-14-采用宽卷积的好处有什么？"><a href="#5-14-采用宽卷积的好处有什么？" class="headerlink" title="5.14 采用宽卷积的好处有什么？"></a>5.14 采用宽卷积的好处有什么？</h2><p>​    宽卷积对应的是窄卷积，实际上并不是卷积操作的类型，指的是卷积过程中的填充方法，对应的是’SAME’填充和’VALID’填充。’SAME’填充通常采用零填充的方式对卷积核不满足整除条件的输入特征进行补全，以使卷积层的输出维度保持与输入特征维度一致；’VALID’填充的方式则相反，实际并不进行任何填充，在输入特征边缘位置若不足以进行卷积操作，则对边缘信息进行舍弃，因此在步长为1的情况下该填充方式的卷积层输出特征维度可能会略小于输入特征的维度。此外，由于前一种方式通过补零来进行完整的卷积操作，可以有效地保留原始的输入特征信息。</p>
<p>​    比如下图左部分为窄卷积。注意到越在边缘的位置被卷积的次数越少。宽卷积可以看作在卷积之前在边缘用0补充，常见有两种情况，一个是全补充，如下图右部分，这样输出大于输入的维度。另一种常用的方法是补充一一部分0值，使得输出和输入的维度一致。</p>
<p><img src="/img/ch5/5.14.1.png" alt="image"></p>
<h2 id="5-15-理解转置卷积与棋盘效应"><a href="#5-15-理解转置卷积与棋盘效应" class="headerlink" title="5.15 理解转置卷积与棋盘效应"></a>5.15 理解转置卷积与棋盘效应</h2><h3 id="5-15-1-标准卷积"><a href="#5-15-1-标准卷积" class="headerlink" title="5.15.1 标准卷积"></a>5.15.1 标准卷积</h3><p>在理解转置卷积之前，需要先理解标准卷积的运算方式。</p>
<p>首先给出一个输入输出结果</p>
<p><img src="/img/ch5/img32.png" alt="image"></p>
<p>那是怎样计算的呢？</p>
<p>卷积的时候需要对卷积核进行180的旋转，同时卷积核中心与需计算的图像像素对齐，输出结构为中心对齐像素的一个新的像素值，计算例子如下：</p>
<p><img src="/img/ch5/5.19.1-2.png" alt="image"></p>
<p>这样计算出左上角(即第一行第一列)像素的卷积后像素值。</p>
<p>给出一个更直观的例子，从左到右看，原像素经过卷积由1变成-8。</p>
<p><img src="/img/ch5/5.19.1-3.png" alt="image"></p>
<p>通过滑动卷积核，就可以得到整张图片的卷积结果。</p>
<h3 id="5-15-2-转置卷积"><a href="#5-15-2-转置卷积" class="headerlink" title="5.15.2 转置卷积"></a>5.15.2 转置卷积</h3><p>图像的deconvolution过程如下：</p>
<p><img src="/img/ch5/5.19.2-5.png" alt="image"></p>
<p>输入：2x2， 卷积核：4x4， 滑动步长：3， 输出：7x7 </p>
<p>过程如下： </p>
<ol>
<li><p>输入图片每个像素进行一次full卷积，根据full卷积大小计算可以知道每个像素的卷积后大小为 1+4-1=4， 即4x4大小的特征图，输入有4个像素所以4个4x4的特征图 </p>
</li>
<li><p>将4个特征图进行步长为3的相加； 输出的位置和输入的位置相同。步长为3是指每隔3个像素进行相加，重叠部分进行相加，即输出的第1行第4列是由红色特阵图的第一行第四列与绿色特征图的第一行第一列相加得到，其他如此类推。  </p>
<p>可以看出翻卷积的大小是由卷积核大小与滑动步长决定， in是输入大小， k是卷积核大小， s是滑动步长， out是输出大小 得到 out = (in - 1) <em> s + k 上图过程就是， (2 - 1) </em> 3 + 4 = 7。</p>
</li>
</ol>
<h3 id="5-15-3-棋盘效应"><a href="#5-15-3-棋盘效应" class="headerlink" title="5.15.3 棋盘效应"></a>5.15.3 棋盘效应</h3><h2 id="5-16-卷积神经网络的参数设置"><a href="#5-16-卷积神经网络的参数设置" class="headerlink" title="5.16 卷积神经网络的参数设置"></a>5.16 卷积神经网络的参数设置</h2><p>​    卷积神经网络中常见的参数在其他类型的神经网络中也是类似的，但是参数的设置还得结合具体的任务才能设置在合理的范围，具体的参数列表如表XX所示。<br>​                                                    表XX 卷积神经网络常见参数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">参数名</th>
<th style="text-align:center">常见设置</th>
<th style="text-align:left">参数说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">学习率(Learning Rate)</td>
<td style="text-align:center">$0-1$</td>
<td style="text-align:left">反向传播网络中更新权值矩阵的步长，在一些常见的网络中会在固定迭代次数或模型不再收敛后对学习率进行指数下降(如$lr=lr\times 0.1$)。当学习率越大计算误差对权值矩阵的影响越大，容易在某个局部最优解附近震荡；越小的学习率对网络权值的更新越精细，但是需要花费更多的时间去迭代</td>
</tr>
<tr>
<td style="text-align:center">批次大小(Batch Size)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">批次大小指定一次性流入模型的数据样本个数，根据任务和计算性能限制判断实际取值，在一些图像任务中往往由于计算性能和存储容量限制只能选取较小的值。在相同迭代次数的前提下，数值越大模型越稳定，泛化能力越强，损失值曲线越平滑，模型也更快地收敛，但是每次迭代需要花费更多的时间</td>
</tr>
<tr>
<td style="text-align:center">数据轮次(Epoch)</td>
<td style="text-align:center">$1-N$</td>
<td style="text-align:left">数据轮次指定所有训练数据在模型中训练的次数，根据数据集规模和分布情况会设置为不同的值。当模型较为简单或训练数据规模较小时，通常轮次不宜过高，否则模型容易过拟合；模型较为复杂或训练数据规模足够大时，可适当提高数据的训练轮次。</td>
</tr>
<tr>
<td style="text-align:center">权重衰减系数(Weight Decay)</td>
<td style="text-align:center">$0-0.001$</td>
<td style="text-align:left">模型训练过程中反向传播权值更新的权重衰减值</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-17-提高卷积神经网络的泛化能力"><a href="#5-17-提高卷积神经网络的泛化能力" class="headerlink" title="5.17 提高卷积神经网络的泛化能力"></a>5.17 提高卷积神经网络的泛化能力</h2><p>​    卷积神经网络与其他类型的神经网络类似，在采用反向传播进行训练的过程中比较依赖输入的数据分布，当数据分布较为极端的情况下容易导致模型欠拟合或过拟合，表XX记录了提高卷积网络泛化能力的方法。<br>​                                                                   表XX 提高卷积网络化能力的方法</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方法</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">使用更多数据</td>
<td style="text-align:left">在有条件的前提下，尽可能多地获取训练数据是最理想的方法，更多的数据可以让模型得到充分的学习，也更容易提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">使用更大批次</td>
<td style="text-align:left">在相同迭代次数和学习率的条件下，每批次采用更多的数据将有助于模型更好的学习到正确的模式，模型输出结果也会更加稳定</td>
</tr>
<tr>
<td style="text-align:center">调整数据分布</td>
<td style="text-align:left">大多数场景下的数据分布是不均匀的，模型过多地学习某类数据容易导致其输出结果偏向于该类型的数据，此时通过调整输入的数据分布可以一定程度提高泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整目标函数</td>
<td style="text-align:left">在某些情况下，目标函数的选择会影响模型的泛化能力，如目标函数$f(y,y’)=</td>
<td>y-y’</td>
<td>$在某类样本已经识别较为准确而其他样本误差较大的侵害概况下，不同类别在计算损失结果的时候距离权重是相同的，若将目标函数改成$f(y,y’)=(y-y’)^2$则可以使误差小的样本计算损失的梯度比误差大的样本更小，进而有效地平衡样本作用，提高模型泛化能力</td>
</tr>
<tr>
<td style="text-align:center">调整网络结构</td>
<td style="text-align:left">在浅层卷积神经网络中，参数量较少往往使模型的泛化能力不足而导致欠拟合，此时通过叠加卷积层可以有效地增加网络参数，提高模型表达能力；在深层卷积网络中，若没有充足的训练数据则容易导致模型过拟合，此时通过简化网络结构减少卷积层数可以起到提高模型泛化能力的作用</td>
</tr>
<tr>
<td style="text-align:center">数据增强</td>
<td style="text-align:left">数据增强又叫数据增广，在有限数据的前提下通过平移、旋转、加噪声等一些列变换来增加训练数据，同类数据的表现形式也变得更多样，有助于模型提高泛化能力，需要注意的是数据变化应尽可能不破坏元数数据的主体特征(如在图像分类任务中对图像进行裁剪时不能将分类主体目标裁出边界)。</td>
</tr>
<tr>
<td style="text-align:center">权值正则化</td>
<td style="text-align:left">权值正则化就是通常意义上的正则化，一般是在损失函数中添加一项权重矩阵的正则项作为惩罚项，用来惩罚损失值较小时网络权重过大的情况，此时往往是网络权值过拟合了数据样本(如$Loss=f(WX+b,y’)+\frac{\lambda}{\eta}\sum{</td>
<td>W</td>
<td>}$)。</td>
</tr>
<tr>
<td style="text-align:center">屏蔽网络节点</td>
<td style="text-align:left">该方法可以认为是网络结构上的正则化，通过随机性地屏蔽某些神经元的输出让剩余激活的神经元作用，可以使模型的容错性更强。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>对大多数神经网络模型同样通用</p>
</blockquote>
<h2 id="5-18-卷积神经网络在不同领域的应用"><a href="#5-18-卷积神经网络在不同领域的应用" class="headerlink" title="5.18 卷积神经网络在不同领域的应用"></a>5.18 卷积神经网络在不同领域的应用</h2><p>​    卷积神经网络中的卷积操作是其关键组成，而卷积操作只是一种数学运算方式，实际上对不同类型的数值表示数据都是通用的，尽管这些数值可能表示的是图像像素值、文本序列中单个字符或是语音片段中单字的音频。只要使原始数据能够得到有效地数值化表示，卷积神经网络能够在不同的领域中得到应用，要关注的是如何将卷积的特性更好地在不同领域中应用，如表XX所示。<br>​                                                    表XX 卷积神经网络不同领域的应用<br>| 应用领域 | 输入数据图示 | 说明 |<br>| :——-: | :—————: | :— |<br>|   图像处理   | <img src="img/ch5/Image-process.png" alt="image_process"> | 卷积神经网络在图像处理领域有非常广泛的应用，这是因为图像数据本身具有的局部完整性非常 |<br>| 自然语言处理 | <img src="img/ch5/NLP.png" alt="NLP"> |  |<br>|   语音处理   | <img src="img/ch5/audio-recognition.png" alt="audio_process"> |  |</p>
<h3 id="5-18-1-联系"><a href="#5-18-1-联系" class="headerlink" title="5.18.1 联系"></a>5.18.1 联系</h3><p>​    自然语言处理是对一维信号（词序列）做操作。<br>​    计算机视觉是对二维（图像）或三维（视频流）信号做操作。</p>
<h3 id="5-18-2-区别"><a href="#5-18-2-区别" class="headerlink" title="5.18.2 区别"></a>5.18.2 区别</h3><p>​    自然语言处理的输入数据通常是离散取值（例如表示一个单词或字母通常表示为词典中的one hot向量），计算机视觉则是连续取值（比如归一化到0，1之间的灰度值）。CNN有两个主要特点，区域不变性(location invariance)和组合性(Compositionality)。</p>
<ol>
<li>区域不变性：滤波器在每层的输入向量(图像)上滑动，检测的是局部信息，然后通过pooling取最大值或均值。pooling这步综合了局部特征，失去了每个特征的位置信息。这很适合基于图像的任务，比如要判断一幅图里有没有猫这种生物，你可能不会去关心这只猫出现在图像的哪个区域。但是在NLP里，词语在句子或是段落里出现的位置，顺序，都是很重要的信息。</li>
<li>局部组合性：CNN中，每个滤波器都把较低层的局部特征组合生成较高层的更全局化的特征。这在CV里很好理解，像素组合成边缘，边缘生成形状，最后把各种形状组合起来得到复杂的物体表达。在语言里，当然也有类似的组合关系，但是远不如图像来的直接。而且在图像里，相邻像素必须是相关的，相邻的词语却未必相关。</li>
</ol>
<h2 id="5-19-卷积神经网络凸显共性的方法？"><a href="#5-19-卷积神经网络凸显共性的方法？" class="headerlink" title="5.19 卷积神经网络凸显共性的方法？"></a>5.19 卷积神经网络凸显共性的方法？</h2><h3 id="5-19-1-局部连接"><a href="#5-19-1-局部连接" class="headerlink" title="5.19.1 局部连接"></a>5.19.1 局部连接</h3><p>​    我们首先了解一个概念，感受野，即每个神经元仅与输入神经元相连接的一块区域。<br>在图像卷积操作中，神经元在空间维度上是局部连接，但在深度上是全连接。局部连接的思想，是受启发于生物学里的视觉系统结构，视觉皮层的神经元就是仅用局部接受信息。对于二维图像，局部像素关联性较强。这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征；<br>下图是一个很经典的图示，左边是全连接，右边是局部连接。</p>
<p><img src="/img/ch5/5.27.1.png" alt="image"></p>
<p>对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此巨大的参数量几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。</p>
<h3 id="5-19-2-权值共享"><a href="#5-19-2-权值共享" class="headerlink" title="5.19.2 权值共享"></a>5.19.2 权值共享</h3><p>​    权值共享，即计算同一深度的神经元时采用的卷积核参数是共享的。权值共享在一定程度上讲是有意义的，是由于在神经网络中，提取的底层边缘特征与其在图中的位置无关。但是在另一些场景中是无意的，如在人脸识别任务，我们期望在不同的位置学到不同的特征。<br>需要注意的是，权重只是对于同一深度切片的神经元是共享的。在卷积层中，通常采用多组卷积核提取不同的特征，即对应的是不同深度切片的特征，而不同深度切片的神经元权重是不共享。相反，偏置这一权值对于同一深度切片的所有神经元都是共享的。<br>权值共享带来的好处是大大降低了网络的训练难度。如下图，假设在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核的大小）。</p>
<p><img src="/img/ch5/5.27.2.png" alt="image"></p>
<p>这里就体现了卷积神经网络的奇妙之处，使用少量的参数，却依然能有非常出色的性能。上述仅仅是提取图像一种特征的过程。如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像不同尺度下的特征，称之为特征图（feature map）。</p>
<h3 id="5-19-3-池化操作"><a href="#5-19-3-池化操作" class="headerlink" title="5.19.3 池化操作"></a>5.19.3 池化操作</h3><p>池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图：</p>
<p><img src="/img/ch5/5.27.3.png" alt="image"></p>
<h2 id="5-20-全连接、局部连接、全卷积与局部卷积"><a href="#5-20-全连接、局部连接、全卷积与局部卷积" class="headerlink" title="5.20 全连接、局部连接、全卷积与局部卷积"></a>5.20 全连接、局部连接、全卷积与局部卷积</h2><p>​    大多数神经网络中高层网络通常会采用全连接层(Global Connected Layer)，通过多对多的连接方式对特征进行全局汇总，可以有效地提取全局信息。但是全连接的方式需要大量的参数，是神经网络中最占资源的部分之一，因此就需要由局部连接(Local Connected Layer)，仅在局部区域范围内产生神经元连接，能够有效地减少参数量。根据卷积操作的作用范围可以分为全卷积(Global Convolution)和局部卷积(Local Convolution)。实际上这里所说的全卷积就是标准卷积，即在整个输入特征维度范围内采用相同的卷积核参数进行运算，全局共享参数的连接方式可以使神经元之间的连接参数大大减少;局部卷积又叫平铺卷积(Tiled Convolution)或非共享卷积(Unshared Convolution)，是局部连接与全卷积的折衷。四者的比较如表XX所示。<br>​                                                     表XX 卷积网络中连接方式的对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">连接方式</th>
<th style="text-align:center">示意图</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">全连接</td>
<td style="text-align:center"><img src="img/ch5/full-connected.png" alt="full-connected"></td>
<td style="text-align:left">层间神经元完全连接，每个输出神经元可以获取到所有输入神经元的信息，有利于信息汇总，常置于网络末层；连接与连接之间独立参数，大量的连接大大增加模型的参数规模。</td>
</tr>
<tr>
<td style="text-align:center">局部连接</td>
<td style="text-align:center"><img src="img/ch5/local-connected.png" alt="local-connected"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，超过这个范围的神经元则没有连接；连接与连接之间独立参数，相比于全连接减少了感受域外的连接，有效减少参数规模</td>
</tr>
<tr>
<td style="text-align:center">全卷积</td>
<td style="text-align:center"><img src="img/ch5/conv.png" alt="convolution"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，在这个范围内采用全连接的方式，连接所采用的参数在不同感受域之间共享，有利于提取特定模式的特征；相比于局部连接，共用感受域之间的参数可以进一步减少参数量。</td>
</tr>
<tr>
<td style="text-align:center">局部卷积</td>
<td style="text-align:center"><img src="img/ch5/local-conv.png" alt="local-conv"></td>
<td style="text-align:left">层间神经元只有局部范围内的连接，感受域内采用全连接的方式，而感受域之间间隔采用局部连接与全卷积的连接方式；相比与全卷积成倍引入额外参数，但有更强的灵活性和表达能力；相比于局部连接，可以有效控制参数量</td>
</tr>
</tbody>
</table>
</div>
<h2 id="5-21-局部卷积的应用"><a href="#5-21-局部卷积的应用" class="headerlink" title="5.21 局部卷积的应用"></a>5.21 局部卷积的应用</h2><p>并不是所有的卷积都会进行权重共享，在某些特定任务中，会使用不权重共享的卷积。下面通过人脸这一任务来进行讲解。在读人脸方向的一些paper时，会发现很多都会在最后加入一个Local Connected Conv，也就是不进行权重共享的卷积层。总的来说，这一步的作用就是使用3D模型来将人脸对齐，从而使CNN发挥最大的效果。<br><img src="/img/ch5/img66.png" alt="image"></p>
<p>截取论文中的一部分图，经过3D对齐以后，形成的图像均是152×152，输入到上述的网络结构中。该结构的参数如下：</p>
<p>Conv：32个11×11×3的卷积核，</p>
<p>Max-pooling: 3×3，stride=2，</p>
<p>Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个9×9的卷积核，</p>
<p>Local-Conv: 16个7×7的卷积核，</p>
<p>Local-Conv: 16个5×5的卷积核，</p>
<p>Fully-connected: 4096维，</p>
<p>Softmax: 4030维。</p>
<p>前三层的目的在于提取低层次的特征，比如简单的边和纹理。其中Max-pooling层使得卷积的输出对微小的偏移情况更加鲁棒。但不能使用更多的Max-pooling层，因为太多的Max-pooling层会使得网络损失图像信息。全连接层将上一层的每个单元和本层的所有单元相连，用来捕捉人脸图像不同位置特征之间的相关性。最后使用softmax层用于人脸分类。<br>中间三层都是使用参数不共享的卷积核，之所以使用参数不共享，有如下原因：</p>
<p>（1）对齐的人脸图片中，不同的区域会有不同的统计特征，因此并不存在特征的局部稳定性，所以使用相同的卷积核会导致信息的丢失。</p>
<p>（2）不共享的卷积核并不增加inference时特征的计算量，仅会增加训练时的计算量。<br>使用不共享的卷积核，由于需要训练的参数量大大增加，因此往往需要通过其他方法增加数据量。</p>
<h2 id="5-22-NetVLAD池化-（贡献者：熊楚原-中国人民大学）"><a href="#5-22-NetVLAD池化-（贡献者：熊楚原-中国人民大学）" class="headerlink" title="5.22 NetVLAD池化    （贡献者：熊楚原-中国人民大学）"></a>5.22 NetVLAD池化    （贡献者：熊楚原-中国人民大学）</h2><p>NetVLAD是论文[15]提出的一个局部特征聚合的方法。</p>
<p>在传统的网络里面，例如VGG啊，最后一层卷积层输出的特征都是类似于Batchsize x 3 x 3 x 512的这种东西，然后会经过FC聚合，或者进行一个Global Average Pooling（NIN里的做法），或者怎么样，变成一个向量型的特征，然后进行Softmax or 其他的Loss。</p>
<p>这种方法说简单点也就是输入一个图片或者什么的结构性数据，然后经过特征提取得到一个长度固定的向量，之后可以用度量的方法去进行后续的操作，比如分类啊，检索啊，相似度对比等等。</p>
<p>那么NetVLAD考虑的主要是最后一层卷积层输出的特征这里，我们不想直接进行欠采样或者全局映射得到特征，对于最后一层输出的W x H x D，设计一个新的池化，去聚合一个“局部特征“，这即是NetVLAD的作用。</p>
<p>NetVLAD的一个输入是一个W x H x D的图像特征，例如VGG-Net最后的3 x 3 x 512这样的矩阵，在网络中还需加一个维度为Batchsize。</p>
<p>NetVLAD还需要另输入一个标量K即表示VLAD的聚类中心数量，它主要是来构成一个矩阵C，是通过原数据算出来的每一个$W \times H$特征的聚类中心，C的shape即$C: K \times D$，然后根据三个输入，VLAD是计算下式的V:</p>
<script type="math/tex; mode=display">V(j, k) = \sum_{i=1}^{N}{a_k(x_i)(x_i(j) - c_k(j))}</script><p>其中j表示维度，从1到D，可以看到V的j是和输入与c对应的，对每个类别k，都对所有的x进行了计算，如果$x_i$属于当前类别k，$a_k=1$，否则$a_k=0$，计算每一个x和它聚类中心的残差，然后把残差加起来，即是每个类别k的结果，最后分别L2正则后拉成一个长向量后再做L2正则，正则非常的重要，因为这样才能统一所有聚类算出来的值，而残差和的目的主要是消减不同聚类上的分布不均，两者共同作用才能得到最后正常的输出。</p>
<p>输入与输出如下图所示：</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad.jpeg" alt="image"></p>
<p>中间得到的K个D维向量即是对D个x都进行了与聚类中心计算残差和的过程，最终把K个D维向量合起来后进行即得到最终输出的$K \times D$长度的一维向量。</p>
<p>而VLAD本身是不可微的，因为上面的a要么是0要么是1，表示要么当前描述x是当前聚类，要么不是，是个离散的，NetVLAD为了能够在深度卷积网络里使用反向传播进行训练，对a进行了修正。</p>
<p>那么问题就是如何重构一个a，使其能够评估当前的这个x和各个聚类的关联程度？用softmax来得到：</p>
<script type="math/tex; mode=display">a_k = \frac{e^{W_k^T x_i + b_k}}{e^{W_{k'}^T x_i + b_{k'}}}</script><p>将这个把上面的a替换后，即是NetVLAD的公式，可以进行反向传播更新参数。</p>
<p>所以一共有三个可训练参数，上式a中的$W: K \times D$，上式a中的$b: K \times 1$，聚类中心$c: K \times D$，而原始VLAD只有一个参数c。</p>
<p>最终池化得到的输出是一个恒定的K x D的一维向量（经过了L2正则），如果带Batchsize，输出即为Batchsize x (K x D)的二维矩阵。</p>
<p>NetVLAD作为池化层嵌入CNN网络即如下图所示，</p>
<p><img src="http://www.ecohnoch.cn/img/netvlad_emb.png" alt="image"></p>
<p>原论文中采用将传统图像检索方法VLAD进行改进后应用在CNN的池化部分作为一种另类的局部特征池化，在场景检索上取得了很好的效果。</p>
<p>后续相继又提出了ActionVLAD、ghostVLAD等改进。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] 卷积神经网络研究综述[J]. 计算机学报, 2017, 40(6):1229-1251.</p>
<p>[2] 常亮, 邓小明, 周明全,等. 图像理解中的卷积神经网络[J]. 自动化学报, 2016, 42(9):1300-1312.</p>
<p>[3] Chua L O. CNN: A Paradigm for Complexity[M]// CNN a paradigm for complexity /.  1998.</p>
<p>[4] He K, Gkioxari G, Dollar P, et al. Mask R-CNN[J]. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, PP(99):1-1.</p>
<p>[5] Hoochang S, Roth H R, Gao M, et al. Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning[J]. IEEE Transactions on Medical Imaging, 2016, 35(5):1285-1298.</p>
<p>[6] 许可. 卷积神经网络在图像识别上的应用的研究[D]. 浙江大学, 2012.</p>
<p>[7] 陈先昌. 基于卷积神经网络的深度学习算法与应用研究[D]. 浙江工商大学, 2014.</p>
<p>[8] <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">CS231n Convolutional Neural Networks for Visual Recognition, Stanford</a></p>
<p>[9] <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.2gfx5zcw3" target="_blank" rel="noopener">Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks</a></p>
<p>[10] cs231n 动态卷积图：<a href="http://cs231n.github.io/assets/conv-demo/index.html" target="_blank" rel="noopener">http://cs231n.github.io/assets/conv-demo/index.html</a></p>
<p>[11] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</p>
<p>[12] Sun Y, Wang X, Tang X. Deep learning face representation from predicting 10,000 classes[C]//Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014: 1891-1898.</p>
<p>[13] 魏秀参.解析深度学习——卷积神经网络原理与视觉实践[M].电子工业出版社, 2018</p>
<p>[14]  Jianxin W U ,  Gao B B ,  Wei X S , et al. Resource-constrained deep learning: challenges and practices[J]. Scientia Sinica(Informationis), 2018.</p>
<p>[15] Arandjelovic R , Gronat P , Torii A , et al. [IEEE 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - Las Vegas, NV, USA (2016.6.27-2016.6.30)] 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) - NetVLAD: CNN Architecture for Weakly Supervised Place Recognition[C]// 2016:5297-5307.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bassyess.github.io/2020/03/03/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Kay">
      <meta itemprop="description" content="千里之行，始于足下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/03/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/" class="post-title-link" itemprop="url">第十二章_网络搭建及训练</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-03 12:32:46 / 修改时间：12:33:03" itemprop="dateCreated datePublished" datetime="2020-03-03T12:32:46+08:00">2020-03-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[TOC]</p>
<h1 id="第十二章-网络搭建及训练"><a href="#第十二章-网络搭建及训练" class="headerlink" title="第十二章 网络搭建及训练"></a>第十二章 网络搭建及训练</h1><h1 id="12-1-TensorFlow"><a href="#12-1-TensorFlow" class="headerlink" title="12.1 TensorFlow"></a>12.1 TensorFlow</h1><h2 id="12-1-1-TensorFlow是什么？"><a href="#12-1-1-TensorFlow是什么？" class="headerlink" title="12.1.1 TensorFlow是什么？"></a>12.1.1 TensorFlow是什么？</h2><p>&emsp;&emsp;TensorFlow支持各种异构平台，支持多CPU/GPU、服务器、移动设备，具有良好的跨平台的特性；TensorFlow架构灵活，能够支持各种网络模型，具有良好的通用性；此外，TensorFlow架构具有良好的可扩展性，对OP的扩展支持，Kernel特化方面表现出众。</p>
<p>&emsp;&emsp;TensorFlow最初由Google大脑的研究员和工程师开发出来，用于机器学习和神经网络方面的研究，于2015.10宣布开源，在众多深度学习框架中脱颖而出，在Github上获得了最多的Star量。</p>
<h2 id="12-1-2-TensorFlow的设计理念是什么？"><a href="#12-1-2-TensorFlow的设计理念是什么？" class="headerlink" title="12.1.2 TensorFlow的设计理念是什么？"></a>12.1.2 TensorFlow的设计理念是什么？</h2><p>TensorFlow的设计理念主要体现在两个方面：</p>
<p>（1）将图定义和图运算完全分开。<br>&emsp;&emsp;TensorFlow 被认为是一个“符号主义”的库。我们知道，编程模式通常分为命令式编程（imperative style programming）和符号式编程（symbolic style programming）。命令式编程就是编写我们理解的通常意义上的程序，很容易理解和调试，按照原有逻辑执行。符号式编程涉及很多的嵌入和优化，不容易理解和调试，但运行速度相对有所提升。现有的深度学习框架中，Torch 是典型的命令式的，Caffe、MXNet 采用了两种编程模式混合的方法，而 TensorFlow 完全采用符号式编程。</p>
<p>&emsp;&emsp;符号式计算一般是先定义各种变量，然后建立一个数据流图，在数据流图中规定各个变量间的计算关系，最后需要对据流图进行编译，但此时的数据流图还是一个空壳儿，里面没有任何实际数据，只有把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。</p>
<p>　　例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t &#x3D; 8 + 9</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在传统的程序操作中，定义了 t 的运算，在运行时就执行了，并输出 17。而在 TensorFlow中，数据流图中的节点，实际上对应的是 TensorFlow API 中的一个操作，并没有真正去运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">t &#x3D; tf.add(8,9)</span><br><span class="line">print(t)</span><br><span class="line"></span><br><span class="line">#输出  Tensor&#123;&quot;Add_1:0&quot;,shape&#x3D;&#123;&#125;,dtype&#x3D;int32&#125;</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;（2）TensorFlow 中涉及的运算都要放在图中，而图的运行只发生在会话（session）中。开启会话后，就可以用数据去填充节点，进行运算；关闭会话后，就不能进行计算了。因此，会话提供了操作运行和 Tensor 求值的环境。</p>
<p>　　例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">#创建图</span><br><span class="line">a &#x3D; tf.constant([4.0,5.0])</span><br><span class="line">b &#x3D; tf.constant([6.0,7.0])</span><br><span class="line">c &#x3D; a * b</span><br><span class="line">#创建会话</span><br><span class="line">sess  &#x3D; tf.Session()</span><br><span class="line">#计算c</span><br><span class="line">print(sess.run(c))   #进行矩阵乘法，输出[24.,35.]</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<h2 id="12-1-3-TensorFlow特点有哪些？"><a href="#12-1-3-TensorFlow特点有哪些？" class="headerlink" title="12.1.3 TensorFlow特点有哪些？"></a>12.1.3 TensorFlow特点有哪些？</h2><h3 id="1-高度的灵活性"><a href="#1-高度的灵活性" class="headerlink" title="1.高度的灵活性"></a>1.高度的灵活性</h3><p>&emsp;&emsp;TensorFlow 并不仅仅是一个深度学习库，只要可以把你的计算过程表示称一个数据流图的过程，我们就可以使用 TensorFlow 来进行计算。TensorFlow 允许我们用计算图的方式建立计算网络，同时又可以很方便的对网络进行操作。用户可以基于 TensorFlow 的基础上用 python 编写自己的上层结构和库，如果TensorFlow没有提供我们需要的API的，我们也可以自己编写底层的 C++ 代码，通过自定义操作将新编写的功能添加到 TensorFlow 中。</p>
<h3 id="2-真正的可移植性"><a href="#2-真正的可移植性" class="headerlink" title="2.真正的可移植性"></a>2.真正的可移植性</h3><p>&emsp;&emsp;TensorFlow 可以在 CPU 和 GPU 上运行，可以在台式机、服务器、移动设备上运行。你想在你的笔记本上跑一下深度学习的训练，或者又不想修改代码，想把你的模型在多个CPU上运行， 亦或想将训练好的模型放到移动设备上跑一下，这些TensorFlow都可以帮你做到。</p>
<h3 id="3-多语言支持"><a href="#3-多语言支持" class="headerlink" title="3.多语言支持"></a>3.多语言支持</h3><p>&emsp;&emsp;TensorFlow采用非常易用的python来构建和执行我们的计算图，同时也支持 C++ 的语言。我们可以直接写python和C++的程序来执行TensorFlow，也可以采用交互式的ipython来方便的尝试我们的想法。当然，这只是一个开始，后续会支持更多流行的语言，比如Lua，JavaScript 或者R语言。</p>
<h3 id="4-丰富的算法库"><a href="#4-丰富的算法库" class="headerlink" title="4.丰富的算法库"></a>4.丰富的算法库</h3><p>&emsp;&emsp;TensorFlow提供了所有开源的深度学习框架里，最全的算法库，并且在不断的添加新的算法库。这些算法库基本上已经满足了大部分的需求，对于普通的应用，基本上不用自己再去自定义实现基本的算法库了。</p>
<h3 id="5-完善的文档"><a href="#5-完善的文档" class="headerlink" title="5.完善的文档"></a>5.完善的文档</h3><p>&emsp;&emsp;TensorFlow的官方网站，提供了非常详细的文档介绍，内容包括各种API的使用介绍和各种基础应用的使用例子，也包括一部分深度学习的基础理论。</p>
<p>&emsp;&emsp;自从宣布开源以来，大量人员对TensorFlow做出贡献，其中包括Google员工，外部研究人员和独立程序员，全球各地的工程师对TensorFlow的完善，已经让TensorFlow社区变成了Github上最活跃的深度学习框架。</p>
<h2 id="12-1-4-TensorFlow的系统架构是怎样的？"><a href="#12-1-4-TensorFlow的系统架构是怎样的？" class="headerlink" title="12.1.4 TensorFlow的系统架构是怎样的？"></a>12.1.4 TensorFlow的系统架构是怎样的？</h2><h3 id="emsp-emsp-整个系统从底层到上层可分为七层："><a href="#emsp-emsp-整个系统从底层到上层可分为七层：" class="headerlink" title="&emsp;&emsp;整个系统从底层到上层可分为七层："></a>&emsp;&emsp;整个系统从底层到上层可分为七层：</h3><p><img src=".\img\ch12\1.bmp" alt=""></p>
<p>&emsp;&emsp;设备层：硬件计算资源，支持CPU、GPU</p>
<p>&emsp;&emsp;网络层：支持两种通信协议</p>
<p>&emsp;&emsp;数值计算层：提供最基础的计算，有线性计算、卷积计算</p>
<p>&emsp;&emsp;高维计算层：数据的计算都是以数组的形式参与计算</p>
<p>&emsp;&emsp;计算图层：用来设计神经网络的结构</p>
<p>&emsp;&emsp;工作流层：提供轻量级的框架调用</p>
<p>&emsp;&emsp;构造层：最后构造的深度学习网络可以通过TensorBoard服务端可视化</p>
<h2 id="12-1-5-TensorFlow编程模型是怎样的？"><a href="#12-1-5-TensorFlow编程模型是怎样的？" class="headerlink" title="12.1.5 TensorFlow编程模型是怎样的？"></a>12.1.5 TensorFlow编程模型是怎样的？</h2><p>TensorFlow的编程模型：让向量数据在计算图里流动。那么在编程时至少有这几个过程：1.构建图，2.启动图，3.给图输入数据并获取结果。</p>
<h3 id="1-构建图"><a href="#1-构建图" class="headerlink" title="1.构建图"></a>1.构建图</h3><p>TensorFlow的图的类型是tf.Graph，它包含着计算节点和tensor的集合。</p>
<p>&emsp;&emsp;这里引用了两个新概念：tensor和计算节点。<br>&emsp;&emsp;我们先介绍tensor，一开始我们就介绍了，我们需要把数据输入给启动的图才能获取计算结果。那么问题来了，在构建图时用什么表示中间计算结果？这个时候tensor的概念就需要引入了。<br>&emsp;&emsp;类型是tf.Tensor，代表某个计算节点的输出，一定要看清楚是“代表”。它主要有两个作用：</p>
<p>1.构建不同计算节点之间的数据流</p>
<p>2.在启动图时，可以设置某些tensor的值，然后获取指定tensor的值。这样就完成了计算的输入输出功能。</p>
<p>如下代码所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inImage &#x3D; tf.placeholder(tf.float32,[32,32,3],&quot;inputImage&quot;)</span><br><span class="line">processedImage &#x3D; tf.image.per_image_standardization(inImage,&quot;processedImage&quot;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里inImage和processedImage都是tensor类型。它们代表着计算节点输出的数据，数据的值具体是多少在启动图的时候才知道。上面两个方法调用都传递了一个字符串，它是计算节点的名字，最好给节点命名，这样我们可以在图上调用get_tensor_by_name(name)获取对应的tensor对象，十分方便。（tensor名字为“&lt;计算节点名字&gt;:<tensor索引>”）</p>
<p>&emsp;&emsp;创建tensor时，需要指定类型和shape。对不同tensor进行计算时要求类型相同，可以使用 tf.cast 进行类型转换。同时也要求 shape (向量维度)满足运算的条件，我们可以使用 tf.reshape 改变shape。</p>
<p>&emsp;&emsp;现在了解计算节点的概念，其功能是对tensor进行计算、创建tensor或进行其他操作，类型是tf.Operation。获取节点对象的方法为get_operation_by_name(name)。</p>
<p>构建图，如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">g&#x3D;tf.Graph()</span><br><span class="line"></span><br><span class="line">with g.as_default():</span><br><span class="line">    input_data&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_data&quot;)</span><br><span class="line">    input_label&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_label&quot;)</span><br><span class="line"></span><br><span class="line">    W1&#x3D;tf.Variable(tf.truncated_normal([2,2]),name&#x3D;&quot;W1&quot;)</span><br><span class="line">    B1&#x3D;tf.Variable(tf.zeros([2]),name&#x3D;&quot;B1&quot;)</span><br><span class="line"></span><br><span class="line">    output&#x3D;tf.add(tf.matmul(input_data,W1),B1,name&#x3D;&quot;output&quot;)</span><br><span class="line">    cross_entropy&#x3D;tf.nn.softmax_cross_entropy_with_logits(logits&#x3D;output,labels&#x3D;input_label)</span><br><span class="line"></span><br><span class="line">    train_step&#x3D;tf.train.AdamOptimizer().minimize(cross_entropy,name&#x3D;&quot;train_step&quot;)</span><br><span class="line"></span><br><span class="line">    initer&#x3D;tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;上面的代码中我们创建了一个图，并在上面添加了很多节点。我们可以通过调用get_default_graph()获取默认的图。</p>
<p>&emsp;&emsp;Input_data，input_label，W1，B1，output，cross_entropy都是tensor类型，train_step，initer，是节点类型。</p>
<p>有几类tensor或节点比较重要，下面介绍一下：</p>
<h4 id="1-placeholder"><a href="#1-placeholder" class="headerlink" title="1.placeholder"></a>1.placeholder</h4><p>&emsp;&emsp;Tensorflow，顾名思义， tensor代表张量数据，flow代表流，其最初的设计理念就是构建一张静态的数据流图。图是有各个计算节点连接而成，计算节点之间流动的便是中间的张量数据。要想让张量数据在我们构建的静态计算图中流动起来，就必须有最初的输入数据流。而placeholder，翻译过来叫做占位符，顾名思义，是给我们的输入数据提供一个接口，也就是说我们的一切输入数据，例如训练样本数据，超参数数据等都可以通过占位符接口输送到数据流图之中。使用实例如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x &#x3D; tf.placeholder(dtype&#x3D;tf.float32,shape&#x3D;[],name&#x3D;&#39;x&#39;)</span><br><span class="line">y &#x3D; tf.placeholder(dtpe&#x3D;tf.float32,shape&#x3D;[],nmae&#x3D;&#39;y&#39;)</span><br><span class="line">z &#x3D; x*y</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	prod &#x3D; sess.run(z,feed_dict&#x3D;&#123;x:1.,y:5.2&#125;)</span><br><span class="line">	print(prod)</span><br><span class="line">[out]:5.2</span><br></pre></td></tr></table></figure>
<h4 id="2-variable"><a href="#2-variable" class="headerlink" title="2. variable"></a>2. variable</h4><p>&emsp;&emsp;无论是传统的机器学习算法，例如线性支持向量机（Support Vector Machine, SVM)，其数学模型为y = <w,x> + b，还是更先进的深度学习算法，例如卷积神经网络（Convolutional Neural Network， CNN）单个神经元输出的模型y = w*x + b。可以看到，w和b就是我们要求的模型，模型的求解是通过优化算法（对于SVM，使用<br>SMO[1]算法，对于CNN，一般基于梯度下降法）来一步一步更新w和b的值直到满足停止条件。因此，大多数机器学习的模型中的w和b实际上是以变量的形式出现在代码中的，这就要求我们在代码中定义模型变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a &#x3D; tf.Variable(2.)</span><br><span class="line">b &#x3D; tf.Variable(3.)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	sess.run(tf.global_variables_initializer()) #变量初始化</span><br><span class="line">    print(sess.run(a*b))</span><br><span class="line">[out]:6.</span><br></pre></td></tr></table></figure>
<p>[1] Platt, John. “Sequential minimal optimization: A fast algorithm for training support vector machines.” (1998).</p>
<h4 id="3-initializer"><a href="#3-initializer" class="headerlink" title="3. initializer"></a>3. initializer</h4><p>&emsp;&emsp;由于tensorflow构建的是静态的计算流图，在开启会话之前，所有的操作都不会被执行。因此为了执行在计算图中所构建的赋值初始化计算节点，需要在开启会话之后，在会话环境下运行初始化。如果计算图中定义了变量，而会话环境下为执行初始化命令，则程序报错，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a &#x3D; tf.Variable(2.)</span><br><span class="line">b &#x3D; tf.Variable(3.)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">	#sess.run(tf.global_variables_initializer()) #注释掉初始化命令</span><br><span class="line">    print(sess.run(a*b))</span><br><span class="line">[Error]: Attempting to use uninitialized value Variable</span><br></pre></td></tr></table></figure>
<h3 id="2-启动图"><a href="#2-启动图" class="headerlink" title="2.启动图"></a>2.启动图</h3><p>&emsp;&emsp;先了解session的概念，然后才能更好的理解图的启动。<br>&emsp;&emsp;图的每个运行实例都必须在一个session里，session为图的运行提供环境。Session的类型是tf.Session，在实例化session对象时我们需要给它传递一个图对象，如果不显示给出将使用默认的图。Session有一个graph属性，我们可以通过它获取session对应的图。</p>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">numOfBatch&#x3D;5</span><br><span class="line">datas&#x3D;np.zeros([numOfBatch,2],np.float32)</span><br><span class="line">labels&#x3D;np.zeros([numOfBatch,2],np.float32)</span><br><span class="line"></span><br><span class="line">sess&#x3D;tf.Session(graph&#x3D;g)</span><br><span class="line">graph&#x3D;sess.graph</span><br><span class="line">sess.run([graph.get_operation_by_name(&quot;initer&quot;)])</span><br><span class="line"></span><br><span class="line">dataHolder&#x3D;graph.get_tensor_by_name(&quot;input_data:0&quot;)</span><br><span class="line">labelHolder&#x3D;graph.get_tensor_by_name(&quot;input_label:0&quot;)</span><br><span class="line">train&#x3D;graph.get_operation_by_name(&quot;train_step&quot;)</span><br><span class="line">out&#x3D;graph.get_tensor_by_name(&quot;output:0&quot;)</span><br><span class="line"></span><br><span class="line">for i inrange(200):</span><br><span class="line">   result&#x3D;sess.run([out,train],feed_dict&#x3D;&#123;dataHolder:datas,labelHolder:labels&#125;)</span><br><span class="line">   if i%100&#x3D;&#x3D;0:</span><br><span class="line">       saver.save(sess,&quot;.&#x2F;moules&quot;)</span><br><span class="line"></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>代码都比较简单，就不介绍了。不过要注意2点：1.别忘记运行初始化节点，2.别忘记close掉session对象以释放资源。</p>
<h4 id="3-给图输入数据并获取结果"><a href="#3-给图输入数据并获取结果" class="headerlink" title="3.给图输入数据并获取结果"></a>3.给图输入数据并获取结果</h4><p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for i inrange(200):</span><br><span class="line">    result&#x3D;sess.run([out,train],feed_dict&#x3D;&#123;dataHolder:datas,labelHolder:labels&#125;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;这里主要用到了session对象的run方法，它用来运行某个节点或tensor并获取对应的值。我们一般会一次传递一小部分数据进行mini-batch梯度下降来优化模型。</p>
<p>&emsp;&emsp;我们需要把我们需要运行的节点或tensor放入一个列表，然后作为第一个参数(不考虑self)传递给run方法，run方法会返回一个计算结果的列表，与我们传递的参数一一对应。</p>
<p>&emsp;&emsp;如果我们运行的节点依赖某个placeholder，那我们必须给这个placeholder指定值，怎么指定代码里面很清楚，给关键字参数feed_dict传递一个字典即可，字典里的元素的key是placeholder对象，value是我们指定的值。值的数据的类型必须和placeholder一致，包括shape。值本身的类型是numpy数组。</p>
<p>这里再解释一个细节，在定义placeholder时代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_data&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_data&quot;)</span><br><span class="line">input_label&#x3D;tf.placeholder(tf.float32,[None,2],&quot;input_label&quot;)</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;shape为[None,2]，说明数据第一个维度是不确定的，然后TensorFlow会根据我们传递的数据动态推断第一个维度，这样我们就可以在运行时改变batch的大小。比如一个数据是2维，一次传递10个数据对应的tensor的shape就是[10,2]。可不可以把多个维度指定为None？理论上不可以！</p>
<h2 id="12-1-6-如何基于tensorflow搭建VGG16"><a href="#12-1-6-如何基于tensorflow搭建VGG16" class="headerlink" title="12.1.6 如何基于tensorflow搭建VGG16"></a>12.1.6 如何基于tensorflow搭建VGG16</h2><p>​    介绍完关于tensorflow的基础知识，是时候来一波网络搭建实战了。虽然网上有很多相关教程，但我想从最标准的tensorflow代码和语法出发（而不是调用更高级的API，失去了原来的味道），向大家展示如何搭建其标准的VGG16网络架构。话不多说，上代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(<span class="string">'weight'</span>, shape=shape, initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(<span class="string">'bias'</span>, shape=shape, initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, w, padding = <span class="string">'SAME'</span>, s=<span class="number">1</span>)</span>:</span></span><br><span class="line">    x = tf.nn.conv2d(x, w, strides=[<span class="number">1</span>, s, s, <span class="number">1</span>], padding = padding)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxPoolLayer</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                          strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_layer</span><span class="params">(x,in_chs, out_chs, ksize, layer_name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(layer_name):</span><br><span class="line">        w = get_weight_variable([ksize, ksize, in_chs, out_chs])</span><br><span class="line">        b = get_bias_variable([out_chs])</span><br><span class="line">        y = tf.nn.relu(tf.bias_add(conv2d(x,w,padding = <span class="string">'SAME'</span>, s=<span class="number">1</span>), b))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fc_layer</span><span class="params">(x,in_kernels, out_kernels, layer_name)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(layer_name):</span><br><span class="line">        w = get_weight_variable([in_kernels,out_kernels])</span><br><span class="line">        b = get_bias_variable([out_kernels])</span><br><span class="line">        y = tf.nn.relu(tf.bias_add(tf.matmul(x,w),b))</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">VGG16</span><span class="params">(x)</span>:</span></span><br><span class="line">    conv1_1 = conv2d_layer(x,tf.get_shape(x).as_list()[<span class="number">-1</span>], <span class="number">64</span>, <span class="number">3</span>, <span class="string">'conv1_1'</span>)</span><br><span class="line">    conv1_2 = conv2d_layer(conv1_1,<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="string">'conv1_2'</span>)</span><br><span class="line">    pool_1 = maxPoolLayer(conv1_2)</span><br><span class="line">    </span><br><span class="line">    conv2_1 = conv2d_layer(pool1,<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="string">'conv2_1'</span>)</span><br><span class="line">    conv2_2 = conv2d_layer(conv2_1,<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="string">'conv2_2'</span>)</span><br><span class="line">    pool2 = maxPoolLayer(conv2_2)</span><br><span class="line">    </span><br><span class="line">	conv3_1 = conv2d_layer(pool2,<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_1'</span>)</span><br><span class="line">    conv3_2 = conv2d_layer(conv3_1,<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_2'</span>)</span><br><span class="line">	conv3_3 = conv2d_layer(conv3_2,<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="string">'conv3_3'</span>)</span><br><span class="line">    pool3 = maxPoolLayer(conv3_3)</span><br><span class="line">    </span><br><span class="line">	conv4_1 = conv2d_layer(pool3,<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_1'</span>)</span><br><span class="line">    conv4_2 = conv2d_layer(conv4_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_2'</span>)</span><br><span class="line">	conv4_3 = conv2d_layer(conv4_2,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv4_3'</span>)</span><br><span class="line">    pool4 = maxPoolLayer(conv4_3)</span><br><span class="line">    </span><br><span class="line">	conv5_1 = conv2d_layer(pool4,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_1'</span>)</span><br><span class="line">    conv5_2 = conv2d_layer(conv5_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_2'</span>)</span><br><span class="line">	conv5_3 = conv2d_layer(conv5_1,<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="string">'conv5_3'</span>)</span><br><span class="line">    pool5 = maxPoolLayer(conv5_3)</span><br><span class="line">    </span><br><span class="line">	pool5_flatten_dims = int(np.prod(pool5.get_shape().as_list()[<span class="number">1</span>:]))</span><br><span class="line">    pool5_flatten = tf.reshape(pool5,[<span class="number">-1</span>,pool5_flatten_dims])</span><br><span class="line">    </span><br><span class="line">    fc_6 = fc_layer(pool5_flatten, pool5_flatten_dims, <span class="number">4096</span>, <span class="string">'fc6'</span>)</span><br><span class="line">	fc_7 = fc_layer(fc_6, <span class="number">4096</span>, <span class="number">4096</span>, <span class="string">'fc7'</span>)</span><br><span class="line">	fc_8 = fc_layer(fc_7, <span class="number">4096</span>, <span class="number">10</span>, <span class="string">'fc8'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> fc_8</span><br></pre></td></tr></table></figure>
<h1 id="12-2-Pytorch"><a href="#12-2-Pytorch" class="headerlink" title="12.2 Pytorch"></a>12.2 Pytorch</h1><h2 id="12-2-1-Pytorch是什么？"><a href="#12-2-1-Pytorch是什么？" class="headerlink" title="12.2.1 Pytorch是什么？"></a>12.2.1 Pytorch是什么？</h2><p>&emsp;&emsp;Pytorch是torch的python版本，是由Facebook开源的神经网络框架，专门针对 GPU 加速的深度神经网络（DNN）编程。Torch 是一个经典的对多维矩阵数据进行操作的张量（tensor ）库，在机器学习和其他数学密集型应用有广泛应用。与Tensorflow的静态计算图不同，pytorch的计算图是动态的，可以根据计算需要实时改变计算图。但由于Torch语言采用 Lua，导致在国内一直很小众，并逐渐被支持 Python 的 Tensorflow 抢走用户。作为经典机器学习库 Torch 的端口，PyTorch 为 Python 语言使用者提供了舒适的写代码选择。</p>
<h2 id="12-2-2-为什么选择-Pytorch？"><a href="#12-2-2-为什么选择-Pytorch？" class="headerlink" title="12.2.2 为什么选择 Pytorch？"></a>12.2.2 为什么选择 Pytorch？</h2><h3 id="1-简洁："><a href="#1-简洁：" class="headerlink" title="1.简洁："></a>1.简洁：</h3><p>&emsp;&emsp;PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像 TensorFlow 中充斥着session、graph、operation、name_scope、variable、tensor、layer等全新的概念，PyTorch 的设计遵循tensor→variable(autograd)→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。<br>简洁的设计带来的另外一个好处就是代码易于理解。PyTorch的源码只有TensorFlow的十分之一左右，更少的抽象、更直观的设计使得PyTorch的源码十分易于阅读。</p>
<h3 id="2-速度："><a href="#2-速度：" class="headerlink" title="2.速度："></a>2.速度：</h3><p>&emsp;&emsp;PyTorch 的灵活性不以速度为代价，在许多评测中，PyTorch 的速度表现胜过 TensorFlow和Keras 等框架。框架的运行速度和程序员的编码水平有极大关系，但同样的算法，使用PyTorch实现的那个更有可能快过用其他框架实现的。</p>
<h3 id="3-易用："><a href="#3-易用：" class="headerlink" title="3.易用："></a>3.易用：</h3><p>&emsp;&emsp;PyTorch 是所有的框架中面向对象设计的最优雅的一个。PyTorch的面向对象的接口设计来源于Torch，而Torch的接口设计以灵活易用而著称，Keras作者最初就是受Torch的启发才开发了Keras。PyTorch继承了Torch的衣钵，尤其是API的设计和模块的接口都与Torch高度一致。PyTorch的设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法，即所思即所得，不需要考虑太多关于框架本身的束缚。</p>
<h3 id="4-活跃的社区："><a href="#4-活跃的社区：" class="headerlink" title="4.活跃的社区："></a>4.活跃的社区：</h3><p>&emsp;&emsp;PyTorch 提供了完整的文档，循序渐进的指南，作者亲自维护的论坛 供用户交流和求教问题。Facebook 人工智能研究院对 PyTorch 提供了强力支持，作为当今排名前三的深度学习研究机构，FAIR的支持足以确保PyTorch获得持续的开发更新，不至于像许多由个人开发的框架那样昙花一现。</p>
<h2 id="12-2-3-PyTorch-的架构是怎样的？"><a href="#12-2-3-PyTorch-的架构是怎样的？" class="headerlink" title="12.2.3 PyTorch 的架构是怎样的？"></a>12.2.3 PyTorch 的架构是怎样的？</h2><p>&emsp;&emsp;PyTorch(Caffe2) 通过混合前端，分布式训练以及工具和库生态系统实现快速，灵活的实验和高效生产。PyTorch 和 TensorFlow 具有不同计算图实现形式，TensorFlow 采用静态图机制(预定义后再使用)，PyTorch采用动态图机制(运行时动态定义)。PyTorch 具有以下高级特征：</p>
<p>&emsp;&emsp;混合前端:新的混合前端在急切模式下提供易用性和灵活性，同时无缝转换到图形模式，以便在C ++运行时环境中实现速度，优化和功能。<br>&emsp;&emsp;分布式训练:通过利用本地支持集合操作的异步执行和可从Python和C ++访问的对等通信，优化了性能。<br>&emsp;&emsp;Python优先: PyTorch为了深入集成到Python中而构建的，因此它可以与流行的库和Cython和Numba等软件包一起使用。<br>&emsp;&emsp;丰富的工具和库:活跃的研究人员和开发人员社区建立了丰富的工具和库生态系统，用于扩展PyTorch并支持从计算机视觉到强化学习等领域的开发。<br>&emsp;&emsp;本机ONNX支持:以标准ONNX（开放式神经网络交换）格式导出模型，以便直接访问与ONNX兼容的平台，运行时，可视化工具等。<br>&emsp;&emsp;C++前端：C++前端是PyTorch的纯C++接口，它遵循已建立的Python前端的设计和体系结构。它旨在实现高性能，低延迟和裸机C++应用程序的研究。<br>使用GPU和CPU优化的深度学习张量库。</p>
<h2 id="12-2-4-Pytorch-与-tensorflow-之间的差异在哪里？"><a href="#12-2-4-Pytorch-与-tensorflow-之间的差异在哪里？" class="headerlink" title="12.2.4 Pytorch 与 tensorflow 之间的差异在哪里？"></a>12.2.4 Pytorch 与 tensorflow 之间的差异在哪里？</h2><p>&emsp;&emsp;上面也将了PyTorch 最大优势是建立的神经网络是动态的, 对比静态的 Tensorflow, 它能更有效地处理一些问题, 比如说 RNN 变化时间长度的输出。各有各的优势和劣势。两者都是大公司发布的, Tensorflow（Google）宣称在分布式训练上下了很大的功夫, 那就默认 Tensorflow 在分布式训练上要超出 Pytorch（Facebook），还有tensorboard可视化工具, 但是 Tensorflow 的静态计算图使得在 RNN 上有一点点被动 (虽然它用其他途径解决了), 不过用 PyTorch 的时候, 会对这种动态的 RNN 有更好的理解。而且 Tensorflow 的高度工业化, 它的底层代码很难看懂， Pytorch 好那么一点点, 如果深入 PytorchAPI, 至少能比看 Tensorflow 多看懂一点点 Pytorch 的底层在干啥。</p>
<h2 id="12-2-5-Pytorch有哪些常用工具包？"><a href="#12-2-5-Pytorch有哪些常用工具包？" class="headerlink" title="12.2.5 Pytorch有哪些常用工具包？"></a>12.2.5 Pytorch有哪些常用工具包？</h2><p>&emsp;&emsp;torch ：类似 NumPy 的张量库，强 GPU 支持 ；<br>&emsp;&emsp;torch.autograd ：基于 tape 的自动区别库，支持 torch 之中的所有可区分张量运行；<br>&emsp;&emsp;torch.nn ：为最大化灵活性未涉及、与 autograd 深度整合的神经网络库；<br>&emsp;&emsp;torch.optim：与 torch.nn 一起使用的优化包，包含 SGD、RMSProp、LBFGS、Adam 等标准优化方式；<br>&emsp;&emsp;torch.multiprocessing： python 多进程并发，进程之间 torch Tensors 的内存共享；<br>&emsp;&emsp;torch.utils：数据载入器。具有训练器和其他便利功能；<br>&emsp;&emsp;torch.legacy(.nn/.optim) ：处于向后兼容性考虑，从 Torch 移植来的 legacy 代码；</p>
<h1 id="12-3-Caffe"><a href="#12-3-Caffe" class="headerlink" title="12.3 Caffe"></a>12.3 Caffe</h1><h2 id="12-3-1-什么是-Caffe？"><a href="#12-3-1-什么是-Caffe？" class="headerlink" title="12.3.1 什么是 Caffe？"></a>12.3.1 什么是 Caffe？</h2><p>&emsp;&emsp;Caffe的全称应该是Convolutional Architecture for Fast Feature Embedding，它是一个清晰、高效的深度学习框架，它是开源的，核心语言是C++，它支持命令行、Python和Matlab接口，它既可以在CPU上运行也可以在GPU上运行。它的license是BSD 2-Clause。</p>
<h2 id="12-3-2-Caffe的特点是什么？"><a href="#12-3-2-Caffe的特点是什么？" class="headerlink" title="12.3.2 Caffe的特点是什么？"></a>12.3.2 Caffe的特点是什么？</h2><p>(1)、模块化：Caffe从一开始就设计得尽可能模块化，允许对新数据格式、网络层和损失函数进行扩展。</p>
<p>(2)、表示和实现分离：Caffe的模型(model)定义是用Protocol Buffer语言写进配置文件的。以任意有向无环图的形式，Caffe支持网络架构。Caffe会根据网络的需要来正确占用内存。通过一个函数调用，实现CPU和GPU之间的切换。</p>
<p>(3)、测试覆盖：在Caffe中，每一个单一的模块都对应一个测试。</p>
<p>(4)、python和Matlab接口：同时提供Python和Matlab接口。</p>
<p>(5)、预训练参考模型：针对视觉项目，Caffe提供了一些参考模型，这些模型仅应用在学术和非商业领域，它们的license不是BSD。</p>
<h2 id="12-3-3-Caffe的设计思想是怎样的？"><a href="#12-3-3-Caffe的设计思想是怎样的？" class="headerlink" title="12.3.3 Caffe的设计思想是怎样的？"></a>12.3.3 Caffe的设计思想是怎样的？</h2><p>&emsp;&emsp;基本上，Caffe 沿用了神经网络的一个简单假设——所有的计算都是以layer的形式表示的，layer做的事情就是take一些数据，然后输出一些计算以后的结果，比如说卷积，就是输入一个图像，然后和这一层的参数（filter）做卷积，然后输出卷积的结果。每一个layer需要做两个计算：forward是从输入计算输出，然后backward是从上面给的gradient来计算相对于输入的gradient，只要这两个函数实现了以后，我们就可以把很多层连接成一个网络，这个网络做的事情就是输入我们的数据（图像或者语音或者whatever），然后来计算我们需要的输出（比如说识别的label），在training的时候，我们可以根据已有的label来计算loss和gradient，然后用gradient来update网络的参数，这个就是Caffe的一个基本流程。</p>
<p>&emsp;&emsp;基本上，最简单地用Caffe上手的方法就是先把数据写成Caffe的格式，然后设计一个网络，然后用Caffe提供的solver来做优化看效果如何，如果你的数据是图像的话，可以从现有的网络，比如说alexnet或者googlenet开始，然后做fine tuning，如果你的数据稍有不同，比如说是直接的float vector，你可能需要做一些custom的configuration，Caffe的logistic regression example兴许会很有帮助。</p>
<p>&emsp;&emsp;Fine tune方法：fine tuning的想法就是说，在imagenet那么大的数据集上train好一个很牛的网络了，那别的task上肯定也不错，所以我们可以把pretrain的网络拿过来，然后只重新train最后几层，重新train的意思是说，比如我以前需要classify imagenet的一千类，现在我只想识别是狗还是猫，或者是不是车牌，于是我就可以把最后一层softmax从一个4096<em>1000的分类器变成一个4096</em>2的分类器，这个strategy在应用中非常好使，所以我们经常会先在imagenet上pretrain一个网络，因为我们知道imagenet上training的大概过程会怎么样。</p>
<h2 id="12-3-4-Caffe架构是怎样的？"><a href="#12-3-4-Caffe架构是怎样的？" class="headerlink" title="12.3.4 Caffe架构是怎样的？"></a>12.3.4 Caffe架构是怎样的？</h2><p>&emsp;&emsp;Caffe的架构与其它的深度学习框架稍微不同，它没有根据算法实现过程的方式来进行编码，而是以系统级的抽象作为整体架构，逐层的封装实现细节，使得上层的架构变得很清晰。Caffe的整体架构如下：</p>
<h3 id="1-SyncedMem"><a href="#1-SyncedMem" class="headerlink" title="1. SyncedMem"></a>1. SyncedMem</h3><p>&emsp;&emsp;这个类的主要功能是封装CPU和GPU的数据交互操作。一般来说，数据的流动形式都是：硬盘-&gt;CPU内存-&gt;GPU内存-&gt;CPU内存-&gt;（硬盘），所以在写代码的过程中经常会写CPU/GPU之间数据传输的代码，同时还要维护CPU和GPU两个处理端的内存指针。这些事情处理起来不会很难，但是会很繁琐。因此SyncedMem的出现就是把CPU/GPU的数据传输操作封装起来，只需要调用简单的接口就可以获得两个处理端同步后的数据。</p>
<h3 id="2-Blob"><a href="#2-Blob" class="headerlink" title="2. Blob"></a>2. Blob</h3><p>&emsp;&emsp;Blob是用于存储数据的对象，在Caffe中各种数据(图像输入、模型参数)都是以Blob的形式在网络中传输的，Blob提供统一的存储操作接口，可用来保存训练数据、模型参数等，同时Blob还能在CPU和GPU之间进行同步以支持CPU/GPU的混合运算。<br>&emsp;&emsp;这个类做了两个封装：一个是操作数据的封装，使用Blob可以操纵高维的数据，快速访问其中的数据，变换数据的维度等；另一个是对原始数据和更新量的封装，每一个Blob中都有data和diff两个数据指针，data用于存储原始数据，diff 用于存储反向传播（Backpropagation）的梯度更新值。Blob使用了SyncedMem，这样便于访问不同的处理端。Blob基本实现了整个Caffe数据结构部分的封装，在Net类中可以看到所有的前后向数据和参数都用Blob来表示就足够了。数据的抽象到这个就可以了，接下来作层级的抽象。神经网络的前后向计算可以做到层与层之间完全独立，只要每个层按照一定的接口规则实现，就可以确保整个网络的正确性。</p>
<h3 id="3-Layer"><a href="#3-Layer" class="headerlink" title="3. Layer"></a>3. Layer</h3><p>&emsp;&emsp;Layer是网络Net的基本单元，也是Caffe中能在外部进行调整的最小网络结构单元，每个Layer都有输入Blob和输出Blob。Layer（层）是Caffe中最庞大最繁杂的模块，它是神经网络的基本计算单元。由于Caffe强调模块化设计，因此只允许每个layer完成一类特定的计算，例如convolution操作、pooling、非线性变换、内积运算，以及数据加载、归一化和损失计算等。Caffe中layer的种类有很多，具体的种类及功能请看官方文档。在创建一个Caffe模型的时候，也是以Layer为基础进行的。Layer是一个父类，它的下面还有各种实现特定功能的子类，例如data_layer，conv_layer，loss_layer等。Layer是通过LayFactory来创建的。</p>
<h3 id="4-Net"><a href="#4-Net" class="headerlink" title="4. Net"></a>4. Net</h3><p>&emsp;&emsp;Net是一个完整的深度网络，包含输入层、隐藏层、输出层，在Caffe中一般是一个卷积神经网络(Convolution Neural Networ，CNN)。通过定义不同类型的Layer，并用Blob将不同的Layer连接起来，就能产生一个Net。Net将数据Blob和层Layer组合起来做进一步的封装，对外提供了初始化和前后传播的接口，使得整体看上去和一个层的功能类似，但内部的组合可以是多种多样的。值得一提的是，每一层的输入输出数据统一保存在Net中，同时每个层内的参数指针也保存在Net中，不同的层可以通过WeightShare共享相同的参数，因此可以通过配置来实现多个神经网络层之间共享参数的功能。一个Net由多个Layer组成。一个典型的网络从data layer（从磁盘中载入数据）出发到loss layer结束。</p>
<h3 id="5-Solver"><a href="#5-Solver" class="headerlink" title="5. Solver"></a>5. Solver</h3><p>&emsp;&emsp;有了Net就可以进行神经网络的前后向传播计算了，但是还缺少神经网络的训练和预测功能，Solver类进一步封装了训练和预测相关的一些功能。它还提供了两个接口：一个是更新参数的接口，继承Solver可以实现不同的参数更新方法，如Momentum，Nesterov，Adagrad等，因此可以使用不同的优化算法。另一个接口是训练过程中每一轮特定状态下的可注入的一些回调函数，在代码中这个回调点的直接使用者就是多GPU训练算法。Solver定义了针对Net网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义Solver能够实现不同的神经网络求解方式。阅读Solver的代码可以了解网络的求解优化过程。Solver是一个父类，它下面还有实现不同优化方法的子类，例如sgd_solver，adagrad_sovler等，Solver是通过SolverFactory来创建的。</p>
<h3 id="6-Proto"><a href="#6-Proto" class="headerlink" title="6. Proto"></a>6. Proto</h3><p>&emsp;&emsp;caffe.proto位于…/src/caffe/proto目录下，在这个文件夹下还有一个.pb.cc和一个.pb.h文件，这两个文件都是由caffe.proto编译而来的。 在caffe.proto中定义了很多结构化数据，包括：<br>BlobProto、Datum、FillerParameter、NetParameter、SolverParameter、SolverState、LayerParameter、ConcatParameter、ConvolutionParameter、DataParameter、DropoutParameter、HDF5DataParameter、HDF5OutputParameter、ImageDataParameter、InfogainLossParameter、InnerProductParameter、LRNParameter、MemoryDataParameter、PoolingParameter、PowerParameter、WindowDataParameter、V0LayerParameter。</p>
<h3 id="7-IO"><a href="#7-IO" class="headerlink" title="7. IO"></a>7. IO</h3><p>&emsp;&emsp;除了上面的东西之外，还需要输入数据和参数。DataReader和DataTransformer帮助准备输入数据，Filler对参数进行初始化，一些Snapshot方法可以对模型进行持久化。</p>
<h2 id="12-3-5-Caffe的有哪些接口？"><a href="#12-3-5-Caffe的有哪些接口？" class="headerlink" title="12.3.5 Caffe的有哪些接口？"></a>12.3.5 Caffe的有哪些接口？</h2><p>&emsp;&emsp;Caffe深度学习框架支持多种编程接口，包括命令行、Python和Matlab,下面将介绍如何使用这些接口。</p>
<h3 id="1-Caffe-Python接口"><a href="#1-Caffe-Python接口" class="headerlink" title="1. Caffe Python接口"></a>1. Caffe Python接口</h3><p>&emsp;&emsp;Caffe提供 Python 接口，即Pycaffe，具体实现在caffe、python文件夹内。在Python代码中import caffe，可以load models（导入模型）、forward and backward （前向、反向迭代）、handle IO（数据输入输出）、visualize networks（绘制net）和instrument model solving（自定义优化方法)。所有的模型数据、计算参数都是暴露在外、可供读写的。<br>&emsp;&emsp;(1)caffe.Net 是主要接口，负责导入数据、校验数据、计算模型。<br>&emsp;&emsp;(2)caffe.Classsifier 用于图像分类。<br>&emsp;&emsp;(3)caffe.Detector 用于图像检测。<br>&emsp;&emsp;(4)caffe.SGDSolver 是露在外的 solver 的接口。<br>&emsp;&emsp;(5)caffe.io 处理输入输出，数据预处理。<br>&emsp;&emsp;(6)caffe.draw 可视化 net 的结构。<br>&emsp;&emsp;(7)caffe blobs 以 numpy ndarrys 的形式表示，方便而且高效。</p>
<h3 id="2-Caffe-MATLAB接口"><a href="#2-Caffe-MATLAB接口" class="headerlink" title="2. Caffe MATLAB接口"></a>2. Caffe MATLAB接口</h3><p>&emsp;&emsp;MATLAB接口（Matcaffe）在 caffe/matlab 目录的 caffe 软件包。在 matcaffe 的基础上，可将Caffe整合到MATLAB代码中。<br>&emsp;&emsp;MATLAB接口包括：<br>&emsp;&emsp;(1)MATLAB 中创建多个网络结构。<br>&emsp;&emsp;(2)网络的前向传播（Forward）与反向传播（Backward）计算。<br>&emsp;&emsp;(3)网络中的任意一层以及参数的存取。<br>&emsp;&emsp;(4)网络参数保存至文件或从文件夹加载。<br>&emsp;&emsp;(5)blob 和 network 形状调整。<br>&emsp;&emsp;(6)网络参数编辑和调整。<br>&emsp;&emsp;(7)创建多个 solvers 进行训练。<br>&emsp;&emsp;(8)从solver 快照（Snapshots）恢复并继续训练。<br>&emsp;&emsp;(9)访问训练网络（Train nets）和测试网络(Test nets)。<br>&emsp;&emsp;(10)迭代后网络交由 MATLAB 控制。<br>&emsp;&emsp;(11)MATLAB代码融合梯度算法。</p>
<h3 id="3-Caffe-命令行接口"><a href="#3-Caffe-命令行接口" class="headerlink" title="3. Caffe 命令行接口"></a>3. Caffe 命令行接口</h3><p>&emsp;&emsp;命令行接口 Cmdcaffe 是 Caffe 中用来训练模型、计算得分以及方法判断的工具。Cmdcaffe 存放在 caffe/build/tools 目录下。</p>
<h4 id="1-caffe-train"><a href="#1-caffe-train" class="headerlink" title="1. caffe train"></a>1. caffe train</h4><p>&emsp;&emsp;caffe train 命令用于模型学习，具体包括：<br>&emsp;&emsp;(1)caffe train 带 solver.prototxt 参数完成配置。<br>&emsp;&emsp;(2)caffe train 带 snapshot mode_iter_1000.solverstate 参数加载 solver snapshot。<br>&emsp;&emsp;(3)caffe train 带 weights 参数 model.caffemodel 完成 Fine-tuning 模型初始化。</p>
<h4 id="2-caffe-test"><a href="#2-caffe-test" class="headerlink" title="2. caffe test"></a>2. caffe test</h4><p>&emsp;&emsp;caffe test 命令用于测试运行模型的得分，并且用百分比表示网络输出的最终结果，比如 accuracyhuoloss 作为其结果。测试过程中，显示每个 batch 的得分，最后输出全部 batch 的平均得分值。</p>
<h4 id="3-caffe-time"><a href="#3-caffe-time" class="headerlink" title="3. caffe time"></a>3. caffe time</h4><p>&emsp;&emsp;caffe time 命令用来检测系统性能和测量模型相对执行时间，此命令通过逐层计时与同步，执行模型检测。</p>
<p>参考文献：<br>1.深度学习：Caffe之经典模型讲解与实战/ 乐毅，王斌</p>
<h3 id="10-4-网络搭建有什么原则？"><a href="#10-4-网络搭建有什么原则？" class="headerlink" title="10.4 网络搭建有什么原则？"></a>10.4 网络搭建有什么原则？</h3><h3 id="10-4-1新手原则。"><a href="#10-4-1新手原则。" class="headerlink" title="10.4.1新手原则。"></a>10.4.1新手原则。</h3><p>刚入门的新手不建议直接上来就开始搭建网络模型。比较建议的学习顺序如下：</p>
<ul>
<li>1.了解神经网络工作原理，熟悉基本概念及术语。</li>
<li>2.阅读经典网络模型论文+实现源码(深度学习框架视自己情况而定)。</li>
<li>3.找数据集动手跑一个网络，可以尝试更改已有的网络模型结构。</li>
<li>4.根据自己的项目需要设计网络。</li>
</ul>
<h3 id="10-4-2深度优先原则。"><a href="#10-4-2深度优先原则。" class="headerlink" title="10.4.2深度优先原则。"></a>10.4.2深度优先原则。</h3><p>通常增加网络深度可以提高准确率，但同时会牺牲一些速度和内存。但深度不是盲目堆起来的，一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率，如果浅层都学不到东西，深了也没效果。</p>
<h3 id="10-4-3卷积核size一般为奇数。"><a href="#10-4-3卷积核size一般为奇数。" class="headerlink" title="10.4.3卷积核size一般为奇数。"></a>10.4.3卷积核size一般为奇数。</h3><p>卷积核为奇数有以下好处：</p>
<ul>
<li>1 保证锚点刚好在中间，方便以 central pixel为标准进行滑动卷积，避免了位置信息发生偏移 。</li>
<li>2 保证在填充（Padding）时，在图像之间添加额外的零层，图像的两边仍然对称。</li>
</ul>
<h3 id="10-4-4卷积核不是越大越好。"><a href="#10-4-4卷积核不是越大越好。" class="headerlink" title="10.4.4卷积核不是越大越好。"></a>10.4.4卷积核不是越大越好。</h3><p>AlexNet中用到了一些非常大的卷积核，比如11×11、5×5卷积核，之前人们的观念是，卷积核越大，感受野越大，看到的图片信息越多，因此获得的特征越好。但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。于是在VGG、Inception网络中，利用2个3×3卷积核的组合比1个5×5卷积核的效果更佳，同时参数量（3×3×2+1=19&lt;26=5×5×1+1）被降低，因此后来3×3卷积核被广泛应用在各种模型中。</p>
<h2 id="10-5-有哪些经典的网络模型值得我们去学习的？"><a href="#10-5-有哪些经典的网络模型值得我们去学习的？" class="headerlink" title="10.5 有哪些经典的网络模型值得我们去学习的？"></a>10.5 有哪些经典的网络模型值得我们去学习的？</h2><p>提起经典的网络模型就不得不提起计算机视觉领域的经典比赛：ILSVRC .其全称是 ImageNet Large Scale Visual Recognition Challenge.正是因为ILSVRC 2012挑战赛上的AlexNet横空出世，使得全球范围内掀起了一波深度学习热潮。这一年也被称作“深度学习元年”。而在历年ILSVRC比赛中每次刷新比赛记录的那些神经网络也成为了人们心中的经典，成为学术界与工业届竞相学习与复现的对象，并在此基础上展开新的研究。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>年份</th>
<th>网络名称</th>
<th>获得荣誉</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2012</td>
<td>AlexNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>2</td>
<td>2014</td>
<td>VGGNet</td>
<td>ILSVRC图像分类亚军</td>
</tr>
<tr>
<td>3</td>
<td>2014</td>
<td>GoogLeNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>4</td>
<td>2015</td>
<td>ResNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
<tr>
<td>5</td>
<td>2017</td>
<td>SeNet</td>
<td>ILSVRC图像分类冠军</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<ul>
<li><p>1 AlexNet<br>论文:<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/alexnet.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.第一次使用非线性激活函数ReLU。</li>
<li>2.增加防加过拟合方法：Droupout层,提升了模型鲁棒性。</li>
<li>3.首次使用数据增强。  </li>
<li>4.首次使用GPU加速运算。</li>
</ul>
</blockquote>
</li>
<li><p>2 VGGNet<br>论文:<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/vgg.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.网络结构更深。</li>
<li>2.普遍使用小卷积核。</li>
</ul>
</blockquote>
</li>
<li><p>3 GoogLeNet<br>论文:<a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Going Deeper with Convolutions</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/inception_v1.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点：</p>
<blockquote>
<ul>
<li>1.增强卷积模块功能。<br>主要的创新在于他的Inception，这是一种网中网（Network In Network）的结构，即原来的结点也是一个网络。Inception一直在不断发展，目前已经V2、V3、V4。其中1*1卷积主要用来降维，用了Inception之后整个网络结构的宽度和深度都可扩大，能够带来2-3倍的性能提升。</li>
<li>2.连续小卷积代替大卷积，保证感受野不变的同时，减少了参数数目。</li>
</ul>
</blockquote>
</li>
<li>4 ResNet<br>论文:<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a><br>代码实现:<a href="https://github.com/tensorflow/tensorflow/blob/361a82d73a50a800510674b3aaa20e4845e56434/tensorflow/contrib/slim/python/slim/nets/inception_v1.py" target="_blank" rel="noopener">tensorflow</a><br>主要特点:<blockquote>
<p>解决了“退化”问题，即当模型的层次加深时，错误率却提高了。</p>
</blockquote>
</li>
<li>5 SeNet<br>论文:<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a><br>代码实现:<a href="https://github.com/ry/tensorflow-resnet" target="_blank" rel="noopener">tensorflow</a><br>主要特点:<blockquote>
<p>提出了feature recalibration，通过引入 attention 重新加权，可以得到抑制无效特征，提升有效特征的权重，并很容易地和现有网络结合，提升现有网络性能，而计算量不会增加太多。</p>
</blockquote>
</li>
</ul>
</blockquote>
<p><strong>CV领域网络结构演进历程：</strong><br><img src="/img/ch12/网络结构演进.png" alt="CV领域网络结构演进历程"></p>
<p><strong>ILSVRC挑战赛历年冠军:</strong><br><img src="/img/ch12/历年冠军.png" alt="ILSVRC挑战赛历年冠军"></p>
<p>此后，ILSVRC挑战赛的名次一直是衡量一个研究机构或企业技术水平的重要标尺。<br>ILSVRC 2017 已是最后一届举办.2018年起，将由WebVision竞赛（Challenge on Visual Understanding by Learning from Web Data）来接棒。因此，即使ILSVRC挑战赛停办了，但其对深度学习的深远影响和巨大贡献，将永载史册。</p>
<h2 id="10-6-网络训练有哪些技巧吗？"><a href="#10-6-网络训练有哪些技巧吗？" class="headerlink" title="10.6 网络训练有哪些技巧吗？"></a>10.6 网络训练有哪些技巧吗？</h2><h3 id="10-6-1-合适的数据集。"><a href="#10-6-1-合适的数据集。" class="headerlink" title="10.6.1.合适的数据集。"></a>10.6.1.合适的数据集。</h3><ul>
<li>1 没有明显脏数据(可以极大避免Loss输出为NaN)。</li>
<li>2 样本数据分布均匀。</li>
</ul>
<h3 id="10-6-2-合适的预处理方法。"><a href="#10-6-2-合适的预处理方法。" class="headerlink" title="10.6.2.合适的预处理方法。"></a>10.6.2.合适的预处理方法。</h3><p>关于数据预处理，在Batch Normalization未出现之前预处理的主要做法是减去均值，然后除去方差。在Batch Normalization出现之后，减均值除方差的做法已经没有必要了。对应的预处理方法主要是数据筛查、数据增强等。</p>
<h3 id="10-6-3-网络的初始化。"><a href="#10-6-3-网络的初始化。" class="headerlink" title="10.6.3.网络的初始化。"></a>10.6.3.网络的初始化。</h3><p>网络初始化最粗暴的做法是参数赋值为全0，这是绝对不可取的。因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的，这可能会直接导致模型失效，无法收敛。吴恩达视频中介绍的方法是将网络权重初始化均值为0、方差为1符合的正态分布的随机数据。</p>
<h3 id="10-6-4-小规模数据试练。"><a href="#10-6-4-小规模数据试练。" class="headerlink" title="10.6.4.小规模数据试练。"></a>10.6.4.小规模数据试练。</h3><p>在正式开始训练之前，可以先用小规模数据进行试练。原因如下：</p>
<ul>
<li>1 可以验证自己的训练流程对否。</li>
<li>2 可以观察收敛速度，帮助调整学习速率。</li>
<li>3 查看GPU显存占用情况，最大化batch_size(前提是进行了batch normalization，只要显卡不爆，尽量挑大的)。</li>
</ul>
<h3 id="10-6-5-设置合理Learning-Rate。"><a href="#10-6-5-设置合理Learning-Rate。" class="headerlink" title="10.6.5.设置合理Learning Rate。"></a>10.6.5.设置合理Learning Rate。</h3><ul>
<li>1 太大。Loss爆炸、输出NaN等。</li>
<li>2 太小。收敛速度过慢，训练时长大大延长。</li>
<li>3 可变的学习速率。比如当输出准确率到达某个阈值后，可以让Learning Rate减半继续训练。</li>
</ul>
<h3 id="10-6-6-损失函数"><a href="#10-6-6-损失函数" class="headerlink" title="10.6.6.损失函数"></a>10.6.6.损失函数</h3><p>损失函数主要分为两大类:分类损失和回归损失</p>
<blockquote>
<p>1.回归损失：</p>
<blockquote>
<ul>
<li>1 均方误差(MSE 二次损失 L2损失)<br>它是我们的目标变量与预测值变量差值平方。</li>
<li>2 平均绝对误差(MAE L1损失)<br>它是我们的目标变量与预测值变量差值绝对值。<br>关于MSE与MAE的比较。MSE更容易解决问题，但是MAE对于异常值更加鲁棒。更多关于MAE和MSE的性能，可以参考<a href="https://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/" target="_blank" rel="noopener">L1vs.L2 Loss Function</a></li>
</ul>
</blockquote>
<p>2.分类损失：</p>
<blockquote>
<ul>
<li>1 交叉熵损失函数。<br>是目前神经网络中最常用的分类目标损失函数。</li>
<li>2 合页损失函数<br>合页损失函数广泛在支持向量机中使用，有时也会在损失函数中使用。缺点:合页损失函数是对错误越大的样本施以更严重的惩罚，但是这样会导致损失函数对噪声敏感。</li>
</ul>
</blockquote>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Kay</p>
  <div class="site-description" itemprop="description">千里之行，始于足下</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kay</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
